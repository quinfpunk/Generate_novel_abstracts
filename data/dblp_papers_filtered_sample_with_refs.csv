id;title;year;references;abstract
3005773274;Malware classification algorithm using advanced Word2vec-based Bi-LSTM for ground control stations;2020.0;[];   Recently, Internet of Drones (IoD) are issued to utilize the diverse kinds of drones for leisure, education and so on. Researchers study to prevent the situations that drones are disabled by cyber-attackers by embedding malwares into the drones and Ground Control Stations (GCS). Therefore, it is required to protect the malwares considering the diverse kinds of features of the drones and GCSs. Signature-based detection approaches are traditionally utilized. However, given that those approaches only scan files partially, some of malwares are not detected. This paper proposes a novel method for finding the malwares in GCSs that utilizes a fastText model to create lower-dimension vectors than those the vectors by one-hot encoding and a bidirectional LSTM model to analyze the correlation with sequential opcodes. In addition, API function names are utilized to increase the classification accuracy of the sequential opcodes. In the experiments, the Microsoft malware classification challenge dataset was utilized and the malwares in the dataset were classified by family types. The proposed method showed the performance improvement of 1.87% comparing with the performance by a one-hot encoding-based approach. When the proposed method was compared with a similar decision tree-based malware detection approach, the performance of the proposed method was improved by 0.76%.
3002219790;Generalized transitivity: A systematic comparison of concepts with an application to preferences in the Babington Smith model;2020.0;[79079207, 170687150, 756957829, 1517040319, 1541230581, 1973469080, 1978829539, 2068655514, 2077313028, 2100683571, 2149427297, 2167861526, 2249408652, 2592965444, 2883737864, 2912748147, 2963611811];   Reciprocal relations are binary relations Q with entries    Q  (  i  ,  j  )  ∈  [  0  ,  1  ]   , and such that    Q  (  i  ,  j  )  +  Q  (  j  ,  i  )  =  1   . Relations of this kind occur quite naturally in various domains, such as preference modeling and preference learning. For example,      q    i  ,  j      could be the fraction of voters in a population who prefer candidate i to candidate j. In the literature, various attempts have been made at generalizing the notion of transitivity to reciprocal relations. In this paper, we compare three important frameworks of generalized transitivity: g-stochastic transitivity, T-transitivity, and cycle-transitivity. To this end, we introduce E-transitivity as an even more general notion. We also use this framework to extend an existing hierarchy of different types of transitivity. As an illustration, we study transitivity properties of probabilities of pairwise preferences, which are induced as marginals of an underlying probability distribution on rankings (strict total orders) of a set of alternatives. In particular, we analyze the interesting case of the so-called Babington Smith model, a parametric family of distributions of that kind.
2999570098;A New DGNSS Positioning Infrastructure for Android Smartphones;2020.0;[1978255623, 2109436662, 2146023544, 2180507614, 2422225805, 2898178966, 2944198247, 2964333408];One’s position has become an important piece of information for our everyday lives in a smart city. Currently, a position can be obtained easily using smartphones that is equipped with low-cost Global Navigation Satellite System (GNSS) chipsets with accuracy varying from 5 m to 10 m. Differential GNSS (DGNSS) is an efficient technology that removes the majority of GNSS errors with the aid of reference stations installed at known locations. The sub-meter accuracy can be achieved when applying the DGNSS technology on the advanced receivers. In 2016, Android has opened the accesses of raw GNSS measurements to developers. However, most of the mid and low-end smartphones only provide the data using the National Marine Electronics Association (NMEA) protocol. They do not provide the raw measurements, and thus do not support the DGNSS operation either. We proposed a DGNSS infrastructure that correct the standalone GNSS position of smartphones using the corrections from the reference station. In the infrastructure, the position correction is generated considering the GNSS satellite IDs that contribute to the standalone solution in smartphones, and the position obtained is equivalent to the solution of using the range-domain correction directly. To serve a large number of smartphone users, a Client/Server architecture is developed to cope with a mass of DGNSS positioning requests efficiently. The comparison of the proposed infrastructure against the ground truth, for all field tests in open areas, showed that the infrastructure achieves the horizontal positioning accuracy better than 2 m. The improvement in accuracy can reach more than 50% for the test in the afternoon. The infrastructure brings benefits to applications that require more accuracy without requiring any hardware modifications.
2998778732;A New Cycle Slip Detection and Repair Method Using a Single Receiver’s Single Station B1 and L1 Frequencies in Ground-Based Positioning Systems;2020.0;[2482503770];The detection and repair of the cycle slip is a key step for high precision navigation and positioning in indoor environments. Different methods have been developed to detect and repair cycle slips for carrier phase processing. However, most approaches are designed to eliminate the effects of the ionosphere in an outdoor environment, and many of them use pseudorange (code) information that is no longer suitable for indoor multipath environments. In this paper, a method based on the geometry-free combination without the pseudorange data is proposed to detect and fix cycle slips. A ground-based navigation system is built for data collection. Unlike the traditional dual-frequency cycle slip detection method, the Beidou B1, GPS L1 carrier phase combination is used instead of the B1, B2, or L1, L2 carrier phase combination, Ublox is used for data collecting. For fixing the cycle slips quickly, an improved adaptive Particle Swarm Optimization (PSO) algorithm is employed. We compared the performance of the new method with the existing two methods using simulated data in different conditions. The results show that the proposed method has better performance than other methods.
2972569439;Existence and concentration of positive ground states for a 1-Laplacian problem in RN;2020.0;[];   This paper is concerned with the following quasilinear elliptic problem:         −  e    Δ    1    u  +  V   (  x  )     u     |  u  |     =  K   (  x  )      |  u  |     p  −  1    u  ,    in      R    N    ,      u  ∈  B  V   (    R    N    )   ,         where     0    p      1    N  −  1      ,     N  ≥  2     and     e  u003e  0     is a small parameter. Under some mild conditions on the nonnegative functions     V   (  x  )      and     K   (  x  )     , we establish the existence of a ground state solution      u    e      for the above problem. Moreover,      u    e      concentrates on the intersection set of global minimum points of     V   (  x  )      and maximum points of     K   (  x  )     . The methods are based on the Nehari manifold technique and the Concentration-Compactness Principle of Lions.
2999284923;Minimal Ramsey graphs on deleting stars for generalized fans and books;2020.0;[1666603719, 2007681157, 2030814745, 2055084468, 2115164934, 2395948124, 2892116073];   Let F, G1 and G2 be simple graphs. Denote by F → (G1, G2) each edge coloring of F by red and blue that contains a red G1 or a blue G2. Define the star-critical Ramsey number      R  S    (   G  1   ,   G  2   )      to be the largest p such that Kr∖K1,p → (G1, G2), where     r  =  R  (   G  1   ,   G  2   )     is the Ramsey number. By Stability Lemma and Regularity Lemma, we determine      R  S    (   C   2  t  +  1    ,   K  1   +  n  G  )      and      R  S    (   C   2  t  +  1    ,   K  s   +  n   K  1   )      for large n.
3004918740;Vehicles Tracking by Combining Convolutional Neural Network Based Segmentation and Optical Flow Estimation;2020.0;[639708223, 1599058448, 1861492603, 1944615693, 2055022211, 2068611653, 2104828970, 2105101328, 2115739848, 2119605622, 2131747574, 2163937424, 2173520492, 2194775991, 2248556341, 2470139095, 2470142083, 2560474170, 2570343428, 2587008894, 2765877758, 2766984662, 2795093135, 2952186347, 2963150697, 2963253230, 2963305757, 2963482775, 2963782415];
2981937409;Dispatching fire trucks under stochastic driving times;2020.0;[1598399595, 1970919575, 1988963644, 2014406364, 2031621632, 2036821962, 2043219147, 2043833509, 2088363515, 2089677243, 2092752818, 2093228967, 2099271358, 2112088608, 2118313703, 2120782266, 2126602679, 2144479651, 2149139265, 2793538957, 2797681513, 2949773717];   To accommodate a swift response to fires and other incidents, fire departments have stations spread throughout their coverage area, and typically dispatch the closest fire truck(s) available whenever a new incident arises. However, it is not obvious that the policy of always dispatching the closest truck(s) minimizes the long-run fraction of late arrivals, since it may leave gaps in the coverage for future incidents. Although the research literature on dispatching of emergency vehicles is substantial, the setting with multiple trucks has received little attention. This is despite the fact that here careful dispatching is even more important, since the potential coverage gap is much larger compared to the single-truck case. Moreover, when dispatching multiple trucks, the uncertainty in the trucks’ driving time plays an important role, in particular due to possible correlation in driving times of the trucks if their routes overlap.  In this paper we discuss optimal dispatching of fire trucks, based on a particular dispatching problem that arises at the Amsterdam Fire Department, where two fire trucks are sent to the same incident location for a quick response. We formulate the dispatching problem as a Markov Decision Process, and numerically obtain the optimal dispatching decisions using policy iteration. We show that the fraction of late arrivals can be significantly reduced by deviating from current practice of dispatching the closest available trucks, with a relative improvement of on average about 20%, and over 50% for certain instances. We also show that driving-time correlation has a non-negligible impact on decision making, and if ignored may lead to performance decrease of over 20% in certain cases. As the optimal policy cannot be computed for problems of realistic size due to the computational complexity of the policy iteration algorithm, we propose a dispatching heuristic based on a queueing approximation for the state of the network. We show that the performance of this heuristic is close to the optimal policy, and requires significantly less computational effort.
2999549136;Fiber-specific variations in anterior transcallosal white matter structure contribute to age-related differences in motor performance;2020.0;[239929985, 1965894642, 1968135525, 1968471282, 1977396216, 2010125850, 2019836674, 2037701101, 2061997902, 2071384456, 2077240096, 2103729454, 2126693856, 2127309075, 2470259419, 2508982726, 2520744072, 2809071881];"   Age-related differences in bimanual motor performance have been extensively documented, but their underlying neural mechanisms remain less clear. Studies applying diffusion MRI in the aging population have revealed evidence for age-related white matter variations in the corpus callosum (CC) which are related to bimanual motor performance. However, the diffusion tensor model used in those studies is confounded by partial volume effects in voxels with complex fiber geometries which are present in up to 90% of white matter voxels, including the bilateral projections of the CC. A recently developed whole-brain analysis framework, known as fixel-based analysis (FBA), enables comprehensive statistical analyses of white matter quantitative measures in the presence of such complex fiber geometries. To investigate the contribution of age-related fiber-specific white matter variations to age-related differences in bimanual performance, a cross-sectional lifespan sample of healthy human adults (N ​= ​95; 20–75 years of age) performed a bimanual tracking task. Furthermore, diffusion MRI data were acquired and the FBA metrics associated with fiber density, cross-section, and combined fiber density and cross-section were estimated. Whole-brain FBA revealed significant negative associations between age and fiber density, cross-section, and combined metrics of multiple white matter tracts, including the bilateral projections of the CC, indicative of white matter micro- and macrostructural degradation with age. More importantly, mediation analyses demonstrated that age-related variations in the combined (fiber density and cross-section) metric of the genu, but not splenium, of the CC contributed to the observed age-related differences in bimanual coordination performance. These findings highlight the contribution of variations in interhemispheric communication between prefrontal (non-motor) cortices to age-related differences in motor performance."
2999248228;A Unified System for Aggression Identification in English Code-Mixed and Uni-Lingual Texts.;2020.0;[1522301498, 1677182931, 1686810756, 1832693441, 1841724727, 1936750108, 2064675550, 2097117768, 2099813784, 2131876387, 2170240176, 2194775991, 2250539671, 2273847690, 2302255633, 2413904250, 2470673105, 2511794618, 2587019100, 2607306668, 2740721704, 2798754355, 2895645998, 2911736392, 2912026968, 2912102236, 2912123473, 2912248199, 2912918913, 2912994242, 2913044599, 2913440437, 2913474415, 2914034656, 2914317128, 2914491155, 2914569437, 2939362715, 2950112902, 2951118135, 2951701153, 2962772361, 2962975459, 2963026768, 2964345792];Wide usage of social media platforms has increased the risk of aggression, which results in mental stress and affects the lives of people negatively like psychological agony, fighting behavior, and disrespect to others. Majority of such conversations contains code-mixed languages[28]. Additionally, the way used to express thought or communication style also changes from one social media plat-form to another platform (e.g., communication styles are different in twitter and Facebook). These all have increased the complexity of the problem. To solve these problems, we have introduced a unified and robust multi-modal deep learning architecture which works for English code-mixed dataset and uni-lingual English dataset both.The devised system, uses psycho-linguistic features and very ba-sic linguistic features. Our multi-modal deep learning architecture contains, Deep Pyramid CNN, Pooled BiLSTM, and Disconnected RNN(with Glove and FastText embedding, both). Finally, the system takes the decision based on model averaging. We evaluated our system on English Code-Mixed TRAC 2018 dataset and uni-lingual English dataset obtained from Kaggle. Experimental results show that our proposed system outperforms all the previous approaches on English code-mixed dataset and uni-lingual English dataset.
2964749018;Unbalanced breast cancer data classification using novel fitness functions in genetic programming;2020.0;[1486003553, 1833977909, 1941659294, 1965173537, 1975580029, 1982934478, 1985842955, 1996313056, 2003087683, 2004320486, 2011170389, 2015776218, 2044675702, 2056137745, 2066393173, 2071529495, 2083780116, 2167389327, 2173226459, 2295985801, 2507278300, 2549437329, 2559900391, 2560205181, 2561467560, 2576787043, 2585770658, 2604677623, 2767106145, 2795726520, 2800577149, 2886310805, 2891321141, 2912889105, 2922034350];   Breast Cancer is a common disease and to prevent it, the disease must be identified at earlier stages. Available breast cancer datasets are unbalanced in nature, i.e. there are more instances of benign (non-cancerous) cases then malignant (cancerous) ones. Therefore, it is a challenging task for most machine learning (ML) models to classify between benign and malignant cases properly, even though they have high accuracy. Accuracy is not a good metric to assess the results of ML models on breast cancer dataset because of biased results. To address this issue, we use Genetic Programming (GP) and propose two fitness functions. First one is F2 score which focuses on learning more about the minority class, which contains more relevant information, the second one is a novel fitness function known as Distance score (D score) which learns about both the classes by giving them equal importance and being unbiased. The GP framework in which we implemented D score is named as D-score GP (DGP) and the framework implemented with F2 score is named as F2GP. The proposed F2GP achieved a maximum accuracy of 99.63%, 99.51% and 100% for 60-40, 70-30 partition schemes and 10 fold cross validation scheme respectively and DGP achieves a maximum accuracy of 99.63%, 98.5% and 100% in 60-40, 70-30 partition schemes and 10 fold cross validation scheme respectively. The proposed models also achieves a recall of 100% for all the test cases. This shows that using a new fitness function for unbalanced data classification improves the performance of a classifier.
2996622920;Training a non-native vowel contrast with a distributional learning paradigm results in improved perception and production;2020.0;[1968703923, 2062429067, 2522016266];   Previous distributional learning research suggests that adults can improve perception of a non-native contrast more efficiently when exposed to a bimodal than a unimodal distribution. Studies have also suggested that perceptual learning can transfer to production. The current study tested whether the addition of visual images to reinforce the contrast and active learning with feedback would result in learning in both conditions and would transfer to gains in production. Native English-speaking adults heard stimuli from a bimodal or unimodal /o/-/œ/ continuum. No group differences were found on a discrimination task, possibly suggesting that the supports eliminated previously documented group differences. On an identification task, listeners in the bimodal group showed better performance than the unimodal group on the endpoint stimuli. Production results indicated that both groups showed increased Euclidean distance between the target vowels after training, suggesting that perceptual training improved production skills in both conditions. Contrary to expectations, degree of perception and production learning were not correlated. Together, these results suggest that a bimodal distribution may aid learning, but that adding images to reinforce the contrast and active learning to the training paradigm could mitigate disadvantages found previously for participants exposed to a unimodal distribution.
2997964213;Damage Spreading, Chaos and Regional Synchronization of a Probabilistic Cellular Automaton.;2020.0;[];
2998646461;Smart computing and cyber technology for cyberization;2020.0;[592218986, 1500464975, 1858904274, 1989634590, 2006761437, 2021941127, 2095223629, 2169239645, 2247776437, 2416799949, 2590019597, 2609552253, 2807337963, 2883995567, 2885161904, 2898896120, 2915594435, 2946502251, 2946627595, 2946699206, 2951627131, 2952417223, 2958718920, 2961595003, 2964008753, 2969522623, 2970032879, 2970888294, 2988958963, 2990135320];
2984273387;Augmented Lagrangian combined to evolutionary heuristic for energy efficiency in OCDMA networks;2020.0;[1987995897, 2105101597, 2126087318, 2150834164, 2320065976, 2483716586, 2498322372, 2783517474];   This paper proposes combining the augmented Lagrangian method (ALM) with evolutionary heuristic methods, as well as quasi-Newton optimization methods applied to the energy efficiency (EE) maximization in the optical code division multiple access (OCDMA) communication network. The particle swarm optimization (PSO) and a hybridization between the PSO and the gravitational search algorithm (GSA) called PSOGSA have been deployed. The ALM structure replaces the objective function and allows a best fit to the problem, and ultimately provide more information about the solution. Numerical results demonstrate the robustness and low-complexity of hybrid ALM-PSO, while the ALM associated with PSOGSA attains robustness at cost of high-complexity. In turn, the usually ALM combined with Broyden-Fletcher-Goldfarb-Shanno (BFGS) method presents convergence for a restrict scenarios, failing to perform suitably for networks with large numbers of users.
2980274035;Balancing mission success probability and risk of system loss by allocating redundancy in systems operating with a rescue option;2020.0;[1550064664, 1971945294, 1980944306, 1987197833, 2001565273, 2005812157, 2006838288, 2014500386, 2022123067, 2025705970, 2033724226, 2051247433, 2067708377, 2069280737, 2082839354, 2093523923, 2116827549, 2148606018, 2165046421, 2334208235, 2620003726, 2742118814, 2751698421, 2801297575, 2938963980];   In many real-world safety-critical applications, a rescue procedure aimed at saving a system can be activated after its failure to execute a primary mission. For such systems, it is essential to keep the balance between the mission success probability and the system survival probability, as both of these contradicting metrics are extremely important in practice. This balance is achieved in the paper by solving the corresponding constrained redundancy optimization problem utilizing the innovative algorithmic approach. The developed methodology is applied to multicomponent systems with overlapping sets of components involved in primary and rescue phases of a mission, respectively. Each component is a subsystem that consists of parallel heterogeneous elements. The random environment in which the system operates is modeled by the Poisson shock process, whereas each shock can result in failures with given probabilities. The detailed numerical example illustrating optimal redundancy solutions is presented. Further directions for generalizations and applications of the obtained results are discussed.
2977258213;Attribute evaluation on attack trees with incomplete information;2020.0;[1365286, 201786483, 1159968740, 1480909796, 1506446282, 1515452738, 1533876796, 1542659806, 1561586523, 2010130012, 2010219174, 2033238208, 2064241518, 2086871667, 2091973517, 2096349415, 2106327281, 2111145992, 2112850401, 2114884316, 2133795641, 2137982913, 2142527556, 2146024157, 2149137922, 2156607688, 2164059558, 2165836036, 2193214538, 2224486821, 2339277944, 2399992960, 2489519702, 2507595475, 2514350728, 2515420228, 2520060407, 2533940363, 2583980768, 2623597181, 2766228646, 2788276534];   Attack trees are considered a useful tool for security modelling because they support qualitative as well as quantitative analysis. The quantitative approach is based on values associated to each node in the tree, expressing, for instance, the minimal cost or probability of an attack. Current quantitative methods for attack trees allow the analyst to, based on an initial assignment of values to the leaf nodes, derive the values of the higher nodes in the tree. In practice, however, it shows to be very difficult to obtain reliable values for all leaf nodes. The main reasons are that data is only available for some of the nodes, that data is available for intermediate nodes rather than for the leaf nodes, or even that the available data is inconsistent. We address these problems by developing a generalisation of the standard bottom-up calculation method in three ways. First, we allow initial attributions of non-leaf nodes. Second, we admit additional relations between attack steps beyond those provided by the underlying attack tree semantics. Third, we support the calculation of an approximative solution in case of inconsistencies. We illustrate our method, which is based on constraint programming, by a comprehensive case study.
2992510549;Mathematical Modeling of Multimodal Transportation Risks.;2020.0;[2007196136, 2154798037];Research has shown that the risks of multimodal transportation depend as both on stochastic and fuzzy parameters.
2997411837;Fast Item Ranking under Neural Network based Measures;2020.0;[17346433, 1455310343, 1556531089, 1591023622, 1603536826, 1966443646, 1967005434, 1974627246, 1978030011, 1983645263, 2002827932, 2010416066, 2012833704, 2111006384, 2128678576, 2136189984, 2147017814, 2147717514, 2153602280, 2186845332, 2512971201, 2536015822, 2605350416, 2610935556, 2648699835, 2725049817, 2740920897, 2753634799, 2783565819, 2810397803, 2890736686, 2892102150, 2897754576, 2963056065, 2963517218];
3000339403;Stable and Accurate Filtering Procedures;2020.0;[303937333, 1972670182, 2012464934, 2753944674, 2769562095];High frequency errors are always present in numerical simulations since no difference stencil is accurate in the vicinity of the $$\pi $$-mode. To remove the defective high wave number information from the solution, artificial dissipation operators or filter operators may be applied. Since stability is our main concern, we are interested in schemes on summation-by-parts (SBP) form with weak imposition of boundary conditions. Artificial dissipation operators preserving the accuracy and energy stability of SBP schemes are available. However, for filtering procedures it was recently shown that stability problems may occur, even for originally energy stable (in the absence of filtering) SBP based schemes. More precisely, it was shown that even the sharpest possible energy bound becomes very weak as the number of filtrations grow. This suggest that successful filtering include a delicate balance between the need to remove high frequency oscillations (filter often) and the need to avoid possible growth (filter seldom). We will discuss this problem and propose a remedy.
2999442061;QoS-Based Robust Cooperative-Jamming-Aided Beamforming for Correlated Wiretap Channels;2020.0;[];This paper studies cooperative-jamming (CJ)-aided beamforming design under Gaussian channel uncertainties to reduce the secrecy loss due to reception correlation. In light of the difficulty of maximizing the outage-probability-constrained secrecy rate, a novel quality-of-service (QoS)-based optimization is considered. By employing the Bernstein-type inequality to approximate the probabilistic constraints, we seek to maximize the target outage signal-to-noise ratio (SNR) at the destination under transmit power and SNR outage constraints. Simulation results verify the designed CJ-aided beamforming substantially enhances the secrecy from a QoS perspective.
3000157810;Understanding Inter-Hemispheric Traveling Ionospheric Disturbances and Their Mechanisms;2020.0;[];
2997108528;HAFLoop: An architecture for supporting Highly Adaptive Feedback Loops in self-adaptive systems;2020.0;[38977886, 1511275332, 1530761836, 1555914462, 1557634793, 1571117462, 1585236401, 1659093230, 1680397247, 1872744038, 1895621607, 1915837116, 1964836145, 1987727544, 1990911977, 2003373522, 2013298510, 2014522999, 2019482491, 2033406822, 2045345923, 2045591981, 2047162235, 2055906872, 2057124872, 2059164916, 2072704264, 2080753128, 2085144121, 2089160254, 2093848495, 2104195067, 2124849328, 2132307560, 2133079113, 2133859873, 2142486130, 2152210408, 2154262285, 2162429446, 2163784380, 2167748275, 2168023489, 2405981446, 2604659713, 2732140844, 2743897093, 2744822502, 2754124646, 2782879278, 2783634296, 2796146433, 2802989963, 2889467404, 2896572817, 2899508431, 2911489589, 2962723569, 2968409638];   Most of the current self-adaptive systems (SASs) rely on static feedback loops such as the IBM’s MAPE-K loop for managing their adaptation process. Static loops do not allow SASs to react to runtime events such as changing adaptation requirements or MAPE-K elements’ faults. In order to address this issue, some solutions have emerged for manually or automatically perform changes on SASs’ feedback loops. However, from the software engineering perspective, most of the proposals cannot be reused or extended by other SASs. In this paper, we present HAFLoop (Highly Adaptive Feedback control Loop), a generic architectural proposal that aims at easing and fastening the design and implementation of adaptive feedback loops in modern SASs. Our solution enables both structural and parameter adaptation of the loop elements. Moreover, it provides a highly modular design that allows SASs’ owners to support a variety of feedback loop settings from centralized to fully decentralized. In this work, HAFLoop has been implemented as a framework for Java-based systems and evaluated in two emerging software application domains: self-driving vehicles and IoT networks. Results demonstrate that our proposal easies and accelerates the development of adaptive feedback loops as well as how it could help to address some of the most relevant challenges of self-driving vehicles and IoT applications. Concretely, HAFLoop has demonstrated to improve SASs’ feedback loops’ runtime availability and operation.
3002203379;Multi-dimensional Dynamic Trust Evaluation Scheme for Cloud Environment;2020.0;[1964344998, 1976409582, 1977367431, 1986484752, 2019438060, 2028837511, 2039637952, 2046229586, 2059407171, 2059415317, 2080550005, 2086726476, 2088979095, 2090072113, 2101391769, 2122129277, 2134894205, 2153199811, 2167521347, 2169822912, 2171104217, 2518226408, 2573428132, 2607002674, 2794240319, 2891996945, 2930250717, 2956007097];   In cloud computing environment, Cloud Customers (CCs) and Cloud Service Providers (CSPs) require to evaluate the trust levels of potential partner prior to appealing in communications. The accurateness of trust evaluation significantly influences the success velocity of the communication. Determining trustworthiness dynamically is a demanding problem in an open and dynamic environment (such as cloud computing) because of huge number of CSPs offering similar types of services. Also it is a challenging job for both CCs and CSPs to mutually recognize and distinguish between the trustworthy and untrustworthy CSPs and CCs. Presently, there are very less number of dynamic trust evaluation scheme that permits CCs to evaluate the trustworthiness of CSPs from multi-dimensional perspectives (i.e. perspectives from Cloud Auditors (CAs), Cloud Brokers (CBs), Service Level Agreement Agents (SLAAs) and Peers). Similarly, there is no scheme that permits CSPs to evaluate trustworthiness of CCs. This paper proposes a Multi-dimensional Dynamic Trust Evaluation Scheme (MDTES) that facilitates CCs to evaluate the trustworthiness of CSPs from various viewpoints. Similar approach can be employed by CSPs to evaluate the trustworthiness of CCs. The proposed MDTES helps CCs to choose trustworthy CSP which provides desired QoS and CSPs to choose desired and legal CCs. The experimental results illustrate that the MDTES is dynamic, efficient and steady in distinguishing trustworthy and untrustworthy CSPs and CCs compared to other trust models.
2982395995;Novel supplier grading approach based on interval probability hesitant fuzzy linguistic TOPSIS;2020.0;[566653638, 1978062397, 1980471025, 1987454296, 2001452963, 2021909147, 2029386095, 2033917481, 2034563113, 2040121225, 2043902857, 2045610944, 2065043952, 2075809784, 2133555750, 2147595926, 2267075006, 2300436939, 2468154960, 2474884296, 2598771954, 2754941027, 2772679429, 2774702408, 2802745231, 2803001434, 2803680438, 2891953200, 2892848289, 2899430821, 2902418216, 2904468863, 2904704195, 2909979782, 2913245224, 2947910571, 2966243013, 2970332774];   The supplier grading, a typical multi-attribute linguistic group decision-making (MALGDM) problem, is one of the important research topic in the uncertain linguistic environment, especially its aggregation of evaluation information and the establishment of grading approaches. In this paper, first we introduce the generalized interval probability hesitant fuzzy linguistic IOWA weighted average (GVIOWAWA) operator to aggregate the uncertain linguistic information with incomplete reliability. The GVIOWAWA operator can allow decision makers to select the appropriate parameters according to their needs. Then the interval probability hesitant fuzzy linguistic TOPSIS (IPHFL-TOPSIS) based on the interval probability hesitant fuzzy linguistic Euclidean distance is established. The IPHFL-TOPSIS model can effectively and objectively help businesses find the strategic cooperation supplier. Eventually, we give the numerical examples concerning the comprehensive assessment of material supplier and energy supplier to illustrate validity and applicability of the proposed approach and compare the proposed method with different parameters and methods to perform its flexibility.
3004636994;Human Comprehension of Fairness in Machine Learning;2020.0;[1648303880, 2100960835, 2530395818, 2963178340];
3000950839;Ergodic Stationary Distribution of a Stochastic Hepatitis B Epidemic Model with Interval-Valued Parameters and Compensated Poisson Process;2020.0;[2572725329, 2725765108, 2790149985, 2801904685, 2885902036, 2908114608];Hepatitis B epidemic was and is still a rich subject that sparks the interest of epidemiological researchers. The dynamics of this epidemic is often modeled by a system with constant parameters. In reality, the parameters associated with the Hepatitis B model are not certain, but the interval in which it belongs to can readily be determined. Our paper focuses on an imprecise Hepatitis B model perturbed by Levy noise due to unexpected environmental disturbances. This model has a global positive solution. Under an appropriate assumption, we prove the existence of a unique ergodic stationary distribution by using the mutually exclusive possibilities lemma demonstrated by Stettner in 1986. Our main effort is to establish an almost perfect condition for the existence of the stationary distribution. Numerical simulations are introduced to illustrate the analytical results.
3003360669;Monotone Probability Distributions over the Boolean Cube Can Be Learned with Sublinear Samples.;2020.0;[];
3008659330;Secure Instant Messaging Application in Prenatal Care;2020.0;[2341607273];The use of mobile phone for medical purposes is rapidly expanding as the number of medical applications rise. Studies show improvement of patient management and communication between medical team members using instant messaging applications. There are currently several smartphone applications routinely used by doctors and nurses. WhatsApp is by far the most common, however, it has several limitations when it comes to medical confidentiality. The aim of this paper is to introduce “Siilo” as an alternative secure messaging application and its advantages in the medical field, specifically in obstetrics. The typical course of consultation for an abnormal fetal finding is very long, cumbersome, frustrating and depends a lot on the patient, whereas, via Siilo the process is fast, efficient, depends more on the medical caregivers and helps ensuring minimum lost to follow-up. This paper demonstrates for the first time the utility of the use of Siilo application in medical management.
2997098112;An algorithm to compute the t-value of a digital net and of its projections;2020.0;[1968708670, 1981743146, 1985018928, 2058890866, 2079366547, 2083253722, 2099375630, 2165254608, 2171637483, 2398934262, 2615953416, 2964258169];   Digital nets are among the most successful methods to construct low-discrepancy point sets for quasi-Monte Carlo integration. Their quality is traditionally assessed by a measure called the    t   -value. A refinement computes the    t   -value of the projections over subsets of coordinates and takes a weighted average (or some other function) of these values. It is also of interest to compute the    t   -values of embedded nets obtained by taking subsets of the points. In this paper, we propose an efficient algorithm to compute such measures and we compare our approach with previously proposed methods both empirically and in terms of computational complexity.
2999738219;A Piezoresistive Array Armband With Reduced Number of Sensors for Hand Gesture Recognition;2020.0;[2136714340, 2137847051, 2286878790, 2383696055, 2589747988, 2766262086, 2767053645, 2769575562, 2793935950, 2795900623, 2845841029, 2885789677, 2889553287, 2904724061, 2905305602, 2981877040];
2996806983;A 7.5 Gb/s/pin 8-Gb LPDDR5 SDRAM With Various High-Speed and Low-Power Techniques;2020.0;[2027886943, 2095658739, 2132943370, 2179530124, 2291303685, 2593688479, 2791561716, 2792446208, 2793439489];A 7.5 Gb/s/pin 8-Gb LPDDR5 SDRAM is implemented in a 1   $\times$   nm DRAM process. Various techniques are applied to achieve higher bandwidth and lower power than LPDDR4X. To increase data rate, a WCK clocking scheme that is less vulnerable to power noise is adopted and a non-target ODT mode is proposed to reduce reflection noise in a two-rank system. A couple of techniques are proposed for saving power. To reduce self-refresh power, this chip supports deep sleep mode (DSM). In DSM, the leakage current of internal voltages decreases by disabling internal voltage generators that are not related with a self-refresh operation. Dynamic voltage frequency scaling (DVFS) is adopted to reduce read and write operation power and when writing all zeros data, an internal data copy function can be used for reducing write operation power. Last, a ZQ calibration scheme that shares one ZQ resistor (RZQ) and automatically executes ZQ calibration is presented. The proposed LPDDR5 DRAM operates up to 7.5 Gb/s on an automatic test equipment (ATE) and 6.4 Gb/s on a prototype system. Read and write power decrease by 21% and 33% compared to LPDDR4X at 4.266 Gb/s, and self-refresh power is reduced by 25% in DSM.
2962682143;On compact packings of the plane with circles of three radii;2020.0;[2070511479, 2104266030, 2417863416];   A compact circle-packing  P  of the Euclidean plane is a set of circles which bound mutually disjoint open discs with the property that, for every circle    S  ∈  P   , there exists a maximal indexed set    {    A    0    ,  …  ,    A    n  −  1    }  ⊆  P    so that, for every    i  ∈  {  0  ,  …  ,  n  −  1  }   , the circle      A    i      is tangent to both circles  S  and      A    i  +  1     mod     n    .     We show that there exist at most 13617 pairs    (  r  ,  s  )    with    0    s    r    1    for which there exist a compact circle-packing of the plane consisting of circles with radii  s ,  r  and 1.  We discuss computing the exact values of such    0    s    r    1    as roots of polynomials and exhibit a selection of compact circle-packings consisting of circles of three radii. We also discuss the apparent infeasibility of computing  all  these values on contemporary consumer hardware.
3007979096;Sector Piezoelectric Sensor Array Transmitter Beamforming MUSIC Algorithm Based Structure Damage Imaging Method;2020.0;[];Elastic-wave-based structural health monitoring technology has a broad application potential for its sensitivity and ability to achieve regional monitoring. For structures with large damping and specific shapes, the traditional damage monitoring method is limited by the sensor arrangement area and affected by low signal-to-noise ratios, so it is difficult to accurately locate the damage in a structure. To solve this problem, this paper proposed a damage monitoring method based on a sector piezoelectric sensor array for multiple signal classification algorithm. By arranging two sector piezoelectric sensor arrays that are suitable for a specific structure, the damage scattering array signal under the multi-excitation source was obtained and synthesized, the signal-to-noise ratios were improved, and the damage location accuracy was thus improved. The effectiveness of the method was verified by monitoring the damage in a circular bonded structure with a metal ring. Compared with the damage localization methods based on the traditional single excitation source multiple signal classification algorithm, path imaging and delay-sum imaging, this method can achieve better damage location and has a higher localization accuracy.
3000631028;Faster initial splitting for small characteristic composite extension degree fields;2020.0;[1482131745, 1570515303, 1758781646, 1865048348, 2056307179, 2075364954, 2091260227, 2120749565, 2396642682, 2400676556, 2804627268, 2950515166];   Let p be a small prime and    n  =    n    1      n    2    u003e  1    be a composite integer. For the function field sieve algorithm applied to      F      p    n       , Guillevic (2019) had proposed an algorithm for initial splitting of the target in the individual logarithm phase. This algorithm generates polynomials and tests them for B-smoothness for some appropriate value of B. The amortised cost of generating each polynomial is    O  (    n    2    2    )    multiplications over      F      p      n    1         . In this work, we propose a new algorithm for performing the initial splitting which also generates and tests polynomials for B-smoothness. The advantage over Guillevic splitting is that in the new algorithm, the cost of generating a polynomial is    O  (  n    log    p    ⁡  (  1  /  π  )  )    multiplications in      F    p     , where π is the relevant smoothness probability.
2996722826;A novel multi-modal machine learning based approach for automatic classification of EEG recordings in dementia;2020.0;[1537837212, 1596717185, 1606551523, 2004641611, 2018924502, 2111296615, 2393256679, 2551054676, 2755032532, 2755940713, 2767544481, 2768956845, 2772766867, 2783371799, 2790139232, 2805246634, 2810419333, 2890564211, 2894895904, 2900290300, 2932060911, 2969853667, 2979077288];   Electroencephalographic (EEG) recordings generate an electrical map of the human brain that are useful for clinical inspection of patients and in biomedical smart Internet-of-Things (IoT) and Brain-Computer Interface (BCI) applications. From a signal processing perspective, EEGs yield a nonlinear and nonstationary, multivariate representation of the underlying neural circuitry interactions. In this paper, a novel multi-modal Machine Learning (ML) based approach is proposed to integrate EEG engineered features for automatic classification of brain states. EEGs are acquired from neurological patients with Mild Cognitive Impairment (MCI) or Alzheimer’s disease (AD) and the aim is to discriminate Healthy Control (HC) subjects from patients. Specifically, in order to effectively cope with nonstationarities, 19-channels EEG signals are projected into the time–frequency (TF) domain by means of the Continuous Wavelet Transform (CWT) and a set of appropriate features (denoted as CWT features) are extracted from    δ   ,    θ   ,      α    1     ,      α    2     ,    β    EEG sub-bands. Furthermore, to exploit nonlinear phase-coupling information of EEG signals, higher order statistics (HOS) are extracted from the bispectrum (BiS) representation. BiS generates a second set of features (denoted as BiS features) which are also evaluated in the five EEG sub-bands. The CWT and BiS features are fed into a number of ML classifiers to perform both 2-way (AD vs. HC, AD vs. MCI, MCI vs. HC) and 3-way (AD vs. MCI vs. HC) classifications. As an experimental benchmark, a balanced EEG dataset that includes 63 AD, 63 MCI and 63 HC is analyzed. Comparative results show that when the concatenation of CWT and BiS features (denoted as multi-modal (CWT+BiS) features) is used as input, the Multi-Layer Perceptron (MLP) classifier outperforms all other models, specifically, the Autoencoder (AE), Logistic Regression (LR) and Support Vector Machine (SVM). Consequently, our proposed multi-modal ML scheme can be considered a viable alternative to state-of-the-art computationally intensive deep learning approaches.
3008619180;Defining the determinants of online impulse buying through a shopping process of integrating perceived risk, expectation-confirmation model, and flow theory issues;2020.0;[];
2979485737;Spatiotemporal dynamics of brightness coding in human visual cortex revealed by the temporal context effect;2020.0;[1970167670, 1974637957, 2094563761, 2110114082, 2124903139];   Human visual perception is modulated by both temporal and spatial contexts. One type of modulation is apparent in the temporal context effect (TCE): In the presence of a constant luminance patch (a long flash), the perceived brightness of a short flash increases monotonically with onset asynchrony. The aim of the current study was to delineate the neural correlates of this illusory effect, particularly focusing on its dynamic neural representation among visual cortical areas. We reconstructed sources of magnetoencephalographic (MEG) data recorded from observers (6 male and 9 female human adults) experiencing the TCE. Together with retinotopic mapping, signals from different occipital lobe areas were extracted to investigate whether different visual areas have differential representation of the onset vs. offset synchronized short flashes. From the data, TCE related responses were observed in LO and V4 in the time window of 200–250 m s, while neuronal responses to physical luminances were observed in the early time window at around 100 m s across early visual cortex, such as V1 and V2, also in V4 and VO. Based on these findings, we suggest that two distinct processes might be involved in brightness coding: one bottom-up process which is stimulus energy driven and responds fast, and another process which may be broadly characterized as top-down or lateral, is context driven, and responds slower. For both processes, we found that V4 might play a critical role in dynamically integrating luminances into brightness perception, a finding that is consistent with the view of V4 as a bottom-up and top-down integration complex.
3006497105;NetVision: On-Demand Video Processing in Wireless Networks;2020.0;[1522734439, 2037754816, 2045403087, 2056510622, 2066260470, 2088692353, 2111200615, 2129861682, 2155893237, 2163605009, 2166646151, 2177847924, 2233116163, 2317337556, 2525851376, 2562526353, 2592843051, 2607172905, 2962707028];The vast adoption of mobile devices with cameras has greatly contributed to the proliferation of the creation and distribution of videos. For a variety of purposes, valuable information may be extracted from these videos. While the computational capability of mobile devices has greatly improved recently, video processing is still a demanding task for mobile devices. We design an on-demand video processing system,  NetVision , that performs distributed video processing using deep learning across a wireless network of mobile and edge devices to answer queries while minimizing the query response time. However, the problem of minimal query response time for processing videos stored across a network is a strongly NP-hard problem. To deal with this, we design a greedy algorithm with bounded performance. To further deal with the dynamics of the transmission rate between mobile and edge devices, we design an adaptive algorithm. We built NetVision and deployed it on a small testbed. Based on the measurements of the testbed and by extensive simulations, we show that the greedy algorithm is close to the optimum and the adaptive algorithm performs better with more dynamic transmission rates. We then perform experiments on the small testbed to examine the realized system performance in both stationary networks and mobile networks.
2963954587;Multistage stochastic demand-side management for price-making major consumers of electricity in a co-optimized energy and reserve market;2020.0;[867003385, 1494277681, 1965419024, 1969007958, 1973266184, 1983183227, 2000722362, 2018682725, 2024816219, 2060013459, 2100131857, 2138000876, 2329959957, 2472763926, 2516230096, 2568007340, 2729664112, 2742900359, 2911561799];   In this paper, we take an optimization-driven heuristic approach, motivated by dynamic programming, to solve a class of non-convex multistage stochastic optimization problems. We apply this to the problem of optimizing the timing of energy consumption for a large manufacturer who is a price-making major consumer of electricity. We introduce a mixed-integer program that co-optimizes consumption bids and interruptible load reserve offers, for such a major consumer over a finite time horizon. By utilizing Lagrangian methods, we decompose our model through approximately pricing the constraints that link the stages together. We construct look-up tables in the form of consumption-utility curves, and use these to determine optimal consumption levels. We also present heuristics, in order to tackle the non-convexities within our model, and improve the accuracy of our policies. In the second part of the paper, we present stochastic solution methods for our model in which, we reduce the size of the scenario tree by utilizing a tailor-made scenario clustering method. Furthermore, we report on a case study that implements our models for a major consumer in the (full) New Zealand Electricity Market and present numerical results.
2999999751;Multipath Suppression for Continuous Wave Radar via Slepian Sequences;2020.0;[];In continuous wave (CW) radar systems, multiple signal copies impinge the receiver simultaneously. Often, undesired multipath and direct-path copies are many times stronger than potential targets. When applying matched filter signal processing techniques, the undesired signal components can mask weaker targets and decrease performance of post-processing techniques, such as target indication or estimation. In this manuscript, we propose a method of rejecting multipath-scattered returns over a continuous region in range and Doppler. We explore the computational cost of this method and additionally propose an approximate method of rejection which leverages the well-known discrete prolate spheroidal sequences (DPSS)–typically referred to as Slepian sequences–to gain a computational advantage. Results are shown to decrease the effective noise floor when applying matched filtering techniques as well as increase target signal-to-interference-plus-noise ratio (SINR) outside of an undesired multipath region. Comparisons are shown to traditional CW multipath removal in terms of rejection performance and run-time.
2973928292;Preparing opportunistic networks for smart cities: Collecting sensed data with minimal knowledge;2020.0;[1504330670, 1513719591, 1617290994, 1626398438, 1677486363, 1965127592, 1966518282, 1966559656, 1966736954, 1971788036, 1974140471, 1981911218, 1982300822, 1986003798, 1987365706, 1990948050, 1992981157, 1995609930, 1996313783, 1997017001, 2001482101, 2005060246, 2005070934, 2006977321, 2010634640, 2012927050, 2021497904, 2028741709, 2033411413, 2037716689, 2039157284, 2040459121, 2048932320, 2060334710, 2068273572, 2079276401, 2088440111, 2099530916, 2109528718, 2122887675, 2125239197, 2131601980, 2133286823, 2133540137, 2134752223, 2135725337, 2136317921, 2138677604, 2139076288, 2140251882, 2143548801, 2147854123, 2153207204, 2162076967, 2162954765, 2166406195, 2181306409, 2279359805, 2308672085, 2406010710, 2538584992, 2560480567, 2579937741, 2586478530, 2751169498, 2767865048, 2773194082, 2791677100, 2963298867];   Opportunistic Networks exploit portable handheld devices to collect delay-tolerant data from sensors to gateways for realizing various Smart City applications. To obtain knowledge for determining suitable routing paths as users go about their daily routine, nodes maintain history of every encounter and exchange the information through summary vectors. Due to large node populations, the size of summary vectors makes it challenging to implement real-world city-scale applications with the technology. In this paper, we take the technology a step towards real-world implementation by proposing a set of adaptive and privacy-preserving mechanisms that can be incorporated into existing encounter-based routing protocols to reduce summary vector sizes without compromising delivery guarantees. We validate our proposals with real-world human movement traces and simulation experiments. In terms of network performance, our proposals reduce the average summary vector size by 75% to achieve up to 21% less energy consumption with about 28% improvement in throughput.
2989387953;The convergence analysis and uniqueness of blow-up solutions for a Dirichlet problem of the general k-Hessian equations;2020.0;[1965839417, 1975118601, 1990954610, 2002769487, 2007521282, 2061193494, 2066482960, 2076627362, 2081344723, 2088615732, 2090226449, 2092542745, 2120578579, 2202969221, 2397273960, 2520843716, 2546663644, 2601688613, 2619998782, 2789947069, 2790179491, 2802084755, 2909525106, 2971146131];   In this paper, we focus on the convergence analysis of the unique solution for a Dirichlet problem of the general    k   -Hessian equation in a ball. By introducing some suitable growth conditions and developing a new iterative technique, the unique solution of the    k   -Hessian equation is obtained. Then we carry out the convergence analysis for the iterative sequences and further obtain the convergence rate and error estimate for the unique solution. The numerical result indicates that the convergence rate is very fast.
3001294237;Rainbow factors in hypergraphs;2020.0;[1971079746, 1984697505, 2014564484, 2030651378, 2051336598, 2054249991, 2093446084, 2098363469, 2126441872, 2126648306, 2127274797, 2768061901, 2773434597];   For any r-graph H, we consider the problem of finding a rainbow H-factor in an r-graph G with large minimum l-degree and an edge-colouring that is suitably bounded. We show that the asymptotic degree threshold is the same as that for finding an H-factor.
2993092475;Lion swarm optimization algorithm for comparative study with application to optimal dispatch of cascade hydropower stations;2020.0;[610200145, 1549119093, 1583171784, 1585611690, 1595159159, 1596195064, 1708394971, 1763200058, 1849730171, 1965442863, 1965811262, 1984762043, 1999284878, 2010659620, 2031183907, 2032338312, 2061438946, 2064622432, 2072955302, 2101812529, 2111393363, 2123682012, 2125281549, 2127931254, 2131613989, 2135383305, 2138784882, 2149295419, 2151554678, 2156773695, 2169064301, 2232317135, 2290883490, 2467327288, 2791899797, 2794083222, 2803360343, 2889545660, 2901565938, 2915596699, 2919979744];   Lion swarm optimization (LSO) algorithm that based on the natural division of labor among lion king, lionesses and lion cubs in a pack of lions is recently introduced. To evaluate the exploration and the exploitation of the LSO algorithm comprehensively, an intensive study based on optimization problems is necessary. In this work, we firstly present the revised version of the LSO algorithm in detail. Secondly, the efficiency of LSO is evaluating using quantitative analysis, convergence analysis, statistical analysis, and robustness analysis on 60 classical numerical test problems, encompassing the Uni-modal, the Multi-modal, the Separable, the Non-separable, and the Multi-dimension problems. For comparison purposes, the results obtained by the LSO algorithm are compared against a large set of state-of-the-art optimization methods. The comparative results show that the LSO can provide significantly superior results for the US, the UN, and the MS problems regarding convergence speed, robustness, success rate, time complexity, and optimization accuracy compared with the other optimizers, and present very competitive results in terms of those indicators compared with the other optimizers. Finally, to check the applicability and robustness of the LSO algorithm, a case study on optimal dispatch problem of China’s Wujiang cascade hydropower stations shows that the LSO can obtain well and reliable optimal results with average generation of 122.421180 108 kW   ⋅   h, 103.463636 108 kW   ⋅   h, and 99.3826340 108 kW   ⋅   h for three different scenarios (i.e. the wet year, the normal year and the dry year), which are satisfying compared with that of the GA, the improved CS, and the PSO in terms of optimization accuracy. Besides, regarding the convergence speed, the results are also competitive. Therefore, we can conclude that the LSO is an efficient method for solving complex problems with correlative decision variables with simple structure and excellent convergence speed.
3004040411;Alles nur noch „E“?;2020.0;[];
3002272455;Reducing Perceived Waiting Time in Theme Park Queues via an Augmented Reality Game;2020.0;[1966197090, 1966911549, 1978038786, 2006697004, 2042601312, 2066426987, 2092361432, 2098755033, 2112565255, 2114690005, 2127473562, 2127503699, 2146774271, 2164551073, 2295996570, 2342405812, 2531428766, 2913799641];
3006643904;Spatial Attention Fusion for Obstacle Detection Using MmWave Radar and Vision Sensor;2020.0;[639708223, 1970686984, 2022686119, 2031489346, 2083044905, 2088049833, 2109255472, 2117539524, 2120010140, 2125994488, 2134963432, 2281954672, 2344934580, 2377836894];For autonomous driving, it is important to detect obstacles in all scales accurately for safety consideration. In this paper, we propose a new spatial attention fusion (SAF) method for obstacle detection using mmWave radar and vision sensor, where the sparsity of radar points are considered in the proposed SAF. The proposed fusion method can be embedded in the feature-extraction stage, which leverages the features of mmWave radar and vision sensor effectively. Based on the SAF, an attention weight matrix is generated to fuse the vision features, which is different from the concatenation fusion and element-wise add fusion. Moreover, the proposed SAF can be trained by an end-to-end manner incorporated with the recent deep learning object detection framework. In addition, we build a generation model, which converts radar points to radar images for neural network training. Numerical results suggest that the newly developed fusion method achieves superior performance in public benchmarking. In addition, the source code will be released in the GitHub.
3001060324;Factors and loose Hamilton cycles in sparse pseudo-random hypergraphs;2020.0;[1623354552, 1969731303, 1979110822, 1983630484, 1994739397, 2001766673, 2011869755, 2054249991, 2059456505, 2062670301, 2099989374, 2105457218, 2110787567, 2113407082, 2149616758, 2153710291, 2170253147, 2901284226, 2963944538, 2964857609];"We investigate the emergence of spanning structures in sparse pseudo-random $k$-uniform hypergraphs, using the following comparatively weak notion of pseudo-randomness. A $k$-uniform hypergraph $H$ on $n$ vertices is called $(p,\alpha,\epsilon)$-pseudo-random if for all (not necessarily disjoint) vertex subsets $A_1,\dots, A_k{\subseteq} V(H)$ with $|A_1|\cdots |A_k|{\geq}\alpha n^{k}$ we have $$e(A_1,\dots, A_k)=(1\pm\epsilon)p |A_1|\cdots |A_k|.$$ For any linear $k$-uniform $F$ we provide a bound on $\alpha=\alpha(n)$ in terms of $p=p(n)$ and $F$, such that (under natural divisibility assumptions on $n$) any $k$-uniform $\big(p,\alpha, o(1)\big)$-pseudo-random $n$-vertex hypergraph $H$ with a mild minimum vertex degree condition contains an $F$-factor. The approach also enables us to establish the existence of loose Hamilton cycles in sufficiently pseudo-random hypergraphs and all results imply corresponding bounds for stronger notions of hypergraph pseudo-randomness such as jumbledness or large spectral gap.  a :[129],""consequence, $\big(p,\alpha, o(1)\big)$-pseudo-random $k$-graphs as above contain: $(i)$ a perfect matching if $\alpha=o(p^{k})$ and $(ii)$ a loose Hamilton cycle if $\alpha=o(p^{k-1})$. This extends the works of Lenz--Mubayi, and Lenz--Mubayi--Mycroft who studied the analogous problems in the dense setting."
3001271068;Logistic Regression for Criteria Weight Elicitation in PROMETHEE-Based Ranking Methods.;2020.0;[1446154311, 2002162495, 2023371922, 2072890208, 2084160675, 2086046934, 2136527772, 2332912335, 2745022580, 2901962466];
2993749016;Research on position correction method for AUV large depth navigation based on ranging positioning;2020.0;[1937351322, 2148021103];   Autonomous Underwater Vehicle (AUV) is an important carrier for studying the ocean. AUV’s accurate navigation and positioning capabilities are most important in ocean exploration. Due to the limitation of communication, the limitation of cost, and the limitation of beacon use conditions, the navigation and positioning of AUV still rely on the sensor equipped with its own dead reckoning algorithm. In the deep sea, Extended Kalman Filtering(EKF), Unscented Kalman Filtering(UKF), and Adaptive Kalman Filtering(AKF) all produce error divergence in navigation and positioning. The article adopted the acoustic-based ranging and positioning method to analyze the error under different conditions, accurately compensate the error in the dead reckoning, and finally improve the navigation and positioning accuracy. Considering the controllability and observability of AUV during ranging positioning, the article proved the stability of AUV model. The article also explained the observability and selection of paths in the ranging and positioning methods to ensure the observability of AUV. The simulation results showed that compared with the dead reckoning method using Kalman filter, the method of introducing ranging location is effective and feasible. The article considered the direction and size of the actual current, and verifies the effectiveness of the method under different conditions. The article considered the divergence of the dead reckoning error and explores the optimal path size at different depths to better obtain the actual AUV motion position. The article also discussed the research that can be done in the future. In the practical application of underwater robots, the article proposed a solution to the inaccuracy of navigation and positioning in the case of large diving depth. The results of the article have practical application value.
2999459970;Knowledge Discovery from Social Media using Big Data provided Sentiment Analysis (SoMABiT);2020.0;[47457999, 1228064353, 1487667621, 1532325895, 1565863475, 1925986356, 1974642802, 1996536857, 2010348199, 2030877481, 2044936152, 2047756776, 2049211729, 2056231502, 2067528438, 2100096506, 2108646579, 2109418048, 2109722477, 2114686809, 2115023510, 2119565742, 2126581182, 2133341045, 2146645312, 2149167588, 2149684865, 2151892596, 2155328222, 2157954477, 2173213060, 2247173740, 2272899302, 2293633461, 2949413858];In todays competitive business world, being aware of customer needs and market-oriented production is a key success factor for industries. To this aim, the use of efficient analytic algorithms ensures a better understanding of customer feedback and improves the next generation of products. Accordingly, the dramatic increase in using social media in daily life provides beneficial sources for market analytics. But how traditional analytic algorithms and methods can scale up for such disparate and multi-structured data sources is the main challenge in this regard. This paper presents and discusses the technological and scientific focus of the SoMABiT as a social media analysis platform using big data technology. Sentiment analysis has been employed in order to discover knowledge from social media. The use of MapReduce and developing a distributed algorithm towards an integrated platform that can scale for any data volume and provide a social media-driven knowledge is the main novelty of the proposed concept in comparison to the state-of-the-art technologies.
3008197904;Productive Hardware Designs using Hybrid HLS-RTL Development;2020.0;[];
2971151417;Social Collateral, Soft Information and Online Peer-to-Peer Lending: A Theoretical Model;2020.0;[430137803, 1836875240, 1962520980, 1974903261, 1980573833, 1989160322, 2018188846, 2042731036, 2079492342, 2085422494, 2092718852, 2125119150, 2144709978, 2157909376, 2161693416, 2492461111, 2747436277, 2775719470];   Traditional credit markets have been criticized as inefficient in allocating credits to borrowers. Powered by advanced Internet technology, online Peer-to-Peer (P2P) lending has emerged as an attractive alternative, especially for small borrowers who have limited assets and are in need of funds urgently. Although several empirical studies have examined factors influencing the micro-level lending outcome, there is a lack of understanding on the overall business model of P2P lending, especially its screening mechanism, and how it helps address the deficiency of the traditional credit market. This paper fills this void. First, we develop a theoretical model incorporating two unique features of P2P lending (soft information and social collateral) and show that in P2P, low-risk borrowers could force high-risk ones off the market under very general conditions. As a result, P2P complements traditional credit markets by serving the unserved (low-risk borrowers with little assets) in the traditional credit markets. Second, we further identify the critical operational settings for P2P success, and the impacts of these settings on borrowers’ welfare. Overall, our model and analyses not only contribute to the literature by showing analytically that P2P and the traditional credit markets are complementary, but also provide practical guidance to P2P platform managers regarding their platform design to help reshape business strategies and enhance business opportunities.
3000103936;Ultra-Sensitive Flexible Pressure Sensor Based on Microstructured Electrode;2020.0;[2964899339];Flexible pressure sensors with a high sensitivity in the lower zone of a subtle-pressure regime has shown great potential in the fields of electronic skin, human–computer interaction, wearable devices, intelligent prosthesis, and medical health. Adding microstructures on the dielectric layer on a capacitive pressure sensor has become a common and effective approach to enhance the performance of flexible pressure sensors. Here, we propose a method to further dramatically increase the sensitivity by adding elastic pyramidal microstructures on one side of the electrode and using a thin layer of a dielectric in a capacitive sensor. The sensitivity of the proposed device has been improved from 3.1 to 70.6 kPa−1 compared to capacitive sensors having pyramidal microstructures in the same dimension on the dielectric layer. Moreover, a detection limit of 1 Pa was achieved. The finite element analysis performed based on electromechanical sequential coupling simulation for hyperelastic materials indicates that the microstructures on electrode are critical to achieve high sensitivity. The influence of the duty ratio of the micro-pyramids on the sensitivity of the sensor is analyzed by both simulation and experiment. The durability and robustness of the device was also demonstrated by pressure testing for 2000 cycles.
2974146121;Supercritical sequences, and the nonrationality of most principal permutation classes;2020.0;[1984841421, 2021848912, 2091015960, 2340541605];   We prove that for any fixed    n   , and for most permutation patterns    q   , the number       Av    n  ,  l     (  q  )      of    q   -avoiding permutations of length    n    that consist of    l    skew blocks is a monotone decreasing function of    l   . We then show that this implies that for most patterns    q   , the generating function       ∑    n  ≥  0      Av    n     (  q  )     z    n       of the sequence       Av    n     (  q  )      of the numbers of    q   -avoiding permutations is not rational. Placing our results in a broader context, we show that for rational power series     F   (  z  )      and     G   (  z  )      with nonnegative real coefficients, the relation     F   (  z  )   =  1  ∕   (  1  −  G   (  z  )   )      is supercritical, while for most permutation patterns    q   , the corresponding relation is not supercritical.
3003641056;Power-Management Strategies for Medical Information Transmission in Wireless Body Sensor Networks;2020.0;[];"To minimize and manage the power drain, and extend battery lifetime of wireless body sensor networks (WBSN) is one of the major challenges. There are three key purposes of this survey article, first, to examine the downsides of the classical power-management methods in WBSNs; second, considering the life-critical applications and emergency contexts that are encompassed by WBSN; and, third, studying the impact of power-management techniques on resource-confined networks for economical healthcare. A specific power-management solution is also discussed."
3008125474;LUXOR: An FPGA Logic Cell Architecture for Efficient Compressor Tree Implementations;2020.0;[1983849809, 2076793873, 2102500637, 2120474114, 2123358761, 2126779549, 2127832194, 2128027813, 2154098113, 2160331356, 2293347572, 2300242332, 2585560244, 2761398658, 2765235648, 2785117529, 2891946740, 2916778556, 2949275038, 2951479717];
2981769453;Technology-enhanced learning in higher education: A bibliometric analysis with latent semantic approach;2020.0;[1488542496, 1505789767, 1606183485, 1761338525, 1805283773, 1862479034, 1877294626, 1880262756, 1945978549, 1976620775, 1978342042, 1978394996, 1986218012, 1990044698, 1992097573, 2004455776, 2021324335, 2031131447, 2032784298, 2056155664, 2058205491, 2058208920, 2064263554, 2066653708, 2094864959, 2120873112, 2121214036, 2125894968, 2130548493, 2132966425, 2135692965, 2145535121, 2147307034, 2147316766, 2160688157, 2162870238, 2166761701, 2214098284, 2248369874, 2313339486, 2356613439, 2495052195, 2508329181, 2519515778, 2571625070, 2586586754, 2606013575, 2740864224, 2741789041, 2774637605, 2778803536, 2791970612, 2798471822, 2800557164, 2803870155];   Technology-enhanced learning (TEL) describes information and communication technology applications as enhancing the outcomes of teaching and learning. Its adoption in higher education is an innovation as well as a disruption to conventional learning mechanisms. To further understand its development from the perspective of academic communities, a hybrid bibliometric approach that combines both direct citation network analysis and text analytics was proposed to examine the related research articles retrieved from the Web of Science database. In addition to visual analytics on the TEL research, a direct citation network approach with cluster analysis was used to delineate the historiographic development of the TEL research domain in higher education. Among the top internally cited articles, five main streams of TEL development were identified, namely adoption, critique, social media, podcasting, and blended learning. Then, the accumulated state of knowledge was summarized by highlighting the essential subgroup topics in each stream with latent semantic analysis. The extraction of the key features of the research domain by the proposed hybrid approach, including the principal streams of development, associated subgroup topics, and a critical article list, contributes a comprehensive method to enable the rapid understanding of the overall research development of the TEL in higher education.
3000136985;SentEmoji: A Dataset to Generate Empathising Conversations.;2020.0;[2153579005, 2798937584, 2901492641, 2949120650, 2963291843];
2999043823;Multi-Authority Revocable Access Control Method Based on CP-ABE in NDN;2020.0;[2009497894, 2033300564, 2039074122, 2122642161, 2169046918, 2172153090, 2328136677, 2611050466, 2730468065];For the future of the Internet, because information-centric network (ICN) have natural advantages in terms of content distribution, mobility, and security, it is regarded as a potential solution, or even the key, to solve many current problems. Named Data Network (NDN) is one of the research projects initiated by the United States for network architecture. NDN is a more popular project than ICN. The information cache in the NDN separates content from content publishers, but content security is threatened because of the lack of security controls. Therefore, a multi-authority revocable access control method based on CP-ABE needs to be proposed. This method constructs a proxy-assisted access control scheme, which can implement effective data access control in NDN networks, and the scheme has high security. Because of the partial decryption on the NDN node, the decryption burden of the consumer client in the solution is reduced, and effective user and attribute revocation is achieved, and forward security and backward security are ensured, and collusion attacks are prevented. Finally, through the other security and performance analysis of the scheme of this paper, it proves that the scheme is safe and efficient.
3000207830;Replicable services for reproducible research: a model for academic libraries;2020.0;[2036318837, 2139598698, 2781579392, 2963508088];
2997869609;Continued Usage and Dependency of Smartphones;2020.0;[822496777, 1527555032, 1791587663, 2032829364, 2049309847, 2057012437, 2067875401, 2100025715, 2109680381, 2163490621, 2553939816];
2999543410;Entropy, Information and Symmetry, Ordered is Symmetrical, II: System of Spins in the Magnetic Field;2020.0;[];
3008633756;Security of Cloud Computing Using Adaptive Neural Fuzzy Inference System;2020.0;[2000900288, 2010739835, 2018634415, 2520811994, 2560164496, 2606906085, 2608689243, 2610551401, 2619104685, 2770446405, 2782876185, 2895353730, 2919803975];
2955329720;Data-driven model checking for errors-in-variables varying-coefficient models with replicate measurements;2020.0;[943491864, 2015777348, 2275719586];   In this work, the adequacy check of errors-in-variables varying-coefficient models is investigated when replicate measurements are available. Estimation using the naive method that ignores measurement errors is biased. After the calibration of the estimators of the regression coefficient functions, we construct an empirical-process-based test statistic by the attenuation of corrected residuals. The asymptotic properties of the test statistic under the null hypothesis, global and various local alternatives are established. Simulation studies and real data analyses reveal that the proposed test performs satisfactorily.
3004292695;Digital Competence and Computational Thinking of Student Teachers;2020.0;[];Digital competence is one of the most demanded skills, and includes, among other aspects, the use of technological, informational, multimedia or communication skills and knowledge. In recent years, different institutions have included computational thinking among the different areas that make up this digital competence. However, there are few publications that deepen the relationship between computational thinking and digital competence. The present study analyzes the level of digital competence and computa-tional thinking of 248 Spanish university students, exploring the relation-ships between both abilities and the existing differences. According to the results, the majority of the students perceive themselves with a medium to a high level of digital competence, highlighting the multimedia and commu-nicative dimensions, as opposed to the more technological aspects. On the other hand, there is a correlation between computational thinking and digi-tal competence, especially with the communicative and technological areas. Likewise, the results indicate that women obtain lower results in their computational thinking and are perceived to be digitally less competent than men, especially in regard to the technological dimension. These results provide relevant information in terms of research and open the door to the development of training actions in student teachers to overcome the still-existing gender gaps.
2998973082;Numerical Simulation of Microflows Using Hermite Spectral Methods;2020.0;[];We propose a Hermite spectral method for the spatially inhomogeneous Boltzmann equation. For the inverse-power-law model, we generalize a class of approximate quadratic collision operators defined ...
3008153996;Enabling Self-defense in Small Drones;2020.0;[2012211082, 2141336889];
3003530160;CESoc Malaysia Chapter Runs a Workshop on Consumer-Centric IoT;2020.0;[];
2995150970;Computing functions of very large matrices with small TT/QTT ranks by quadrature formulas;2020.0;[413889016, 1483804921, 1540455107, 1954993463, 1983543686, 1984160320, 1993047937, 1993482030, 1994338028, 1996869553, 1998258511, 2016170913, 2024165284, 2028212620, 2037007846, 2040882071, 2042308234, 2106565812, 2130727150, 2138249740, 2767309578, 2790318515, 2796494547, 2903477899];   The computation of matrix functions using quadrature formulas and rational approximations of very large structured matrices using tensor trains (TT), and quantized tensor trains (QTT) is considered here. The focus is on matrices with a small TT/QTT rank. Some analysis of the error produced by the use of the TT/QTT representation and the underlying approximation formula used is also provided. Promising experiments on exponential, power, Mittag-Leffler and logarithm function of multilevel Toeplitz matrices, that are among those which generate a low TT/QTT rank representation, are also provided, confirming that the proposed approach is feasible.
2997161743;Gamification of Formative Feedback in Language Arts and Mathematics Classrooms: Application of the Learning Error and Formative Feedback (LEAFF) Model;2020.0;[153693754, 2049080106, 2065071081, 2076545077, 2165136898, 2194213766, 2619597416];
3000232543;Coastal Mangrove Response to Marine Erosion: Evaluating the Impacts of Spatial Distribution and Vegetation Growth in Bangkok Bay from 1987 to 2017;2020.0;[2048957655, 2082148511, 2126236168, 2581656659, 2970364834];
3000263883;Hybrid biogeography-based optimization with enhanced mutation and CMA-ES for global optimization problem;2020.0;[1971554611, 1977186065, 1987092188, 1992589780, 2025148441, 2112036188, 2168081761, 2180046157, 2238935679, 2344305373, 2345115205, 2416573145, 2518812438, 2738231971, 2916340702, 2919373174];In recent years, scheduling problems have attracted enormous attentions from practitioners and researches in manufacturing systems, for instance, the scheduling of computing resource in cloud infrastructure and cloud services. The scheduling problems in cloud services, big data and other service-oriented computing problems are regarded as non-separable problems. In this paper, a hybrid biogeography-based optimization with the enhanced mutation operator and CMA-ES (HBBO-CMA) is proposed to enhance the ability of exploitation on non-separable problems and alleviate the rotational variance. In the migration operator, the rotationally invariant migration operator is designed to reduce the dependence of BBO on the coordinate system and control the diversity of population. In the mutation operator, an enhanced mutation operator, which is sampled from the mean value and stand deviation of the variables of population, is employed to effectively escape the local optimum. Furthermore, the CMA-ES, which has outstanding performance on the non-separable problem, is applied to extend the exploitation of HBBO-CMA. Experimental results on CEC-2017 demonstrated the effectiveness of the proposed HBBO-CMA.
2972555422;Exploring predictors of online gambling in a nationally representative sample of Spanish adolescents;2020.0;[2010835019, 2019037190, 2020558201, 2401752081, 2518185797, 2569711038, 2901536328];   The Internet plays a central role in the daily life of most adolescents. Despite the numerous potential benefits the Internet affords, it can also lead to problematic behaviours. This study analyses several risk and protective factors associated with online gambling among a Spanish representative sample of students aged 14–18 years old (N = 37,486). Ordered logistic regression estimates show a positive and increasing association between online gambling prevalence and the frequency by which adolescents report cyberbullying victimization. However, having caring parents, having less disposable income, not participating in sports, and reading for entertainment appear as protective factors.
2981432975;Two-stage Storage Assignment to Minimize Travel Time and Congestion for Warehouse Order Picking Operations;2020.0;[1597172804, 1970732620, 1973179696, 1983481963, 1988752523, 1993530334, 2002772301, 2013014665, 2017711125, 2020320008, 2022485595, 2023462284, 2050357215, 2053900989, 2064491739, 2078014196, 2083830795, 2101668304, 2106334424, 2116661285, 2118794032, 2126105956, 2128970394, 2140066605, 2308426365, 2557894284, 2565646189, 2594575977, 2793029474, 2804191162];   This research presents a systematic and integrated approach that extends the correlated storage assignment strategy to improve the efficiency of warehouse order picking operations. The correlated storage assignment can reduce a significant amount of travel costs, but could lead to traffic congestion due to the imbalanced traffic flow. Hence, this research proposes the correlated and traffic balanced storage assignment (Cu0026TBSA) to minimize the travel time and picking delays, which is modeled in two stages: clustering and assignment. In the clustering stage, a bi-objective optimization model is formulated to group items with the consideration of both travel efficiency and traffic flow balance, which is solved using multi-objective evolutionary algorithms (MOEAs). In the assignment stage, items in each cluster are distributed to the available storage locations. Cu0026TBSA is evaluated with an actual warehouse case study and the results show that Cu0026TBSA outperforms random, class-based, and correlated storage assignment methods by 48.74%, 23.82%, and 7.58% respectively, regarding the total time consisting of travel time and picking delays.
2984243831;Aircraft detection in remote sensing image based on corner clustering and deep learning;2020.0;[1799366690, 1904073808, 1997800186, 2042316011, 2065557289, 2088049833, 2091224289, 2163605009, 2164500538, 2166210850, 2241295367, 2308318555, 2513787030, 2577537809, 2803696563, 2883145848, 2964121718];   Owing to the variations of aircraft type, pose, size and complex background, it remains difficult to detect aircraft effectively in remote sensing images, which plays a great significance in civilian and military. Classical aircraft detection algorithms still produce thousands of candidate regions and extract the features of candidate regions manually, which affects the detection performance. To address these difficulties encountered, an aircraft detection scheme based on corner clustering and Convolutional Neural Network (CNN) is proposed in this paper. The scheme is divided into two main steps: region proposal and classification. First, candidate regions are generated by utilizing mean-shift clustering algorithm to the corners detected on binary images. Then, the CNN is used for the feature extraction and classification of candidate regions that possibly contain the aircraft, and the location of the aircraft is finally determined after further screening. Compared with other classical methods, such as selective search (SS) + CNN, Edgeboxes + CNN and histogram of oriented gradient (HOG) + support vector machine (SVM), the proposed approach has a high accuracy and efficiency since it can automatically learn the essential features of the object from a large amount of data and produce fewer high quality candidate regions.
3001795195;A more decentralized vision for Linked Data;2020.0;[23305702, 109245773, 178136164, 237995741, 342586941, 1528140876, 1537617436, 1582479658, 1587260920, 1978799369, 2013667086, 2015191210, 2034912561, 2048562911, 2080133951, 2100170800, 2113310608, 2114292603, 2133109597, 2153225416, 2157327941, 2187569332, 2231771044, 2281806033, 2341964010, 2514438535, 2579007238, 2594144796, 2771909823, 2805780866, 2885947181, 2886555363, 2912478203, 2933602294, 3000383752];
3006692893;Multiple conclusion linear logic: cut elimination and more;2020.0;[29513559, 1774823771, 1800613700, 2006319938, 2065261777, 2078730256, 2083147990, 2112970627, 2126768050];
3008591352;MLPerf: An industry standard benchmark suite for machine learning performance;2020.0;[];
2999582779;Airborne Electromagnetic and Radiometric Peat Thickness Mapping of a Bog in Northwest Germany (Ahlen-Falkenberger Moor);2020.0;[2020016035];
2973786973;Application of a complete radiation boundary condition for the Helmholtz equation in locally perturbed waveguides;2020.0;[1930431862, 1978390553, 2025023024, 2067939568, 2090515334, 2095328651, 2102295719, 2907221105, 2963199603];   This paper deals with an application of a complete radiation boundary condition (CRBC) for the Helmholtz equation in locally perturbed waveguides. The CRBC, one of efficient high-order absorbing boundary conditions, has been analyzed in straight waveguides in Hagstrom and Kim (2019). In this paper, we apply CRBC to the Helmholtz equation posed in locally perturbed waveguides and establish the well-posedness of the problem and convergence of CRBC approximate solutions. The new CRBC proposed in this paper improves the one studied in Hagstrom and Kim (2019) in two aspects. The first one is that the new CRBC involves more damping parameters with the same computational cost as that of CRBC in Hagstrom and Kim (2019), which results in 50% smaller reflection errors. The second one is that the new CRBC takes a Neumann terminal condition of three term recurrence relations of auxiliary variables instead of a Dirichlet terminal condition used in Hagstrom and Kim (2019) so that it can treat cutoff modes effectively. Finally, we present numerical experiments illustrating the convergence theory.
2999214889;Temporal-spatial-frequency depth extraction of brain-computer interface based on mental tasks;2020.0;[1970242638, 1990746954, 2066783542, 2132360759, 2136794114, 2344681960, 2466248227, 2513539187, 2540507509, 2549162964, 2559463885, 2657631929, 2751159520, 2768956845, 2789939861, 2894424125, 2900786131, 2907080401, 2924079966, 2944813347];   With the help of brain-computer interface (BCI) systems, the electroencephalography (EEG) signals can be translated into control commands. It is rare to extract temporal-spatial-frequency features of the EEG signals at the same time by conventional deep neural networks. In this study, two types of series and parallel structures are proposed by combining convolutional neural network (CNN) and long short term memory (LSTM). The frequency and spatial features of EEG are extracted by CNN, and the temporal features are extracted by LSTM. The EEG signals of mental tasks with speech imagery are extracted and classified by these architectures. In addition, the proposed methods are further validated by the 2008 BCI competition IV-2a EEG data set, and its mental task is motor imagery. The series structure with compact CNN obtains the best results for two data sets. Compared with the algorithms of other literatures, our proposed method achieves the best result. Better classification results can be obtained by designing the well structured deep neural network.
3006036236;Sofware engneering challenges for machine learning applications: A literature review;2020.0;[1501005121, 1984020445, 1986014385, 1997543377, 1999798506, 2046353767, 2065267227, 2091118421, 2094834818, 2099419573, 2115627867, 2130834086, 2187483593, 2240086230, 2260771218, 2296242593, 2323909273, 2395579298, 2407212869, 2493343568, 2565516711, 2588336250, 2757656223, 2809684781, 2895658904, 2905588001, 2923735516, 2926314329, 2941110559, 2962689739];
2990968997;Fourth-order direction finding in antenna arrays with partial channel gain/phase calibration;2020.0;[1969067552, 2013646023, 2023354680, 2023634537, 2063903336, 2068297245, 2076329543, 2087757910, 2105108073, 2117345021, 2144661574, 2148372454, 2151105244, 2156985756, 2763708625, 2792333172, 2883589083, 2910050977, 2921062571, 2942508766];   In this paper, we consider the problem of direction finding in partly calibrated uniform linear array (ULA) system with channel gain/phase mismatch, by taking advantage of the high-order statistics (HOS) of the array observations. In particular, we devise a     (  2  M  −  1  )  ×  (  2  M  −  1  )     fourth-order cumulant (FOC) matrix for an M-element ULA and exploit it to estimate the directions-of-arrival (DOAs) with subspace techniques. The proposed FOC direction finding method is computationally attractive without spatial spectrum search and robust against spatially correlated noise. It is also able to achieve better DOA estimation performance than the existing method, as illustrated by numerical simulations.
3000941124;Multiple attribute group decision‐making based on order‐α divergence and entropy measures under q‐rung orthopair fuzzy environment;2020.0;[1480399662, 1509561367, 1668569279, 1954281074, 1965239040, 1971875039, 1977803996, 1978953450, 1981107087, 1989995659, 1996937753, 2000763566, 2008010437, 2010123060, 2010740403, 2013908206, 2014417436, 2017846705, 2026244788, 2029074979, 2037554805, 2040121225, 2040986566, 2041455711, 2056788985, 2060907774, 2063738313, 2079457935, 2086869454, 2091275864, 2109707919, 2125268144, 2276957120, 2289047752, 2311142332, 2345848912, 2396040005, 2414387118, 2514754342, 2549733045, 2589233777, 2593530789, 2604920622, 2735483683, 2746747652, 2756292711, 2769278513, 2789682920, 2790715562, 2796547704, 2884166416, 2899304489, 2900501367, 2906809961, 2920990744, 2924973947, 2934195809, 2951762009, 2968632614, 2969179193];
2998962490;Method of defining diagnostic features to monitor the condition of the belt conveyor gearbox with the use of the legged inspection robot;2020.0;[];
3001319253;Deep Magnetic Resonance Image Reconstruction: Inverse Problems Meet Neural Networks;2020.0;[];Image reconstruction from undersampled k-space data has been playing an important role in fast magnetic resonance imaging (MRI). Recently, deep learning has demonstrated tremendous success in various fields and also shown potential in significantly accelerating MRI reconstruction with fewer measurements. This article provides an overview of deep-learning-based image reconstruction methods for MRI. Two types of deep-learningbased approaches are reviewed, those that are based on unrolled algorithms and those that are not, and the main structures of both are explained. Several signal processing issues for maximizing the potential of deep reconstruction in fast MRI are discussed, which may facilitate further development of the networks and performance analysis from a theoretical point of view.
2971311320;A comparative study on network alignment techniques;2020.0;[1554807050, 1686696692, 1888005072, 1973956316, 1974682889, 1980680715, 1999531283, 2014374374, 2022580894, 2037137187, 2056069438, 2056124433, 2056835025, 2058036501, 2075633077, 2096939802, 2104812688, 2118674552, 2123235204, 2124637492, 2125031621, 2127607093, 2137648512, 2153204928, 2154851992, 2167467982, 2168689650, 2247394048, 2289831356, 2294347342, 2294865516, 2366141641, 2387462954, 2391555403, 2393319904, 2532629220, 2571692900, 2572926828, 2583803680, 2585831915, 2598689838, 2604795503, 2612174832, 2612872092, 2741602058, 2756095124, 2761896323, 2763929596, 2789457647, 2789600914, 2792234394, 2793022729, 2795152721, 2808935172, 2888657195, 2950133940, 2962767366];   Network alignment is a method to align nodes that belong to the same entity from different networks. A well-known application of network alignment is to map user accounts from different social networks that belong to the same person. As network alignment has a wide range of applications from recommendation to link prediction, there are several proposed approaches to aligning nodes from different networks. These techniques, however, have been rarely compared and analyzed under the same setting, rendering a right choice for a particular set of networks very difficult. Addressing this problem, this paper presents a benchmark that offers a comprehensive empirical study on the performance comparison of network alignment methods. Specifically, we integrate several state-of-the-art network alignment techniques in a comparable manner, and measure distinct characteristics of these techniques with various settings. We then provide in-depth analysis of the benchmark results, obtained by using both real data and synthetic data. We believe that the findings from the benchmark will serve as a practical guideline for potential applications.
3002749393;Correction: Zhu, Q., et al. Hyperspectral Remote Sensing of Phytoplankton Species Composition Based on Transfer Learning. Remote Sensing 2019, 11, 2001;2020.0;[];
2963869273;Discovering attractive segments in the user-generated video streams;2020.0;[1486723856, 1527575280, 1586939924, 1668832360, 1687679254, 2009096031, 2029535444, 2060505035, 2064675550, 2093697352, 2099808581, 2107878631, 2114025269, 2115752676, 2125844878, 2130942839, 2139501017, 2144577430, 2170224461, 2194775991, 2326678594, 2562476021, 2618609858, 2742947535, 2751013881, 2752930373, 2791295466, 2803620531, 2807548702, 2810334075, 2884326683, 2887712318, 2896196878, 2897230578, 2897298014, 2903559293, 2907166662, 2907536214, 2914011592, 2950133940, 2950138901, 2963410018, 2963843052, 2964011431, 2964173101, 2976125790, 2981578638];   With the rapid development of digital equipment and the continuous upgrading of online media, a growing number of people are willing to post videos on the web to share their daily lives ( Jelodar, Paulius, u0026 Sun, 2019 ). Generally, not all video segments are popular with audiences, some of which may be boring. If we can predict which segment in a newly generated video stream would be popular, the audiences can only enjoy this segment rather than watch the whole video to find the funny point. And if we can predict the emotions that the audiences would induce when they watch a video, this must be helpful for video analysis and for guiding the video-makers to improve their videos. In recent years, crowd-sourced time-sync video comments have emerged worldwide, supporting further research on temporal video labeling. In this paper, we propose a novel framework to achieve the following goal: Predicting which segment in a newly generated video stream (hasn’t been commented with the time-sync comments) will be popular among the audiences. At last, experimental results on real-world data demonstrate the effectiveness of the proposed framework and justify the idea of predicting the popularities of segments in a video exploiting crowd-sourced time-sync comments as a bridge to analyze videos.
2973646423;SoTaRePo: Society-Tag Relationship Protocol based architecture for UIP construction;2020.0;[1489253734, 1561976191, 1788181460, 1965971975, 1980672078, 1985131669, 1990766815, 2014124897, 2023261081, 2031999643, 2040256447, 2044591816, 2064173066, 2064213435, 2073083495, 2079607477, 2085925999, 2095976990, 2100755716, 2105981469, 2122841972, 2130196654, 2141599568, 2147709600, 2153134505, 2157025439, 2165806612, 2169223373, 2293286118, 2314700336, 2345559987, 2497672654, 2802583537];   As with the advancement of web services, there has been a rapid proliferation in web size and number of web users, where, each user holds a different viewpoint towards the same information. This, in turn, has become a big challenge for the web search platforms to interpret the preferences of the users and provide the desired information to them. The most suitable solution to the problem of search platforms is personalization of web search. A personalization system is a kind of expert and intelligent system which can automatically learn about the preferences of a user so that the system can provide the search results as per their relevance to a user. The process of acquiring knowledge about user’s preferences by a personalization system is known as User Interest Profile (UIP). In the field of search personalization, it can also not be denied that only an efficient and complete UIP can lead to an effective and high performing web search personalization methodology design. But most of the studies conducted for web search personalization have only focused on UIP modeling without any thought about the quality of UIP. Rather limited attention has been paid to sparsity issue of UIP modeling. In this paper, we propose a novel protocol based architecture model to create an efficient UIP by exploiting direct and indirect interest of a user. Direct interest aims at mining user’s preferences from his own activities on a social information platform. The explicitly defined society and real-world activity relationships of a user on a social platform are used to predict his indirect interest as UIP constructed solely on the basis of direct interest is sparse and ineffective. In order to unearth user’s activity relationships the concept of semantic relatedness, computed using Word2vec model, has been used. Moreover, different trust levels in society relationships have also been incorporated into the proposed model to facilitate the prediction of user’s indirect interest. A series of experiments have been conducted on a del.icio.us dataset to evaluate the effectiveness of the proposed model. The results show that the model has outperformed each and every baseline in relation to complete and efficient UIP construction.
3003566925;Shift Invariance Property of a Non-Negative Matrix Factorization;2020.0;[];
2972653537;Global and Local Multi-View Multi-Label Learning;2020.0;[221337630, 1201875361, 1978169406, 2002370809, 2007972815, 2011390239, 2049365101, 2060059578, 2076365336, 2100453679, 2108013467, 2116373735, 2129414564, 2142742813, 2156935079, 2169529055, 2210977594, 2343246984, 2414996405, 2605997098, 2620419788, 2747053038, 2756095124, 2787932447, 2898233200, 2963139378];"   In order to process multi-view multi-label data sets, we propose global and local multi-view multi-label learning (GLMVML). This method can exploit global and local label correlations of both the whole data set and each view simultaneously. What’s more, GLMVML introduces a consensus multi-view representation which encodes the complementary information from different views. Related experiments on three multi-view data sets, fourteen multi-label data sets, and one multi-view multi-label data set have validated that (1) GLMVML has a better average AUC and precision and it is superior to the classical multi-view learning methods and multi-label learning methods in statistical; (2) the running time of GLMVML won’t add too much; (3) GLMVML has a good convergence and ability to process multi-view multi-label data sets; (4) since the model of GLMVML consists of both the global label correlations and local label correlations, so parameter values should be moderate rather than too large or too small."
2997404769;Hidden Hamiltonian Cycle Recovery via Linear Programming;2020.0;[];
2992197405;An IoT platform for the analysis of brain CT images based on Parzen analysis;2020.0;[1951747108, 1970150866, 2044465660, 2094056275, 2100199701, 2103496339, 2122111042, 2129526018, 2149298154, 2159498975, 2163352848, 2168822268, 2169491762, 2187206419, 2260424533, 2466535637, 2528884737, 2569168369, 2586433286, 2586890322, 2591940486, 2606295484, 2606808050, 2616444582, 2619568410, 2668739872, 2768506985, 2804426336, 2805182940, 2897064646, 2897829713, 2911722481, 2913929186, 2921483513, 2941914178, 2942571977];   Stroke or cerebrovascular accidents are among the three leading causes of death worldwide. Furthermore, strokes are a major cause of morbidity, hospitalizations, and acquired disabilities. Stroke diagnoses are usually made based on the set of symptoms exhibited and, more specifically, on the results of neuroimaging exams. Among the different types of neuroimaging, computed tomography (CT) is the most commonly used because it can show the extent and severity of the accident. Furthermore, CT exams are faster, more accessible than other systems, and are financially viable. Thus, due to its emergency nature, CAD systems that can analyze CT images are essential to obtain information that can accelerate the diagnosis and definition of appropriate treatment. This work presents a new feature extractor from CT images of the brain to assist in the detection and classification of strokes, called Parzen Analysis of Brain Tissue Densities. The proposed method can reduce the subjectivity present in the brain tissue bands using Parzen Window Estimation by calculating the probability that each pixel belongs to a preconfigured range. In addition, the method is fully integrated with an Internet of Things framework and can be used remotely to assist medical specialists in the diagnosis and treatment of strokes. The method presented promising results, reaching the highest values of accuracy, 98.41%, F1-Score, 97.61%, negative predictive value, 98.80%, and positive predictive value, 95.45%, among all the methods evaluated here. The results demonstrated the effectiveness of the method in extracting relevant features from brain CTs to describe if there is a stroke as well as determining its type.
2997443541;Super-weak force and neutrino masses;2020.0;[];We consider an anomaly free extension of the standard model gauge group     G SM     by an abelian group to      G SM  ⊗ U   ( 1 )  Z     . The condition of anomaly cancellation is known to fix the Z-charges of the particles, but two. We fix one remaining charge by allowing for all possible Yukawa interactions of the known left-handed neutrinos and new right-handed ones that obtain their masses through interaction with a new scalar field with spontaneously broken vacuum. We discuss some of the possible consequences of the model.
2997291405;Seismic Observation and Analysis Based on Three-Component Fiber Optic Seismometer;2020.0;[];
3000392525;Inertial sensors-based torso motion mode recognition for an active postural support brace*;2020.0;[1540557062, 1913582850, 1978332670, 2144781570, 2167221577, 2298294084, 2743692535, 2883617685, 2954614122];ABSTRACTIn order to achieve high-level control for an active postural support brace, it is important for a wearable robot to be capable of recognizing human motion intentions. An inertial sensors-b...
3001683536;Design, Analysis, and Verification of an Electro- Hydrostatic Actuator for Distributed Actuation System;2020.0;[];In order to provide a simplified and low-cost solution of the terminal for a distributed actuation system, this paper proposes an electro-hydrostatic actuator (EHA) based on the linear drive principle. The proposed actuator is directly driven by a linear pump with a collaborative rectification mechanism, whose performance relies on the collaboration of the internal two units. A pair of linear oscillating motors are employed to drive the two pump units respectively. The control of the actuator is based on the modulation of the oscillating amplitude, frequency, and phase difference of the two motors. The advantage of this actuator is that no more valve control is needed to rectify the linear pump besides the high efficiency of the direct pump drive. In this paper, both schematic and detailed structure of the actuator is presented. The kinematic and dynamic characteristics are analyzed and modeled, based on which the control method is proposed. The experiments verify the validity of the actuator structure and control.
1365286;Limiting Adversarial Budget in Quantitative Security Assessment;2014.0;[];We present the results of research of limiting adversarial budget in attack games, and, in particular, in the failure-free attack tree models presented by Buldas-Stepanenko in 2012 and improved in 2013 by Buldas and Lenin. In the previously presented models attacker’s budget was assumed to be unlimited. It is natural to assume that the adversarial budget is limited and such an assumption would allow us to model the adversarial decision making more close to the one that might happen in real life. We analyze three atomic cases – the single atomic case, the atomic AND, and the atomic OR. Even these elementary cases become quite complex, at the same time, limiting adversarial budget does not seem to provide any better or more precise results compared to the failure-free models. For the limited model analysis results to be reliable, it is required that the adversarial reward is estimated with high precision, probably not achievable by providing expert estimations for the quantitative annotations on the attack steps, such as the cost or the success probability. It is doubtful that it is reasonable to face this complexity, as the failure-free model provides reliable upper bounds, being at the same time computationally less complex.
17346433;Fast approximate nearest-neighbor search with k-nearest neighbor graph;2011.0;[];We introduce a new nearest neighbor search algorithm. The algorithm builds a nearest neighbor graph in an offline phase and when queried with a new point, performs hill-climbing starting from a randomly sampled node of the graph. We provide theoretical guarantees for the accuracy and the computational complexity and empirically show the effectiveness of this algorithm.
23305702;Observing Linked Data Dynamics;2013.0;[];"In this paper, we present the design and first results of the Dynamic Linked Data Observatory: a long-term experiment to monitor the two-hop neighbourhood of a core set of eighty thousand diverse Linked Data documents on a weekly basis. We present the methodology used for sampling the URIs to monitor, retrieving the documents, and further crawling part of the two-hop neighbourhood. Having now run this experiment for six months, we analyse the dynamics of the monitored documents over the data collected thus far. We look at the estimated lifespan of the core documents, how often they go on-line or off-line, how often they change; we further investigate domain-level trends. Next we look at changes within the RDF content of the core documents across the weekly snapshots, examining the elements (i.e., triples, subjects, predicates, objects, classes) that are most frequently added or removed. Thereafter, we look at how the links between dereferenceable documents evolves over time in the two-hop neighbourhood."
38977886;An Organic Architecture for Traffic Light Controllers.;2006.0;[];Efficient control of traffic networks is a complex but important task. A successful network management vitally depends on the abilities of the traffic light controllers to adapt to changing traffic situations. In this paper a control architecture for traffic nodes is presented that is inspired by the principles of Organic Computing. It allows a node to quickly adapt to changing traffic situations and enables it to autonomously learn new control strategies if necessary.
47457999;Semantic technologies for enterprise cloud management;2010.0;[];Enterprise clouds apply the paradigm of cloud computing to enterprise IT infrastructures, with the goal of providing easy, flexible, and scalable access to both computing resources and IT services. Realizing the vision of the fully automated enterprise cloud involves addressing a range of technological challenges. In this paper, we focus on the challenges related to intelligent information management in enterprise clouds and discuss how semantic technologies can help to fulfill them. In particular, we address the topics of data integration, collaborative documentation and annotation and intelligent information access and analytics and present solutions that are implemented in the newest addition to our eCloudManager product suite: The Intelligence Edition.
79079207;Preference-Based Rank Elicitation using Statistical Models: The Case of Mallows;2014.0;[];We address the problem of rank elicitation assuming that the underlying data generating process is characterized by a probability distribution on the set of all rankings (total orders) of a given set of items. Instead of asking for complete rankings, however, our learner is only allowed to query pairwise preferences. Using information of that kind, the goal of the learner is to reliably predict properties of the distribution, such as the most probable top-item, the most probable ranking, or the distribution itself. More specifically, learning is done in an online manner, and the goal is to minimize sample complexity while guaranteeing a certain level of confidence.
109245773;Weaving the Pedantic Web;2010.0;[];"Over a decade after RDF has been published as a W3C recommendation, publishing open and machine-readable content on the Web has recently received a lot more attention, including from corporate and governmental bodies; notably thanks to the Linked Open Data community, there now exists a rich vein of heterogeneous RDF data published on the Web (the so-called \Web of Data"") accessible to all. However, RDF publishers are prone to making errors which compromise the e ectiveness of applications leveraging the resulting data. In this paper, we discuss common errors in RDF publishing, their consequences for applications, along with possible publisher-oriented approaches to improve the quality of structured, machine-readable and open data on the Web."
153693754;Gamification of Education: A Review of Literature;2014.0;[];We synthesized the literature on gamification of education by conducting a review of the literature on gamification in the educational and learning context. Based on our review, we identified several game design elements that are used in education. These game design elements include points, levels/stages, badges, leaderboards, prizes, progress bars, storyline, and feedback. We provided examples from the literature to illustrate the application of gamification in the educational context.
170687150;The quest for transitivity, a showcase of fuzzy relational calculus;2012.0;[];"We present several relational frameworks for expressing similarities and preferences in a quantitative way. The main focus is on the occurrence of various types of transitivity in these frameworks. The first framework is that of fuzzy relations; the corresponding notion of transitivity is C-transitivity, with C a conjunctor. We discuss two approaches to the measurement of similarity of fuzzy sets: a logical approach based on biresidual operators and a cardinal approach based on fuzzy set cardinalities. The second framework is that of reciprocal relations; the corresponding notion of transitivity is cycle-transitivity. It plays a crucial role in the description of different types of transitivity arising in the comparison of (artificially coupled) random variables in terms of winning probabilities. It also embraces the study of mutual rank probability relations of partially ordered sets."
178136164;LOD Laundromat: A Uniform Way of Publishing Other People's Dirty Data;2014.0;[];"It is widely accepted that proper data publishing is difficult. The majority of Linked Open Data (LOD) does not meet even a core set of data publishing guidelines. Moreover, datasets that are clean at creation, can get stains over time. As a result, the LOD cloud now contains a high level of dirty data that is difficult for humans to clean and for machines to  :[65],""solutions for cleaning data (standards, guidelines, tools) are targeted towards human data creators, who can (and do) choose not to use them. This paper presents the LOD Laundromat which removes stains from data without any human intervention. This fully automated approach is able to make very large amounts of LOD more easily available for further processing right  Laundromat is not a :[123],""new dataset, but rather a uniform point of entry to a collection of cleaned siblings of existing datasets. It provides researchers and application developers a wealth of data that is guaranteed to conform to a specified set of best practices, thereby greatly improving the chance of data actually being (re)used."
201786483;Time-Dependent Analysis of Attacks;2014.0;[];"The success of a security attack crucially depends on time: the more time available to the attacker, the higher the probability of a successful attack; when given enough time, any system can be compromised. Insight in time-dependent behaviors of attacks and the evolution of the attacker’s success as time progresses is therefore a key for effective countermeasures in securing  :[59],""paper presents an efficient technique to analyze attack times for an extension of the prominent formalism of attack trees. If each basic attack step, i.e., each leaf in an attack tree, is annotated with a probability distribution of the time needed for this step to be successful, we show how this information can be propagated to an analysis of the entire tree. In this way, we obtain the probability distribution for the entire system to be attacked successfully as time progresses. For our approach to be effective, we take great care to always work with the best possible compression of the representations of the probability distributions arising. This is achieved by an elegant calculus of acyclic phase type distributions, together with an effective compositional compression technique. We demonstrate the effectiveness of this approach on three case studies, exhibiting orders of magnitude of compression."
221337630;Multi-view based multi-label propagation for image annotation;2015.0;[];Multi-view learning and multi-label propagation are two common approaches to address the problem of image annotation. Traditional multi-view methods disregard the consistencies among different views while existing algorithms toward multi-label propagation ignore the underlying mutual correlations among different labels. In this paper, we present a novel image annotation algorithm by exploring the heterogeneities from both the view level and the label level. For a single label, its propagation from one view should agree with the propagation from another view. Similarly, for a single view, the propagations of related labels should be similar. We call the proposed approach as Multi-view based Multi-label Propagation for image annotation (MMP). MMP handles the consistencies among different views by requiring them to generate the same annotation result, and captures the correlations among different labels by imposing the similarity constraints. By taking full advantage of the dual-heterogeneity from views and labels, MMP is able to propagate the labels better than state of the art. Furthermore, we introduce an iterative algorithm to solve the optimization problem. Extensive experiments on real image data have shown that the proposed framework has effective image annotation performance.
237995741;Strategies for Executing Federated Queries in SPARQL1.1;2014.0;[];A common way for exposing RDF data on the Web is by means of SPARQL endpoints which allow end users and applications to query just the RDF data they want. However, servers hosting SPARQL endpoints often restrict access to the data by limiting the amount of results returned per query or the amount of queries per time that a client may issue. As this may affect query completeness when using SPARQL1.1u0027s federated query extension, we analysed different strategies to implement federated queries with the goal to circumvent endpoint limits. We show that some seemingly intuitive methods for decomposing federated queries provide unsound results in the general case, and provide fixes or discuss under which restrictions these recipes are still applicable. Finally, we evaluate the proposed strategies for checking their feasibility in practice.
239929985;Connectivity-based fixel enhancement: Whole-brain statistical analysis of diffusion MRI measures in the presence of crossing fibres.;2015.0;[];In brain regions containing crossing fibre bundles, voxel-average diffusion MRI measures such as fractional anisotropy (FA) are difficult to interpret, and lack within-voxel single fibre population specificity. Recent work has focused on the development of more interpretable quantitative measures that can be associated with a specific fibre population within a voxel containing crossing fibres (herein we use fixel to refer to a specific fibre population within a single voxel). Unfortunately, traditional 3D methods for smoothing and cluster-based statistical inference cannot be used for voxel-based analysis of these measures, since the local neighbourhood for smoothing and cluster formation can be ambiguous when adjacent voxels may have different numbers of fixels, or ill-defined when they belong to different tracts. Here we introduce a novel statistical method to perform whole-brain fixel-based analysis called connectivity-based fixel enhancement (CFE). CFE uses probabilistic tractography to identify structurally connected fixels that are likely to share underlying anatomy and pathology. Probabilistic connectivity information is then used for tract-specific smoothing (prior to the statistical analysis) and enhancement of the statistical map (using a threshold-free cluster enhancement-like approach). To investigate the characteristics of the CFE method, we assessed sensitivity and specificity using a large number of combinations of CFE enhancement parameters and smoothing extents, using simulated pathology generated with a range of test-statistic signal-to-noise ratios in five different white matter regions (chosen to cover a broad range of fibre bundle features). The results suggest that CFE input parameters are relatively insensitive to the characteristics of the simulated pathology. We therefore recommend a single set of CFE parameters that should give near optimal results in future studies where the group effect is unknown. We then demonstrate the proposed method by comparing apparent fibre density between motor neurone disease (MND) patients with control subjects. The MND results illustrate the benefit of fixel-specific statistical inference in white matter regions that contain crossing fibres.
303937333;Summation-By-Parts in Time: The Second Derivative;2016.0;[];We analyze the extension of summation-by-parts operators and weak boundary conditions for solving initial boundary value problems involving second derivatives in time. A wide formulation is obtained by first rewriting the problem on first order form. This formulation leads to optimally sharp fully discrete energy estimates that are unconditionally stable and high order accurate. Furthermore, it provides a natural way to impose mixed boundary conditions of Robin type, including time and space derivatives. We apply the new formulation to the wave equation and derive optimal fully discrete energy estimates for general Robin boundary conditions, including nonreflecting ones. The scheme utilizes wide stencil operators in time, whereas the spatial operators can have both wide and compact stencils. Numerical calculations verify the stability and accuracy of the method. We also include a detailed discussion on the added complications when using compact operators in time and give an example showing that an energy ...
342586941;Schema-Agnostic Query Rewriting in SPARQL 1.1;2014.0;[];"SPARQLa1.1 supports the use of ontologies to enrich query results with logical entailments, and OWLa2 provides a dedicated fragment OWLaQL for this purpose. Typical implementations use the OWLaQL schema to rewrite a conjunctive query into an equivalent set of queries, to be answered against the non-schema part of the data. With the adoption of the recent SPARQLa1.1 standard, however, RDF databases are capable of answering much more expressive queries directly, and we ask how this can be exploited in query rewriting. We find that SPARQLa1.1 is powerful enough to ""implement"" a full-fledged OWLaQL reasoner in a single query. Using additional SPARQLa1.1 features, we develop a new method of schema-agnostic query rewriting, where arbitrary conjunctive queries over OWLaQL are rewritten into equivalent SPARQLa1.1 queries in a way that is fully independent of the actual schema. This allows us to query RDF data under OWLaQL entailment without extracting or preprocessing OWL axioms."
413889016;Numerical Evaluation of Two and Three Parameter Mittag-Leffler Functions;2015.0;[];The Mittag-Leffler (ML) function plays a fundamental role in fractional calculus but very few methods are available for its numerical evaluation. In this work we present a method for the efficient computation of the ML function based on the numerical inversion of its Laplace transform (LT): an optimal parabolic contour is selected on the basis of the distance and the strength of the singularities of the LT, with the aim of minimizing the computational effort and reducing the propagation of errors. Numerical experiments are presented to show accuracy and efficiency of the proposed approach. The application to the three parameter ML (also known as Prabhakar) function is also presented.
430137803;Instance-based credit risk assessment for investment decisions in P2P lending;2016.0;[];Recent years have witnessed increased attention on peer-to-peer (P2P) lending, which provides an alternative way of financing without the involvement of traditional financial institutions. A key challenge for personal investors in P2P lending marketplaces is the effective allocation of their money across different loans by accurately assessing the credit risk of each loan. Traditional rating-based assessment models cannot meet the needs of individual investors in P2P lending, since they do not provide an explicit mechanism for asset allocation. In this study, we propose a data-driven investment decision-making framework for this emerging market. We designed an instance-based credit risk assessment model, which has the ability of evaluating the return and risk of each individual loan. Moreover, we formulated the investment decision in P2P lending as a portfolio optimization problem with boundary constraints. To validate the proposed model, we performed extensive experiments on real-world datasets from two notable P2P lending marketplaces. Experimental results revealed that the proposed model can effectively improve investment performances compared with existing methods in P2P lending.
566653638;Linguistic Interval Hesitant Fuzzy Sets and Their Application in Decision Making;2016.0;[];To cope with the hesitancy and uncertainty of the decision makers’ cognitions to decision-making problems, this paper introduces a new type of fuzzy sets called linguistic interval hesitant fuzzy sets. A linguistic interval hesitant fuzzy set is composed of several linguistic terms with each one having several interval membership degrees. Considering the application of linguistic interval hesitant fuzzy sets in decision making, an ordered relationship is offered, and several operational laws are defined. After that, several aggregation operators based on additive and fuzzy measures are introduced, by which the comprehensive attribute values can be obtained. Based on the defined distance measure, models for the optimal weight vectors are constructed. In addition, an approach to multi-attribute decision making with linguistic interval hesitant fuzzy information is developed. Finally, two numerical examples are provided to show the concrete application of the procedure.
610200145;Lion Optimization Algorithm (LOA): A nature-inspired metaheuristic algorithm;2016.0;[];During the past decade, solving complex optimization problems with metaheuristic algorithms has received considerable attention among practitioners and researchers. Hence, many metaheuristic algorithms have been developed over the last years. Many of these algorithms are inspired by various phenomena of nature. In this paper, a new population based algorithm, the Lion Optimization Algorithm (LOA), is introduced. Special lifestyle of lions and their cooperation characteristics has been the basic motivation for development of this optimization algorithm. Some benchmark problems are selected from the literature, and the solution of the proposed algorithm has been compared with those of some well-known and newest meta-heuristics for these problems. The obtained results confirm the high performance of the proposed algorithm in comparison to the other algorithms used in this paper.
639708223;Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks;2017.0;[];State-of-the-art object detection networks depend on region proposal algorithms to hypothesize object locations. Advances like SPPnet  [1]  and Fast R-CNN  [2]  have reduced the running time of these detection networks, exposing region proposal computation as a bottleneck. In this work, we introduce a  Region Proposal Network  (RPN) that shares full-image convolutional features with the detection network, thus enabling nearly cost-free region proposals. An RPN is a fully convolutional network that simultaneously predicts object bounds and objectness scores at each position. The RPN is trained end-to-end to generate high-quality region proposals, which are used by Fast R-CNN for detection. We further merge RPN and Fast R-CNN into a single network by sharing their convolutional features—using the recently popular terminology of neural networks with ’attention’ mechanisms, the RPN component tells the unified network where to look. For the very deep VGG-16 model  [3]  , our detection system has a frame rate of 5 fps ( including all steps ) on a GPU, while achieving state-of-the-art object detection accuracy on PASCAL VOC 2007, 2012, and MS COCO datasets with only 300 proposals per image. In ILSVRC and COCO 2015 competitions, Faster R-CNN and RPN are the foundations of the 1st-place winning entries in several tracks. Code has been made publicly available.
756957829;A frequentist view on cycle-transitivity of reciprocal relations;2015.0;[];We establish a connection between two transitivity frameworks: the transitivity of fuzzy relations based on a commutative quasi-copula and the cycle-transitivity of reciprocal relations w.r.t. the dual quasi-copula as an upper bound function. Loosely speaking, it turns out that the latter can be characterized by imposing a lower bound on the relative frequency with which the former is fulfilled, when applied to reciprocal relations. We provide two compelling cases: the 4/6 theorem, expressing that the winning probability relation of a set of independent random variables is at least 66.66% product-transitive, and the 5/6 theorem, expressing that the mutual rank probability relation associated with a given poset is at least 83.33% product-transitive. Moreover, these lower bounds turn out be rather conservative, illustrating that, from a frequentist point of view, transitivity is abundant.
822496777;Understanding Continued Usage Intention in e-Learning Context;2011.0;[];With the latest development of the Internet technologies, it has offered many e-learning systems available for the educators to conduct courses online. The advantage of using such systems in connection with on-site courses is that it increases flexibility through resources that facilitate learning anytime anywhere. However, there is little empirical evidence to suggest what factors underpin educators continued usage of such systems. This study builds a model based on the Unified Theory of Acceptance and Use of Technology to identify the factors. The model is tested among the university educators (n = 175) who use a popular e-learning system, Moodle. The results suggest that continuance intention is driven by perceived usefulness and access. Perceived ease of use, perceived behavioral control, compatibility, and social influence do not have significant impact on continuance intention. These core determinants of continuance intention altogether explained around 70% of the total variance of intention.
867003385;Energy-efficient scheduling in manufacturing companies: A review and research framework;2016.0;[];Because sustainable scheduling is attracting increasing amounts of attention from many manufacturing companies and energy is a central concern regarding sustainability, the purpose of this paper is to develop a research framework for “energy-efficient scheduling” (EES). EES approaches are scheduling approaches that have the objective of improving energy efficiency. Based on an iterative methodology, we review, analyze, and synthesize the current state of the literature and propose a completely new research framework to structure the research field. In doing so, the three dimensions “energetic coverage”, “energy supply”, and “energy demand” are introduced and used to classify the literature. Each of these dimensions contains categories and attributes to specify energy-related characteristics that are relevant for EES. We further provide an empirical analysis of the reviewed literature and emphasize the benefits that can be achieved by EES in practice.
943491864;Consistent test of error-in-variables partially linear model with auxiliary variables;2015.0;[];In this paper, we investigate the model checking problem of a partially linear model when some covariates are measured with error and some auxiliary variables are supplied. The often-used assumptions on the measurement error, such as a known error variance or a known distribution of the error variable, are not required. Also repeated measurements are not needed. Instead, a nonparametric calibration method is applied to deal with the measurement error. An estimating method for the null hypothetical model is proposed and the asymptotic properties of the proposed estimators are established. A testing method based on a residual-marked empirical process is then developed to check the null hypothetical partially linear model. The tests are shown to be consistent and can detect the alternative hypothesis close to the null hypothesis at the rate n - r with 0 ? r ? 1 / 2 . Simulation studies and real data analysis are conducted to examine the finite sample behavior of the proposed methods.
1159968740;Pareto Efficient Solutions of Attack-Defence Trees;2015.0;[];"Attack-defence trees are a promising approach for representing threat scenarios and possible countermeasures in a concise and intuitive manner. An attack-defence tree describes the interaction between an attacker and a defender, and is evaluated by assigning parameters to the nodes, such as probability or cost of attacks and defences. In case of multiple parameters most analytical methods optimise one parameter at a time, e.g., minimise cost or maximise probability of an attack. Such methods may lead to sub-optimal solutions when optimising conflicting parameters, e.g., minimising cost while maximising  :[88],""order to tackle this challenge, we devise automated techniques that optimise all parameters at once. Moreover, in the case of conflicting parameters our techniques compute the set of all optimal solutions, defined in terms of Pareto efficiency. The developments are carried out on a new and general formalism for attack-defence trees."
1201875361;Multi-view low-rank dictionary learning for image classification;2016.0;[];Recently, a multi-view dictionary learning (DL) technique has received much attention. Although some multi-view DL methods have been presented, they suffer from the problem of performance degeneration when large noise exists in multiple views. In this paper, we propose a novel multi-view DL approach named multi-view low-rank DL (MLDL) for image classification. Specifically, inspired by the low-rank matrix recovery theory, we provide a multi-view dictionary low-rank regularization term to solve the noise problem. We further design a structural incoherence constraint for multi-view DL, such that redundancy among dictionaries of different views can be reduced. In addition, to enhance efficiency of the classification procedure, we design a classification scheme for MLDL, which is based on the idea of collaborative representation based classification. We apply MLDL for face recognition, object classification and digit classification tasks. Experimental results demonstrate the effectiveness and efficiency of the proposed approach. We offer a multi-view low-rank dictionary learning method for image classification.Multi-view dictionary low-rank regularization term is designed to handle noise.Structural incoherence constraint is given to reduce redundancy in dictionaries.Multi-view collaborative representation based classification scheme is provided.Effectiveness and efficiency of our method are demonstrated on four datasets.
1446154311;A new method for elicitation of criteria weights in additive models: Flexible and interactive tradeoff;2016.0;[];This paper proposes the Flexible and Interactive Tradeoff (FITradeoff) method, for eliciting scaling constants or weights of criteria. The FITradeoff uses partial information about decision maker (DM) preferences to determine the most preferred in a specified set of alternatives, according to an additive model in MAVT (Multi-Attribute Value Theory) scope. This method uses the concept of flexible elicitation for improving the applicability of the traditional tradeoff elicitation procedure. FITradeoff offers two main benefits: the information required from the DM is reduced and the DM does not have to make adjustments for the indifference between two consequences (trade-off), which is a critical issue on the traditional tradeoff procedure. It is easier for the DM to make comparisons of consequences (or outcomes) based on strict preference rather than on indifference. The method is built into a decision support system and applied to two cases on supplier selection, already published in the literature.
1455310343;Asymmetric Minwise Hashing for Indexing Binary Inner Products and Set Containment;2015.0;[];Minwise hashing (Minhash) is a widely popular indexing scheme in practice. Minhash is designed for estimating set resemblance and is known to be suboptimal in many applications where the desired measure is set overlap (i.e., inner product between binary vectors) or set containment. Minhash has inherent bias towards smaller sets, which adversely affects its performance in applications where such a penalization is not desirable. In this paper, we propose asymmetric minwise hashing ({\em MH-ALSH}), to provide a solution to this well-known problem. The new scheme utilizes asymmetric transformations to cancel the bias of traditional minhash towards smaller sets, making the final ``collision probabilityu0027u0027 monotonic in the inner product. Our theoretical comparisons show that, for the task of retrieving with binary inner products, asymmetric minhash is provably better than traditional minhash and other recently proposed hashing algorithms for general inner products. Thus, we obtain an algorithmic improvement over existing approaches in the literature. Experimental evaluations on four publicly available high-dimensional datasets validate our claims. The proposed scheme outperforms, often significantly, other hashing algorithms on the task of near neighbor retrieval with set containment. Our proposal is simple and easy to implement in practice.
1480399662;R-norm entropy on intuitionistic fuzzy sets;2015.0;[];Fuzzy sets have led to study of vague phenomena. Generalizations of fuzzy sets have led to deeper analysis of these types of studies. The problem that then arises is to finding quantitative measures for vagueness and other features of these phenomena. In the present paper, based on the concept of R-norm fuzzy entropy, an R-norm intuitionistic fuzzy entropy measure is proposed in the setting of intuitionistic fuzzy set theory. This measure is a generalized version of R-norm fuzzy entropy proposed by Hooda in 2004. Some properties of this measure are proved. Finally, a numerical example is given to show that the proposed entropy measure for intuitionistic fuzzy set is reasonable by comparing it with other existing intuitionistic fuzzy entropy measures.
1480909796;Z3: an efficient SMT solver;2008.0;[];Satisfiability Modulo Theories (SMT) problem is a decision problem for logical first order formulas with respect to combinations of background theories such as: arithmetic, bit-vectors, arrays, and uninterpreted functions. Z3 is a new and efficient SMT Solver freely available from Microsoft Research. It is used in various software verification and analysis applications.
1482131745;The function field sieve;1994.0;[];The fastest method known for factoring integers is the ‘number field sieve’. An analogous method over function fields is developed, the ‘function field sieve’, and applied to calculating discrete logarithms over GF(pn). An heuristic analysis shows that there exists a ce ℜu003e0 such that the function field sieve computes discrete logarithms within random time: L p n [1/3, c] when log(p) ≤ n9(n) where g is any function such that g: N → ℜ u003e0 u003c.98 approaches zero as n → ∞.
1486003553;Fitness functions in genetic programming for classification with unbalanced data;2007.0;[];This paper describes a genetic programming (GP) approach to binary classification with class imbalance problems. This approach is examined on two benchmark and two synthetic data sets. The results show that when using the overall classification accuracy as the fitness function, the GP system is strongly biased toward the majority class. Two new fitness functions are developed to deal with the class imbalance problem. The experimental results show that both of them substantially improve the performance for the minority class, and the performance for the majority and minority classes is much more balanced.
1486723856;Combining Language and Vision with a Multimodal Skip-gram Model;2015.0;[];We extend the SKIP-GRAM model of Mikolov et al. (2013a) by taking visual information into account. Like SKIP-GRAM, our multimodal models (MMSKIP-GRAM) build vector-based word representations by learning to predict linguistic contexts in text corpora. However, for a restricted set of words, the models are also exposed to visual representations of the objects they denote (extracted from natural images), and must predict linguistic and visual features jointly. The MMSKIP-GRAM models achieve good performance on a variety of semantic benchmarks. Moreover, since they propagate visual information to all words, we use them to improve image labeling and retrieval in the zero-shot setup, where the test concepts are never seen during model training. Finally, the MMSKIP-GRAM models discover intriguing visual properties of  words, paving the way to realistic implementations of embodied theories of meaning.
1487667621;Towards Semantic Social Networks;2007.0;[];Computer manipulated social networks are usually built from the explicit assertion by users that they have some relation with other users or by the implicit evidence of such relations (e.g., co-authoring). However, since the goal of social network analysis is to help users to take advantage of these networks, it would be convenient to take more information into account. We introduce a three-layered model which involves the network between people (social network), the network between the ontologies they use (ontology network) and a network between concepts occurring in these ontologies. We explain how relationships in one network can be extracted from relationships in another one based on analysis techniques relying on this network specificity. For instance, similarity in the ontology network can be extracted from a similarity measure on the concept network. We illustrate the use of these tools for the emergence of consensus ontologies in the context of semantic peer-to-peer systems.
1489253734;Personalizing web search with folksonomy-based user and document profiles;2010.0;[];Web search personalization aims to adapt search results to a user based on his tastes, interests and needs. The way in which such personal preferences are captured, modeled and exploited distinguishes the different personalization strategies. In this paper, we propose to represent a user profile in terms of social tags, manually provided by users in folksonomy systems to describe, categorize and organize items of interest, and investigate a number of novel techniques that exploit the users’ social tags to re-rank results obtained with a Web search engine. An evaluation conducted with a dataset from Delicious social bookmarking system shows that our personalization techniques clearly outperform state of the art approaches.
1494277681;Scenarios for Multistage Stochastic Programs;2000.0;[];A major issue in any application of multistage stochastic programming is the representation of the underlying random data process. We discuss the case when enough data paths can be generated according to an accepted parametric or nonparametric stochastic model. No assumptions on convexity with respect to the random parameters are required. We emphasize the notion of representative scenarios (or a representative scenario tree) relative to the problem being modeled.
1500464975;State-of-the-art in MAC protocols for underwater acoustics sensor networks;2007.0;[];Many potential applications such as ocean sampling network, environment monitoring, undersea exploration, disaster prevention, assisted navigation, and mine reconnaissance can be provided by deploying the Underwater Acoustic Sensor Networks. Because of the peculiarities of acoustic communication in underwater, the MAC protocol which play a role of managing and controlling the channels, must overcome the required of energy consumption, propagation delay and time synchronize as well as other factors. In this paper, we summarize and classify some current proposed MAC protocols as well as make a comparison table in order to bring out the current development of a very interesting research area. Beside, we briefly introduce our suggestion of MAC mechanisms for Underwater Acoustic Sensor Networks (UWASNs) named Gain-time and Guard-time TDMA mechanism and UWANAV mechanism.
1501005121;Power to the People: The Role of Humans in Interactive Machine Learning;2014.0;[];"Intelligent systems that learn interactively from their end-users are quickly becoming widespread. Until recently, this progress has been fueled mostly by advances in machine learning; however, more and more researchers are realizing the importance of studying users of these systems. In this article we promote this approach and demonstrate how it can result in better user experiences and more effective learning systems. We present a number of case studies that characterize the impact of interactivity, demonstrate ways in which some existing systems fail to account for the user, and explore new ways for learning systems to interact with their users. We argue that the design process for interactive machine learning systems should involve users at all stages: explorations that reveal human interaction patterns and inspire novel interaction methods, as well as refinement stages to tune details of the interface and choose among alternatives. After giving a glimpse of the progress that has been made so far, we discuss the challenges that we face in moving the field forward."
1504330670;A venues-aware message routing scheme for delay-tolerant networks;2015.0;[];With their proliferation and increasing capabilities, mobile devices with local wireless interfaces can be organized into delay-tolerant networks DTNs that exploit communication opportunities arising out of the movement of their users. As the mobile devices are usually carried by people, these DTNs can also be viewed as social networks. Unfortunately, most existing routing algorithms for DTNs rely on relatively simple mobility models that rarely consider these social network characteristics, and therefore, the mobility models in these algorithms cannot accurately describe usersu0027 real mobility traces. In this paper, we propose two predict and spread PreS message routing algorithms for DTNs. We employ an adapted Markov chain to model a nodeu0027s mobility pattern and capture its social characteristics. A comparison with state-of-the-art algorithms demonstrates that PreS can yield better performance in terms of delivery ratio and delivery latency, and it can provide a comparable performance with the epidemic routing algorithm with lower resource consumption. Copyright © 2013 John Wiley u0026 Sons, Ltd.
1505789767;Computer-mediated collaborative learning: an empirical evaluation;1994.0;[];"National commissions and scholarly reports on the status of contemporary higher education have frequently been critical of the college experience; the emphasis on transmitting fixed bodies of information and a failure to develop problem solving and critical thinking skills have been cited as serious weaknesses in higher education systems. Colleges and universities have additional reasons to redevelop central pedagogies for students. Individuals need to learn at higher rates of effectiveness and efficiency than ever before because of rapidly growing bodies of relevant information and the escalation of knowledge and skill requirements for most jobs.Recent developments incomputer hardware, software, and communication technologies create exciting new opportunities for the educational use of these technologies. The objective of this study is to go beyond the traditional classroom instructional modesl (e.g., lectures and class discussions) to develop and evaluate computer-supported pedagogical approaches. More specifically, this study investigates whether the use of a group decision support system (GDSS) in a collaborative learning process enhances student learning and evaluation of classroom experiences.The findings of a study involving 127 MBA students indicate that GDSS-supported collaborative learning leads to higher levels of perceived skill development, self-reported learning, and evaluation of classroom experience in comparison with non-GDSS supported collaborative learning. Furthermore, the final test grades of the group of students who were exposed to GDSS-supported collaborative learning were significantly higher than those of the other groups of students who participated in the experiment."
1506446282;Quantitative questions on attack: defense trees;2012.0;[];Attack---defense trees are a novel methodology for graphical security modeling and assessment. The methodology includes intuitive and formal components that can be used for quantitative analysis of attack---defense scenarios. In practice, we use intuitive questions to ask about aspects of scenarios we are interested in. Formally, a computational procedure, using a bottom-up algorithm, is applied to derive the corresponding numerical values. This paper bridges the gap between the intuitive and the formal way of quantitatively assessing attack---defense scenarios. We discuss how to properly specify a question, so that it can be answered unambiguously. Given a well-specified question, we then show how to derive an appropriate attribute domain which constitutes the corresponding formal model.
1509561367;Exponential entropy on intuitionistic fuzzy sets;2013.0;[];In the present paper, based on the concept of fuzzy entropy, an exponential intuitionistic fuzzy entropy measure is proposed in the setting of Atanassovu0027s intuitionistic fuzzy set theory. This measure is a generalized version of exponential fuzzy entropy proposed by Pal and Pal. A connection between exponential fuzzy entropy and exponential intuitionistic fuzzy entropy is also established. Some interesting properties of this measure are analyzed. Finally, a numerical example is given to show that the proposed entropy measure for Atanassovu0027s intuitionistic fuzzy set is consistent by comparing it with other existing entropies.
1511275332;Organic Control of Traffic Lights;2008.0;[];"In recent years, Autonomic and Organic Computing have become areas of active research in the computer science community. Both initiatives aim at handling the growing complexity in technical systems by creating systems with adaptation and self-optimisation capabilities. One application scenario for such ""life-like"" systems is the control of road traffic signals in urban areas. This paper presents an organic approach to traffic light control and analyses its performance by an experimental validation of the proposed architecture which demonstrates its benefits compared to classical traffic control."
1513719591;Activity-aware map: identifying human daily activity pattern using mobile phone data;2010.0;[];Being able to understand dynamics of human mobility is essential for urban planning and transportation management. Besides geographic space, in this paper, we characterize mobility in a profile-based space (activity-aware map) that describes most probable activity associated with a specific area of space. This, in turn, allows us to capture the individual daily activity pattern and analyze the correlations among different peopleu0027s work areau0027s profile. Based on a large mobile phone data of nearly one million records of the users in the central Metro-Boston area, we find a strong correlation in daily activity patterns within the group of people who share a common work areau0027s profile. In addition, within the group itself, the similarity in activity patterns decreases as their work places become apart.
1515452738;Models and tools for quantitative assessment of operational security;1996.0;[];This paper proposes a novel approach to help computing system administrators in monitoring the security of their systems. The approach is based on modeling the system as a privilege graph exhibiting operational security vulnerabilities and on transforming this privilege graph into a Markov chain corresponding to all possible successful attack scenarios. A set of tools has been developed to support this approach and to provide automatic security evaluation of Unix systems in operation.
1517040319;Rank centrality: Ranking from pairwise comparisons;2017.0;[];"The question of aggregating pairwise comparisons to obtain a global ranking over a collection of objects has been of interest for a very long time: be it ranking of online gamers (e.g., MSR’s TrueSkill system) and chess players, aggregating social opinions, or deciding which product to sell based on transactions. In most settings, in addition to obtaining a ranking, finding ‘scores’ for each object (e.g., player’s rating) is of interest for understanding the intensity of the preferences.In this paper, we propose Rank Centrality , an iterative rank aggregation algorithm for discovering scores for objects (or items) from pairwise comparisons. The algorithm has a natural random walk interpretation over the graph of objects with an edge present between a pair of objects if they are compared; the score, which we call Rank Centrality, of an object turns out to be its stationary probability under this random walk.To study the efficacy of the algorithm, we consider the popular Bradley-Terry-Luce (BTL) model (equivalent to the Multinomial Logit (MNL) for pairwise comparisons) in which each object has an associated score that determines the probabilistic outcomes of pairwise comparisons between objects. In terms of the pairwise marginal probabilities, which is the main subject of this paper, the MNL model and the BTL model are identical. We bound the finite sample error rates between the scores assumed by the BTL model and those estimated by our algorithm. In particular, the number of samples required to learn the score well with high probability depends on the structure of the comparison graph. When the Laplacian of the comparison graph has a strictly positive spectral gap, e.g., each item is compared to a subset of randomly chosen items, this leads to dependence on the number of samples that is nearly order optimal.Experimental evaluations on synthetic data sets generated according to the BTL model show that our algorithm performs as well as the maximum likelihood estimator for that model and outperforms other popular ranking algorithms."
1522734439;Learning Spatiotemporal Features with 3D Convolutional Networks;2015.0;[];We propose a simple, yet effective approach for spatiotemporal feature learning using deep 3-dimensional convolutional networks (3D ConvNets) trained on a large scale supervised video dataset. Our findings are three-fold: 1) 3D ConvNets are more suitable for spatiotemporal feature learning compared to 2D ConvNets, 2) A homogeneous architecture with small 3x3x3 convolution kernels in all layers is among the best performing architectures for 3D ConvNets, and 3) Our learned features, namely C3D (Convolutional 3D), with a simple linear classifier outperform state-of-the-art methods on 4 different benchmarks and are comparable with current best methods on the other 2 benchmarks. In addition, the features are compact: achieving 52.8% accuracy on UCF101 dataset with only 10 dimensions and also very efficient to compute due to the fast inference of ConvNets. Finally, they are conceptually very simple and easy to train and use.
1527555032;Educational use of smart phone technology: A survey of mobile phone application use by undergraduate university students;2013.0;[];Purpose – This paper aims to present the results of a survey of undergraduate student use of smart phone applications. Design/methodology/approach – Undergraduate students currently enrolled in an information literacy course answered an online survey regarding their use of applications (apps) on their smart phones. Findings – However, still a small percentage of most frequently used apps (10.4 percent), search engines, online encyclopedias, and libraries are used by undergraduate students. The apps used most often are familiar to them and allow mobile access to popular web sites available on personal computers. Furthermore, a significant number (76 percent) of undergraduate students also report that they use apps to find academic information. The type of app most frequently used to find academic information is search engines. Originality/value – This research provides evidence on the actual use of mobile devices by students for library administrators and educators interested in developing integrated mobil...
1528140876;Watson, more than a Semantic Web search engine;2011.0;[];In this tool report, we present an overview of the Watson system, a Semantic Web search engine providing various functionalities not only to find and locate ontologies and semantic data online, but also to explore the content of these semantic documents. Beyond the simple facade of a search engine for the Semantic Web, we show that the availability of such a component brings new possibilities in terms of developing semantic applications that exploit the content of the Semantic Web. Indeed, Watson provides a set of APIs containing high level functions for finding, exploring and querying semantic data and ontologies that have been published online. Thanks to these APIs, new applications have emerged that connect activities such as ontology construction, matching, sense disambiguation and question answering to the Semantic Web, developed by our group and others. In addition, we also describe Watson as a unprecedented research platform for the study the Semantic Web, and of formalised knowledge in general.
1530761836;Towards Reusability in Autonomic Computing;2015.0;[];Reusability of software artifacts reduces development time, effort, and error-proneness. Nevertheless, in the development of autonomic systems, developers often start from scratch when building a new system instead of reusing existing components. Many frameworks offer reusability on a higher level of abstraction, but neglect reusability on the lower component implementation level. In this short paper, we present a reusable adaptation logic by separating the generic structure and mechanisms of Autonomic Computing systems from its custom functionality. That is, we provide a reusable communication architecture with  component templates that enables a faster development and easier runtime adaptation. We evaluate our approach in a case study with two implementations.
1533876796;Attack Trees with Sequential Conjunction;2015.0;[];We provide the first formal foundation of SAND attack trees which are a popular extension of the well-known attack trees. The SAND attack tree formalism increases the expressivity of attack trees by introducing the sequential conjunctive operator \({\mathtt {SAND}}\). This operator enables the modeling of ordered events.
1537617436;BioPortal as a dataset of linked biomedical ontologies and terminologies in RDF;2013.0;[];BioPortal is a repository of biomedical ontologies --the largest such repository, with more than 300 ontologies to date. This set includes ontologies that were developed in OWL, OBO and other formats, as well as a large number of medical terminologies that the US National Library of Medicine distributes in its own proprietary format. We have published the RDF version of all these ontologies at http://sparql.bioontology.org. This dataset contains 190M triples, representing both metadata and content for the 300 ontologies. We use the metadata that the ontology authors provide and simple RDFS reasoning in order to provide dataset users with uniform access to key properties of the ontologies, such as lexical properties for the class names and provenance data. The dataset also contains 9.8M cross-ontology mappings of different types, generated both manually and automatically, which come with their own metadata.
1537837212;Early detection of alzheimer’s disease by blind source separation, time frequency representation, and bump modeling of EEG signals;2005.0;[];"The early detection Alzheimer’s disease (AD) is an important challenge. In this paper, we propose a novel method for early detection of AD using electroencephalographic (EEG) recordings: first a blind source separation algorithm is applied to extract the most significant spatio-temporal components; these components are subsequently wavelet transformed; the resulting time-frequency representation is approximated by sparse “bump modeling”; finally, reliable and discriminant features are selected by orthogonal forward regression and the random probe method. These features are fed to a simple neural network classifier. The method was applied to EEG recorded in patients with Mild Cognitive Impairment (MCI) who later developed AD, and in age-matched controls. This method leads to a substantially improved performance (93% correctly classified, with improved sensitivity and specificity) over classification results previously published on the same set of data. The method is expected to be applicable to a wide variety of EEG classification problems."
1540455107;Approximation of functions of large matrices with Kronecker structure;2017.0;[];We consider the numerical approximation of $$f(\mathcal{A})b$$f(A)b where $$b\in {\mathbb {R}}^{N}$$bźRN and $$\mathcal A$$A is the sum of Kronecker products, that is $$\mathcal{A}=M_2 \otimes I + I \otimes M_1\in {\mathbb {R}}^{N\times N}$$A=M2źI+IźM1źRN×N. Here f is a regular function such that $$f(\mathcal{A})$$f(A) is well defined. We derive a computational strategy that significantly lowers the memory requirements and computational efforts of the standard approximations, with special emphasis on the exponential and on completely monotonic functions, for which the new procedure becomes particularly advantageous. Our findings are illustrated by numerical experiments with typical functions used in applications.
1540557062;Dynamic brace for correction of abnormal postures of the human spine;2015.0;[];This paper describes the design and control architectures for a novel active thoracolumbosacral orthosis targeted at correction of abnormal postures and treatment of the human spine, often seen in adolescent idiopathic or neuromuscular scoliosis. Our novel device is motivated by the current limitations of the rigid braces used for this purpose which do not adapt to changes in the skeletal system in response to treatment. In addition, the dynamic brace can open possibilities for new treatment methods which currently do not exist. Previous brace designs were not capable of providing dynamic controlled forces. Our design utilizes two Stewart-Gough platforms in series, each controlled independently, either in position or force modes. The design can provide controlled forces/torques on different regions of spine to modify the posture. Additionally, it can control the motion of different regions of the spine through independent position control of each platform using six parallel actuators. Both control methods were validated in benchtop tests. A range of motion study was also performed with a healthy subject wearing the device while the system was controlled in transparent mode.
1541230581;Cyclic Evaluation of Transitivity of Reciprocal Relations;2006.0;[];A general framework for studying the transitivity of reciprocal relations is presented. The key feature is the cyclic evaluation of transitivity: triangles (i.e. any three points) are visited in a cyclic manner. An upper bound function acting upon the ordered weights encountered provides an upper bound for the ‘sum minus 1’ of these weights. Commutative quasi-copulas allow to translate a general definition of fuzzy transitivity (when applied to reciprocal relations) elegantly into the framework of cycle-transitivity. Similarly, a general notion of stochastic transitivity corresponds to a particular class of upper bound functions. Ample attention is given to self-dual upper bound functions.
1542659806;A Probabilistic Framework for Security Scenarios with Dependent Actions;2014.0;[];This work addresses the growing need of performing meaningful probabilistic analysis of security. We propose a framework that integrates the graphical security modeling technique of attack–defense trees with probabilistic information expressed in terms of Bayesian networks. This allows us to perform probabilistic evaluation of attack–defense scenarios involving dependent actions. To improve the efficiency of our computations, we make use of inference algorithms from Bayesian networks and encoding techniques from constraint reasoning. We discuss the algebraic theory underlying our framework and point out several generalizations which are possible thanks to the use of semiring theory.
1549119093;A Continuous Genetic Algorithm Designed for the Global Optimization of Multimodal Functions;2000.0;[];"Genetic algorithms are stochastic search approaches based on randomized operators, such as selection, crossover and mutation, inspired by the natural reproduction and evolution of the living creatures. However, few published works deal with their application to the global optimization of functions depending on continuous  :[44],""new algorithm called Continuous Genetic Algorithm (CGA) is proposed for the global optimization of multiminima functions. In order to cover a wide domain of possible solutions, our algorithm first takes care over the choice of the initial population. Then it locates the most promising area of the solution space, and continues the search through an “intensification” inside this area. The selection, the crossover and the mutation are performed by using the decimal code. The efficiency of CGA is tested in detail through a set of benchmark multimodal functions, of which global and local minima are known. CGA is compared to Tabu Search and Simulated Annealing, as alternative algorithms."
1550064664;System Reliability Theory: Models, Statistical Methods, and Applications;2003.0;[];Preface to the Second Edition. Preface to the First Edition. Acknowledgments. 1. Introduction. 2. Failure Models. 3. Qualitative Systems Analysis. 4. Systems of Independent Components. 5. Component Importance. 6. Dependent Failures. 7. Counting Processes. 8. Markov Models. 9. Reliability of Maintained Systems. 10. Reliability of Safety Systems. 11. Life Data Analysis. 12. Accelerated Life Testing. 13. Bayesian Reliability Analysis. 14. Reliability Data Sources. Appendix A. The Gamma and Beta Functions. Appendix B. Laplace Transforms. Appendix C. Kronecker Products. Appendix D. Distribution Theorems. Appendix E. Maximum Likelihood Estimation. Appendix F. Statistical Tables. Acronyms. Glossary. References. Author Index. Subject Index.
1554807050;Graph matching applications in pattern recognition and image processing;2003.0;[];In this paper we will try to characterize the role that graphs are conquering within the pattern recognition field. To this aim, a taxonomy built considering the most common applications of graph based techniques in the pattern recognition and image processing field is presented and discussed.
1555914462;Self-Adaptation to Mobile Resources in Service Oriented Architecture;2015.0;[];Mobile or pervasive systems continuously change their environments and resources (e.g. Battery or bandwidth). Mobile applications require different services when they enter or exit environments and as their resources change. In this paper, we propose a service oriented architectural approach that supports self-adaptation to changes in resources and location topology when mobility occurs, by reconfiguring the software architecture at runtime. The location topology and mobility primitives are inspired from ambient calculus. Our approach considers ambients to be autonomic elements that can manage elements located in them to their environment and provide them with new services suited to the available resources, when mobility occurs. Ambients implement a hierarchical and decentralized MAPE-K loop to adapt the distributed and mobile service oriented architecture to the resource requirements. We have designed an algorithm based on swarm optimization technique in order to allow ambients to optimally plan the reconfiguration process according to available services and resources. Throughout the paper, we use a scenario to illustrate our approach and perform initial evaluations on the swarm algorithm.
1556531089;Hamming Embedding and Weak Geometric Consistency for Large Scale Image Search;2008.0;[];This paper improves recent methods for large scale image search. State-of-the-art methods build on the bag-of-features image representation. We, first, analyze bag-of-features in the framework of approximate nearest neighbor search. This shows the sub-optimality of such a representation for matching descriptors and leads us to derive a more precise representation based on 1) Hamming embedding (HE) and 2) weak geometric consistency constraints (WGC). HE provides binary signatures that refine the matching based on visual words. WGC filters matching descriptors that are not consistent in terms of angle and scale. HE and WGC are integrated within the inverted file and are efficiently exploited for all images, even in the case of very large datasets. Experiments performed on a dataset of one million of images show a significant improvement due to the binary signature and the weak geometric consistency constraints, as well as their efficiency. Estimation of the full geometric transformation, i.e., a re-ranking step on a short list of images, is complementary to our weak geometric consistency constraints and allows to further improve the accuracy.
1557634793;DYNAMICO: A Reference Model for Governing Control Objectives and Context Relevance in Self-Adaptive Software Systems;2013.0;[];"Despite the valuable contributions on self-adaptation, most implemented approaches assume adaptation goals and monitoring infrastructures as non-mutable, thus constraining their applicability to systems whose context awareness is restricted to static monitors. Therefore, separation of concerns, dynamic monitoring, and runtime requirements variability are critical for satisfying system goals under highly changing environments. In this chapter we present DYNAMICO, a reference model for engineering adaptive software that helps guaranteeing the coherence of (i) adaptation mechanisms with respect to changes in adaptation goals; and (ii) monitoring mechanisms with respect to changes in both adaptation goals and adaptation mechanisms. DYNAMICO improves the engineering of self-adaptive systems by addressing (i) the management of adaptation properties and goals as control objectives; (ii) the separation of concerns among feedback loops required to address control objectives over time; and (iii) the management of dynamic context as an independent control function to preserve context-awareness in the adaptation mechanism."
1561586523;Computational aspects of attack---defense trees;2011.0;[];Attack---defense trees extend attack trees with defense nodes. This richer formalism allows for a more precise modeling of a systemu0027s vulnerabilities, by representing interactions between possible attacks and corresponding defensive measures. In this paper we compare the computational complexity of both formalisms. We identify semantics for which extending attack trees with defense nodes does not increase the computational complexity. This implies that, for these semantics, every query that can be solved efficiently on attack trees can also be solved efficiently on attack---defense trees. Furthermore, every algorithm for attack trees can directly be used to process attack---defense trees.
1561976191;Location Aware Web: Concept, protocol and system;2015.0;[];When users look for information on the Internet through their smart devices, their physical location is usually highly correlated with the contents they are trying to find. Actually, localization (as well as language) is an integral part of the input data used by web search engines to customize the information provided to the users. Based on this concept, we have designed a Location Based Service (LBS), called LAW (Location Aware Web), which exploits the relationship between Web based applications and the context, including mainly the position and other sensor or personal information. In this paper, we outline the LAW concept, the design of the protocol to support the idea and the description of a system implementing both concept and protocol. We have designed a protocol to support the LAW service, intended for the application OSI layer based on a clientserver architecture. We also present the design and prototypes of subsystem implementing both idea and protocol, including all the necessary details: hardware selection, network technology and developed software. In the industrial environment, the Location Aware Web service could be used to provide feasible information for the user, for example, support to procedure operations such as maintenance or stock control.
1565863475;Learning Subjective Adjectives from Corpora;2000.0;[];Subjectivity tagging is distinguishing sentences used to present opinions and evaluations from sentences used to objectively present factual information. There are numerous applications for which subjectivity tagging is relevant, including information extraction and information retrieval. This paper identifies strong clues of subjectivity using the results of a method for clustering words according to distributional similarity (Lin 1998), seeded by a small amount of detailed manual annotation. These features are then further refined with the addition of lexical semantic features of adjectives, specifically polarity and gradability (Hatzivassiloglou u0026 McKeown 1997), which can be automatically learned from corpora. In 10-fold cross validation experiments, features based on both similarity clusters and the lexical semantic features are shown to have higher precision than features based on each alone.
1582479658;Semantic sitemaps: efficient and flexible access to datasets on the semantic web;2008.0;[];Increasing amounts of RDF data are available on the Web for consumption by Semantic Web browsers and indexing by Semantic Web search engines. Current Semantic Web publishing practices, however, do not directly support efficient discovery and high-performance retrieval by clients and search engines. We propose an extension to the Sitemaps protocol which provides a simple and effective solution: Data publishers create Semantic Sitemaps to announce and describe their data so that clients can choose the most appropriate access method. We show how this protocol enables an extended notion of authoritative information across different access methods.
1583171784;Solving Engineering Design Problems by Social Cognitive Optimization;2004.0;[];Swarm systems are products of natural evolution. The complex collective behavior can emerge from a society of N autonomous cognitive entities [2], called as agents [5]. Each agent acquires knowledge in socially biased individual learning [4]. For human, the extrasomatic arbitrary symbols that manipulated by language allows for cognition on a grand scale [3], since agent can acquire social information that is no longer limited to direct observation to other agents. The individual learning then only plays secondary role due to the ubiquity and efficiency of social learning [1].
1585236401;Runtime Evolution of the Adaptation Logic in Self-Adaptive Systems;2015.0;[];Self-adaptive systems, which are highly related to Autonomic Computing, are a response to the increasing complexity and size of information systems. They are able to adapt their behavior to changes in the environment or system resources. A self-adaptive system consists of managed resources that realize functionality and an adaptation logic that controls the adaptations. So far, many research has been performed on adapting the managed resources. However, only few works cover adapting the adaptation logic, which might be necessary in several cases, e.g., When the architecture of the managed resources changes. This work adresses why adaptation of the adaptation logic might be beneficial, how it can be achieved, and what challenges arise.
1585611690;Social Emotional Optimization Algorithm for Nonlinear Constrained Optimization Problems;2010.0;[];Nonlinear programming problem is one important branch in operational research, and has been successfully applied to various real-life problems. In this paper, a new approach called Social emotional optimization algorithm (SEOA) is used to solve this problem which is a new swarm intelligent technique by simulating the human behavior guided by emotion. Simulation results show that the social emotional optimization algorithm proposed in this paper is effective and efficiency for the nonlinear constrained programming problems.
1586939924;Describing Videos by Exploiting Temporal Structure;2015.0;[];Recent progress in using recurrent neural networks (RNNs) for image description has motivated the exploration of their application for video description. However, while images are static, working with videos requires modeling their dynamic temporal structure and then properly integrating that information into a natural language description model. In this context, we propose an approach that successfully takes into account both the local and global temporal structure of videos to produce descriptions. First, our approach incorporates a spatial temporal 3-D convolutional neural network (3-D CNN) representation of the short temporal dynamics. The 3-D CNN representation is trained on video action recognition tasks, so as to produce a representation that is tuned to human motion and behavior. Second we propose a temporal attention mechanism that allows to go beyond local temporal modeling and learns to automatically select the most relevant temporal segments given the text-generating RNN. Our approach exceeds the current state-of-art for both BLEU and METEOR metrics on the Youtube2Text dataset. We also present results on a new, larger and more challenging dataset of paired video and natural language descriptions.
1587260920;Weaving a social data web with semantic Pingback;2010.0;[];In this paper we tackle some pressing obstacles of the emerging Linked Data Web, namely the quality, timeliness and coherence of data, which are prerequisites in order to provide direct end user benefits. We present an approach for complementing the Linked Data Web with a social dimension by extending the well-known Pingback mechanism, which is a technological cornerstone of the blogosphere, towards a Semantic Pingback. It is based on the advertising of an RPC service for propagating typed RDF links between Data Web resources. Semantic Pingback is downwards compatible with conventional Pingback implementations, thus allowing to connect and interlink resources on the Social Web with resources on the Data Web. We demonstrate its usefulness by showcasing use cases of the Semantic Pingback implementations in the semantic wiki OntoWiki and the Linked Data interface for database-backed Web applications Triplify.
1591023622;Dual-tree fast exact max-kernel search;2014.0;[];"The problem of max-kernel search arises everywhere: given a query point \documentclass{article}\usepackage{amsmath}\usepackage{amssymb}\usepackage{amsbsy}\usepackage{amsfonts}\pagestyle{empty}\begin{document}$p_q$ \end{document}, a set of reference objects \documentclass{article}\usepackage{amsmath}\usepackage{amssymb}\usepackage{amsbsy}\usepackage{amsfonts}\pagestyle{empty}\begin{document}$S_r$ \end{document} and some kernel \documentclass{article}\usepackage{amsmath}\usepackage{amssymb}\usepackage{amsbsy}\usepackage{amsfonts}\pagestyle{empty}\begin{document}$\mathcal{K}$ \end{document}, find \documentclass{article}\usepackage{amsmath}\usepackage{amssymb}\usepackage{amsbsy}\usepackage{amsfonts}\pagestyle{empty}\begin{document}$arg\,max_{p_r \in S_r} \mathcal{K}p_q, p_r$ \end{document}. Max-kernel search is ubiquitous and appears in countless domains of science, thanks to the wide applicability of kernels. A few domains include image matching, information retrieval, bio-informatics, similarity search, and collaborative filtering to name just a few. However, there is no generalized technique for efficiently solving max-kernel search. This paper presents a single-tree algorithm called single-tree FastMKS which returns the max-kernel solution for a single query point in provably \documentclass{article}\usepackage{amsmath}\usepackage{amssymb}\usepackage{amsbsy}\usepackage{amsfonts}\pagestyle{empty}\begin{document}$O\log N$ \end{document} time where \documentclass{article}\usepackage{amsmath}\usepackage{amssymb}\usepackage{amsbsy}\usepackage{amsfonts}\pagestyle{empty}\begin{document}$N$ \end{document} is the number of reference objects, and also a dual-tree algorithm dual-tree FastMKS which is useful for max-kernel search with many query points. If the set of query points is of size \documentclass{article}\usepackage{amsmath}\usepackage{amssymb}\usepackage{amsbsy}\usepackage{amsfonts}\pagestyle{empty}\begin{document}$ON$ \end{document}, this algorithm returns a solution in provably \documentclass{article}\usepackage{amsmath}\usepackage{amssymb}\usepackage{amsbsy}\usepackage{amsfonts}\pagestyle{empty}\begin{document}$ON$ \end{document} time, which is significantly better than the \documentclass{article}\usepackage{amsmath}\usepackage{amssymb}\usepackage{amsbsy}\usepackage{amsfonts}\pagestyle{empty}\begin{document}$ON^2$ \end{document} linear scan solution; these bounds are dependent on the expansion constant of the data. These algorithms work for  objects, as they do not require explicit representation of the points in kernel space. Empirical results for a variety of datasets show up to five orders of magnitude speedup in some cases. In addition, we present approximate extensions of the FastMKS algorithms that can achieve further speedups."
1595159159;Differential Evolution – A Simple and Efficient Heuristic for Global Optimization over Continuous Spaces;1997.0;[];A new heuristic approach for minimizing possibly nonlinear and non-differentiable continuous space functions is presented. By means of an extensive testbed it is demonstrated that the new method converges faster and with more certainty than many other acclaimed global optimization methods. The new method requires few control variables, is robust, easy to use, and lends itself very well to parallel computation.
1596195064;An Electromagnetism-like Mechanism for Global Optimization;2003.0;[];This paper proposes a new heuristic for global optimization. The method utilizes an attraction-repulsion mechanism to move the sample points towards the optimality. The proposed scheme can be used either as a stand-alone approach or as an accompanying procedure for other methods. Some test results on nonlinear test functions in the category of ``minor to moderate difficultyu0027u0027 are included. The ease of implementation and flexibility of the heuristic show the potential of this new approach.
1596717185;Least Squares Support Vector Machine Classifiers;1999.0;[];In this letter we discuss a least squares version for support vector machine (SVM) classifiers. Due to equality type constraints in the formulation, the solution follows from solving a set of linear equations, instead of quadratic programming for classical SVM‘s. The approach is illustrated on a two-spiral benchmark classification problem.
1597172804;Comparison study of SPEA2+, SPEA2, and NSGA-II in diesel engine emissions and fuel economy problem;2005.0;[];Recently, the technology that can control NOx and Soot values of diesel engines by changing the electronically controllable parameters has been developed. However, there is a trade-off relationship between fuel economy and NOx values. Therefore, the diesel engines that can change their characteristics with along to the driving environment should be emerged in the future. For designing these kinds of engines, the Pareto solutions that can express the trade-off between fuel economy and NOx values are needed. In that case, the derived non dominated solutions should have the diversity not only in the objective space but also in the design variable space. SPEA2+ is one of multi objective genetic algorithms and is developed based on SPEA2. The derived non dominated solutions by SPEA2+ have the diversity in both objective space and design variable space. In this study, the diesel engines that have high fuel economy and small amounts of NOx and Soot are designed by SEPA2+. The results are compared with those of SPEA2 and NSGA-II. From the discussions, it is found that the solutions of SPEA2+ have the diversity not only in the objective space but also in the design variable space. These characteristics are very suitable for designing diesel engines whose parameters are changing against the driving environment
1598399595;Recommendations for dispatching emergency vehicles under multitiered response via simulation;2014.0;[];"Emergency medical service (EMS) systems provide medical care and transportation. While many real-world systems use multiple vehicle types to attend different call priorities, few guidelines exist about which vehicles to allocate in multitiered responses where more than one vehicle is sent per call. This paper makes recommendations for multiple-unit dispatch to multiple call priorities based on simulation optimization and heuristics. The objective is to maximize the overall expected survival probability of patients classified as “life-threatening”. We assume two types of medical units and three call priorities; and that information may be updated when the medical unit arrives on-scene. First, we study the optimal dispatching policies through several examples. Numerical results show that dispatching while considering call priorities, rather than dispatching the closest units, improves EMS system effectiveness. A heuristic algorithm is developed for large-scale problems. A comparison between the heuristic and closest policy is demonstrated using real-world data."
1532325895;Introduction to Information Retrieval;2008.0;[];"Class-tested and coherent, this groundbreaking new textbook teaches web-era information retrieval, including web search and the related areas of text classification and text clustering from basic concepts. Written from a computer science perspective by three leading experts in the field, it gives an up-to-date treatment of all aspects of the design and implementation of systems for gathering, indexing, and searching documents; methods for evaluating systems; and an introduction to the use of machine learning methods on text collections. All the important ideas are explained using examples and figures, making it perfect for introductory courses in information retrieval for advanced undergraduates and graduate students in computer science. Based on feedback from extensive classroom experience, the book has been carefully structured in order to make teaching more natural and effective. Although originally designed as the primary text for a graduate or advanced undergraduate course in information retrieval, the book will also create a buzz for researchers and professionals alike."
1606183485;Institutional drivers and barriers to faculty adoption of blended learning in higher education;2016.0;[];Relatively little research on blended learning ( BL) addresses institutional adoption, although such research would benefit institutions of higher education in strategically adopting and implementing BL. In a prior study, the authors proposed a framework for institutional BL adoption, identifying three stages: (1) awareness/exploration, (2) adoption/early implementation, and (3) mature implementation/growth. The framework also identified key strategy, structure and support issues universities may address at each stage. The current study applies that framework as well as Rogersu0027 diffusion of innovations theory to determine the degree to which institutional strategy, structure and support decisions facilitate or impede BL adoption among higher education faculty. The authors also explore whether faculty membersu0027 innovation adoption category (innovator, early adopter, early majority, late majority or laggard) affects which decisions facilitate or impede BL adoption. To achieve these objectives, the authors surveyed 214 faculty members at a university in the adoption/early implementation stage, Brigham Young University-Idaho. We found that the availability of sufficient infrastructure, technological support, pedagogical support, evaluation data and an institutionu0027s purpose for adopting BL would most significantly influence faculty adoption. We also identified a wide range of factors that would influence each category of innovation adopters. [ABSTRACT FROM AUTHOR]
1617290994;Smartphone-based data collection from wireless sensor networks in an urban environment;2015.0;[];Using smartphones as mobile basestations and leveraging human mobility is a promising approach for urban data collection from Wireless Sensor Networks (WSNs). In this paper, we evaluate the feasibility of this approach applying analyses on a city-wide mobility dataset. Our spatial analysis shows that popular locations cluster close to each other and sensor nodes located in rarely visited locations can transmit their data in a few hops to smartphones visiting these popular locations. Our energy-efficiency analysis indicates the feasibility of employing energy-conserving approaches on both smartphones and WSN nodes based on mobility behavior of smartphone users. We evaluated and compared on-demand and continuous data collection protocols on several WSN islands with different size and connectivity regarding to data collection efficiency. Our simulation results show that continuous data collection protocols surpass on-demand data collection protocols in terms of data delivery ratio and latency. We found that data collection protocols run more efficiently in many-connected small islands compared to fewer connected large islands. HighlightsAccording to the GPS readings falling in each cell, hot cells are found to be close enough to cold cells for cold cell nodes to transmit data in multi-hop manner to a visiting smartphone user traveling in a hot cell.We grouped nearby frozen cells and call it an island. Since islands occur adjacent to city roads, multi-hop data collection is feasible from island nodes by smartphone users traveling on city roads.If island nodes run a WSN data collection protocol, continuous data collection is a better choice than on-demand data collection regarding to data delivery ratio and latency.For data collection by smartphones, to achieve energy-efficiency at the smartphone side, smartphone radio can only be turned on when its user is mobile.For conserving energy at islands, island nodes can run various duty cycling algorithms based on smartphone user visits.
1623354552;Explicit Ramsey graphs and orthonormal labelings;1994.0;[];We describe an explicit construction of triangle-free graphs with no independent sets of size m and with ›(m 3=2 ) vertices, improving a sequence of previous constructions by various authors. As a byproduct we show that the maximum possible value of the Lovasz µ-function of a graph on n vertices with no independent set of size 3 is £(n 1=3 ), slightly improving a result of Kashin and Konyagin who showed that this maximum is at least ›(n 1=3 = logn) and at most O(n 1=3 ). Our results imply that the maximum possible Euclidean norm of a sum of n unit vectors in R n , so that among any three of them some two are orthogonal, is £(n 2=3 ).
1626398438;GeoLife: A Collaborative Social Networking Service among User, location and trajectory;2010.0;[];"People travel in the real world and leave their location history in a form of trajectories. These trajectories do not only connect locations in the physical world but also bridge the gap between people and locations. This paper introduces a social networking service, called GeoLife, which aims to understand trajectories, locations and users, and mine the correlation between users and locations in terms of usergenerated GPS trajectories. GeoLife offers three key applications scenarios: 1) sharing life experiences based on GPS trajectories; 2) generic travel recommendations, e.g., the top interesting locations, travel sequences among locations and travel experts in a given region; and 3) personalized friend and location recommendation."
1648303880;Automated Experiments on Ad Privacy Settings;2015.0;[];To partly address people’s concerns over web tracking, Google has created the Ad Settings webpage to provide information about and some choice over the profiles Google creates on users. We present AdFisher, an automated tool that explores how user behaviors, Google’s ads, and Ad Settings interact. Our tool uses a rigorous experimental design and analysis to ensure the statistical significance of our results. It uses machine learning to automate the selection of a statistical test. We use AdFisher to find that Ad Settings is opaque about some features of a user’s profile, that it does provide some choice on ads, and that these choices can lead to seemingly discriminatory ads. In particular, we found that visiting webpages associated with substance abuse will change the ads shown but not the settings page. We also found that setting the gender to female results in getting fewer instances of an ad related to high paying jobs than setting it to male.
1659093230;Interaction-driven self-adaptation of service ensembles;2010.0;[];The emergence of large-scale online collaboration requires current information systems to be apprehended as service ensembles comprising human and software service entities. The software services in such systems cannot adapt to user needs based on autonomous principles alone. Instead system requirements need to reflect global interaction characteristics that arise from the overall collaborative effort. Interaction monitoring and analysis, therefore, must become a central aspect of system self-adaptation. We propose to dynamically evaluate and update system requirements based on interaction characteristics. Subsequent reconfiguration and replacement of services enables the ensemble to mature in parallel with the evolution of its user community. We evaluate our approach in a case study focusing on adaptive storage services.
1666603719;On star-critical and upper size Ramsey numbers;2016.0;[];"In this paper, we study the upper size Ramsey number u ( G 1 , G 2 ) , defined by Erd?s and Faudree, as well as the star-critical Ramsey number r ? ( G 1 , G 2 ) , defined by Hook and Isaak. We define Ramsey-full graphs and size Ramsey good graphs, and perform a detailed study on these graphs. We generalize earlier results by determining u ( n K k , m K l ) and r ? ( n K k , m K l ) for k , l ? 3 and large m , n ; u ( C n , C m ) for m odd, with n m ? 3 ; and r ? ( C n , C m ) for m odd, with n ? m ? 3 and ( m , n ) ? ( 3 , 3 ) ."
1668569279;Pythagorean Membership Grades, Complex Numbers, and Decision Making;2013.0;[];We describe the idea of Pythagorean membership grades and the related idea of Pythagorean fuzzy subsets. We focus on the negation and its relationship to the Pythagorean theorem. We look at the basic set operations for the case of Pythagorean fuzzy subsets. A relationship is shown between Pythagorean membership grades and complex numbers. We specifically show that Pythagorean membership grades are a subclass of complex numbers called Π-i numbers. We investigate operations that are closed under Π-i numbers. We consider the problem of multicriteria decision making with satisfactions expressed as Pythagorean membership grades, Π-i numbers. We look at the use of the geometric mean and ordered weighted geometric operator for aggregating criteria satisfaction.
1668832360;Time-Constrained Keyframe Selection Technique;2000.0;[];In accessing large collections of digitized videos, it is often difficult to find both the appropriate video file and the portion of the video that is of interest. This paper describes a novel technique for determining keyframes that are different from each other and provide a good representation of the whole video. We use keyframes to distinguish videos from each other, to summarize videos, and to provide access points into them. The technique can determine any number of keyframes by clustering the frames in a video and by selecting a representative frame from each cluster. Temporal constraints are used to filter out some clusters and to determine the representative frame for a cluster. Desirable visual features can be emphasized in the set of keyframes. An application for browsing a collection of videos makes use of the keyframes to support skimming and to provide visual summaries.
1677182931;Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification;2015.0;[];Rectified activation units (rectifiers) are essential for state-of-the-art neural networks. In this work, we study rectifier neural networks for image classification from two aspects. First, we propose a Parametric Rectified Linear Unit (PReLU) that generalizes the traditional rectified unit. PReLU improves model fitting with nearly zero extra computational cost and little overfitting risk. Second, we derive a robust initialization method that particularly considers the rectifier nonlinearities. This method enables us to train extremely deep rectified models directly from scratch and to investigate deeper or wider network architectures. Based on the learnable activation and advanced initialization, we achieve 4.94% top-5 test error on the ImageNet 2012 classification dataset. This is a 26% relative improvement over the ILSVRC 2014 winner (GoogLeNet, 6.66% [33]). To our knowledge, our result is the first to surpass the reported human-level performance (5.1%, [26]) on this dataset.
1677486363;Urban Automation Networks: Current and Emerging Solutions for Sensed Data Collection and Actuation in Smart Cities;2015.0;[];Urban Automation Networks (UANs) are being deployed worldwide in order to enable Smart City applications. Given the crucial role of UANs, as well as their diversity, it is critically important to assess their properties and trade-offs. This article introduces the requirements and challenges for UANs, characterizes the main current and emerging UAN paradigms, provides guidelines for their design and/or choice, and comparatively examines their performance in terms of a variety of parameters including coverage, power consumption, latency, standardization status and economic cost.
1680397247;Modeling Dimensions of Self-Adaptive Software Systems;2009.0;[];It is commonly agreed that a self-adaptive software system is one that can modify itself at run-time due to changes in the system, its requirements, or the environment in which it is deployed. A cursory review of the software engineering literature attests to the wide spectrum of software systems that are described as self-adaptive. The way self-adaptation is conceived depends on various aspects, such as the usersu0027 requirements, the particular properties of a system, and the characteristics of the environment. In this paper, we propose a classification of modeling dimensions for self-adaptive software systems. Each modeling dimension describes a particular facet of the system that is relevant to self-adaptation. The modeling dimensions provide the engineers with a common set of vocabulary for specifying the self-adaptive properties under consideration and select suitable solutions. We illustrate how the modeling dimensions apply to several application scenarios.
1687679254;Content-based medical image retrieval with metric learning via rank correlation;2010.0;[];A novel content-based medical image retrieval method with metric learning via rank correlation is proposed in this paper. A new rank correlation measure is proposed to learn a metric encoding the pairwise similarity between images via direct optimization. Our method has been evaluated with a large population-based dataset composed of 5000 slit-lamp images with different nuclear cataract severities. Experimental results and statistical analysis demonstrate the superiority of our method over several popular metric learning methods in content-based slit-lamp image retrieval.
1686696692;MAGNA: Maximizing Accuracy in Global Network Alignment;2014.0;[];
1708394971;A comparative study of differential evolution, particle swarm optimization, and evolutionary algorithms on numerical benchmark problems;2004.0;[];Several extensions to evolutionary algorithms (EAs) and particle swarm optimization (PSO) have been suggested during the last decades offering improved performance on selected benchmark problems. Recently, another search heuristic termed differential evolution (DE) has shown superior performance in several real-world applications. In this paper, we evaluate the performance of DE, PSO, and EAs regarding their general applicability as numerical optimization techniques. The comparison is performed on a suite of 34 widely used benchmark problems. The results from our study show that DE generally outperforms the other algorithms. However, on two noisy functions, both DE and PSO were outperformed by the EA.
1758781646;A Heuristic Quasi-Polynomial Algorithm for Discrete Logarithm in Finite Fields of Small Characteristic;2014.0;[];The difficulty of computing discrete logarithms in fields \(\mathbb{F}_{q^k}\) depends on the relative sizes of k and q. Until recently all the cases had a sub-exponential complexity of type L(1/3), similar to the factorization problem. In 2013, Joux designed a new algorithm with a complexity of L(1/4 + e) in small characteristic. In the same spirit, we propose in this article another heuristic algorithm that provides a quasi-polynomial complexity when q is of size at most comparable with k. By quasi-polynomial, we mean a runtime of n O(logn) where n is the bit-size of the input. For larger values of q that stay below the limit \(L_{q^k}(1/3)\), our algorithm loses its quasi-polynomial nature, but still surpasses the Function Field Sieve. Complexity results in this article rely on heuristics which have been checked experimentally.
1761338525;How students and instructors using a virtual learning environment perceive the fit between technology and task;2007.0;[];Virtual learning environments (VLEs) are widespread in higher education today, typically used to deliver instructional materials and facilitate communication within a course. This study aimed to investigate the task–technology fit of VLEs for their two main groups of users: instructors and students, using the VLE WebCT. Task–technology fit, user satisfaction, attitude towards use and anticipated consequences of use were found to be significantly higher for students than for instructors. Instructors were found to have higher perceptions of social norms and higher perceptions of facilitating conditions than students. However, there was no difference between the instructors and students in level of utilization of the VLE. Students perceived that the VLE had higher impacts on their learning compared with instructorsu0027 perceptions regarding their teaching. These results suggest that despite high levels of support acknowledged by instructors, they may still be unsure about the contribution of VLEs to their teaching.
1763200058;Fast Evolution Strategies;1997.0;[];Evolution strategies are a class of general optimisation algorithms which are applicable to functions that are multimodal, non-differentiable, or even discontinuous. Although recombination operators have been introduced into evolution strategies, their primary search operator is still mutation. Classical evolution strategies rely on Gaussian mutations. A new mutation operator based on the Cauchy distribution is proposed in this paper. It is shown empirically that the new evolution strategy based on Cauchy mutation outperforms the classical evolution strategy on most of the 23 benchmark problems tested in this paper. These results, along with those obtained by fast evolutionary programming
1774823771;A Formulation of Linear Logic Based on Dependency-Relations;1997.0;[];In this paper we describe a solution to the problem of proving cut-elimination for FILL, a variant of exponential-free and multiplicative Linear Logic originally introduced by Hyland and de Paiva. In the work of Hyland and de Paiva, a term assignment system is used to describe the intuitionistic character of FILL and a proof of cut-elimination is barely sketched. In the present paper, as well as correcting a small mistake in their work and extending the system to deal with exponentals, we introduce a different formal system describing the intuitionistic character of FILL and we provide a full proof of the cut-elimination theorem. The formal system is based on a dependency-relation between formulae occurrences within a given proof and seems of independent interest. The procedure for cut-elimination applies to (multiplicative and exponential) Classical Linear Logic, and we can (with care) restrict our attention to the subsystem FILL. The proof, as usual with cut-elimination proofs, is a little involved and we have not seen it published anywhere.
1788181460;Web search personalization via social bookmarking and tagging;2007.0;[];In this paper, we present a new approach to web search personalization based on user collaboration and sharing of information about web documents. The proposed personalization technique separates data collection and user profiling from the information system whose contents and indexed documents are being searched for, i.e. the search engines, and uses social bookmarking and tagging to re-rank web search results. It is independent of the search engine being used, so users are free to choose the one they prefer, even if their favorite search engine does not natively support personalization. We show how to design and implement such a system in practice and investigate its feasibility and usefulness with large sets of real-word data and a user study.
1791587663;Perceived usefulness, perceived ease of use, and user acceptance of information technology;1989.0;[];Valid measurement scales for predicting user acceptance of computers are in short supply. Most subjective measures used in practice are unvalidated, and their relationship to system usage is unknown. The present research develops and validates new scales for two specific variables, perceived usefulness and perceived ease of use, which are hypothesized to be fundamental determinants of user acceptance. Definitions of these two variables were used to develop scale items that were pretested for content validity and then tested for reliability and construct validity in two studies involving a total of 152 users and four application programs. The measures were refined and streamlined, resulting in two six-item scales with reliabilities of .98 for usefulness and .94 for ease of use. The scales exhibited hgih convergent, discriminant, and factorial validity. Perceived usefulness was significnatly correlated with both self-reported current usage r = .63, Study 1) and self-predicted future usage r = .85, Study 2). Perceived ease of use was also significantly correlated with current usage r = .45, Study 1) and future usage r = .59, Study 2). In both studies, usefulness had a signficnatly greater correaltion with usage behavior than did ease of use. Regression analyses suggest that perceived ease of use may actually be a causal antecdent to perceived usefulness, as opposed to a parallel, direct determinant of system usage. Implications are drawn for future research on user acceptance.
1805283773;An algorithmic historiography of the Ebola research specialty: mapping the science behind Ebola;2015.0;[];The objective of this paper was to identify the intellectual profile of the Ebola research specialty and its behavior from its inception to 2014. This objective was met by chronologically mapping the information flows within the specialty using bibliometric and citation data extracted from 1638 Ebola research documents in conjunction with Histcite to produce an algorithmic historiography representing a view of the Ebola specialtyu0027s intellectual profile and evolution. The present study was guided by the following research questions. What is the bibliometric profile of the Ebola specialty in terms of publication output and the impact of its authors, journals, institutions, countries, and years? What influential Ebola research has been produced since its discovery, and how has the research evolved? The most significant results show the Ebola specialty citation network as a small-world and highly cohesive network. The Ebola specialty citation network was found to be symmetrical in structure and segmented into four distinct cliques representing specific research focuses (i.e., uncovering divergent strains, immune responses and vaccines, Ebolau0027s pathogenesis, Ebolau0027s molecular structure and physiology). Key authors and contributing journals were identified. The most substantial contributions to the specialty were from the government and academia. The Ebola specialty had a slow publication output and oscillating citation activity for the first few decades, coinciding with several outbreaks. The greatest production of Ebola research articles occurred after 2000, along with exponential citation behavior.
1832693441;Convolutional Neural Networks for Sentence Classification;2014.0;[];We report on a series of experiments with convolutional neural networks (CNN) trained on top of pre-trained word vectors for sentence-level classification tasks. We show that a simple CNN with little hyperparameter tuning and static vectors achieves excellent results on multiple benchmarks. Learning task-specific vectors through fine-tuning offers further gains in performance. We additionally propose a simple modification to the architecture to allow for the use of both task-specific and static vectors. The CNN models discussed herein improve upon the state of the art on 4 out of 7 tasks, which include sentiment analysis and question classification.
1833977909;Improved use of continuous attributes in C4.5;1996.0;[];A reported weakness of C4.5 in domains with continuous attributes is addressed by modifying the formation and evaluation of tests on continuous attributes. An MDL-inspired penalty is applied to such tests, eliminating some of them from consideration and altering the relative desirability of all tests. Empirical trials show that the modifications lead to smaller decision trees with higher predictive accuracies. Results also confirm that a new version of C4.5 incorporating these changes is superior to recent approaches that use global discretization and that construct small trees with multi-interval splits.
1836875240;Screening Peers Softly: Inferring the Quality of Small Borrowers;2016.0;[];"This paper examines the performance of new online lending markets that rely on nonexpert individuals to screen their peersu0027 creditworthiness. We find that these peer lenders predict an individualu0027s likelihood of defaulting on a loan with 45% greater accuracy than the borroweru0027s exact credit score unobserved by the lenders, who only see a credit category. Moreover, peer lenders achieve 87% of the predictive power of an econometrician who observes all standard financial information about borrowers. Screening through soft or nonstandard information is relatively more important when evaluating lower-quality borrowers. Our results highlight how aggregating over the views of peers and leveraging nonstandard information can enhance lending  paper :[106],""was accepted by Amit Seru, finance."
1841724727;Effective Use of Word Order for Text Categorization with Convolutional Neural Networks;2014.0;[];Convolutional neural network (CNN) is a neural network that can make use of the internal structure of data such as the 2D structure of image data. This paper studies CNN on text categorization to exploit the 1D structure (namely, word order) of text data for accurate prediction. Instead of using low-dimensional word vectors as input as is often done, we directly apply CNN to high-dimensional text data, which leads to directly learning embedding of small text regions for use in classification. In addition to a straightforward adaptation of CNN from image to text, a simple but new variation which employs bag-of-word conversion in the convolution layer is proposed. An extension to combine multiple convolution layers is also explored for higher accuracy. The experiments demonstrate the effectiveness of our approach in comparison with state-of-the-art methods.
1849730171;A New Bio-inspired Algorithm: Chicken Swarm Optimization;2014.0;[];A new bio-inspired algorithm, Chicken Swarm Optimization (CSO), is proposed for optimization applications. Mimicking the hierarchal order in the chicken swarm and the behaviors of the chicken swarm, including roosters, hens and chicks, CSO can efficiently extract the chickens’ swarm intelligence to optimize problems. Experiments on twelve benchmark problems and a speed reducer design were conducted to compare the performance of CSO with that of other algorithms. The results show that CSO can achieve good optimization results in terms of both optimization accuracy and robustness. Future researches about CSO are finally suggested.
1858904274;Data prediction, compression, and recovery in clustered wireless sensor networks for environmental monitoring applications;2016.0;[];Environmental monitoring is one of the most important applications of wireless sensor networks (WSNs), which usually requires a lifetime of several months, or even years. However, the inherent restriction of energy carried within the battery of sensor nodes brings an extreme difficulty to obtain a satisfactory network lifetime, which becomes a bottleneck in scale of such applications in WSNs. In this paper, we propose a novel framework with dedicated combination of data prediction, compression, and recovery to simultaneously achieve accuracy and efficiency of the data processing in clustered WSNs. The main aim of the framework is to reduce the communication cost while guaranteeing the data processing and data prediction accuracy. In this framework, data prediction is achieved by implementing the Least Mean Square (LMS) dual prediction algorithm with optimal step size by minimizing the mean-square derivation (MSD), in a way that the cluster heads (CHs) can obtain a good approximation of the real data from the sensor nodes. On this basis, a centralized Principal Component Analysis (PCA) technique is utilized to perform the compression and recovery for the predicted data on the CHs and the sink, separately in order to save the communication cost and to eliminate the spatial redundancy of the sensed data about environment. All errors generated in these processes are finally evaluated theoretically, which come out to be controllable. Based on the theoretical analysis, we design a number of algorithms for implementation. Simulation results by using the real world data demonstrate that our framework provides a cost-effective solution to such as environmental monitoring applications in cluster based WSNs.
1861492603;Microsoft COCO: Common Objects in Context;2014.0;[];We present a new dataset with the goal of advancing the state-of-the-art in object recognition by placing the question of object recognition in the context of the broader question of scene understanding. This is achieved by gathering images of complex everyday scenes containing common objects in their natural context. Objects are labeled using per-instance segmentations to aid in precise object localization. Our dataset contains photos of 91 objects types that would be easily recognizable by a 4 year old. With a total of 2.5 million labeled instances in 328k images, the creation of our dataset drew upon extensive crowd worker involvement via novel user interfaces for category detection, instance spotting and instance segmentation. We present a detailed statistical analysis of the dataset in comparison to PASCAL, ImageNet, and SUN. Finally, we provide baseline performance analysis for bounding box and segmentation detection results using a Deformable Parts Model.
1862479034;The appropriation and repurposing of social technologies in higher education;2009.0;[];This paper presents some of the findings from a recent project that conducted a virtual ethnographic study of three formal courses in higher education that use u0027Web 2.0u0027 or social technologies for learning and teaching. It describes the pedagogies adopted within these courses, and goes on to explore some key themes emerging from the research and relating to the pedagogical use of weblogs and wikis in particular. These themes relate primarily to the academyu0027s tendency to constrain and contain the possibly more radical effects of these new spaces. Despite this, the findings present a range of student and tutor perspectives which show that these technologies have significant potential as new collaborative, volatile and challenging environments for formal learning.
1865048348;An Analytic Approach to Smooth Polynominals over Finite Fields;1998.0;[];We consider the largest degrees that occur in the decomposition of polynomials over finite fields into irreducible factors. We expand the range of applicability of the Dickman function as an approximation for the number of smooth polynomials, which provides precise estimates for the discrete logarithm problem. In addition, we characterize the distribution of the two largest degrees of irreducible factors, a problem relevant to polynomial factorization. As opposed to most earlier treatments, our methods are based on a combination of exact descriptions by generating functions and a specific complex asymptotic method.
1872744038;ACon: A learning-based approach to deal with uncertainty in contextual requirements at runtime;2016.0;[];"Context: Runtime uncertainty such as unpredictable operational environment and failure of sensors that gather environmental data is a well-known challenge for adaptive  :[22],""To execute requirements that depend on context correctly, the system needs up-to-date knowledge about the context relevant to such requirements. Techniques to cope with uncertainty in contextual requirements are currently underrepresented. In this paper we present ACon (Adaptation of Contextual requirements), a data-mining approach to deal with runtime uncertainty affecting contextual  ACon :[74],""uses feedback loops to maintain up-to-date knowledge about contextual requirements based on current context information in which contextual requirements are valid at runtime. Upon detecting that contextual requirements are affected by runtime uncertainty, ACon analyses and mines contextual data, to (re-)operationalize context and therefore update the information about contextual  :[125],""We evaluate ACon in an empirical study of an activity scheduling system used by a crew of 4 rowers in a wild and unpredictable environment using a complex monitoring infrastructure. Our study focused on evaluating the data mining part of ACon and analysed the sensor data collected onboard from 46 sensors and 90,748 measurements per  ACon is an :[181],""important step in dealing with uncertainty affecting contextual requirements at runtime while considering end-user interaction. ACon supports systems in analysing the environment to adapt contextual requirements and complements existing requirements monitoring approaches by keeping the requirements monitoring specification up-to-date. Consequently, it avoids manual analysis that is usually costly in today’s complex system environments."
1877294626;What are the affordances and constraints of handheld devices for learning in higher education;2011.0;[];
1888005072;LINE: Large-scale Information Network Embedding;2015.0;[];This paper studies the problem of embedding very large information networks into low-dimensional vector spaces, which is useful in many tasks such as visualization, node classification, and link prediction. Most existing graph embedding methods do not scale for real world information networks which usually contain millions of nodes. In this paper, we propose a novel network embedding method called the ``LINE,u0027u0027 which is suitable for arbitrary types of information networks: undirected, directed, and/or weighted. The method optimizes a carefully designed objective function that preserves both the local and global network structures. An edge-sampling algorithm is proposed that addresses the limitation of the classical stochastic gradient descent and improves both the effectiveness and the efficiency of the inference. Empirical experiments prove the effectiveness of the LINE on a variety of real-world information networks, including language networks, social networks, and citation networks. The algorithm is very efficient, which is able to learn the embedding of a network with millions of vertices and billions of edges in a few hours on a typical single machine. The source code of the LINE is available online\footnote{\url{https://github.com/tangjianpku/LINE}}.
1895621607;Adaptive Knowledge Bases in Self-Adaptive System Design;2015.0;[];Self-adaptive systems allow for flexible solutions in changing environments. Usually, a fixed set of predefined rules is used to define the adaptation possibilities of a system. The main problem of such systems is to cope with environment behaviours that were not anticipated at design-time. In this case, no adaptation rule might be applicable or adaptations might not have the expected effect. In this paper, we propose an extended architecture of IBMu0027s MAPE-K loop to cope with this problem. We impose a structure on the knowledge base consisting of an  system and environment model, a global goal model, and a set of (current) adaptation rules. Furthermore, we introduce an evaluation component that deletes failed adaptation rules, as well as a learning component that uses run-time models to autonomously generate new rules if the current ones are not applicable. With our approach, not only functional components can dynamically be adapted but also the adaptation logic itself.
1880262756;Latent dirichlet allocation;2003.0;[];We describe latent Dirichlet allocation (LDA), a generative probabilistic model for collections of discrete data such as text corpora. LDA is a three-level hierarchical Bayesian model, in which each item of a collection is modeled as a finite mixture over an underlying set of topics. Each topic is, in turn, modeled as an infinite mixture over an underlying set of topic probabilities. In the context of text modeling, the topic probabilities provide an explicit representation of a document. We present efficient approximate inference techniques based on variational methods and an EM algorithm for empirical Bayes parameter estimation. We report results in document modeling, text classification, and collaborative filtering, comparing to a mixture of unigrams model and the probabilistic LSI model.
1904073808;Vehicle Detection in High-Resolution Aerial Images via Sparse Representation and Superpixels;2016.0;[];This paper presents a study of vehicle detection from high-resolution aerial images. In this paper, a superpixel segmentation method designed for aerial images is proposed to control the segmentation with a low breakage rate. To make the training and detection more efficient, we extract meaningful patches based on the centers of the segmented superpixels. After the segmentation, through a training sample selection iteration strategy that is based on the sparse representation, we obtain a complete and small training subset from the original entire training set. With the selected training subset, we obtain a dictionary with high discrimination ability for vehicle detection. During training and detection, the grids of histogram of oriented gradient descriptor are used for feature extraction. To further improve the training and detection efficiency, a method is proposed for the defined main direction estimation of each patch. By rotating each patch to its main direction, we give the patches consistent directions. Comprehensive analyses and comparisons on two data sets illustrate the satisfactory performance of the proposed algorithm.
1913582850;Active capacitive sensing: exploring a new wearable sensing modality for activity recognition;2010.0;[];The paper describes the concept, implementation, and evaluation of a new on-body capacitive sensing approach to derive activity related information. Using conductive textile based electrodes that are easy to integrate in garments, we measure changes in capacitance inside the human body. Such changes are related to motions and shape changes of muscle, skin, and other tissue, which can in turn be related to a broad range of activities and physiological parameters. We describe the physical principle, the analog hardware needed to acquire and pre-process the signal, and example signals from different body locations and actions. We perform quantitative evaluations of the recognition accuracy, focused on the specific example of collar-integrated electrodes and actions, such as chewing, swallowing, speaking, sighing (taking a deep breath), as well as different head motions and positions.
1915837116;SACRE: A tool for dealing with uncertainty in contextual requirements at runtime;2015.0;[];Self-adaptive systems are capable of dealing with uncertainty at runtime handling complex issues as resource variability, changing user needs, and system intrusions or faults. If the requirements depend on context, runtime uncertainty will affect the execution of these contextual requirements. This work presents SACRE, a proof-of-concept implementation of an existing approach, ACon, developed by researchers of the Univ. of Victoria (Canada) in collaboration with the UPC (Spain). ACon uses a feedback loop to detect contextual requirements affected by uncertainty and data mining techniques to determine the best operationalization of contexts on top of sensed data. The implementation is placed in the domain of smart vehicles and the contextual requirements provide functionality for drowsy drivers.
1925986356;Survey of social search from the perspectives of the village paradigm and online social networks;2013.0;[];Two paradigms currently exist for information search. The first is the library paradigm, which has been largely automated and is the prevailing paradigm in todayu0027s web search. The second is the village paradigm, and although it is older than the library paradigm, its automation has not been considered, yet certain elements of its key aspects have been automated, as in the cases of the Qu0026A communities or novel services such as Quora. The increasing popularity and availability of online social networks and question-answering communities have encouraged revisiting of the automation of the village paradigm owing to new helpful developments, primarily that people are more connected with their acquaintances on the internet and their contact lists are available. In this survey, we study how the village paradigm is today partially automated: we consider the selection of candidates for answering questions, answering questions automatically and helping candidates to decide what questions to answer. Other aspects are also considered, for example, the automation of a reward system. We conclude that a next step towards the automation of the village paradigm involves intelligent agents that can leverage a P2P (peer-to-peer) social network, which will create new and interesting issues deeply entwined with social networks in the form of information processing by agents in parallel and side by side with people.
1930431862;Near-optimal perfectly matched layers for indefinite Helmholtz problems;2016.0;[];A new construction of an absorbing boundary condition for indefinite Helmholtz problems on  unbounded domains  is presented. This construction is based on a near-best uniform rational interpolant of the inverse square root function on the union of a negative and positive real interval,  designed with the help of a classical result by Zolotarev. Using Kreinu0027s interpretation of a Stieltjes continued fraction, this interpolant can be converted into a three-term finite difference discretization of a perfectly matched layer (PML) which converges exponentially fast in the number of grid points. The convergence rate is asymptotically optimal  for both propagative and evanescent wave modes. Several numerical experiments and illustrations are included.
1936750108;Efficient object localization using Convolutional Networks;2015.0;[];Recent state-of-the-art performance on human-body pose estimation has been achieved with Deep Convolutional Networks (ConvNets). Traditional ConvNet architectures include pooling and sub-sampling layers which reduce computational requirements, introduce invariance and prevent over-training. These benefits of pooling come at the cost of reduced localization accuracy. We introduce a novel architecture which includes an efficient ‘position refinement’ model that is trained to estimate the joint offset location within a small region of the image. This refinement model is jointly trained in cascade with a state-of-the-art ConvNet model [21] to achieve improved accuracy in human joint location estimation. We show that the variance of our detector approaches the variance of human annotations on the FLIC [20] dataset and outperforms all existing approaches on the MPII-human-pose dataset [1].
1937351322;Tightly coupled ultrashort baseline and inertial navigation system for underwater vehicles: An experimental validation;2013.0;[];This paper presents a new ultrashort baseline (USBL) tightly coupled integration technique to enhance error estimation in low-cost strapdown inertial navigation systems (INSs), with application to underwater vehicles. In the proposed strategy, the acoustic array spatial information is directly exploited in an extended Kalman filter (EKF) implemented in a direct feedback structure. Instead of using the USBL position fixes or computed range and elevation/bearing angles to correct the INS error drifts, as in classical loosely coupled strategies, the novel tightly coupled strategy directly embeds in the EKF the round-trip-time and time-difference-of-arrival of the acoustic signals arriving at the onboard receivers. The enhanced performance of the proposed filtering technique is evidenced both through extensive numerical simulations and with experimental data obtained in field tests at sea. The tightly coupled filter is also shown to be able to operate closer to theoretical performance lower bounds, such as the posterior Cramer-Rao lower bound, using Monte-Carlo simulations. This paper details the design and description of an USBL/INS prototype to be used as a low-cost navigation system, including the acoustic processing and positioning system, fully developed in-house. The developed system validates the usage of the proposed technique with real data in real world operation scenarios, and its enhanced performance compared to classical strategies is evaluated experimentally (median improvement level of 15% in typical operating conditions). Improved and faster convergence to nominal trajectories from multiple initial conditions, as well as enhanced accelerometer and rate gyros estimation capabilities, are also demonstrated experimentally for the new tightly coupled filter. © 2013 Wiley Periodicals, Inc.
1941659294;The class imbalance problem: A systematic study;2002.0;[];In machine learning problems, differences in prior class probabilities -- or class imbalances -- have been reported to hinder the performance of some standard classifiers, such as decision trees. This paper presents a systematic study aimed at answering three different questions. First, we attempt to understand the nature of the class imbalance problem by establishing a relationship between concept complexity, size of the training set and class imbalance level. Second, we discuss several basic re-sampling or cost-modifying methods previously proposed to deal with the class imbalance problem and compare their effectiveness. The results obtained by such methods on artificial domains are linked to results in real-world domains. Finally, we investigate the assumption that the class imbalance problem does not only affect decision tree systems but also affects other classification systems such as Neural Networks and Support Vector Machines.
1944615693;Action recognition with trajectory-pooled deep-convolutional descriptors;2015.0;[];"Visual features are of vital importance for human action understanding in videos. This paper presents a new video representation, called trajectory-pooled deep-convolutional descriptor (TDD), which shares the merits of both hand-crafted features [31] and deep-learned features [24]. Specifically, we utilize deep architectures to learn discriminative convolutional feature maps, and conduct trajectory-constrained pooling to aggregate these convolutional features into effective descriptors. To enhance the robustness of TDDs, we design two normalization methods to transform convolutional feature maps, namely spatiotemporal normalization and channel normalization. The advantages of our features come from (i) TDDs are automatically learned and contain high discriminative capacity compared with those hand-crafted features; (ii) TDDs take account of the intrinsic characteristics of temporal dimension and introduce the strategies of trajectory-constrained sampling and pooling for aggregating deep-learned features. We conduct experiments on two challenging datasets: HMD-B51 and UCF101. Experimental results show that TDDs outperform previous hand-crafted features [31] and deep-learned features [24]. Our method also achieves superior performance to the state of the art on these datasets."
1945978549;The use of computer technology in university teaching and learning: a critical perspective;2007.0;[];Abstract  Despite huge efforts to position information and communication technology (ICT) as a central tenet of university teaching and learning, the fact remains that many university students and faculty make only limited formal academic use of computer technology. Whilst this is usually attributed to a variety of operational deficits on the part of students, faculty, and universities, this paper considers the wider social relations underpinning the relatively modest use of technology in higher education. The paper explores how university use of computer technology is shaped into marginalized and curtailed positions by a variety of actors. From the ‘writing’ of ICT at a national policy level through to the marginalization of ICT within the lived ‘student experience’, a consistent theme emerges where computer technology use is constructed in limited, linear, and rigid terms far removed from the creative, productive, and empowering uses which are often celebrated by educational technologists. In the light of such constraints, the paper considers how these dominant constructions of a peripheral and limited use of ICT may be challenged by the higher education community. In particular, it concludes by reflecting on current critical thinking about how educational technologists can foster a more expansive and empowered use of computer technology within university settings.
1951747108;State-of-the-Art of computer-aided detection/diagnosis (CAD);2010.0;[];This paper summarizes the presentations given in the special ICMB2010 session on state-of-the-art of computer-aided detection/diagnosis (CAD). The topics are concerned with the latest development of technologies and applications in CAD, which include brain MR images, fundus photographs, dynamic chest radiography, chest CT images, whole breast ultrasonography, CT colonography and torso FDG-PET scans.
1954281074;A new measure of inaccuracy with its application to multi-criteria decision making under intuitionistic fuzzy environment;2014.0;[];Mathematics has evolved to study vague phenomena that do not show statistical stability. Intuitionistic fuzzy sets best represent these vague phenomena, and admit set operations that do not arise otherwise, because of the functions involved in their definition. This has greatly enriched mathematics and has potential new directions for quantitative studies and applications. There is need to define quantitative measures for contents, vagueness, distance, etc. over intuitionistic fuzzy sets. In this paper a measure of inaccuracy between two u0027intuitionistic fuzzy setsu0027 is introduced and studied. The measure is demonstrated to satisfy some very interesting properties, which prepare ground for applications in multi-criteria decision making problems. We develop a method to solve multi-criteria decision making problems with the help of new measure. Finally, three numerical examples are given to illustrate the proposed method to solve multi-criteria decision-making problem under intuitionistic fuzzy environment.
1954993463;Simultaneous state‐time approximation of the chemical master equation using tensor product formats;2015.0;[];" :[0],""apply the novel tensor product formats (tensor train, quantized TT [QTT], and QTT-Tucker) to the solution of d-dimensional chemical master equations for gene regulating networks (signaling cascades, toggle switches, and phage- λ). For some important cases, for example, signaling cascade models, we prove analytical tensor product representations of the system operator. The quantized tensor representations (QTT, QTT-Tucker) are employed in both state space and time, and the global state-time (d + 1)-dimensional system is solved in the tensor product form by the alternating minimal energy iteration, the ALS-type algorithm. This approach leads to the logarithmic dependence of the computational complexity on the volume of the state space. We investigate the proposed technique numerically and compare it with the direct chemical master equation solution and some previously known approximate schemes, where possible. We observe that the newer tensor methods demonstrate a good potential in simulation of relevant biological systems. Copyright © 2014 John Wiley u0026 Sons, Ltd."
1962520980;Online Peer-to-Peer Lending: A Lenders' Perspective;2008.0;[];Online Peer-to-Peer lending platforms claim to be beneficial for both borrowers and lenders by eliminating expensive intermediaries and reducing transaction costs. However, are the often inexperienced lenders who operate in a pseudonymous online environment with potentially significant information asymmetries really able to obtain an attractive return on their investment? This paper discusses the question by presenting profitability data from the US platform Prosper. Although the overall investment performance has not been satisfactory for most rating categories, it is shown that following some simple investment rules improves profitability of a portfolio and leads to acceptable returns for all credit rating categories with exception of the high-risk one.
1964344998;Establishing Trust in Cloud Computing;2010.0;[];"The paper discussed the emerging technologies that can help address the challenges of trust in cloud computing. Cloud computing provides many opportunities for enterprises by offering a range of computing services. In todayu0027s competitive environment, the service dynamism, elasticity, and choices offered by this highly scalable technology are too attractive for enterprises to ignore. These opportunities, however, donu0027t come without challenges. Cloud computing has opened up a new frontier of challenges by introducing a different type of trust scenario. Today, the problem of trusting cloud computing is a paramount concern for most enterprises. Itu0027s not that the enterprises donu0027t trust the cloud providersu0027 intentions; rather, they question cloud computingu0027s capabilities."
1964836145;A versatile policy toolkit supporting run-time policy reconfiguration;2008.0;[];This paper describes an autonomics development tool which serves as both a powerful and flexible policy-expression language and a policy-based framework that supports the integration and dynamic composition of several autonomic computing techniques including signal processing, automated trend analysis and utility functions. Each of these technologies has specific advantages and applicability to different types of dynamic adaptation. The AGILE platform enables seamless interoperability of the different technologies to each perform various aspects of self-management within a single application. Self-management behaviour is specified using the policy language semantics to bind the various technologies together as required. Since the policy semantics support run-time re-configuration, the self-management architecture is dynamically composable. The policy language and implementation library have integrated support for self-stabilising behaviour, enabling oscillation and other forms of instability to be handled at the policy level with very little effort on the part of the application developer. Example applications are presented to illustrate the integration of different autonomics techniques, and the achievement of dynamic composition.
1965127592;A novel destination-based routing protocol (DBRP) in DTNs;2012.0;[];In Delay Tolerant Networks (DTNs), the aim of any forwarding/routing protocols is to achieve a high delivery ratio of packets/bundles at the lowest possible bandwidth cost, buffer space and energy. Therefore, finding a protocol which uses less resource to achieve high delivery ratio and low latency is an open research question. This paper proposes a quota-based protocol which confines the number of replicas and forwards them based on the meeting history of nodes. The unique aspect of our protocol is to weight any encounter with the final destination to be much higher than any other node encounter. This aspect of the protocol is based on the idea that regardless of how small an encounter rate with the destination, given a highly correlated movement model (i.e., human behaviour) we will end up with a high delivery ratio. The results of our simulation support this hypothesis.
1965173537;Comparing performances of backpropagation and genetic algorithms in the data classification;2011.0;[];Artificial neural networks (ANN) have a wide ranging usage area in the data classification problems. Backpropagation algorithm is classical technique used in the training of the artificial neural networks. Since this algorithm has many disadvantages, the training of the neural networks has been implemented with the binary and real-coded genetic algorithms. These algorithms can be used for the solutions of the classification problems. The real-coded genetic algorithm has been compared with other training methods in the few works. It is known that the comparison of the approaches is as important as proposing a new classification approach. For this reason, in this study, a large-scale comparison of performances of the neural network training methods is examined on the data classification datasets. The experimental comparison contains different real classification data taken from the literature and a simulation study. A comparative analysis on the real data sets and simulation data shows that the real-coded genetic algorithm may offer efficient alternative to traditional training methods for the classification problem.
1965239040;Fuzzy entropy and conditioning;1986.0;[];   A new nonprobabilistic entropy measure is introduced in the context of fuzzy sets or messages. Fuzzy units, or  fits , replace bits in a new framework of fuzzy information theory. An appropriate measure of entropy or fuzziness of messages is shown to be a simple ratio of distances: the distances between the fuzzy message and its nearest and farthest nonfuzzy neighbors. Fuzzy conditioning is examined as the degree of subsethood (submessagehood) of one fuzzy set or message in another. This quantity is shown to behave as a conditional probability in many contexts. It is also shown that the entropy of  A  is the degree to which  A  ∪  A   c   is a subset of  A  ∩  A   c  , an intuitive relationship that cannot occur in probability theory. This theory of subsethood is then shown to solve one of the major problems with Bayes-theorem learning and its variants—the problem of requiring that the space of alternatives be partitioned into disjoint exhaustive hypotheses. Any fuzzy subsets will do. However, a rough inverse relationship holds between number and fuzziness of partitions and the information gained from experience. All results reduce to fuzzy cardinality.
1965419024;Modeling and Stochastic Control for Home Energy Management;2013.0;[];The problem of modeling and stochastic optimization for home energy management is considered. Several different types of load classes are discussed, including heating, ventilation, and air conditioning unit, plug-in hybrid electric vehicle, and deferrable loads such as washer and dryer. A first-order thermal dynamic model is extracted and validated using real measurements collected over an eight months time span. A mixed integer multi-time scale stochastic optimization is formulated for the scheduling of loads of different characteristics. A model predictive control based heuristic is proposed. Numerical simulations coupled with real data measurements are used for performance evaluation and comparison studies.
1965442863;An elitism based multi-objective artificial bee colony algorithm;2015.0;[];In this paper, we suggest a new multi-objective artificial bee colony (ABC) algorithm by introducing an elitism strategy. The algorithm uses a fixed-size archive that is maintained based on crowding-distance to store non-dominated solutions found during the search process. In the proposed algorithm, an improved artificial bee colony algorithm with an elitism strategy is adopted for the purpose of avoiding premature convergence. Specifically, the elites in the archive are selected and used to generate new food sources in both employed and onlooker bee phases in each cycle. To keep diversity, a member located at the most crowded region will be removed when the archive overflows. The algorithm is very easy to be implemented and it employs only a few control parameters. The proposed algorithm is tested on a wide range of multi-objective problems, and compared with other state-of-the-art algorithms in terms of often-used quality indicators with the help of a nonparametric test. It is revealed by the test procedure that the algorithm produces better or comparable results when compared with other well-known algorithms, and it can be used as a promising alternative tool to solve multi-objective problems with the advantage of being simple and effective.
1965811262;Rosenbrock artificial bee colony algorithm for accurate global optimization of numerical functions;2011.0;[];A Rosenbrock artificial bee colony algorithm (RABC) that combines Rosenbrocku0027s rotational direction method with an artificial bee colony algorithm (ABC) is proposed for accurate numerical optimization. There are two alternative phases of RABC: the exploration phase realized by ABC and the exploitation phase completed by the rotational direction method. The proposed algorithm was tested on a comprehensive set of complex benchmark problems, encompassing a wide range of dimensionality, and it was also compared with several algorithms. Numerical results show that the new algorithm is promising in terms of convergence speed, success rate, and accuracy. The proposed RABC is also capable of keeping up with the direction changes in the problems.
1965839417;The uniqueness of positive solution for a fractional order model of turbulent flow in a porous medium;2014.0;[];   In this paper, we establish the uniqueness of positive solution for a fractional model of turbulent flow in a porous medium by using the fixed point theorem of the mixed monotone operator. An example is also given to illustrate the application of the main result.
1965894642;Multi-tissue constrained spherical deconvolution for improved analysis of multi-shell diffusion MRI data;2014.0;[];   Constrained spherical deconvolution (CSD) has become one of the most widely used methods to extract white matter (WM) fibre orientation information from diffusion-weighted MRI (DW-MRI) data, overcoming the crossing fibre limitations inherent in the diffusion tensor model. It is routinely used to obtain high quality fibre orientation distribution function (fODF) estimates and fibre tractograms and is increasingly used to obtain apparent fibre density (AFD) measures. Unfortunately, CSD typically only supports data acquired on a single shell in q-space. With multi-shell data becoming more and more prevalent, there is a growing need for CSD to fully support such data. Furthermore, CSD can only provide high quality fODF estimates in voxels containing WM only. In voxels containing other tissue types such as grey matter (GM) and cerebrospinal fluid (CSF), the WM response function may no longer be appropriate and spherical deconvolution produces unreliable, noisy fODF estimates.  The aim of this study is to incorporate support for multi-shell data into the CSD approach as well as to exploit the unique b-value dependencies of the different tissue types to estimate a multi-tissue ODF. The resulting approach is dubbed multi-shell, multi-tissue CSD (MSMT-CSD) and is compared to the state-of-the-art single-shell, single-tissue CSD (SSST-CSD) approach. Using both simulations and real data, we show that MSMT-CSD can produce reliable WM/GM/CSF volume fraction maps, directly from the DW data, whereas SSST-CSD has a tendency to overestimate the WM volume in voxels containing GM and/or CSF. In addition, compared to SSST-CSD, MSMT-CSD can substantially increase the precision of the fODF fibre orientations and reduce the presence of spurious fODF peaks in voxels containing GM and/or CSF. Both effects translate into more reliable AFD measures and tractography results with MSMT-CSD compared to SSST-CSD.
1965971975;Use of social network information to enhance collaborative filtering performance;2010.0;[];When people make decisions, they usually rely on recommendations from friends and acquaintances. Although collaborative filtering (CF), the most popular recommendation technique, utilizes similar neighbors to generate recommendations, it does not distinguish friends in a neighborhood from strangers who have similar tastes. Because social networking Web sites now make it easy to gather social network information, a study about the use of social network information in making recommendations will probably produce productive results. In this study, we developed a way to increase recommendation effectiveness by incorporating social network information into CF. We collected data about usersu0027 preference ratings and their social network relationships from a social networking Web site. Then, we evaluated CF performance with diverse neighbor groups combining groups of friends and nearest neighbors. Our results indicated that more accurate prediction algorithms can be produced by incorporating social network information into CF.
1966197090;Cognitive shifts related to interactive information retrieval;2007.0;[];"– Interactive information retrieval (IR) involves many human cognitive shifts at different information behaviour levels. Cognitive science defines a cognitive shift or shift in cognitive focus as triggered by the brainu0027s response and change due to some external force. This paper aims to provide an explication of the concept of “cognitive shift” and then report results from a study replicating Spinku0027s study of cognitive shifts during interactive IR. This work aims to generate promising insights into aspects of cognitive shifts during interactive IR and a new IR evaluation measure – information problem shift., – The study participants (n=9) conducted an online search on an in‐depth personal medical information problem. Data analysed included the pre‐ and post‐search questionnaires completed by each study participant. Implications for web services and further research are discussed., – Key findings replicated the results in Spinku0027s study, including: all study participants reported some level of cognitive shift in their information problem, information seeking and personal knowledge due to their search interaction; and different study participants reported different levels of cognitive shift. Some study participants reported major cognitive shifts in various user‐based variables such as information problem or information‐seeking stage. Unlike Spinku0027s study, no participant experienced a negative shift in their information problem stage or level of information problem understanding., – This study builds on the previous study by Spink using a different dataset. The paper provides valuable insights for further research into cognitive shifts during interactive IR."
1966443646;Learning to Rank Short Text Pairs with Convolutional Deep Neural Networks;2015.0;[];Learning a similarity function between pairs of objects is at the core of learning to rank approaches. In information retrieval tasks we typically deal with query-document pairs, in question answering -- question-answer pairs. However, before learning can take place, such pairs needs to be mapped from the original space of symbolic words into some feature space encoding various aspects of their relatedness, e.g. lexical, syntactic and semantic. Feature engineering is often a laborious task and may require external knowledge sources that are not always available or difficult to obtain. Recently, deep learning approaches have gained a lot of attention from the research community and industry for their ability to automatically learn optimal feature representation for a given task, while claiming state-of-the-art performance in many tasks in computer vision, speech recognition and natural language processing. In this paper, we present a convolutional neural network architecture for reranking pairs of short texts, where we learn the optimal representation of text pairs and a similarity function to relate them in a supervised way from the available training data. Our network takes only words in the input, thus requiring minimal preprocessing. In particular, we consider the task of reranking short text pairs where elements of the pair are sentences. We test our deep learning system on two popular retrieval tasks from TREC: Question Answering and Microblog Retrieval. Our model demonstrates strong performance on the first task beating previous state-of-the-art systems by about 3\% absolute points in both MAP and MRR and shows comparable results on tweet reranking, while enjoying the benefits of no manual feature engineering and no additional syntactic parsers.
1966518282;DTN routing as a resource allocation problem;2007.0;[];Many DTN routing protocols use a variety of mechanisms, including discovering the meeting probabilities among nodes, packet replication, and network coding. The primary focus of these mechanisms is to increase the likelihood of finding a path with limited information, so these approaches have only an incidental effect on such routing metrics as maximum or average delivery latency. In this paper, we present  RAPID , an intentional DTN routing protocol that can optimize a specific routing metric such as worst-case delivery latency or the fraction of packets that are delivered within a deadline. The key insight is to treat DTN routing as a resource allocation problem that translates the routing metric into per-packet utilities which determine how packets should be replicated in the system.   We evaluate RAPID rigorously through a prototype of  RAPID  deployed over a vehicular DTN testbed of 40 buses and simulations based on real traces. To our knowledge, this is the first paper to report on a routing protocol deployed on a real DTN at this scale. Our results suggest that  RAPID  significantly outperforms existing routing protocols for several metrics. We also show empirically that for small loads  RAPID  is within 10% of the optimal performance.
1966559656;Context information prediction for social-based routing in opportunistic networks;2012.0;[];Context information can be used to streamline routing decisions in opportunistic networks. We propose a novel social context-based routing scheme that considers both the spatial and the temporal dimensions of the activity of mobile nodes to predict the mobility patterns of nodes based on the BackPropagation Neural Networks model.
1966736954;Energy-efficient rate-adaptive GPS-based positioning for smartphones;2010.0;[];Many emerging smartphone applications require position information to provide location-based or context-aware services. In these applications, GPS is often preferred over its alternatives such as GSM/WiFi based positioning systems because it is known to be more accurate. However, GPS is extremely power hungry. Hence a common approach is to periodically duty-cycle GPS. However, GPS duty-cycling trades-off positioning accuracy for lower energy. A key requirement for such applications, then, is a positioning system that provides accurate position information while spending minimal energy.   In this paper, we present RAPS, rate-adaptive positioning system for smartphone applications. It is based on the observation that GPS is generally less accurate in urban areas, so it suffices to turn on GPS only as often as necessary to achieve this accuracy. RAPS uses a collection of techniques to cleverly determine when to turn on GPS. It uses the location-time history of the user to estimate user velocity and adaptively turn on GPS only if the estimated uncertainty in position exceeds the accuracy threshold. It also efficiently estimates user movement using a duty-cycled accelerometer, and utilizes Bluetooth communication to reduce position uncertainty among neighboring devices. Finally, it employs celltower-RSS blacklisting to detect GPS unavailability (e.g., indoors) and avoid turning on GPS in these cases. We evaluate RAPS through real-world experiments using a prototype implementation on a modern smartphone and show that it can increase phone lifetimes by more than a factor of 3.8 over an approach where GPS is always on.
1966911549;Designing sports: a framework for exertion games;2011.0;[];Exertion games require investing physical effort. The fact that such games can support physical health is tempered by our limited understanding of how to design for engaging exertion experiences. This paper introduces the Exertion Framework as a way to think and talk about Exertion Games, both for their formative design and summative analysis. Our Exertion Framework is based on the ways in which we can conceive of the body investing in game-directed exertion, supported by four perspectives on the body (the Responding Body, Moving Body, Sensing Body and Relating Body) and three perspectives on gaming (rules, play and context). The paper illustrates how this framework was derived from prior systems and theory, and presents a case study of how it has been used to inspire novel exertion interactions.
1967005434;Voronoi diagrams—a survey of a fundamental geometric data structure;1991.0;[];
1968135525;Age-related changes in grey and white matter structure throughout adulthood.;2010.0;[];Normal ageing is associated with gradual brain atrophy. Determining spatial and temporal patterns of change can help shed light on underlying mechanisms. Neuroimaging provides various measures of brain structure that can be used to assess such age-related change but studies to date have typically considered single imaging measures. Although there is consensus on the notion that brain structure deteriorates with age, evidence on the precise time course and spatial distribution of changes is mixed. We assessed grey matter (GM) and white matter (WM) structure in a group of 66 adults aged between 23 and 81. Multimodal imaging measures included voxel-based morphometry (VBM)-style analysis of GM and WM volume and diffusion tensor imaging (DTI) metrics of WM microstructure. We found widespread reductions in GM volume from middle age onwards but earlier reductions in GM were detected in frontal cortex. Widespread age-related deterioration in WM microstructure was detected from young adulthood onwards. WM decline was detected earlier and more sensitively using DTI-based measures of microstructure than using markers of WM volume derived from conventional T1-weighted imaging.
1968471282;Frontal circuitry degradation marks healthy adult aging: Evidence from diffusion tensor imaging;2005.0;[];"   In vivo study of white matter microstructural integrity through magnetic resonance diffusion tensor imaging (DTI) permits examination of degradation of axonal circuitry that may underlie functional decline of frontally-based processes in normal adult aging. Determination of the pattern of age-related degradation of white matter microstructure requires quantitative comparison of the rostral–caudal and superior–inferior extents of the brainu0027s white matter. To date, this has not been accomplished, probably because of significant artifacts from spatial distortion and poor signal resolution that precludes accurate analysis in prefrontal and inferior brain regions. Here, we report a profile analysis of the integrity of white matter microstructure across the supratentorium and in selected focal regions using DTI data collected at high-field strength (3 T), with isotropic voxel acquisition, and an analysis based on a concurrently-acquired field map to permit accurate quantification of artifact-prone, anterior and inferior brain regions. The groups comprised 10 younger and 10 older individuals; all were high functioning, highly educated, and in excellent health. The DTI profile analysis revealed a robust frontal distribution of low white matter anisotropy and high bulk mean diffusivity in healthy older compared with younger adults. In contrast to frontal fiber systems, posterior systems were largely preserved with age. A second analysis, based on focal samples of FA, confirmed that the age-related FA decline was restricted to frontal regions, leaving posterior and inferior brain regions relatively intact. The selective decline of anterior anisotropy with advancing age provides evidence for the potential of a microstructural white matter mechanism for the commonly observed decline in frontally-based functions."
1968703923;Perception of French vowels by American English adults with and without French language experience;2008.0;[];   This study investigated the effects of language experience and consonantal context on American English (AE) listeners’ discrimination of contrasts involving Parisian French vowels /y, œ, u, i/. Vowels were produced in /rabVp/ and /rabVt/ nonsense disyllables in carrier phrases by 3 speakers and presented in a categorial AXB discrimination task. Two groups were tested: AE listeners who had studied French extensively beginning after age 13 (Exp) and non-French-speaking AE listeners (Inexp). The Exp group performed better than the Inexp group on /u-œ/, /i-y/ and /y-œ/ (mean errors: Exp=5%, Inexp=24%). However, for /u-y/, the groups did not differ (Exp=30% vs Inexp=24% errors). The Inexp group confused /i-y/ more often in bilabial context, but /u-y/ more often in alveolar context, whereas the Exp group confused /u-y/ in both contexts. Overall, the Inexp group performed better in bilabial than in alveolar context (16% vs 32% errors), whereas the Exp group revealed no context effect. Results suggest that learning a second language (L2) includes learning its coarticulatory rules. Implications for models of L2-speech perception are discussed.
1968708670;A Gray Code for Compositions;1982.0;[];   The paper describes a rapid algorithm for the sequential listing of the compositions of  n  into  k  parts.
1969007958;Scenarios and Policy Aggregation in Optimization Under Uncertainty;1991.0;[];A common approach in coping with multiperiod optimization problems under uncertainty where statistical information is not really enough to support a stochastic programming model, has been to set up and analyze a number of scenarios. The aim then is to identify trends and essential features on which a robust decision policy can be based. This paper develops for the first time a rigorous algorithmic procedure for determining such a policy in response to any weighting of the scenarios. The scenarios are bundled at various levels to reflect the availability of information, and iterative adjustments are made to the decision policy to adapt to this structure and remove the dependence on hindsight.
1969731303;Dirac-type results for loose Hamilton cycles in uniform hypergraphs;2010.0;[];A classic result of G.A. Dirac in graph theory asserts that for nu003e=3 every n-vertex graph with minimum degree at least n/2 contains a spanning (so-called Hamilton) cycle. G.Y. Katona and H.A. Kierstead suggested a possible extension of this result for k-uniform hypergraphs. There a Hamilton cycle of an n-vertex hypergraph corresponds to an ordering of the vertices such that every k consecutive (modulo n) vertices in the ordering form an edge. Moreover, the minimum degree is the minimum (k-1)-degree, i.e. the minimum number of edges containing a fixed set of k-1 vertices. V. Rodl, A. Rucinski, and E. Szemeredi verified (approximately) the conjecture of Katona and Kierstead and showed that every n-vertex, k-uniform hypergraph with minimum (k-1)-degree (1/2+o(1))n contains such a tight Hamilton cycle. We study the similar question for Hamilton @?-cycles. A Hamilton @?-cycle in an n-vertex, k-uniform hypergraph (1=u003c@?
1970150866;Digital Pattern Recognition by Moments;1962.0;[];
1970167670;Correction of distortion in flattened representations of the cortical surface allows prediction of V1-V3 functional organization from anatomy.;2014.0;[];Several domains of neuroscience offer map-like models that link location on the cortical surface to properties of sensory representation. Within cortical visual areas V1, V2, and V3, algebraic transformations can relate position in the visual field to the retinotopic representation on the flattened cortical sheet. A limit to the practical application of this structure-function model is that the cortex, while topologically a two-dimensional surface, is curved. Flattening of the curved surface to a plane unavoidably introduces local geometric distortions that are not accounted for in idealized models. Here, we show that this limitation is overcome by correcting the geometric distortion induced by cortical flattening. We use a mass-spring-damper simulation to create a registration between functional MRI retinotopic mapping data of visual areas V1, V2, and V3 and an algebraic model of retinotopy. This registration is then applied to the flattened cortical surface anatomy to create an anatomical template that is linked to the algebraic retinotopic model. This registered cortical template can be used to accurately predict the location and retinotopic organization of these early visual areas from cortical anatomy alone. Moreover, we show that prediction accuracy remains when extrapolating beyond the range of data used to inform the model, indicating that the registration reflects the retinotopic organization of visual cortex. We provide code for the mass-spring-damper technique, which has general utility for the registration of cortical structure and function beyond the visual cortex.
1970242638;Brain–Computer Interfaces Using Sensorimotor Rhythms: Current State and Future Perspectives;2014.0;[];Many studies over the past two decades have shown that people can use brain signals to convey their intent to a computer using brain-computer interfaces (BCIs). BCI systems extract specific features of brain activity and translate them into control signals that drive an output. Recently, a category of BCIs that are built on the rhythmic activity recorded over the sensorimotor cortex, i.e., the sensorimotor rhythm (SMR), has attracted considerable attention among the BCIs that use noninvasive neural recordings, e.g., electroencephalography (EEG), and have demonstrated the capability of multidimensional prosthesis control. This paper reviews the current state and future perspectives of SMR-based BCI and its clinical applications, in particular focusing on the EEG SMR. The characteristic features of SMR from the human brain are described and their underlying neural sources are discussed. The functional components of SMR-based BCI, together with its current clinical applications, are reviewed. Finally, limitations of SMR-BCIs and future outlooks are also discussed.
1970686984;Data fusion of radar and image measurements for multi-object tracking via Kalman filtering;2014.0;[];Data fusion is an important issue for object tracking in autonomous systems such as robotics and surveillance. In this paper, we present a multiple-object tracking system whose design is based on multiple Kalman filters dealing with observations from two different kinds of physical sensors. Hardware integration which combines a cheap radar module and a CCD camera has been developed and data fusion method has been proposed to process measurements from those modules for multi-object tracking. Due to the limited resolution of bearing angle measurements of the cheap radar module, CCD measurements are used to compensate for the low angle resolution. Conversely, the radar module provides radial distance information which cannot be measured easily by the CCD camera. The proposed data fusion enables the tracker to efficiently utilize the radial measurements of objects from the cheap radar module and 2D location measurements of objects in image space of the CCD camera. To achieve the multi-object tracking we combine the proposed data fusion method with the integrated probability data association (IPDA) technique underlying the multiple-Kalman filter framework. The proposed complementary system based on the radar and CCD camera is experimentally evaluated through a multi-person tracking scenario. The experimental results demonstrate that the implemented system with fused observations considerably enhances tracking performance over a single sensor system.
1970732620;Storage assignment problem with travel distance and blocking considerations for a picker-to-part order picking system;2012.0;[];"Order picking is a key operation in managing a warehouse efficiently. Most previous studies on picking only considered single-picker operation; however, many pickers frequently work concurrently in the same region. Since congestion may occur in such a multi-picker system, waiting time must be taken into account together with travel time and distance when evaluating the efficiency of picking operations. The picking model under investigation can be formulated as a queueing network, and a heuristic storage assignment policy that considers both the travel time and the waiting time simultaneously by minimizing the average order fulfillment time is developed in the paper. An approximation method and a simulation model using eM-plant software are presented to implement the proposed heuristic algorithm and to compare the mean travel time for different storage assignment polices as well. The results indicate that the proposed heuristic policy outperforms existing storage assignment policies in a multi-picker warehouse environment."
1970919575;Optimal assignments in a markovian queueing system;1981.0;[];   A generalization of the Hypercube queueing model for exponential queueing systems is presented which allows for distinguishable servers and multiple types of customers. Given costs associated with each server-customer pair, the determination of the assignment policy which minimizes time-averaged costs is formulated as a Markov decision problem. A characterization of optimal policies is obtained and used in an efficient algorithm for determining the optimum. The algorithm combines the method of successive approximations and “Howardu0027s method” in a manner which is particularly applicable to Markov decision problems having large, sparse transition matrices.
1971079746;New versions of Suen's correlation inequality;1998.0;[];
1971554611;An analysis of the equilibrium of migration models for biogeography-based optimization;2010.0;[];Motivated by the migration mechanisms of ecosystems, various extensions to biogeography-based optimization (BBO) are proposed here. As a global optimization method, BBO is an original algorithm based on the mathematical model of organism distribution in biological systems. BBO is an evolutionary process that achieves information sharing by biogeography-based migration operators. In BBO, habitats represent candidate problem solutions, and species migration represents the sharing of features between candidate solutions according to the fitness of the habitats. This paper generalizes equilibrium species count results in biogeography theory, explores the behavior of six different migration models in BBO, and investigates performance through 23 benchmark functions with a wide range of dimensions and diverse complexities. The performance study shows that sinusoidal migration curves provide the best performance among the six different models that we explored. In addition, comparison with other biology-based optimization algorithms is investigated, and the influence of the population size, problem dimension, mutation rate, and maximum migration rate of BBO are also studied.
1971788036;Exploiting social preferences for congestion control in opportunistic networks;2014.0;[];It has been observed that opportunistic networks exhibit a highly unbalanced traffic load distribution, mainly because of the heterogeneity in mobility and the greedy routing decisions, leading to packet drops due to storage constraints. The existing strategies rely either on fairness techniques or on diverting traffic to alternative routes in order to control congestion. The result is a dilemma between performance and fairness. In this work, we introduce a congestion control mechanism that provides a tunable trade-off between efficiency and fairness. We rely on the social preferences of the nodes for dynamically tuning the aforementioned trade-off. Our simulations show that the proposed algorithm achieves high delivery ratio, combined with low end-to-end delay and routing cost, without sacrificing fairness under high traffic load.
1971875039;Intuitionistic Fuzzy Aggregation Operators;2007.0;[];An intuitionistic fuzzy set, characterized by a membership function and a non-membership function, is a generalization of fuzzy set. In this paper, based on score function and accuracy function, we introduce a method for the comparison between two intuitionistic fuzzy values and then develop some aggregation operators, such as the intuitionistic fuzzy weighted averaging operator, intuitionistic fuzzy ordered weighted averaging operator, and intuitionistic fuzzy hybrid aggregation operator, for aggregating intuitionistic fuzzy values and establish various properties of these operators.
1971945294;Tabu search for the redundancy allocation problem of homogenous series–parallel multi-state systems;2008.0;[];   This paper develops an efficient tabu search (TS) heuristic to solve the redundancy allocation problem for multi-state series–parallel systems. The system has a range of performance levels from perfect functioning to complete failure. Identical redundant elements are included in order to achieve a desirable level of availability. The elements of the system are characterized by their cost, performance and availability. These elements are chosen from a list of products available in the market. System availability is defined as the ability to satisfy consumer demand, which is represented as a piecewise cumulative load curve. A universal generating function technique is applied to evaluate system availability. The proposed TS heuristic determines the minimal cost system configuration under availability constraints. An originality of our approach is that it proceeds by dividing the search space into a set of disjoint subsets, and then by applying TS to each subset. The design problem, solved in this study, has been previously analyzed using genetic algorithms (GAs). Numerical results for the test problems from previous research are reported, and larger test problems are randomly generated. Comparisons show that the proposed TS out-performs GA solutions, in terms of both the solution quality and the execution time.
1972670182;REVIEW OF SUMMATION-BY-PARTS SCHEMES FOR INITIAL-BOUNDARY-VALUE PROBLEMS;2014.0;[];High-order finite difference methods are efficient, easy to program, scale well in multiple dimensions and can be modified locally for various reasons (such as shock treatment for example). The main drawback has been the complicated and sometimes even mysterious stability treatment at boundaries and interfaces required for a stable scheme. The research on summation-by-parts operators and weak boundary conditions during the last 20 years has removed this drawback and now reached a mature state. It is now possible to construct stable and high order accurate multi-block finite difference schemes in a systematic building-block-like manner. In this paper we will review this development, point out the main contributions and speculate about the next lines of research in this area.
1973179696;A novel hybrid multi-objective immune algorithm with adaptive differential evolution;2015.0;[];In this paper, we propose a novel hybrid multi-objective immune algorithm with adaptive differential evolution, named ADE-MOIA, in which the introduction of differential evolution (DE) into multi-objective immune algorithm (MOIA) combines their respective advantages and thus enhances the robustness to solve various kinds of MOPs. In ADE-MOIA, in order to effectively cooperate DE with MOIA, we present a novel adaptive DE operator, which includes a suitable parent selection strategy and a novel adaptive parameter control approach. When performing DE operation, two parents are respectively picked from the current evolved and dominated population in order to provide a correct evolutionary direction. Moreover, based on the evolutionary progress and the success rate of offspring, the crossover rate and scaling factor in DE operator are adaptively varied for each individual. The proposed adaptive DE operator is able to improve both of the convergence speed and population diversity, which are validated by the experimental studies. When comparing ADE-MOIA with several nature-inspired heuristic algorithms, such as NSGA-II, SPEA2, AbYSS, MOEA/D-DE, MIMO and D2MOPSO, simulations show that ADE-MOIA performs better on most of 21 well-known benchmark problems. Differential evolution is embedded into the multi-objective immune algorithm.A suitable parent selection strategy provides a correct evolutionary direction.A novel adaptive control approach enhances the algorithmic robustness.
1973266184;Tackling the Load Uncertainty Challenges for Energy Consumption Scheduling in Smart Grid;2013.0;[];In this paper, we propose a novel optimization-based real-time residential load management algorithm that takes into account load uncertainty in order to minimize the energy payment for each user. Unlike most existing demand side management algorithms that assume perfect knowledge of usersu0027 energy needs, our design only requires knowing some statistical estimates of the future load demand. Moreover, we consider real-time pricing combined with inclining block rate tariffs. In our problem formulation, we take into account different types of constraints on the operation of different appliances such as must-run appliances, controllable appliances that are interruptible, and controllable appliances that are not interruptible. Our design is multi-stage. As the demand information of the appliances is gradually revealed over time, the operation schedule of controllable appliances is updated accordingly. Simulation results confirm that the proposed energy consumption scheduling algorithm can benefit both users, by reducing their energy expenses, and utility companies, by improving the peak-to-average ratio of the aggregate load demand.
1973469080;A study on the transitivity of probabilistic and fuzzy relations;2011.0;[];Given a set of alternatives we consider a fuzzy relation and a probabilistic relation defined on such a set. We investigate the relation between the T-transitivity of the fuzzy relation and the cycle-transitivity of the associated probabilistic relation. We provide a general result, valid for any t-norm and we later provide explicit expressions for important particular cases. We also apply the results obtained to explore the transitivity satisfied by the probabilistic relation defined on a set of random variables. We focus on uniform continuous random variables.
1973956316;ComSoc: adaptive transfer of user behaviors over composite social network;2012.0;[];"Accurate prediction of user behaviors is important for many social media applications, including social marketing, personalization and recommendation, etc. A major challenge lies in that, the available behavior data or interactions between users and items in a given social network are usually very limited and sparse (e.g., u003e= 99.9% empty). Many previous works model user behavior from only historical user logs. We observe that many people are members of several social networks in the same time, such as Facebook, Twitter and Tencentu0027s QQ. Importantly, their behaviors and interests in different networks influence one another. This gives us an opportunity to leverage the knowledge of user behaviors in different networks, in order to alleviate the data sparsity problem, and enhance the predictive performance of user modeling. Combining different networks ""simply and naively"" does not work well. Instead, we formulate the problem to model multiple networks as ""composite network knowledge transfer"". We first select the most suitable networks inside a composite social network via a hierarchical Bayesian model, parameterized for individual users, and then build topic models for user behavior prediction using both the relationships in the selected networks and related behavior data. To handle big data, we have implemented the algorithm using Map/Reduce. We demonstrate that the proposed composite network-based user behavior model significantly improve the predictive accuracy over a number of existing approaches on several real world applications, such as a very large social-networking dataset from Tencent Inc."
1974140471;Opportunistic Routing in Intermittently Connected Mobile P2P Networks;2013.0;[];Mobile P2P networking is an enabling technology for mobile devices to self-organize in an unstructured style and communicate in a peer-to-peer fashion. Due to user mobility and/or the unrestricted switching on/off of the mobile devices, links are intermittently connected and end-to-end paths may not exist, causing routing a very challenging problem. Moreover, the limited wireless spectrum and device resources together with the rapidly growing number of portable devices and amount of transmitted data make routing even harder. To tackle these challenges, the routing algorithms must be scalable, distributed, and light-weighted. Nevertheless, existing approaches usually cannot simultaneously satisfy all these three requirements. In this paper, we propose two opportunistic routing algorithms for intermittently connected mobile P2P networks, which exploit the spatial locality, spatial regularity, and activity heterogeneity of human mobility to select relays. The first algorithm employs a depth-search approach to diffuse the data towards the destination. The second one adopts a depth-width-search approach in a sense that it diffuses the data not only towards the destination but also to other directions determined by the actively moving nodes (activists) to find better relays. We perform both theoretical analysis as well as a comparison based simulation study. Our results obtained from both the synthetic data and the real world traces reveal that the proposed algorithms outperform the state-of-the-art in terms of delivery latency and delivery ratio.
1974627246;Speeding up the Xbox recommender system using a euclidean transformation for inner-product spaces;2014.0;[];A prominent approach in collaborative filtering based recommender systems is using dimensionality reduction (matrix factorization) techniques to map users and items into low-dimensional vectors. In such systems, a higher inner product between a user vector and an item vector indicates that the item better suits the useru0027s preference. Traditionally, retrieving the most suitable items is done by scoring and sorting all items. Real world online recommender systems must adhere to strict response-time constraints, so when the number of items is large, scoring all items is intractable.   We propose a novel order preserving transformation, mapping the maximum inner product search problem to Euclidean space nearest neighbor search problem. Utilizing this transformation, we study the efficiency of several (approximate) nearest neighbor data structures. Our final solution is based on a novel use of the PCA-Tree data structure in which results are augmented using paths one hamming distance away from the query (neighborhood boosting). The end result is a system which allows approximate matches (items with relatively high inner product, but not necessarily the highest one). We evaluate our techniques on two large-scale recommendation datasets, Xbox Movies and Yahoo~Music, and show that this technique allows trading off a slight degradation in the recommendation quality for a significant improvement in the retrieval time.
1974637957;Automated model selection in covariance estimation and spatial whitening of MEG and EEG signals.;2015.0;[];   Magnetoencephalography and electroencephalography (M/EEG) measure non-invasively the weak electromagnetic fields induced by post-synaptic neural currents. The estimation of the spatial covariance of the signals recorded on M/EEG sensors is a building block of modern data analysis pipelines. Such covariance estimates are used in brain–computer interfaces (BCI) systems, in nearly all source localization methods for spatial whitening as well as for data covariance estimation in beamformers. The rationale for such models is that the signals can be modeled by a zero mean Gaussian distribution. While maximizing the Gaussian likelihood seems natural, it leads to a covariance estimate known as empirical covariance (EC). It turns out that the EC is a poor estimate of the true covariance when the number of samples is small. To address this issue the estimation needs to be regularized. The most common approach downweights off-diagonal coefficients, while more advanced regularization methods are based on shrinkage techniques or generative models with low rank assumptions: probabilistic PCA (PPCA) and factor analysis (FA). Using cross-validation all of these models can be tuned and compared based on Gaussian likelihood computed on unseen data.  We investigated these models on simulations, one electroencephalography (EEG) dataset as well as magnetoencephalography (MEG) datasets from the most common MEG systems. First, our results demonstrate that different models can be the best, depending on the number of samples, heterogeneity of sensor types and noise properties. Second, we show that the models tuned by cross-validation are superior to models with hand-selected regularization. Hence, we propose an automated solution to the often overlooked problem of covariance estimation of M/EEG signals. The relevance of the procedure is demonstrated here for spatial whitening and source localization of MEG signals.
1974642802;Visual analysis of large dental imaging data in caries research;2014.0;[];With dental imaging data acquired at unprecedented speed and resolution, traditional serial image processing and single-node storage need to be re-examined in a “BigData” context. Furthermore, most previous dental computing has focused on the actual imaging acquisition and image analysis tools, while much less research has focused on enabling caries assessment via visual analysis of large dental imaging data. In this paper we present DENVIS, an end-to-end solution for cariologists to manage, mine, visualize, and analyze large dental imaging data for investigative carious lesion studies. DENVIS consists of two main parts: data driven image analysis modules triggered by imaging data acquisition that exploit parallel MapReduce tasks and ingest visualization archive into a distributed NoSQL store, and user driven modules that allow investigative analysis at run time. DENVIS has seen early use by our collaborators in oral health research, where our system has been used to pose and answer domain-specific questions for quantitative assessment of dynamic carious lesion activities.
1974682889;BIG-ALIGN: Fast Bipartite Graph Alignment;2013.0;[];How can we find the virtual twin (i.e., the same or similar user) on Linked In for a user on Facebook? How can we effectively link an information network with a social network to support cross-network search? Graph alignment - the task of finding the node correspondences between two given graphs - is a fundamental building block in numerous application domains, such as social networks analysis, bioinformatics, chemistry, pattern recognition. In this work, we focus on aligning bipartite graphs, a problem which has been largely ignored by the extensive existing work on graph matching, despite the ubiquity of those graphs (e.g., users-groups network). We introduce a new optimization formulation and propose an effective and fast algorithm to solve it. We also propose a fast generalization of our approach to align unipartite graphs. The extensive experimental evaluations show that our method outperforms the state-of-art graph matching algorithms in both alignment accuracy and running time, being up to 10x more accurate or 174x faster on real graphs.
1974903261;Updating a credit-scoring model based on new attributes without realization of actual data;2014.0;[];Funding small and medium-sized enterprises (SMEs) to support technological innovation is critical for national competitiveness. Technology credit scoring models are required for the selection of appropriate funding beneficiaries. Typically, a technology credit-scoring model consists of several attributes and new models must be derived every time these attributes are updated. However, it is not feasible to develop new models until sufficient historical evaluation data based on these new attributes will have accumulated. In order to resolve this limitation, we suggest the framework to update the technology credit scoring model. This framework consists of ways to construct new technology credit-scoring model by comparing alternative scenarios for various relationships between existing and new attributes based on explanatory factor analysis, analysis of variance, and logistic regression. Our approach can contribute to find the optimal scenario for updating a scoring model.
1975118601;The eigenvalue problem for a singular higher order fractional differential equation involving fractional derivatives;2012.0;[];   In this paper, we study the following singular eigenvalue problem for a higher order fractional differential equation           -    D    α    x  (  t  )  =  λ  f  (  x  (  t  )  ,    D      μ    1      x  (  t  )  ,    D      μ    2      x  (  t  )  ,  …  ,    D      μ    n  -  1      x  (  t  )  )  ,    0    t    1  ,      x  (  0  )  =  0  ,      D      μ    i      x  (  0  )  =  0  ,      D    μ    x  (  1  )  =    ∑    j  =  1    p  -  2      a    j      D    μ    x  (    ξ    j    )  ,    1  ⩽  i  ⩽  n  -  1  ,           where     n  ≥  3  ,    n  ∈  N    ,     n  -  1    α  ⩽  n  ,    n  -  l  -  1    α  -    μ    l      n  -  l    , for     l  =  1  ,  2  ,  …  ,  n  -  2    , and     μ  -    μ    n  -  1    u003e  0  ,    α  -    μ    n  -  1    ≤  2  ,    α  -  μ  u003e  1    ,       a    j    ∈  [  0  ,  +  ∞  )  ,    0      ξ    1        ξ    2      ⋯      ξ    p  -  2      1    ,     0      ∑    j  =  1    p  -  2      a    j      ξ    j    α  -  μ  -  1      1    ,       D    α       is the standard Riemann–Liouville derivative, and     f  :  (  0  ,  +  ∞    )    n    →  [  0  ,  +  ∞  )     is continuous. Firstly, we give the Green function and its properties. Then we established an eigenvalue interval for the existence of positive solutions from Schauder’s fixed point theorem and the upper and lower solutions method. The interesting point of this paper is that     f     may be singular at       x    i    =  0  ,     for     i  =  1  ,  2  ,  …  ,  n    .
1975580029;Developing New Fitness Functions in Genetic Programming for Classification With Unbalanced Data;2012.0;[];Machine learning algorithms such as genetic programming (GP) can evolve biased classifiers when data sets are unbalanced. Data sets are unbalanced when at least one class is represented by only a small number of training examples (called the minority class) while other classes make up the majority. In this scenario, classifiers can have good accuracy on the majority class but very poor accuracy on the minority class(es) due to the influence that the larger majority class has on traditional training criteria in the fitness function. This paper aims to both highlight the limitations of the current GP approaches in this area and develop several new fitness functions for binary classification with unbalanced data. Using a range of real-world classification problems with class imbalance, we empirically show that these new fitness functions evolve classifiers with good performance on both the minority and majority classes. Our approaches use the original unbalanced training data in the GP learning process, without the need to artificially balance the training examples from the two classes (e.g., via sampling).
1976409582;Establishing Trust in Hybrid Cloud Computing Environments;2011.0;[];Establishing trust for resource sharing and collaboration has become an important issue in distributed computing environment. In this paper, we investigate the problem of establishing trust in hybrid cloud computing environments. As the scope of federated cloud computing enlarges to ubiquitous and pervasive computing, there will be a need to assess and maintain the trustworthiness of the cloud computing entities. We present a fully distributed framework that enable trust-based cloud customer and cloud service provider interactions. The framework aids a service consumer in assigning an appropriate weight to the feedback of different raters regarding a prospective service provider. Based on the framework, we developed a mechanism for controlling falsified feedback ratings from iteratively exerting trust level contamination due to falsified feedback ratings. The experimental analysis shows that the proposed framework successfully dilutes the effects of falsified feedback ratings, thereby facilitating accurate and fair assessment of the service reputations.
1976620775;Historiographic Mapping of Knowledge Domains Literature;2004.0;[];To better understand the topic of this colloquium, we have created a series of databases related to knowledge domains (dynamic systems [small world/Milgram], information visualization [Tufte], co-citation [Small], bibliographic coupling [Kessler], and scientometrics [Scientometrics]). I have used a software package called HistCiteTM which generates chronological maps of subject (topical) collections resulting from searches of the ISI Web of Science1 or ISI citation indexes (SCI, SSCI, and/or AHCI) on CD-ROM. When a marked list is created on WoS, an export file is created which contains all cited references for each source document captured. These bibliographic collections, saved as ASCII files, are processed by HistCite in order to generate chronological and other tables as well as historiographs which highlight the most-cited works in and outside the collection. HistCite also includes a module for detecting and editing errors or variations in cited references as well as a vocabulary analyzer which genera...
1977186065;Biogeography-based optimization with improved migration operator and self-adaptive clear duplicate operator;2014.0;[];Biogeography-based optimization (BBO) is a new emerging population-based algorithm that has been shown to be competitive with other evolutionary algorithms. However, there are some insufficiencies in solving complex problems, such as poor population diversity and slow convergence speed in the later stage. To overcome these shortcomings, we propose an improved BBO (IBBO) algorithm integrating a new improved migration operator, Gaussian mutation operator, and self-adaptive clear duplicate operator. The improved migration operator simultaneously adopts more information from other habitats, maintains population diversity, and preserves exploitation ability. The self-adaptive clear duplicate operator can clear duplicate or almost identical habitats, while also preserving population diversity through a self-adaptation threshold within the evolution process. Simulation results and comparisons from the experimental tests conducted on 23 benchmark functions show that IBBO achieves excellent performance in solving complex problems compared with other variants of the BBO algorithm and other evolutionary algorithms. The performance of the improved migration operator is also discussed.
1977367431;Cloud computing and emerging IT platforms: Vision, hype, and reality for delivering computing as the 5th utility;2009.0;[];With the significant advances in Information and Communications Technology (ICT) over the last half century, there is an increasingly perceived vision that computing will one day be the 5th utility (after water, electricity, gas, and telephony). This computing utility, like all other four existing utilities, will provide the basic level of computing service that is considered essential to meet the everyday needs of the general community. To deliver this vision, a number of computing paradigms have been proposed, of which the latest one is known as Cloud computing. Hence, in this paper, we define Cloud computing and provide the architecture for creating Clouds with market-oriented resource allocation by leveraging technologies such as Virtual Machines (VMs). We also provide insights on market-based resource management strategies that encompass both customer-driven service management and computational risk management to sustain Service Level Agreement (SLA)-oriented resource allocation. In addition, we reveal our early thoughts on interconnecting Clouds for dynamically creating global Cloud exchanges and markets. Then, we present some representative Cloud platforms, especially those developed in industries, along with our current work towards realizing market-oriented resource allocation of Clouds as realized in Aneka enterprise Cloud technology. Furthermore, we highlight the difference between High Performance Computing (HPC) workload and Internet-based services workload. We also describe a meta-negotiation infrastructure to establish global Cloud exchanges and markets, and illustrate a case study of harnessing u0027Storage Cloudsu0027 for high performance content delivery. Finally, we conclude with the need for convergence of competing IT paradigms to deliver our 21st century vision.
1977396216;Differential callosal contributions to bimanual control in young and older adults;2011.0;[];Our recent work has shown that older adults are disproportionately impaired at bimanual tasks when the two hands are moving out of phase with each other [Bangert, A. S., Reuter-Lorenz, P. A., Walsh, C. M., Schachter, A. B., u0026 Seidler, R. D. Bimanual coordination and aging: Neurobehavioral implications. Neuropsychologia, 48, 1165-1170, 2010]. Interhemispheric interactions play a key role during such bimanual movements to prevent interference from the opposite hemisphere. Declines in corpus callosum (CC) size and microstructure with advancing age have been well documented, but their contributions to age deficits in bimanual function have not been identified. In the current study, we used structural magnetic resonance and diffusion tensor imaging to investigate age-related changes in the relationships between callosal macrostructure, microstructure, and motor performance on tapping tasks requiring differing degrees of interhemispheric interaction. We found that older adults demonstrated disproportionately poorer performance on out-of-phase bimanual control, replicating our previous results. In addition, older adults had smaller anterior CC size and poorer white matter integrity in the callosal midbody than their younger counterparts. Surprisingly, larger CC size and better integrity of callosal microstructure in regions connecting sensorimotor cortices were associated with poorer motor performance on tasks requiring high levels of interhemispheric interaction in young adults. Conversely, in older adults, better performance on these tasks was associated with larger size and better CC microstructure integrity within the same callosal regions. These findings implicate age-related declines in callosal size and integrity as a key contributor to bimanual control deficits. Further, the differential age-related involvement of transcallosal pathways reported here raises new questions about the role of the CC in bimanual control.
1977803996;Fractional programming methodology for multi-attribute group decision-making using IFS;2009.0;[];Owing to more vague concepts frequently represented in decision data, mathematical objects introduced by K.T. Atanassov and studied under the name u0027u0027intuitionistic fuzzy setu0027u0027 (IFS) are more flexibly used to model real-life decision situations. The aim of this paper is to develop a new methodology for solving multi-attribute group decision-making problems using IFS, in which multiple attributes are explicitly considered. In this methodology, for each decision maker in the group two auxiliary fractional programming models are derived from the TOPSIS to determine the relative closeness coefficient intervals of alternatives, which are aggregated for the group to generate the ranking order of all alternatives by computing their optimal degrees of membership based on the ranking method of interval numbers. The implementation process of the method proposed in this paper is illustrated with a numerical example.
1978030011;Fast and unified local search for random walk based k-nearest-neighbor query in large graphs;2014.0;[];Given a large graph and a query node, finding its k-nearest-neighbor (kNN) is a fundamental problem. Various random walk based measures have been developed to measure the proximity (similarity) between nodes. Existing algorithms for the random walk based top-k proximity search can be categorized as global and local methods based on their search strategies. Global methods usually require an expensive precomputing step. By only searching the nodes near the query node, local methods have the potential to support more efficient query. However, most existing local search methods cannot guarantee the exactness of the solution. Moreover, they are usually designed for specific proximity measures. Can we devise an efficient local search method that applies to different measures and also guarantees result exactness? In this paper, we present FLoS (Fast Local Search), a unified local search method for efficient and exact top-k proximity query in large graphs. FLoS is based on the no local optimum property of proximity measures. We show that many measures have no local optimum. Utilizing this property, we introduce several simple operations on transition probabilities, which allow developing lower and upper bounds on the proximity. The bounds monotonically converge to the exact proximity when more nodes are visited. We further show that FLoS can also be applied to measures having local optimum by utilizing relationship among different measures. We perform comprehensive experiments to evaluate the efficiency and applicability of the proposed method.
1978038786;Managing Capacity and Flow at Theme Parks;1997.0;[];"The growth of service industries and their impact on the U.S. economy have attracted considerable attention in recent years. While some service sectors, most notably airline and telecommunication industries, have been in the forefront of model development, the industry is rather fragmented, and similar rigor is lacking in most other  :[50],""paper describes an application of a model-based approach to some of the short-term ride capacity and visitor flow issues faced by the Six Flags Magic Mountain SFMM, a major national theme park. Specifically, we consider daily operations at the theme park and focus on the generation and evaluation of alternative strategies for managing ride capacities and visitor flow. Management of demand involves two aspects: a understanding customer preferences as revealed by routing behavior, and b using the model to evaluate the implications of changes in  :[136],""crucial component of the study relates to the empirical data collected. Besides verifying the validity of the models, these data provide several insights for developing schemes to manage the day-to-day operations of the park. The SFMM management was actively involved in various phases of this study and as a result has been introducing the proposed models in a phased manner."
1978062397;Hesitant Fuzzy Linguistic Term Sets for Decision Making;2012.0;[];"Dealing with uncertainty is always a challenging problem, and different tools have been proposed to deal with it. Recently, a new model that is based on hesitant fuzzy sets has been presented to manage situations in which experts hesitate between several values to assess an indicator, alternative, variable, etc. Hesitant fuzzy sets suit the modeling of quantitative settings; however, similar situations may occur in qualitative settings so that experts think of several possible linguistic values or richer expressions than a single term for an indicator, alternative, variable, etc. In this paper, the concept of a hesitant fuzzy linguistic term set is introduced to provide a linguistic and computational basis to increase the richness of linguistic elicitation based on the fuzzy linguistic approach and the use of context-free grammars by using comparative terms. Then, a multicriteria linguistic decision-making model is presented in which experts provide their assessments by eliciting linguistic expressions. This decision model manages such linguistic expressions by means of its representation using hesitant fuzzy linguistic term sets."
1978169406;Multiple-View Multiple-Learner Semi-Supervised Learning;2011.0;[];Some recent successful semi-supervised learning methods construct more than one learner from both labeled and unlabeled data for inductive learning. This paper proposes a novel multiple-view multiple-learner (MVML) framework for semi-supervised learning, which differs from previous methods in possession of both multiple views and multiple learners. This method adopts a co-training styled learning paradigm in enlarging labeled data from a much larger set of unlabeled data. To the best of our knowledge it is the first attempt to combine the advantages of multiple-view learning and ensemble learning for semi-supervised learning. The use of multiple views is promising to promote performance compared with single-view learning because information is more effectively exploited. At the same time, as an ensemble of classifiers is learned from each view, predictions with higher accuracies can be obtained than solely adopting one classifier from the same view. Experiments on different applications involving both multiple-view and single-view data sets show encouraging results of the proposed MVML method.
1978255623;Cooperative Positioning for Vehicular Networks: Facts and Future;2013.0;[];Intelligent transportation systems (ITSs) are increasingly being considered to mitigate the impacts of road transportation, including road injuries, energy waste, and environmental pollution. Vehicular positioning is a fundamental part of many ITS applications. Although global navigation satellite systems (GNSSs), e.g., Global Positioning System (GPS), are applicable for navigation and fleet management, the accuracy and availability of GNSSs do not meet the requirements for some applications, including collision avoidance or lane-level positioning. Cooperative positioning (CP) based on vehicular communications is an approach to tackle these shortcomings. The applicability of vehicular CP techniques proposed in the literature is questionable due to viability issues, including internode distance estimation, which is an important part of many CP techniques. Conventional CP systems such as differential GPS (DGPS) and other augmentation systems are also effectively incapable of addressing the given ITS applications. In this paper, modern and conventional CP systems are discussed, and the viability of radio ranging/range rating and constraints of vehicular communications as main pieces of modern CP systems are investigated. The general performance boundaries for modern CP systems are explained, as is the gap existing between the positioning accuracy required for crucial ITS applications and what modern CP can provide. This is followed by introduction of a novel trend for vehicular CP research, which is a potential reliable solution using a modified concept of real-time kinematic (RTK) GPSs for vehicular environments.
1978332670;A Noncontact Capacitive Sensing System for Recognizing Locomotion Modes of Transtibial Amputees;2014.0;[];This paper presents a noncontact capacitive sensing system (C-Sens) for locomotion mode recognition of transtibial amputees. C-Sens detects changes in physical distance between the residual limb and the prosthesis. The sensing front ends are built into the prosthetic socket without contacting the skin. This novel signal source improves the usability of locomotion mode recognition systems based on electromyography (EMG) signals and systems based on capacitance signals obtained from skin contact. To evaluate the performance of C-Sens, we carried out experiments among six transtibial amputees with varying levels of amputation when they engaged in six common locomotive activities. The capacitance signals were consistent and stereotypical for different locomotion modes. Importantly, we were able to obtain sufficiently informative signals even for amputees with severe muscle atrophy (i.e., amputees lacking of quality EMG from shank muscles for mode classification). With phase-dependent quadratic classifier and selected feature set, the proposed system was capable of making continuous judgments about locomotion modes with an average accuracy of 96.3% and 94.8% for swing phase and stance phase, respectively (Experiment 1). Furthermore, the system was able to achieve satisfactory recognition performance after the subjects redonned the socket (Experiment 2). We also validated that C-Sens was robust to load bearing changes when amputees carried 5-kg weights during activities (Experiment 3). These results suggest that noncontact capacitive sensing is capable of circumventing practical problems of EMG systems without sacrificing performance and it is, thus, promising for automatic recognition of human motion intent for controlling powered prostheses.
1978342042;A technological acceptance of e-learning tools used in practical and laboratory teaching, according to the European higher education area;2008.0;[];The application of scientific tools to analyse the use of Internet-based e-learning tools in academic settings is in general an ignored area. E-learning tools are actually an emergent topic as a result of the new ideas introduced by the European Higher Education Area. Lifelong learning, or the promotion of student initiative, is the new paradigm of a learner-centred education. In this context, e-learning tools can represent an effective way of supporting this new trend in education. Assuming the premise that successful use of these web-based tools depends primarily on a useru0027s behaviour, the objective of this research is to examine the technology acceptance model (TAM) of web-based e-learning tools used in practical and laboratory teaching. The research hypotheses derived from this model have empirically been validated using the responses to a survey on e-learning usage among 220 users. These responses have been examined through partial least square. The obtained results strongly support the extended TAM in predicting a studentu0027s intention to use e-learning and define a set of external variables with a significant influence in the original TAM variables. Surprisingly, perceived ease of use did not posit a significant impact on student attitude or intention towards e-learning tool usage. Therefore, early evaluation of e-learning material is considered essential to providing a framework for further improvements of the tool.
1978394996;Term-weighting approaches in automatic text retrieval;1988.0;[];The experimental evidence accumulated over the past 20 years indicates that textindexing systems based on the assignment of appropriately weighted single terms produce retrieval results that are superior to those obtainable with other more elaborate text representations. These results depend crucially on the choice of effective term weighting systems. This paper summarizes the insights gained in automatic term weighting, and provides baseline single term indexing models with which other more elaborate content analysis procedures can be compared.
1978799369;Swoogle: a search and metadata engine for the semantic web;2004.0;[];Swoogle is a crawler-based indexing and retrieval system for the Semantic Web. It extracts metadata for each discovered document, and computes relations between documents. Discovered documents are also indexed by an information retrieval system which can use either character N-Gram or URIrefs as keywords to find relevant documents and to compute the similarity among a set of documents. One of the interesting properties we compute is ontology rank, a measure of the importance of a Semantic Web document.
1978829539;On the cycle-transitive comparison of artificially coupled random variables;2008.0;[];"Given a collection of random variables, we build a probabilistic relation that, in the case of continuous random variables, expresses for each couple of random variables the probability that the first one takes a greater value than the second one. In order to compute this probability, the random variables are artificially coupled by means of a fixed commutative copula. The main result of this paper pertains to the transitivity of this probabilistic relation. Provided the commutative copula satisfies some additional condition, this transitivity can be described elegantly within the cycle-transitivity framework. It ranges between two known types of transitivity: T""L-transitivity and partial stochastic transitivity."
1978953450;An Atanassov's intuitionistic Fuzzy Kernel Clustering for Medical Image segmentation;2014.0;[];AbstractThis paper suggests a novel method for medical image segmentation using kernel based Atanassovu0027s intuitionistic fuzzy clustering. The widely used fuzzy c means clustering that uses Euclidean distance has many limitations in clustering the regions accurately. To overcome these difficulties, we introduce a new method using Atanassovu0027s intuitionistic fuzzy set theory that incorporates a robust kernel based distance function. As the membership degrees are not precise and may contain hesitation, Sugeno type fuzzy complement is used to find the non-membership values and then hesitation degree is computed. The algorithm uses all the three kernels – Gaussian, radial basis, and hyper tangent kernels. In the algorithm, for each pixel, two features are considered - pixel energy and mean and the average of the two features are taken. The method clusters the tumors/lesions/clots almost accurately especially in a noisy environment. Experiments are performed on several noisy medical images and to assess the perf...
1979110822;On the second eigenvalue of hypergraphs;1995.0;[];
1980471025;Operators and Comparisons of Hesitant Fuzzy Linguistic Term Sets;2014.0;[];The theory of hesitant fuzzy linguistic term sets (HFLTSs) is very useful in objectively dealing with situations in which people are hesitant in providing linguistic assessments. The purpose of this paper is to develop comparison methods and study the aggregation theory for HFLTSs. We first define operations on HFLTSs and give possibility degree formulas for comparing HFLTSs. We then define two aggregation operators for HFLTSs: a hesitant fuzzy LWA operator and a hesitant fuzzy LOWA operator. In actual application, we use these operators and the comparison methods to deal with multicriteria decision-making problems with different situations in which importance weights of criteria or experts are known or unknown.
1980573833;Rational Herding in Microloan Markets;2012.0;[];"Microloan markets allow individual borrowers to raise funding from multiple individual lenders. We use a unique panel data set that tracks the funding dynamics of borrower listings on Prosper.com, the largest microloan market in the United States. We find evidence of rational herding among lenders. Well-funded borrower listings tend to attract more funding after we control for unobserved listing heterogeneity and payoff externalities. Moreover, instead of passively mimicking their peers (irrational herding), lenders engage in active observational learning (rational herding); they infer the creditworthiness of borrowers by observing peer lending decisions and use publicly observable borrower characteristics to moderate their inferences. Counterintuitively, obvious defects (e.g., poor credit grades) amplify a listingu0027s herding momentum, as lenders infer superior creditworthiness to justify the herd. Similarly, favorable borrower characteristics (e.g., friend endorsements) weaken the herding effect, as lenders attribute herding to these observable merits. Follow-up analysis shows that rational herding beats irrational herding in predicting loan  :[153],""paper was accepted by Pradeep Chintagunta, marketing."
1980672078;Recommending twitter users to follow using content and collaborative filtering approaches;2010.0;[];Recently the world of the web has become more social and more real-time. Facebook and Twitter are perhaps the exemplars of a new generation of social, real-time web services and we believe these types of service provide a fertile ground for recommender systems research. In this paper we focus on one of the key features of the social web, namely the creation of relationships between users. Like recent research, we view this as an important recommendation problem -- for a given user, UT which other users might be recommended as followers/followees -- but unlike other researchers we attempt to harness the real-time web as the basis for profiling and recommendation. To this end we evaluate a range of different profiling and recommendation strategies, based on a large dataset of Twitter users and their tweets, to demonstrate the potential for effective and efficient followee recommendation.
1980680715;Connecting users across social media sites: a behavioral-modeling approach;2013.0;[];"People use various social media for different purposes. The information on an individual site is often incomplete. When sources of complementary information are integrated, a better profile of a user can be built to improve online services such as verifying online information. To integrate these sources of information, it is necessary to identify individuals across social media sites. This paper aims to address the cross-media user identification problem. We introduce a methodology (MOBIUS) for finding a mapping among identities of individuals across social media sites. It consists of three key components: the first component identifies usersu0027 unique behavioral patterns that lead to information redundancies across sites; the second component constructs features that exploit information redundancies due to these behavioral patterns; and the third component employs machine learning for effective user identification. We formally define the cross-media user identification problem and show that MOBIUS is effective in identifying users across social media sites. This study paves the way for analysis and mining across social media sites, and facilitates the creation of novel online services across sites."
1980944306;A reliability system under different types of shock governed by a Markovian arrival process and maintenance policy K;2014.0;[];A reliability system subject to shocks producing damage and failure is considered. The source of shocks producing failures is governed by a Markovian arrival process. All the shocks produce deterioration and some of them failures, which can be repairable or non-repairable. Repair times are governed by a phase-type distribution. The number of deteriorating shocks that the system can stand is fixed. After a fatal failure the system is replaced by another identical one. For this model the availability, the reliability, and the rate of occurrence of the different types of failures are calculated. It is shown that this model extends other previously published in the literature.
1981107087;Some geometric aggregation operators based on intuitionistic fuzzy sets;2006.0;[];The weighted geometric (WG) operator and the ordered weighted geometric (OWG) operator are two common aggregation operators in the field of information fusion. But these two aggregation operators are usually used in situations where the given arguments are expressed as crisp numbers or linguistic values. In this paper, we develop some new geometric aggregation operators, such as the intuitionistic fuzzy weighted geometric (IFWG) operator, the intuitionistic fuzzy ordered weighted geometric (IFOWG) operator, and the intuitionistic fuzzy hybrid geometric (IFHG) operator, which extend the WG and OWG operators to accommodate the environment in which the given arguments are intuitionistic fuzzy sets which are characterized by a membership function and a non-membership function. Some numerical examples are given to illustrate the developed operators. Finally, we give an application of the IFHG operator to multiple attribute decision making based on intuitionistic fuzzy sets.
1981743146;When Are Quasi-Monte Carlo Algorithms Efficient for High Dimensional Integrals?;1998.0;[];Recently, quasi-Monte Carlo algorithms have been successfully used for multivariate integration of high dimensiond, and were significantly more efficient than Monte Carlo algorithms. The existing theory of the worst case error bounds of quasi-Monte Carlo algorithms does not explain this phenomenon. This paper presents a partial answer to why quasi-Monte Carlo algorithms can work well for arbitrarily larged. It is done by identifying classes of functions for which the effect of the dimensiondis negligible. These areweightedclasses in which the behavior in the successive dimensions is moderated by a sequence of weights. We prove that the minimalworst caseerror of quasi-Monte Carlo algorithms does not depend on the dimensiondiff the sum of the weights is finite. We also prove that the minimal number of function values in the worst case setting needed to reduce the initial error by ? is bounded byC??p, where the exponentp? 1, 2], andCdepends exponentially on the sum of weights. Hence, the relatively small sum of the weights makes some quasi-Monte Carlo algorithms strongly tractable. We show in a nonconstructive way that many quasi-Monte Carlo algorithms are strongly tractable. Even random selection of sample points (done once for the whole weighted class of functions and then the worst case error is established for that particular selection, in contrast to Monte Carlo where random selection of sample points is carried out for a fixed function) leads to strong tractable quasi-Monte Carlo algorithms. In this case the minimal number of function values in theworst casesetting is of order ??pwith the exponentp= 2. The deterministic construction of strongly tractable quasi-Monte Carlo algorithms as well as the minimal exponentpis open.
1981911218;Less is more: energy-efficient mobile sensing with senseless;2009.0;[];We present SenseLess, a system that leverages the different energy consumption characteristics of sensors to maximise battery life in mobile-sensing applications. We use the less expensive sensors more often, thereby enabling us to use the more expensive sensors less frequently. In the context of location-aware services, experimental results indicate that for a typical indoor and outdoor walk, compared to a simple GPS-based system, our SenseLess system can reduce energy consumption by more than 58% when determining a useru0027s location, while maintaining the fidelity of the sensed data. This extends the battery life of a typical handheld device from 9 hours to 22 hours.
1982934478;A cascade learning system for classification of diabetes disease: Generalized Discriminant Analysis and Least Square Support Vector Machine;2008.0;[];The aim of this study is to diagnosis of diabetes disease, which is one of the most important diseases in medical field using Generalized Discriminant Analysis (GDA) and Least Square Support Vector Machine (LS-SVM). Also, we proposed a new cascade learning system based on Generalized Discriminant Analysis and Least Square Support Vector Machine. The proposed system consists of two stages. The first stage, we have used Generalized Discriminant Analysis to discriminant feature variables between healthy and patient (diabetes) data as pre-processing process. The second stage, we have used LS-SVM in order to classification of diabetes dataset. While LS-SVM obtained 78.21% classification accuracy using 10-fold cross validation, the proposed system called GDA-LS-SVM obtained 82.05% classification accuracy using 10-fold cross validation. The robustness of the proposed system is examined using classification accuracy, k-fold cross-validation method and confusion matrix. The obtained classification accuracy is 82.05% and it is very promising compared to the previously reported classification techniques.
1983183227;On stochastic dynamic programming for solving large-scale planning problems under uncertainty;2009.0;[];The stochastic dynamic programming approach outlined here, makes use of the scenario tree in a back-to-front scheme. The multi-period stochastic problems, related to the subtrees whose root nodes are the starting nodes (i.e., scenario groups), are solved at each given stage along the time horizon. Each subproblem considers the effect of the stochasticity of the uncertain parameters from the periods of the given stage, by using curves that estimate the expected future value (EFV) of the objective function. Each subproblem is solved for a set of reference levels of the variables that also have nonzero elements in any of the previous stages besides the given stage. An appropriate sensitivity analysis of the objective function for each reference level of the linking variables allows us to estimate the EFV curves applicable to the scenario groups from the previous stages, until the curves for the first stage have been computed. An application of the scheme to the problem of production planning with logical constraints is presented. The aim of the problem consists of obtaining the planning of tactical production over the scenarios along the time horizon. The expected total cost is minimized to satisfy the product demand. Some computational experience is reported. The proposed approach compares favorably with a state-of-the-art optimization engine in instances on a very large scale. Scope and purpose: For quite some time, we have known that traditional methods of deterministic optimization are not suitable to capture the truly dynamic nature of most real-life problems, in view of the fact that the parameters which represent information concerning the future are uncertain. Many of the problems in planning under uncertainty, have logical constraints that require 0-1 variables in their formulation and can be solved via stochastic integer programming using scenario tree analysis. Given the dimensions of the deterministic equivalent model (DEM) of the stochastic problem, certain decomposition approaches can be considered by exploiting the structure of the models. Traditional decomposition schemes, such as the Benders and Lagrangean approaches, do not appear to provide the solution for large-scale problems (mainly in the cardinality of the scenario tree) in affordable computing effort. In this work, a stochastic dynamic programming approach is suggested, which we feel is particularly suited to exploit the scenario tree structure and, therefore, very amenable to finding solutions to very large-scale DEMs. The pilot case used involves a classical tactical production planning problem, where the structure is not exploited by the proposed approach so that it is generally applicable.
1983481963;Mathematical modeling and solving procedure of the planar storage location assignment problem;2009.0;[];This paper focuses on the planar storage location assignment problem (PSLAP) that needs to be clearly defined and newly formulated. In addition, the solving procedure should be developed. The PSLAP can be defined as the assignment of the inbound and outbound objects to the storage yard with aim of minimizing the number of obstructive object moves. The storage yard allows only planar moves of objects. The PSLAP usually occurs in the assembly block stockyard operations at a shipyard. This paper formulates the PSLAP using a mathematical programming model, but which belongs to the NP-hard problems category. Thus this paper utilizes an efficient genetic algorithm (GA) to solve the PSLAP for real-sized instances. The performance of the proposed mathematical programming model and developed GA is verified by a number of numerical experiments.
1983543686;The strong Kronecker product;1994.0;[];"   The strong Kronecker product has proved a powerful new multiplication tool for orthogonal matrices. This paper obtains algebraic structure theorems and properties for this new product.  The results are then applied to give new multiplication theorems for Hadamard matrices, complex Hadamard matrices and other related orthogonal matrices.  We obtain complex Hadamard matrices of order 8abcd from complex Hadamard matrices of order 2a, 2b, 2c, and 2d, and complex Hadamard matrices of order 32abcdef from Hadamard matrices of orders 4a, 4b, 4c, 4d, 4e, and 4f. We also obtain a pair of disjoint amicable OD(8hn; 2hn, 2hn)s from Hadamard matrices of orders 4h and 4n, and Plotkinu0027s result that a pair of amicable OD(4h; 2h, 2h)s and an OD(8h; 2h, 2h, 2h, 2h) can be constructed from an Hadamard matrix of order 4h as a corollary."
1983630484;Some graphs with small second eigenvalue;1995.0;[];For any prime,p, we construct a Cayley graph on the group,G, of affine linear transformations ofℤ/pℤ of degree 2(p−1) and second eigenvalue\(2\sqrt p \) with the following special property: the adjacency matrix of the graph is supported on the “blocks” associated to the trivial representation and the irreducible representation of sizep−1. SinceG is of orderp(p−1), the correspondingt-uniform Cayley hypergraph has essentially optimal second eigenvalue for this degree and size of the graph (see [2] for definitions). En route we give, for any integerku003e1, a simple Cayley graph onp k nodes of degreep of second eigenvalue\( \leqslant (k - 1)\sqrt p \).
1983645263;An Algorithm for Finding Nearest Neighbors;1975.0;[];An algorithm that finds the k nearest neighbors of a point, from a sample of size N in a d-dimensional space, with an expected number of distance calculations is described, its properties examined, and the validity of the estimate verified with simulated data.
1983849809;A Suggestion for a Fast Multiplier;1964.0;[];It is suggested that the economics of present large-scale scientific computers could benefit from a greater investment in hardware to mechanize multiplication and division than is now common. As a move in this direction, a design is developed for a multiplier which generates the product of two numbers using purely combinational logic, i.e., in one gating step. Using straightforward diode-transistor logic, it appears presently possible to obtain products in under 1, ?sec, and quotients in 3 ?sec. A rapid square-root process is also outlined. Approximate component counts are given for the proposed design, and it is found that the cost of the unit would be about 10 per cent of the cost of a modern large-scale computer.
1984020445;Big Data Deep Learning: Challenges and Perspectives;2014.0;[];"Deep learning is currently an extremely active research area in machine learning and pattern recognition society. It has gained huge successes in a broad area of applications such as speech recognition, computer vision, and natural language processing. With the sheer size of data available today, big data brings big opportunities and transformative potential for various sectors; on the other hand, it also presents unprecedented challenges to harnessing data and information. As the data keeps getting bigger, deep learning is coming to play a key role in providing big data predictive analytics solutions. In this paper, we provide a brief overview of deep learning, and highlight current research efforts and the challenges to big data, as well as the future trends."
1984160320;Evaluation of generalized Mittag---Leffler functions on the real line;2013.0;[];"This paper addresses the problem of the numerical computation of generalized Mittag---Leffler functions with two parameters, with applications in fractional calculus. The inversion of their Laplace transform is an effective tool in this direction; however, the choice of the integration contour is crucial. Here parabolic contours are investigated and combined with quadrature rules for the numerical integration. An in-depth error analysis is carried out to select suitable contouru0027s parameters, depending on the parameters of the Mittag---Leffler function, in order to achieve any fixed accuracy. We present numerical experiments to validate theoretical results and some computational issues are discussed."
1984762043;A new optimization method: Big Bang-Big Crunch;2006.0;[];"Nature is the principal source for proposing new optimization methods such as genetic algorithms (GA) and simulated annealing (SA) methods. All traditional evolutionary algorithms are heuristic population-based search procedures that incorporate random variation and selection. The main contribution of this study is that it proposes a novel optimization method that relies on one of the theories of the evolution of the universe; namely, the Big Bang and Big Crunch Theory. In the Big Bang phase, energy dissipation produces disorder and randomness is the main feature of this phase; whereas, in the Big Crunch phase, randomly distributed particles are drawn into an order. Inspired by this theory, an optimization algorithm is constructed, which will be called the Big Bang-Big Crunch (BB-BC) method that generates random points in the Big Bang phase and shrinks those points to a single representative point via a center of mass or minimal cost approach in the Big Crunch phase. It is shown that the performance of the new (BB-BC) method demonstrates superiority over an improved and enhanced genetic search algorithm also developed by the authors of this study, and outperforms the classical genetic algorithm (GA) for many benchmark test functions."
1984841421;Combinatorics of Permutations;2004.0;[];In One Line and Close. Permutations as Linear Orders. Descents Alternating Runs Alternating Subsequences In One Line and Anywhere. Permutations as Linear Orders. Inversions. Inversions Inversion in Permutations of Multisets In Many Circles. Permutations as Products of Cycles. Decomposing a Permutation into Cycles Type and Stirling Numbers Cycle Decomposition versus Linear Order Permutations with Restricted Cycle Structure In Any Way but This. Pattern Avoidance. The Basics. The Notion of Pattern Avoidance Patterns of Length Three Monotone Patterns Patterns of Length Four The Proof of the Stanley-Wilf Conjecture In This Way but Nicely. Pattern Avoidance. Follow-Up. Polynomial Recurrences Containing a Pattern Many Times Containing a Pattern a Given Number of Times Mean and Insensitive. Random Permutations. The Probabilistic Viewpoint Expectation Variance and Standard Deviation An Application: Longest Increasing Subsequences Permutations versus Everything Else. Algebraic Combinatorics of Permutations. The Robinson-Schensted-Knuth Correspondence Posets of Permutations Simplicial Complexes of Permutations Get Them All. Algorithms and Permutations. Generating Permutations Stack Sorting Permutations Variations of Stack Sorting How Did We Get Here? Permutations as Genome Rearrangements. Introduction Block Transpositions Block Interchanges Block Transpositions Revisited Solutions to Odd-Numbered Exercises References List of Frequently Used Notation Index Exercises, Problems, and Problem Solutions appear at the end of each chapter.
1985018928;Extensible Lattice Sequences for Quasi-Monte Carlo Quadrature;2000.0;[];Integration lattices are one of the main types of low discrepancy sets used in quasi-Monte Carlo methods. However, they have the disadvantage of being of fixed size. This article describes the construction of an infinite sequence of points, the first bm of which forms a lattice for any nonnegative integer m. Thus, if the quadrature error using an initial lattice is too large, the lattice can be extended without discarding the original points. Generating vectors for extensible lattices are found by minimizing a loss function based on some measure of discrepancy or nonuniformity of the lattice. The spectral test used for finding pseudorandom number generators is one important example of such a discrepancy. The performance of the extensible lattices proposed here is compared to that of other methods for some practical quadrature problems.
1985131669;Exploiting social bookmarking services to build clustered user interest profile for personalized search;2014.0;[];Search engine users tend to write short queries, generally comprising of two or three query words. As these queries are often ambiguous or incomplete, search engines tend to return results whose rankings reflect a community of intent. Moreover, search engines are designed to satisfy the needs of the general populace, not those of a specific searcher. To address these issues, we propose two methods that use Singular Value Decomposition (SVD) to build a Clustered User Interest Profile (CUIP), for each user, from the tags annotated by a community of users to web resources of interest. A CUIP consists of clusters of semantically or syntactically related tags, each cluster identifying a topic of the useru0027s interest. The matching cluster, to the given useru0027s query, aids in disambiguation of user search needs and assists the search engine to generate a set of personalized search results. A series of experiments was executed against two data sets to judge the clustering tendency of the cluster structure CUIP, and to evaluate the quality of personalized search. The experiment results indicate that the CUIP based personalized search outperforms the baseline search and is better than the other approaches that use social bookmarking services for building a user profile and use it for personalized search.
1985842955;Breast cancer diagnosis using Genetically Optimized Neural Network model;2015.0;[];   One in every eight women is susceptible to breast cancer, at some point of time in her life. Early detection and effective treatment is the only rescue to reduce breast cancer mortality. Accurate classification of a breast cancer tumor is an important task in medical diagnosis. Machine learning techniques are gaining importance in medical diagnosis because of their classification capability. In this paper, we propose a new, Genetically Optimized Neural Network (GONN) algorithm, for solving classification problems. We evolve a neural network genetically to optimize its architecture (structure and weight) for classification. We introduce new crossover and mutation operators which differ from standard crossover and mutation operators to reduce the destructive nature of these operators. We use the GONN algorithm to classify breast cancer tumors as benign or malignant. To demonstrate our results, we had taken the WBCD database from UCI Machine Learning repository and compared the classification accuracy, sensitivity, specificity, confusion matrix, ROC curves and AUC under ROC curves of GONN with classical model and classical back propagation model. Our algorithm gives classification accuracy of 98.24%, 99.63% and 100% for 50–50, 60–40, 70–30 training–testing partition respectively and 100% for 10 fold cross validation. The results show that our approach works well with the breast cancer database and can be a good alternative to the well-known machine learning methods.
1986003798;LOC algorithm: location-aware opportunistic forwarding by using node’s approximate location;2014.0;[];" :[0],""This paper aims to propose an algorithm, location-aware opportunistic content forwarding (LOC), to improve message directivity using direction vectors in opportunistic networks. The LOC is based on the assumption that if approximate location of the destination node is known, then overall message delivery and cost can be improved. Efficient message delivery with low communication cost is a major challenge in current opportunistic networks. In these networks, nodes do not have prior knowledge of their recipients, and message forwarding can be achieved by selecting suitable forwarder based on some forwarding criteria, as compared to its ancestor mobile ad hoc networks.  In :[100],""this paper, the authors tested LOC in two sets of mobility models, synthetic movement model and real mobility data sets. In the first set, working day movement is used as synthetic movement model, where proposed algorithm is compared against Lobby Influence (LI) and Epidemic algorithms. In the second set of experiments, the new algorithm is tested in three mobility data sets, namely, Cambridge, Reality and Sassy, and results compared against LI algorithm. The reason of using various movement models is to establish strengths and weaknesses of the proposed algorithm in different scenarios.  The :[194],""experimental results show that the new algorithm performed extremely well in different scenarios, not only in terms of overall message delivery but also successfully managed to reduce the communication cost.  The new :[226],""contribution increases the overall energy and storage efficiency of nodes by targeting relevant forwarding nodes in the network."
1986014385;A survey of robot learning from demonstration;2009.0;[];We present a comprehensive survey of robot Learning from Demonstration (LfD), a technique that develops policies from example state to action mappings. We introduce the LfD design choices in terms of demonstrator, problem space, policy derivation and performance, and contribute the foundations for a structure in which to categorize LfD research. Specifically, we analyze and categorize the multiple ways in which examples are gathered, ranging from teleoperation to imitation, as well as the various techniques for policy derivation, including matching functions, dynamics models and plans. To conclude we discuss LfD limitations and related promising areas for future research.
1986218012;Blended learning in higher education: Students' perceptions and their relation to outcomes;2011.0;[];New information and communication technologies (ICTs) provide educators and learners with an innovative learning environment to stimulate and enhance the teaching and learning process. In this context, novel educational concepts such as blended learning are being developed. In the present paper, we present the results obtained from a blended learning experience carried out at the University of Granada. A total of 17 groups took part, with 1431 students registered for the 2009-2010 academic year. In this study, we use objective outcomes and the studentsu0027 perceptions regarding the blended learning activities performed. The study shows that the use of blended learning has a positive effect in reducing dropout rates and in improving exam marks. Moreover, the studentsu0027 perceptions on blended learning are interrelated, with their final marks depending on the blended learning activities, and on the studentsu0027 age, background and class attendance rate.
1986484752;An SLA-based Framework for Estimating Trustworthiness of a Cloud;2012.0;[];In cloud computing consumers often seek some assurance from cloud service providers (CSPs) that services will be provided according to consumersu0027 requirements. The u0027u0027service-level-agreementu0027u0027 (SLA) between a CSP and a consumer solves this issue to an extent and provides some level of assurance. However, SLAs alone do not solve the issue entirely as there is no unique rule to create an SLA. They vary in description, length, and types of information released. Therefore, consumers need a better way of estimating `trustworthinessu0027 of a cloud (CSP). In this work we propose a framework to alleviate the above issue. Our framework estimates trustworthiness of a cloud using a quantitative model of trust. We identified and formalized several parameters that can be extracted from SLA or retrieved during the sessions and are used to estimate trust.
1987092188;Backtracking Search Optimization Algorithm for numerical optimization problems;2013.0;[];This paper introduces the Backtracking Search Optimization Algorithm (BSA), a new evolutionary algorithm (EA) for solving real-valued numerical optimization problems. EAs are popular stochastic search algorithms that are widely used to solve non-linear, non-differentiable and complex numerical optimization problems. Current research aims at mitigating the effects of problems that are frequently encountered in EAs, such as excessive sensitivity to control parameters, premature convergence and slow computation. In this vein, development of BSA was motivated by studies that attempt to develop simpler and more effective search algorithms. Unlike many search algorithms, BSA has a single control parameter. Moreover, BSAu0027s problem-solving performance is not over sensitive to the initial value of this parameter. BSA has a simple structure that is effective, fast and capable of solving multimodal problems and that enables it to easily adapt to different numerical optimization problems. BSAu0027s strategy for generating a trial population includes two new crossover and mutation operators. BSAu0027s strategies for generating trial populations and controlling the amplitude of the search-direction matrix and search-space boundaries give it very powerful exploration and exploitation capabilities. In particular, BSA possesses a memory in which it stores a population from a randomly chosen previous generation for use in generating the search-direction matrix. Thus, BSAu0027s memory allows it to take advantage of experiences gained from previous generations when it generates a trial preparation. This paper uses the Wilcoxon Signed-Rank Test to statistically compare BSAu0027s effectiveness in solving numerical optimization problems with the performances of six widely used EA algorithms: PSO, CMAES, ABC, JDE, CLPSO and SADE. The comparison, which uses 75 boundary-constrained benchmark problems and three constrained real-world benchmark problems, shows that in general, BSA can solve the benchmark problems more successfully than the comparison algorithms.
1987197833;Reliability optimization of a series system with multiple-choice and budget constraints;2000.0;[];   This paper considers a reliability optimization problem for a series system with multiple-choice constraints incorporated at each subsystem to maximize the system reliability subject to the system budget. The problem is formulated as a nonlinear binary integer programming problem and characterized as an NP-hard problem. In the problem analysis, some solution properties are characterized to reduce the solution space in advance. A branch-and-bound solution algorithm is then derived based on the reduced solution space to search for the optimal solution. The algorithm is tested for its efficiency with randomly generated numerical examples.
1987365706;Extracting human mobility patterns from GPS-based traces;2010.0;[];In this paper we analyze few GPS-based traces to infer human mobility patterns. We propose a clustering method to extract the main points of interest, called geo-locations, from GPS data. Starting from geo-locations we propose a definition of community, the geo-community, which captures the relation between a spatial description of human movements and the social context where users live. A statistical analysis of the principal characteristics of human walks provide the fitting distributions of distances covered by people inside a geo-location and among geo-locations and pause time. Finally we analyze factors influencing people when choosing successive location in their movement.
1987454296;Induced 2-tuple linguistic generalized aggregation operators and their application in decision-making;2013.0;[];The induced 2-tuple linguistic generalized ordered weighted averaging (2-TILGOWA) operator is presented. This new aggregation operator extends previous approaches by using generalized means, order-inducing variables in the reordering of the arguments and linguistic information represented with the 2-tuple linguistic approach. Its main advantage is that it includes a wide range of linguistic aggregation operators. Thus, its analyses can be seen from different perspectives and we obtain a much more complete picture of the situation considered and are able to select the alternative that best fits with our interests or beliefs. We further generalize the 2-TILGOWA by using quasi-arithmetic means and Choquet integrals. The result is the Quasi-2-TILOWA operator and the 2-tuple linguistic induced quasi-arithmetic Choquet integral aggregation. We conclude this paper by analysing the applicability of this new approach in a multi-person linguistic decision-making problem concerning product management.
1987727544;Improving context-awareness in self-adaptation using the DYNAMICO reference model;2013.0;[];Self-adaptation mechanisms modify target systems dynamically to address adaptation goals, which may evolve continuously due to changes in system requirements. These changes affect values and thresholds of observed context variables and monitoring logic, or imply the addition and/or deletion of context variables, thus compromising self-adaptivity effectiveness under static monitoring infrastructures. Nevertheless, self-adaptation approaches often focus on adapting target systems only rather than monitoring infrastructures. Previously, we proposed DYNAMICO, a reference model for self-adaptive systems where adaptation goals and monitoring requirements change dynamically. This paper presents an implementation of DYNAMICO comprising our SMARTERCONTEXT monitoring infrastructure and QoS-CARE adaptation framework in a self-adaptation solution that maintains its context-awareness relevance. To evaluate our reference model we use self-adaptive system properties and the Znn.com exemplar to compare the Rainbow system with our DYNAMICO implementation. The results of the evaluation demonstrate the applicability, feasibility, and effectiveness of Dynamico, especially for self-adaptive systems with context-awareness requirements.
1987995897;On Augmented Lagrangian Methods with General Lower-Level Constraints;2007.0;[];Augmented Lagrangian methods with general lower-level constraints are considered in the present research. These methods are useful when efficient algorithms exist for solving subproblems in which the constraints are only of the lower-level type. Inexact resolution of the lower-level constrained subproblems is considered. Global convergence is proved using the constant positive linear dependence constraint qualification. Conditions for boundedness of the penalty parameters are discussed. The resolution of location problems in which many constraints of the lower-level set are nonlinear is addressed, employing the spectral projected gradient method for solving the subproblems. Problems of this type with more than $3 \times 10^6$ variables and $ 14 \times 10^6$ constraints are solved in this way, using moderate computer time. All the codes are available at http://www.ime.usp.br/$\sim$egbirgin/tango/.
1988752523;A new multi-objective particle swarm optimization method for solving reliability redundancy allocation problems;2013.0;[];In this paper, a new dynamic self-adaptive multi-objective particle swarm optimization (DSAMOPSO) method is proposed to solve binary-state multi-objective reliability redundancy allocation problems (MORAPs). A combination of penalty function and modification strategies is used to handle the constraints in the MORAPs. A dynamic self-adaptive penalty function strategy is utilized to handle the constraints. A heuristic cost-benefit ratio is also supplied to modify the structure of violated swarms. An adaptive survey is conducted using several test problems to illustrate the performance of the proposed DSAMOPSO method. An efficient version of the epsilon-constraint (AUGMECON) method, a modified non-dominated sorting genetic algorithm (NSGA-II) method, and a customized time-variant multi-objective particle swarm optimization (cTV-MOPSO) method are used to generate non-dominated solutions for the test problems. Several properties of the DSAMOPSO method, such as fast-ranking, evolutionary-based operators, elitism, crowding distance, dynamic parameter tuning, and tournament global best selection, improved the best known solutions of the benchmark cases of the MORAP. Moreover, different accuracy and diversity metrics illustrated the relative preference of the DSAMOPSO method over the competing approaches in the literature.
1988963644;Solving the dynamic ambulance relocation and dispatching problem using approximate dynamic programming;2012.0;[];Emergency service providers are supposed to locate ambulances such that in case of emergency patients can be reached in a time-efficient manner. Two fundamental decisions and choices need to be made real-time. First of all immediately after a request emerges an appropriate vehicle needs to be dispatched and send to the requests’ site. After having served a request the vehicle needs to be relocated to its next waiting location. We are going to propose a model and solve the underlying optimization problem using approximate dynamic programming (ADP), an emerging and powerful tool for solving stochastic and dynamic problems typically arising in the field of operations research. Empirical tests based on real data from the city of Vienna indicate that by deviating from the classical dispatching rules the average response time can be decreased from 4.60 to 4.01 minutes, which corresponds to an improvement of 12.89%. Furthermore we are going to show that it is essential to consider time-dependent information such as travel times and changes with respect to the request volume explicitly. Ignoring the current time and its consequences thereafter during the stage of modeling and optimization leads to suboptimal decisions.
1989160322;Establishing online trust through a community responsibility system;2001.0;[];Much of the research on trust building in electronic commerce takes a descriptive approach. The question of what social structures are more appropriate to promote trust in the online world has not been extensively studied. We analyze in this paper, using a prescriptive approach, how a certain social structure—a community responsibility system, supported by present technology, can be set up. We use game theoretic tools to prove that under the community responsibility system for trust building, online transactions that are impersonal can be supported and can preserve at the same time anonymity to a large extent.
1989634590;When target motion matters: Doppler coverage in radar sensor networks;2013.0;[];Radar sensors, which actively transmit radio waves and collect RF energy scattered by objects in the environment, offer a number of advantages over purely passive sensors. An important issue in radar is that the transmitted energy may be scattered by objects that are not of interest as well as objects of interest (e.g., targets). The detection performance of radar systems is affected by such clutter as well as noise. Further, in many applications, clutter can be substantially stronger than the signals of interest. To combat the effect of clutter, a popular method is to take advantage of the Doppler frequency shift (DFS) extracted from the echo signal due to the relative motion of a target with respect to the radar. Unfortunately, a sensor coverage model that only depends on the distance to a target would fail to capture the DFS. In this paper, we set forth the concept of Doppler coverage for a network of spatially distributed radars. Specifically, a target is said to be Doppler-covered if, regardless of its direction of motion, there exists some radar in the network whose signalto-noise ratio (SNR) is sufficiently high and the DFS at that radar is sufficiently large. Based on the Doppler coverage model, we first propose an efficient method to characterize Dopplercovered regions for arbitrarily deployed radars. Then we design an algorithm for deriving the minimum radar density required to achieve Doppler coverage in a region under any polygonal deployment pattern, and further apply it to investigate the regular triangle based deployment.
1989995659;PROJECTION MODELS FOR INTUITIONISTIC FUZZY MULTIPLE ATTRIBUTE DECISION MAKING;2010.0;[];The aim of this paper is to investigate the intuitionistic fuzzy multiple attribute decision-making problems where the attribute values are expressed in intuitionistic fuzzy numbers or interval-valued intuitionistic fuzzy numbers. We introduce some notions, such as intuitionistic fuzzy ideal point, interval-valued intuitionistic fuzzy ideal point, the modules of intuitionistic fuzzy numbers, and interval-valued intuitionistic fuzzy numbers. We also introduce the cosine of the included angle between the attribute value vectors of each alternative and the intuitionistic fuzzy ideal point, and the cosine of the included angle between the attribute value vectors of each alternative and the interval-valued intuitionistic fuzzy ideal point. Then we establish two projection models to measure the similarity degrees between each alternative and the intuitionistic fuzzy ideal point, and between each alternative and the interval-valued intuitionistic fuzzy ideal point. Based on the projection models, we can rank the given alternatives and then select the most desirable one. Finally, we illustrate the developed projection models with a numerical example.
1990044698;An investigation of mobile learning readiness in higher education based on the theory of planned behavior;2012.0;[];This study investigated the current state of college studentsu0027 perceptions toward mobile learning in higher education. Mobile learning is a new form of learning utilizing the unique capabilities of mobile devices. Although mobile devices are ubiquitous on college campuses, student readiness for mobile learning has yet to be fully explored in the United States. The paper describes a conceptual model, based on the theory of planned behavior (TPB), which explains how college studentsu0027 beliefs influence their intention to adopt mobile devices in their coursework. Structural equation modeling was used to analyze self-report data from 177 college students. The findings showed that the TPB explained college studentsu0027 acceptance of m-learning reasonably well. More specifically, attitude, subjective norm, and behavioral control positively influenced their intention to adopt mobile learning. The results provide valuable implications for ways to increase college studentsu0027 acceptance of mobile learning.
1990911977;The vision of autonomic computing;2003.0;[];A 2001 IBM manifesto observed that a looming software complexity crisis -caused by applications and environments that number into the tens of millions of lines of code - threatened to halt progress in computing. The manifesto noted the almost impossible difficulty of managing current and planned computing systems, which require integrating several heterogeneous environments into corporate-wide computing systems that extend into the Internet. Autonomic computing, perhaps the most attractive approach to solving this problem, creates systems that can manage themselves when given high-level objectives from administrators. Systems manage themselves according to an administratoru0027s goals. New components integrate as effortlessly as a new cell establishes itself in the human body. These ideas are not science fiction, but elements of the grand challenge to create self-managing computing systems.
1990948050;Quota-control routing in delay-tolerant networks;2015.0;[];Delay-tolerant networks (DTNs) are network environments that are subject to delays and disruptions. Traditional end-to-end routing protocols fail in such challenging network conditions because of intermittent connections and/or long delays. Research results have shown that per-hop forwarding of multiple copies of the same message to the destination can produce satisfactory routing performance in DTNs. Current methods rely on the fixed setting of a quota value to limit the number of message copies. This paper proposes a dynamic quota-control mechanism, allowing routing to operate effectively with different traffic loads. To remove useless message copies from the network, a low-cost probability-based method is also presented. The proposed routing framework is then extended to interest-based information dissemination, which is used to efficiently disseminate an event message to all interested users. A performance evaluation was conducted using a real social contact trace, and performance comparisons with other DTN routing protocols are provided.
1990954610;Positive solutions for singular second order differential equations with integral boundary conditions;2013.0;[];   In this paper, we study the existence of positive solutions for the singular second order integral boundary value problem       {       u    ″     (  t  )   +  a   (  t  )     u    ′     (  t  )   +  b   (  t  )   u   (  t  )   +  c   (  t  )   f   (  u  )   =  0  ,    t  ∈   (  0  ,  1  )   ,      u   (  0  )   =    ∫    0    1    g   (  s  )   u   (  s  )   d  s  ,    u   (  1  )   =    ∫    0    1    h   (  s  )   u   (  s  )   d  s  ,          where    c   (  t  )     is allowed to be singular at    t  =  0  ,  1    and    f   (  u  )     may be singular at    u  =  0   . The existence of positive solutions for the above problem is established by applying the fixed point index theorems under some weaker conditions concerning the first eigenvalue corresponding to the relevant linear operator. The results obtained herein generalize and improve some known results including singular and non-singular cases.
1992097573;Implementing Web 2.0 technologies in higher education: A collective case study;2012.0;[];Web 2.0 technologies are becoming more popular in the everyday lives of students. As a result, teachers and designers have begun to explore their use in formal education. This paper presents evaluation findings from a collective case study of six Web 2.0 implementations in Australian higher education. The research was undertaken as part of a larger study that sought to understand how todayu0027s students use information and communication technologies to support their learning. Conducted across three universities, the research included a range of disciplines, class sizes and year levels. A common evaluation strategy was used in order to collect comparable data from which commonalities and differences could be identified. This paper provides an overview of the study, describes the methodology used, summarises the implementation experiences of staff and students involved and presents the key findings. The results suggest that most students had little prior experience with relevant technologies and that many struggled to see the value of using Web 2.0 technologies for learning and teaching, both of which have important implications for the design of appropriate learning tasks. While the argument can be made for improving the design through better task-technology alignment, this study also highlights inherent tensions between Web 2.0 and educational practices.
1992589780;Drift analysis of mutation operations for biogeography-based optimization;2015.0;[];As an essential factor of evolutionary algorithms (EAs), mutation operator plays an important role in exploring the search space, maintaining the diversity of individuals and breaking away local optimums. In most standard evolutionary algorithms, the mutation operator is independent from the recombination operator. Nevertheless, in biogeography-based optimization (BBO), the mutation operator is affected not only by predefined constants but also by recombination models, namely the migration operator. However to date, the relationship between the mutation and migration has never been investigated. To reveal the relationship and evaluate the mutation models, we utilize drift analysis to investigate the expected first hitting time of BBO with different migration models. The analysis compares three different kinds of mutation models in a mathematical way and the conclusion is helpful for designing migration models of BBO. The simulation results are also in agreement with our analysis.
1992981157;Vehicle-assisted data delivery for smart grid: An optimal stopping approach;2013.0;[];The booming smart grid produces a large amount of data that should be transmitted to the utility control center (UCC), typically by means of the cellular network. This may pose a prohibitive transmission cost and choke the cellular network. As an effort to address this issue, we propose a vehicle assisted data delivery method to offload the cellular network, in which vehicles are utilized to carry and deliver the data from distributed locations to the UCC through the deployed roadside units. Two data forwarding schemes are developed based on the theory of optimal stopping rules to increase the data delivery probability. Simulation results are given to demonstrate that the proposed method can achieve high data delivery ratio through the roadside network so that it can efficiently offload the cellular network and reduce the communication cost.
1993047937;On the use of matrix functions for fractional partial differential equations;2011.0;[];Abstract: The main focus of this paper is the solution of some partial differential equations of fractional order. Promising methods based on matrix functions are taken in consideration. The features of different approaches are discussed and compared with results provided by classical convolution quadrature rules. By means of numerical experiments accuracy and performance are examined.
1993482030;Tensor-Train Decomposition;2011.0;[];A simple nonrecursive form of the tensor decomposition in $d$ dimensions is presented. It does not inherently suffer from the curse of dimensionality, it has asymptotically the same number of parameters as the canonical decomposition, but it is stable and its computation is based on low-rank approximation of auxiliary unfolding matrices. The new form gives a clear and convenient way to implement all basic operations efficiently. A fast rounding procedure is presented, as well as basic linear algebra operations. Examples showing the benefits of the decomposition are given, and the efficiency is demonstrated by the computation of the smallest eigenvalue of a 19-dimensional operator.
1993530334;The Pareto Envelope-Based Selection Algorithm for Multi-objective Optimisation;2000.0;[];We introduce a new multiobjective evolutionary algorithm called PESA (the Pareto Envelope-based Selection Algorithm), in which selection and diversity maintenance are controlled via a simple hyper-grid based scheme. PESAu0027s selection method is relatively unusual in comparison with current well known multiobjective evolutionary algorithms, which tend to use counts based on the degree to which solutions dominate others in the population. The diversity maintenance method is similar to that used by certain other methods. The main attraction of PESA is the integration of selection and diversity maintenance, whereby essentially the same technique is used for both tasks. The resulting algorithm is simple to describe, with full pseudocode provided here and real code available from the authors. We compare PESA with two recent strong-performing MOEAs on some multiobjective test problems recently proposed by Deb. We find that PESA emerges as the best method overall on these problems.
1994338028;Parabolic and Hyperbolic Contours for Computing the Bromwich Integral;2007.0;[];Some of the most eectiv e methods for the numerical inversion of the Laplace transform are based on the approximation of the Bromwich contour integral. The accuracy of these methods often hinges on a good choice of contour, and several such contours have been proposed in the literature. Here we analyze two recently proposed contours, namely a parabola and a hyperbola. Using a representative model problem, we determine estimates for the optimal parameters that dene these contours. An application to a fractional diusion equation is presented.
1994739397;Weak hypergraph regularity and linear hypergraphs;2010.0;[];We consider conditions which allow the embedding of linear hypergraphs of fixed size. In particular, we prove that any k-uniform hypergraph H of positive uniform density contains all linear k-uniform hypergraphs of a given size. More precisely, we show that for all integers @?u003e=ku003e=2 and every du003e0 there exists @ru003e0 for which the following holds: if H is a sufficiently large k-uniform hypergraph with the property that the density of H induced on every vertex subset of size @rn is at least d, then H contains every linear k-uniform hypergraph F with @? vertices. The main ingredient in the proof of this result is a counting lemma for linear hypergraphs, which establishes that the straightforward extension of graph @e-regularity to hypergraphs suffices for counting linear hypergraphs. We also consider some related problems.
1995609930;A self-regulating protocol for efficient routing in mobile delay tolerant networks;2012.0;[];"In this paper, we present a novel routing protocol for mobile delay tolerant networks, called Community-based Adaptive Spray (CAS). How to maximize routing performance (delivery ratio) and minimize resource consumption (number of message copies) is a common goal in these networks. Our protocol considers the following two aspects toward this goal: 1) selecting the intermediate nodes that are closest to the destination, based on their mobility patterns; 2) dynamically controlling the number of message copies according to the time-to-live of a message. Experiment results demonstrate that our protocol can improve routing performance and resource consumption compared to the state-of-the-art Spray-and-Wait and BUBBLE protocols."
1996313056;A hybrid approach to medical decision support systems: Combining feature selection, fuzzy weighted pre-processing and AIRS;2007.0;[];This paper presents a hybrid approach based on feature selection, fuzzy weighted pre-processing and artificial immune recognition system (AIRS) to medical decision support systems. We have used the heart disease and hepatitis disease datasets taken from UCI machine learning database as medical dataset. Artificial immune recognition system has shown an effective performance on several problems such as machine learning benchmark problems and medical classification problems like breast cancer, diabetes, and liver disorders classification. The proposed approach consists of three stages. In the first stage, the dimensions of heart disease and hepatitis disease datasets are reduced to 9 from 13 and 19 in the feature selection (FS) sub-program by means of C4.5 decision tree algorithm (CBA program), respectively. In the second stage, heart disease and hepatitis disease datasets are normalized in the range of [0,1] and are weighted via fuzzy weighted pre-processing. In the third stage, weighted input values obtained from fuzzy weighted pre-processing are classified using AIRS classifier system. The obtained classification accuracies of our system are 92.59% and 81.82% using 50-50% training-test split for heart disease and hepatitis disease datasets, respectively. With these results, the proposed method can be used in medical decision support systems.
1996313783;Fairness-related challenges in mobile opportunistic networking;2013.0;[];"The fundamental challenge in opportunistic networking, regardless of the application, is when and how to forward a message. Rank-based forwarding techniques currently represent one of the most promising methods for addressing this message forwarding challenge. While these techniques have demonstrated great efficiency in performance, they do not address the rising concern of fairness amongst various nodes in the network. Higher ranked nodes typically carry the largest burden in delivering messages, which creates a high potential of dissatisfaction amongst them. In this paper, we adopt a real-trace driven approach to study and analyze the trade-offs between efficiency, cost, and fairness of rank-based forwarding techniques in mobile opportunistic networks. Our work comprises three major contributions. First, we quantitatively analyze the trade-off between fair and efficient environments. Second, we demonstrate how fairness coupled with efficiency can be achieved based on real mobility traces. Third, we propose FOG, a real-time distributed framework to ensure efficiency-fairness trade-off using local information. Our framework, FOG, enables state-of-the-art rank-based opportunistic forwarding algorithms to ensure a better fairness-efficiency trade-off while maintaining a low overhead. Within FOG, we implement two real-time distributed fairness algorithms; Proximity Fairness Algorithm (PFA), and Message Context Fairness Algorithm (MCFA). Our data-driven experiments and analysis show that mobile opportunistic communication between users may fail with the absence of fairness in participating high-ranked nodes, and an absolute fair treatment of all users yields inefficient communication performance. Finally our analysis shows that FOG-based algorithms ensure relative equality in the distribution of resource usage among neighbor nodes while keeping the success rate and cost performance near optimal."
1996536857;Scalable decision support for digital preservation;2014.0;[];Purpose – Preservation environments such as repositories need scalable and context-aware preservation planning and monitoring capabilities to ensure continued accessibility of content over time. This article identifies a number of gaps in the systems and mechanisms currently available and presents a new, innovative architecture for scalable decision-making and control in such environments. Design/methodology/approach – The paper illustrates the state of the art in preservation planning and monitoring, highlights the key challenges faced by repositories to provide scalable decision-making and monitoring facilities, and presents the contributions of the SCAPE Planning and Watch suite to provide such capabilities. Findings – The presented architecture makes preservation planning and monitoring context-aware through a semantic representation of key organizational factors, and integrates this with a business intelligence system that collects and reasons upon preservation-relevant information. Research limitati...
1996869553;Alternating Minimal Energy Methods for Linear Systems in Higher Dimensions;2014.0;[];We propose algorithms for the solution of high-dimensional symmetrical positive definite (SPD) linear systems with the matrix and the right-hand side given and the solution sought in a low-rank format. Similarly to density matrix renormalization group (DMRG) algorithms, our methods optimize the components of the tensor product format subsequently. To improve the convergence, we expand the search space by an inexact gradient direction. We prove the geometrical convergence and estimate the convergence rate of the proposed methods utilizing the analysis of the steepest descent algorithm. The complexity of the presented algorithms is linear in the mode size and dimension, and the demonstrated convergence is comparable to or even better than the one of the DMRG algorithm. In the numerical experiment we show that the proposed methods are also efficient for non-SPD systems, for example, those arising from the chemical master equation describing the gene regulatory model at the mesoscopic scale.
1996937753;Fuzzy entropy on intuitionistic fuzzy sets;2006.0;[];In this article we exploit the concept of probability for defining the fuzzy entropy of intuitionistic fuzzy sets (IFSs). We then propose two families of entropy measures for IFSs and also construct the axiom definition and properties. Two definitions of entropy for IFSs proposed by Burillo and Bustince in 1996 and Szmidt and Kacprzyk in 2001 are used. The first one allows us to measure the degree of intuitionism of an IFS, whereas the second one is a nonprobabilistic-type entropy measure with a geometric interpretation of IFSs used in comparison with our proposed entropy of IFSs in the numerical comparisons. The results show that the proposed entropy measures seem to be more reliable for presenting the degree of fuzziness of an IFS. © 2006 Wiley Periodicals, Inc. Int J Int Syst 21: 443–451, 2006.
1997017001;Data Pre-Forwarding for Opportunistic Data Collection in Wireless Sensor Networks;2014.0;[];Opportunistic data collection in wireless sensor networks uses passing smartphones to collect data from sensor nodes, thus avoiding the cost of multiple static sink nodes. Based on the observed mobility patterns of smartphone users, sensor data should be preforwarded to the nodes that are visited more frequently with the aim of improving network throughput. In this article, we construct a formal network model and an associated theoretical optimization problem to maximize the throughput subject to energy constraints of sensor nodes. Since a centralized controller is not available in opportunistic data collection, data pre-forwarding (DPF) must operate as a distributed mechanism in which each node decides when and where to forward data based on local information. Hence, we develop a simple distributed DPF mechanism with two heuristic algorithms, implement this proposal in Contiki-OS, and evaluate it thoroughly. We demonstrate empirically, in simulations, that our approach is close to the optimal solution obtained by a centralized algorithm. We also demonstrate that this approach performs well in scenarios based on real mobility traces of smartphone users. Finally, we evaluate our proposal on a small laboratory testbed, demonstrating that the distributed DPF mechanism with heuristic algorithms performs as predicted by simulations, and thus that it is a viable technique for opportunistic data collection through smartphones.
1997543377;Model learning for robot control: a survey.;2011.0;[];Models are among the most essential tools in robotics, such as kinematics and dynamics models of the robot’s own body and controllable external objects. It is widely believed that intelligent mammals also rely on internal models in order to generate their actions. However, while classical robotics relies on manually generated models that are based on human insights into physics, future autonomous, cognitive robots need to be able to automatically generate models that are based on information which is extracted from the data streams accessible to the robot. In this paper, we survey the progress in model learning with a strong focus on robot control on a kinematic as well as dynamical level. Here, a model describes essential information about the behavior of the environment and the influence of an agent on this environment. In the context of model-based learning control, we view the model from three different perspectives. First, we need to study the different possible model learning architectures for robotics. Second, we discuss what kind of problems these architecture and the domain of robotics imply for the applicable learning methods. From this discussion, we deduce future directions of real-time learning algorithms. Third, we show where these scenarios have been used successfully in several case studies.
1997800186;Object localization based on sparse representation for remote sensing imagery;2014.0;[];In this paper, we propose a new object localization method named sparse representation based object localization (SROL), which is based on the generalized Hough-transform-based approach using sparse representations for parts detection. The proposed method was applied to car and ship detection in remote sensing images and its performance was compared to those of state-of-the-art methods. Experimental results showed that the SROL algorithm can accurately localize categorical objects or a specific object using a small size of training data.
1998258511;Multilevel Toeplitz Matrices Generated by Tensor-Structured Vectors and Convolution with Logarithmic Complexity;2013.0;[];We study the tensor structure of two operations: the transformation of a given multidimensional vector into a multilevel Toeplitz matrix and the convolution of two given multidimensional vectors. We show that the low-rank tensor structure of the input is preserved in the output and propose efficient algorithms for these operations in the newly introduced quantized tensor train (QTT) format. Consider a $d$-dimensional $2n \times\cdots\times 2n$-vector $\boldsymbol{x}$. If it is represented elementwise, the number of parameters is $(2n)^{d}$. However, if we assume that $\boldsymbol{x}$ is given in a QTT representation with ranks bounded by $p$, the number of parameters is reduced to $\mathcal{O}\left(dp^{2} \log n\right)$. Under this assumption we show how the multilevel Toeplitz matrix generated by $\boldsymbol{x}$ can be obtained in the QTT format with ranks bounded by $2p$ in $\mathcal{O}\left(dp^{2} \log n\right)$ operations. We also describe how the convolution $\boldsymbol{x}\star\boldsymbol{y}$ of $\...
1999284878;Teaching-learning-based optimization: A novel method for constrained mechanical design optimization problems;2011.0;[];A new efficient optimization method, called u0027Teaching-Learning-Based Optimization (TLBO)u0027, is proposed in this paper for the optimization of mechanical design problems. This method works on the effect of influence of a teacher on learners. Like other nature-inspired algorithms, TLBO is also a population-based method and uses a population of solutions to proceed to the global solution. The population is considered as a group of learners or a class of learners. The process of TLBO is divided into two parts: the first part consists of the u0027Teacher Phaseu0027 and the second part consists of the u0027Learner Phaseu0027. u0027Teacher Phaseu0027 means learning from the teacher and u0027Learner Phaseu0027 means learning by the interaction between learners. The basic philosophy of the TLBO method is explained in detail. To check the effectiveness of the method it is tested on five different constrained benchmark test functions with different characteristics, four different benchmark mechanical design problems and six mechanical design optimization problems which have real world applications. The effectiveness of the TLBO method is compared with the other population-based optimization algorithms based on the best solution, average solution, convergence rate and computational effort. Results show that TLBO is more effective and efficient than the other optimization methods for the mechanical design optimization problems considered. This novel optimization method can be easily extended to other engineering design optimization problems.
1999531283;Personalized video recommendation based on cross-platform user modeling;2013.0;[];Online propagation of videos has surged up to an unparalleled level. Most personalized video recommendation methods are based on single-platform user modeling, which suffer from data sparsity and cold-start issues. In this paper, we introduce cross-platform user modeling as a solution by smartly aggregating user information from different platforms. Unlike traditional recommendation methods where sufficient user information is assumed available in the target platform, this proposed method works well when there is little knowledge about usersu0027 interests in the target platform. While considering the difference of user behaviors in different platforms, on one hand, we enrich user profile in the target platform with related information in the auxiliary platform. On the other hand, we transfer the collaborative relationship defined in behaviors from the auxiliary platform to the target platform. Carefully designed experiments have demonstrated the effectiveness of the proposed method.
1999798506;Guidelines for conducting systematic mapping studies in software engineering: An update;2015.0;[];"   Context  Systematic mapping studies are used to structure a research area, while systematic reviews are focused on gathering and synthesizing evidence. The most recent guidelines for systematic mapping are from 2008. Since that time, many suggestions have been made of how to improve systematic literature reviews (SLRs). There is a need to evaluate how researchers conduct the process of systematic mapping and identify how the guidelines should be updated based on the lessons learned from the existing systematic maps and SLR guidelines.    Objective  To identify how the systematic mapping process is conducted (including search, study selection, analysis and presentation of data, etc.); to identify improvement potentials in conducting the systematic mapping process and updating the guidelines accordingly.    Method  We conducted a systematic mapping study of systematic maps, considering some practices of systematic review guidelines as well (in particular in relation to defining the search and to conduct a quality assessment).    Results  In a large number of studies multiple guidelines are used and combined, which leads to different ways in conducting mapping studies. The reason for combining guidelines was that they differed in the recommendations given.    Conclusion  The most frequently followed guidelines are not sufficient alone. Hence, there was a need to provide an update of how to conduct systematic mapping studies. New guidelines have been proposed consolidating existing findings."
2000722362;Optimal production planning under time-sensitive electricity prices for continuous power-intensive processes;2012.0;[];   Power-intensive processes can lower operating expenses when adjusting production planning according to time-dependent electricity pricing schemes. In this paper, we describe a discrete-time, deterministic MILP model that allows optimal production planning for continuous power-intensive processes. We emphasize the systematic modeling of operational transitions, that result from switching the operating modes of the plant equipment, with logic constraints. We prove properties on the tightness of several logic constraints. For the time horizon of 1 week and hourly changing electricity prices, we solve an industrial case study on air separation plants, where transitional modes help us capture ramping behavior. We also solve problem instances on cement plants where we show that the appropriate choice of operating modes allows us to obtain practical schedules, while limiting the number of changeovers. Despite the large size of the MILPs, the required solution times are small due to the explicit modeling of transitions.
2000763566;A note on fuzzy information measures;1997.0;[];   A new entropy of a fuzzy set is defined based on the membership functions of the intersection and union of the set and its complement set. The concept of fuzzy cross-entropy is also introduced and its definition is given by analogy with the cross-entropy of probability distributions. Their possible applications are highlighted.
2000900288;Distribution Grid Impacts of Smart Electric Vehicle Charging From Different Perspectives;2015.0;[];As a consequence of the developments in electric transportation and the evolution toward smart grids, large-scale deployment of smart charging strategies for electric vehicles (EVs) becomes feasible. This leads to opportunities for different market parties to use the flexibility of EVs for various objectives that may be conflicting and result in a nonoptimal shifting of peak demands for the distribution grids. In this paper, we assess the financial impact of various EV charging strategies on distribution grids. We compare a strategy that minimizes network peak loads (from a network operators perspective) with a strategy to minimize charging costs (from the perspective of a commercial party). In a scenario with a high wind penetration in the system, the electricity prices are, for a significant part, determined by the instantaneous wind production. Therefore, we additionally study the effect of wind energy on electricity prices and, consequently, on the resulting EV load and network impacts. We obtain the network costs by calculating the impacts expressed in the net present value (NPV) of the investments costs and energy losses. We found that, in the case where EVs are basing their charge schedules on electricity prices, the increase in NPV compared with a no EV scenario was found to be 25% higher than in the case where the extra peak load due to EVs was minimized. The large difference in network impacts between the price based and network based charging strategies was only observed in the case with a high wind penetration. The results strongly suggest that the situation where EVs are controlled with a strategy to minimize charging costs that does not take the distribution grids into account may not lead to an optimal situation when the entire electricity delivery system is regarded.
2001452963;Linguistic group decision making with induced aggregation operators and probabilistic information;2014.0;[];A new approach for linguistic multi-criteria group decision making.The induced linguistic probabilistic OWA operator.A generalization with moving averages and Bonferroni means.An application in the management of import strategies. A new approach for linguistic group decision making by using probabilistic information and induced aggregation operators is presented. It is based on the induced linguistic probabilistic ordered weighted average (ILPOWA). It is an aggregation operator that uses probabilities and OWA operators in the same formulation considering the degree of importance that each concept has in the formulation. It uses complex attitudinal characters that can be assessed by using order inducing variables. Furthermore, it deals with an uncertain environment where the information cannot be studied in a numerical scale but it is possible to use linguistic variables. Several extensions to this approach are presented by using moving averages and Bonferroni means. The applicability of this approach is also studied with a focus on multi-criteria group decision making by using multi-person aggregation operators in order to deal with the opinion of several experts in the analysis. An illustrative example regarding importation strategies in the administration of a country is developed.
2001482101;Cellular Traffic Offloading through WiFi Networks;2011.0;[];Cellular networks are currently facing the challenges of mobile data explosion. High-end mobile phones and laptops double their mobile data traffic every year and this trend is expected to continue given the rapid development of mobile social applications. It is imperative that novel architectures be developed to handle such voluminous mobile data. In this paper, we propose and evaluate an integrated architecture exploiting the opportunistic networking paradigm to migrate data traffic from cellular networks to metropolitan WiFi access points (APs). To quantify the benefits of deploying such an architecture, we consider the case of bulk file transfer and video streaming over 3G networks and simulate data delivery using real mobility data set of 500 taxis in an urban area. We are the first to quantitatively evaluate the gains of citywide WiFi offloading using large scale real traces. Our results give the numbers of APs needed for different requirements of quality of service for data delivery in large metropolitan area. We show that even with a sparse WiFi network the delivery performance can be significantly improved. This effort serves as an important feasibility study and provides guidelines for operators to evaluate the possibility and cost of this solution. Keywords-Cellular traffic offloading, delay tolerant, WiFi access points, trace-driven simulation.
2001565273;Heterogeneous Redundancy Allocation for Series-Parallel Multi-State Systems Using Hybrid Particle Swarm Optimization and Local Search;2012.0;[];A hybrid algorithm of particle swarm optimization (PSO) and local search (LS) is proposed to solve the redundancy allocation problem for series-parallel multi-state systems. The proposed hybrid algorithm is able to design the system structure with a minimum cost to provide a desired level of availability. Unlike most of the previous studies that only consider homogeneous redundancy, the proposed algorithm allows for the heterogeneous redundancy technique. The universal generating function method is applied to evaluate system availability. The standard PSO is modified and novel LS strategies are integrated to adapt to the redundancy allocation problem. Case studies that facilitate comparisons between the proposed hybrid algorithm and other non-hybrid heuristics as well as meta-heuristics reported in the literature (such as genetic algorithms, tabu search, and ant colony optimization) are provided. The results illustrate the advantages of the proposed hybrid algorithm in terms of the solution quality or algorithm efficiency.
2001766673;Minimum vertex degree conditions for loose Hamilton cycles in 3-uniform hypergraphs;2013.0;[];"We investigate minimum vertex degree conditions for 3-uniform hypergraphs which ensure the existence of loose Hamilton cycles. A loose Hamilton cycle is a spanning cycle in which only consecutive edges intersect and these intersections consist of precisely one vertex. We prove that every 3-uniform n-vertex (n even) hypergraph H with minimum vertex degree @d""1(H)u003e=(716+o(1))(n2) contains a loose Hamilton cycle. This bound is asymptotically best possible."
2002162495;"Preference disaggregation and statistical learning for multicriteria decision support: A review ;)";2011.0;[];Disaggregation methods have become popular in multicriteria decision aiding (MCDA) for eliciting preferential information and constructing decision models from decision examples. From a statistical point of view, data mining and machine learning are also involved with similar problems, mainly with regard to identifying patterns and extracting knowledge from data. Recent research has also focused on the introduction of specific domain knowledge in machine learning algorithms. Thus, the connections between disaggregation methods in MCDA and traditional machine learning tools are becoming stronger. In this paper the relationships between the two fields are explored. The differences and similarities between the two approaches are identified, and a review is given regarding the integration of the two fields.
2002370809;Block-Row Sparse Multiview Multilabel Learning for Image Classification;2016.0;[];In image analysis, the images are often represented by multiple visual features (also known as multiview features), that aim to better interpret them for achieving remarkable performance of the learning. Since the processes of feature extraction on each view are separated, the multiple visual features of images may include overlap, noise, and redundancy. Thus, learning with all the derived views of the data could decrease the effectiveness. To address this, this paper simultaneously conducts a hierarchical feature selection and a multiview multilabel (MVML) learning for multiview image classification, via embedding a proposed a new block-row regularizer into the MVML framework. The block-row regularizer concatenating a Frobenius norm (   ${F}$   -norm) regularizer and an    $\boldsymbol {\ell }_{\textbf {2,1}}$   -norm regularizer is designed to conduct a hierarchical feature selection, in which the    ${F}$   -norm regularizer is used to conduct a high-level feature selection for selecting the informative views (i.e., discarding the uninformative views) and the    $\boldsymbol {\ell }_{\textbf {2,1}}$   -norm regularizer is then used to conduct a low-level feature selection on the informative views. The rationale of the use of a block-row regularizer is to avoid the issue of the over-fitting (via the block-row regularizer), to remove redundant views and to preserve the natural group structures of data (via the    ${F}$   -norm regularizer), and to remove noisy features (the    $\boldsymbol {\ell }_{\textbf {2,1}}$   -norm regularizer), respectively. We further devise a computationally efficient algorithm to optimize the derived objective function and also theoretically prove the convergence of the proposed optimization method. Finally, the results on real image datasets show that the proposed method outperforms two baseline algorithms and three state-of-the-art algorithms in terms of classification performance.
2002769487;The spectral analysis for a singular fractional differential equation with a signed measure;2015.0;[];In this paper, by using the spectral analysis of the relevant linear operator and Gelfandu0027s formula, we obtain some properties of the first eigenvalue of a fractional differential equation. Based on these properties, the fixed point index of the nonlinear operator is calculated explicitly and some sufficient conditions for the existence of positive solutions are established.
2002772301;Research on warehouse operation: A comprehensive review;2007.0;[];An extensive review on warehouse operation planning problems is presented. The problems are classified according to the basic warehouse functions, i.e., receiving, storage, order picking, and shipping. The literature in each category is summarized with an emphasis on the characteristics of various decision support models and solution algorithms. The purpose is to provide a bridge between academic researchers and warehouse practitioners, explaining what planning models and methods are currently available for warehouse operations, and what are the future research opportunities.
2002827932;Two algorithms for constructing a Delaunay triangulation;1980.0;[];This paper provides a unified discussion of the Delaunay triangulation. Its geometric properties are reviewed and several applications are discussed. Two algorithms are presented for constructing the triangulation over a planar set ofN points. The first algorithm uses a divide-and-conquer approach. It runs inO(N logN) time, which is asymptotically optimal. The second algorithm is iterative and requiresO(N2) time in the worst case. However, its average case performance is comparable to that of the first algorithm.
2003087683;Obtaining interpretable fuzzy classification rules from medical data;1999.0;[];For many application problems classifiers can be used to support a decision making process. In some domains-in areas like medicine especially-it is preferable not to use black box approaches. The user should be able to understand the classifier and to evaluate its results. Fuzzy rule based classifiers are especially suitable, because they consist of simple linguistically interpretable rules and do not have some of the drawbacks of symbolic or crisp rule based classifiers. Classifiers must often be created from data by a learning process, because there is not enough expert knowledge to determine their parameters completely. A simple and convenient way to learn fuzzy classifiers from data is provided by neuro-fuzzy approaches. In this paper we discuss extensions to the learning algorithms of neuro-fuzzy classification (NEFCLASS), a neuro-fuzzy approach for data analysis that we have presented before. We present interactive strategies for pruning rules and variables from a trained classifier to enhance its readability, and demonstrate our approach on a small example.
2003373522;From goals to components: a combined approach to self-management;2008.0;[];Autonomous or semi-autonomous systems are deployed in environments where contact with programmers or technicians is infrequent or undesirable. To operate reliably, such systems should be able to adapt to new circumstances on their own. This paper describes our combined approach for adaptable software architecture and task synthesis from high-level goals, which is based on a three-layer model. In the uppermost layer, reactive plans are generated from goals expressed in a temporal logic. The middle layer is responsible for plan execution and assembling a configuration of domain-specific software components, which reside in the lowest layer. Moreover, the middle layer is responsible for selecting alternative components when the current configuration is no longer viable for the circumstances that have arisen. The implementation demonstrates that the approach enables us to handle non-determinism in the environment and unexpected failures in software components.
2004320486;A support vector machine classifier with rough set-based feature selection for breast cancer diagnosis;2011.0;[];Breast cancer is becoming a leading cause of death among women in the whole world, meanwhile, it is confirmed that the early detection and accurate diagnosis of this disease can ensure a long survival of the patients. Expert systems and machine learning techniques are gaining popularity in this field because of the effective classification and high diagnostic capability. In this paper, a rough set (RS) based supporting vector machine classifier (RS_SVM) is proposed for breast cancer diagnosis. In the proposed method (RS_SVM), RS reduction algorithm is employed as a feature selection tool to remove the redundant features and further improve the diagnostic accuracy by SVM. The effectiveness of the RS_SVM is examined on Wisconsin Breast Cancer Dataset (WBCD) using classification accuracy, sensitivity, specificity, confusion matrix and receiver operating characteristic (ROC) curves. Experimental results demonstrate the proposed RS_SVM can not only achieve very high classification accuracy but also detect a combination of five informative features, which can give an important clue to the physicians for breast diagnosis.
2004455776;Identifying a better measure of relatedness for mapping science;2006.0;[];Measuring the relatedness between bibliometric units (journals, documents, authors, or words) is a central task in bibliometric analysis. Relatedness measures are used for many different tasks, among them the generating of maps, or visual pictures, showing the relationship between all items from these data. Despite the importance of these tasks, there has been little written on how to quantitatively evaluate the accuracy of relatedness measures or the resulting maps. The authors propose a new framework for assessing the performance of relatedness measures and visualization algorithms that contains four factors: accuracy, coverage, scalability, and robustness. This method was applied to 10 measures of journal–journal relatedness to determine the best measure. The 10 relatedness measures were then used as inputs to a visualization algorithm to create an additional 10 measures of journal–journal relatedness based on the distances between pairs of journals in two-dimensional space. This second step determines robustness (i.e., which measure remains best after dimension reduction). Results show that, for low coverage (under 50p), the Pearson correlation is the most accurate raw relatedness measure. However, the best overall measure, both at high coverage, and after dimension reduction, is the cosine index or a modified cosine index. Results also showed that the visualization algorithm increased local accuracy for most measures. Possible reasons for this counterintuitive finding are discussed. © 2006 Wiley Periodicals, Inc.
2004641611;Spectral and complexity analysis of scalp EEG characteristics for mild cognitive impairment and early Alzheimer's disease;2014.0;[];Amnestic mild cognitive impairment (aMCI) often is an early stage of Alzheimeru0027s disease (AD). MCI is characterized by cognitive decline departing from normal cognitive aging but that does not significantly interfere with daily activities. This study explores the potential of scalp EEG for early detection of alterations from cognitively normal status of older adults signifying MCI and AD. Resting 32-channel EEG records from 48 age-matched participants (mean age 75.7 years)-15 normal controls (NC), 16 early MCI, and 17 early stage AD-are examined. Regional spectral and complexity features are computed and used in a support vector machine model to discriminate between groups. Analyses based on three-way classifications demonstrate overall discrimination accuracies of 83.3%, 85.4%, and 79.2% for resting eyes open, counting eyes closed, and resting eyes closed protocols, respectively. These results demonstrate the great promise for scalp EEG spectral and complexity features as noninvasive biomarkers for detection of MCI and early AD.
2005060246;Data Offloading Techniques in Cellular Networks: A Survey;2015.0;[];One of the most engaging challenges for mobile operators today is how to manage the exponential data traffic increase. Mobile data offloading stands out as a promising and low-cost solution to reduce the burden on the cellular network. To make this possible, we need a new hybrid network paradigm that leverages the existence of multiple alternative communication channels. This entails significant modifications in the way data are handled, affecting also the behavior of network protocols. In this paper, we present a comprehensive survey of data offloading techniques in cellular networks and extract the main requirements needed to integrate data offloading capabilities into todayu0027s mobile networks. We classify existing strategies into two main categories, according to their requirements in terms of content delivery guarantees: delayed and nondelayed offloading. We overview the technical aspects and discuss the state of the art in each category. Finally, we describe in detail the novel functionalities needed to implement mobile data offloading in the access network, as well as current and future research challenges in the field, with an eye toward the design of hybrid architectures.
2005070934;Epidemic Strategies in Delay Tolerant Networks from an Energetic Point of View: Main Issues and Performance Evaluation;2015.0;[];"In this work we investigate Epidemic novel strategies called SNPS and EAER-SNPS applied to a DTN Network. These strategies extend the approach of the basic epidemic routing by using the node density estimation and the nodes energy levels; they adopt the forwarding scheme of the n-Epidemic routing. However, differently from n-Epidemic, it applies a dynamic forwarding scheme based on nodes density, that is able to reduce energy consumption and increase message delivery probability. A deep campaign of simulations was carried out in order to verify the effectiveness of the SNPS and EAER-SNPS. Simulation campaigns were conducted to evaluate the message delivery ratio, the average hop count, the average end-to-end delay and the average residual energy. Different mobility scenarios have been considered through the Working Day mobility model and by simulating pedestrians, buses and cars, in order to see how the performance can degrade on the basis of mobility and nodes characteristics"
2005812157;Optimal task allocation and hardware redundancy policies in distributed computing systems;2003.0;[];   A distributed computing system (DCS) in general consists of processing nodes, communication channels, and tasks. Achieving a reliable DCS thus comprises three parts: the realization of reliable task processing, reliable communication among processing nodes, and a good task allocation strategy. In this study, we examine the relationship between system cost and system reliability in a cycle-free hardware-redundant DCS where multiple processors are available at each processing node and multiple communication links are available at each communication channel. Intuitively, higher hardware redundancy leads to higher system reliability which results in the reduction of communication cost. Such an endowment of hardware redundancy, however, incurs higher hardware operating cost. A unified model of system cost is therefore developed in this study that is a complex function of task allocation and hardware redundancy policies, and a hybrid genetic algorithm (HGA) based on genetic algorithms and a local search procedure is proposed to seek the optimal task allocation and hardware redundancy policies. The proposed algorithm is tested on randomly generated DCSs and compared with a simple genetic algorithm (SGA). The simulation results show that the HGA gives higher solution quality in less computational time than the SGA.
2006319938;Relating Categorical Semantics for Intuitionistic Linear Logic;2005.0;[];There are several kinds of linear typed calculus in the literature, some with their associated notion of categorical model. Our aim in this paper is to systematise the relationship between three of these linear typed calculi and their models. We point out that mere soundness and completeness of a linear typed calculus with respect to a class of categorical models are not sufficient to identify the most appropriate class uniquely. We recommend instead to use the notion of internal language when relating a typed calculus to a class of models. After clarifying the internal languages of the categories of models in the literature we relate these models via reflections and coreflections.
2006697004;Foot-based mobile interaction with games;2004.0;[];"Interaction with mobile applications is often awkward due to the limited and miniaturized input modalities available. This is especially problematic for games where the only incentive to use an application is the pleasure derived from the interaction. It is therefore interesting to examine novel forms of interaction in order to increase the ""playability"" of mobile games.In this paper we present a simple mobile gaming application on a standard Pocket PC PDA that employs computer vision (CV) as itu0027s main interaction modality. Practical experience with the application demonstrates the feasibility of CV as a primary interaction modality and indicates the high potential of CV as an input modality for mobile devices in the future. Our approach exploits the video capabilities that are becoming ubiquitous on camera equipped smart-phones and PDAs to provide a fun solution for interaction tasks in games like ""Pong"", ""Break-out"" or soccer."
2006761437;DynaMMo: mining and summarization of coevolving sequences with missing values;2009.0;[];"Given multiple time sequences with missing values, we propose DynaMMo which summarizes, compresses, and finds latent variables. The idea is to discover hidden variables and learn their dynamics, making our algorithm able to function even when there are missing values.   We performed experiments on both real and synthetic datasets spanning several megabytes, including motion capture sequences and chlorine levels in drinking water. We show that our proposed DynaMMo method (a) can successfully learn the latent variables and their evolution; (b) can provide high compression for little loss of reconstruction accuracy; (c) can extract compact but powerful features for segmentation, interpretation, and forecasting; (d) has complexity linear on the duration of sequences."
2006838288;Optimal allocation of redundancies in series systems;2012.0;[];It is of great interest for the problem of how to allocate redundancies in a system so as to optimize the system performance in reliability engineering and system security. In this paper, we consider the problems of optimal allocation of both active and standby redundancies in series systems in the sense of various stochastic orderings. For the case of allocating one redundancy to a series system with two exponential components, we establish two likelihood ratio order results for active redundancy case and standby redundancy case, respectively. We also discuss the case of allocating K active redundancies to a series system and establish some new results. The results developed here strengthen and generalize some of the existing results in the literature. Specifically, we give an answer to an open problem mentioned in Hu and Wang [T. Hu, Y. Wang, Optimal allocation of active redundancies in r-out-of-n systems, Journal of Statistical Planning and Inference 139 (2009) 3733–3737]. Numerical examples are provided to illustrate the theoretic results established here.
2006977321;Traffic management strategy for delay-tolerant networks;2012.0;[];Messages in delay-tolerant networks (DTNs) are generally classified by relative priority into low, medium, and high priority classes, thus creating challenges in structuring scheduling and drop policies of a traffic management system. This study proposes a novel traffic management strategy for DTNs. The proposed scheme improves message deliveries of different priority classes by utilizing message properties such as estimated total number of replicas, elapsed time, and remaining time-to-live. A delete mechanism of delivered messages is also incorporated to use buffer space efficiently. The performance of the proposed strategy is verified via implementation of a simulation model along with existing scheduling and drop policies and is tested with two well-known real-world trace datasets. The results demonstrate that the proposed strategy yields improved message deliveries of high priority as well as medium and low priority classes.
2007196136;A FTA-Based Method for Risk Decision Making in Emergency Response;2011.0;[];Emergency decision making problem is a crucial issue of emergency management and is a valuable academic research topic. Although some research has been conducted, there has been no attempt to solve emergency decision making problems on the basis of analyzing the influence of alternatives on the development and evolvement of emergency. In this paper, a novel FTA-based method is proposed for risk decision making in emergency response. In the method, firstly, a fault tree of the undesirable state of emergency is constructed, by which the influence of alternatives on the emergency can be analyzed. On this basis, the probabilities that the undesirable state will occur given that different alternatives are chosen are estimated. Then, according to the determined probabilities, the overall ranking values of alternatives are calculated based on multiple criteria risk decision making (MCRDM). Furthermore, a ranking of alternatives is determined according to the overall ranking values. Finally, a practical example is used to illustrate the feasibility and validity of the proposed method. The proposed method overcomes the limitations of existing methods that the influence of alternatives on emergency is not considered, and enriches the theories and methods for emergency decision making.
2007521282;Positive solutions of an infinite boundary value problem for nth-order nonlinear impulsive singular integro-differential equations in Banach spaces;2012.0;[];   In this paper, the cone theory and Monch fixed point theorem combined with a monotone iterative technique are used to investigate the positive solutions of a class of boundary problems for  n th-order nonlinear impulsive singular integro-differential equations of mixed type on an infinite interval in Banach spaces. The conditions for the existence of a positive solution are established. In addition, an explicit iterative approximation of the solution for the boundary value problem is derived.
2007681157;Wheel and star-critical Ramsey numbers for quadrilateral;2015.0;[];The star-critical Ramsey number r ? ( H 1 , H 2 ) is the smallest integer k such that every red/blue coloring of the edges of K n - K 1 , n - k - 1 contains either a red copy of H 1 or a blue copy of H 2 , where n is the graph Ramsey number R ( H 1 , H 2 ) . We study the cases of r ? ( C 4 , C n ) and R ( C 4 , W n ) . In particular, we prove that r ? ( C 4 , C n ) = 5 for all n ? 4 , obtain a general characterization of Ramsey-critical ( C 4 , C n ) -graphs, and establish the exact values of R ( C 4 , W n ) for 9 cases of n between 18 and 44 .
2007972815;NUS-WIDE: a real-world web image database from National University of Singapore;2009.0;[];"This paper introduces a web image dataset created by NUSu0027s Lab for Media Search. The dataset includes: (1) 269,648 images and the associated tags from Flickr, with a total of 5,018 unique tags; (2) six types of low-level features extracted from these images, including 64-D color histogram, 144-D color correlogram, 73-D edge direction histogram, 128-D wavelet texture, 225-D block-wise color moments extracted over 5x5 fixed grid partitions, and 500-D bag of words based on SIFT descriptions; and (3) ground-truth for 81 concepts that can be used for evaluation. Based on this dataset, we highlight characteristics of Web image collections and identify four research issues on web image annotation and retrieval. We also provide the baseline results for web image annotation by learning from the tags using the traditional k-NN algorithm. The benchmark results indicate that it is possible to learn effective models from sufficiently large image dataset to facilitate general image retrieval."
2008010437;On the J-divergence of intuitionistic fuzzy sets with its application to pattern recognition;2008.0;[];The importance of suitable distance measures between intuitionistic fuzzy sets (IFSs) arises because of the role they play in the inference problem. A concept closely related to one of distance measures is a divergence measure based on the idea of information-theoretic entropy that was first introduced in communication theory by Shannon (1949). It is known that J-divergence is an important family of divergences. In this paper, we construct J-divergence between IFSs. The proposed J-divergence can induce some useful distance and similarity measures between IFSs. Numerical examples demonstrate that the proposed measures perform well in clustering and pattern recognition.
2009096031;Video Highlight Shot Extraction with Time-Sync Comment;2015.0;[];"Benefit from abundance of mobile applications, portability of large-screen mobile devices and accessibility of media resources, users nowadays much more prefer to watch videos on their mobiles no matter whether they are at home or on the way. However, constrained by available time and network flow, users may only choose to watch some hot video segments that are manually annotated by video editors. In this paper, we aim to automatically extract video highlight shot with the help of video sentimental feature of time-sync comments. First, analyzing statistical feature of real data. After, we simulate the generation process of time-sync comment after. Then, we propose a shot boundary detection method to extract highlight shot, which is proved to be more effective than traditional methods based on comment density. This experiment attests the time-sync comment is particularly suitable for sentiment-based video segment extraction for 2 reasons. 1) Text-based similarity calculation of is much faster than image-based process depending on every frame of video; 2) Time-sync comment reflects user subjective emotion therefore is useful in personalised video recommendation."
2009497894;Privacy-preserving personal health record using multi-authority attribute-based encryption with revocation;2015.0;[];Personal health record (PHR) service is an emerging model for health information exchange. In PHR systems, patientu0027s health records and information are maintained by the patient himself through the Web. In reality, PHRs are often outsourced to be stored at the third parties like cloud service providers. However, there have been serious privacy concerns about cloud service as it may expose useru0027s sensitive data like PHRs to those cloud service providers or unauthorized users. Using attribute-based encryption (ABE) to encrypt patientu0027s PHRs in cloud environment, secure and flexible access control can be achieved. Yet, problems like scalability in key management, fine-grained access control, and efficient user revocation remain to be addressed. In this paper, we propose a privacy-preserving PHR, which supports fine-grained access control and efficient revocation. To be specific, our scheme achieves the goals (1) scalable and fine-grained access control for PHRs by using multi-authority ABE scheme, and (2) efficient on-demand user/attribute revocation and dynamic policy update. In our scheme, we consider the situation that multiple data owners exist, and patientu0027s PHRs are encrypted and stored in semi-trust servers. The access structure in our scheme is expressive access tree structure, and the security of our scheme can be reduced to the standard decisional bilinear Diffie---Hellman assumption.
2010123060;Maximizing deviation method for multiple attribute decision making in intuitionistic fuzzy setting;2008.0;[];With respect to multiple attribute decision making problems with intuitionistic fuzzy information, some operational laws of intuitionistic fuzzy numbers, score function and accuracy function of intuitionistic fuzzy numbers are introduced. An optimization model based on the maximizing deviation method, by which the attribute weights can be determined, is established. For the special situations where the information about attribute weights is completely unknown, we establish another optimization model. By solving this model, we get a simple and exact formula, which can be used to determine the attribute weights. We utilize the intuitionistic fuzzy weighted averaging (IFWA) operator to aggregate the intuitionistic fuzzy information corresponding to each alternative, and then rank the alternatives and select the most desirable one(s) according to the score function and accuracy function. Finally, an illustrative example is given to verify the developed approach and to demonstrate its practicality and effectiveness.
2010125850;Apparent Fibre Density: a novel measure for the analysis of diffusion-weighted magnetic resonance images.;2012.0;[];   This article proposes a new measure called Apparent Fibre Density (AFD) for the analysis of high angular resolution diffusion-weighted images using higher-order information provided by fibre orientation distributions (FODs) computed using spherical deconvolution. AFD has the potential to provide specific information regarding differences between populations by identifying not only the location, but also the orientations along which differences exist. In this work, analytical and numerical Monte-Carlo simulations are used to support the use of the FOD amplitude as a quantitative measure (i.e. AFD) for population and longitudinal analysis. To perform robust voxel-based analysis of AFD, we present and evaluate a novel method to modulate the FOD to account for changes in fibre bundle cross-sectional area that occur during spatial normalisation. We then describe a novel approach for statistical analysis of AFD that uses cluster-based inference of differences extended throughout space and orientation. Finally, we demonstrate the capability of the proposed method by performing voxel-based AFD comparisons between a group of Motor Neurone Disease patients and healthy control subjects.  A significant decrease in AFD was detected along voxels and orientations corresponding to both the corticospinal tract and corpus callosal fibres that connect the primary motor cortices. In addition to corroborating previous findings in MND, this study demonstrates the clear advantage of using this type of analysis by identifying differences along single fibre bundles in regions containing multiple fibre populations.
2010130012;Necessary measures: metric-driven information security risk assessment and decision making;2007.0;[];Use measurable, reliable real-world metrics to improve information security decision making.
2010348199;TrendFocus: Visualization of trends in financial news with indicator sets;2014.0;[];Although many current visualization approaches to time-series data can show a good general trend of a topic or theme over time, most cannot help people gain a deeper understanding of key issues. They may illustrate how some key issues come up and disappear from time to time, but they typically provide limited information about relationships of interest. Financial and economic analysts are often interested in how financial or economic performance indicators interplay or change over time. With a set of well-defined and commonly used equations or indicators in the domain of Economics and Finance, we can refine our visualization by keeping track of these variables. In this paper, we make use of a line plot layout to track some of these indicators and leverage word clouds to reveal how these indicators are affected by the changes in social and economic environments.
2010416066;Maximum inner-product search using cone trees;2012.0;[];The problem of efficiently finding the best match for a query in a given set with respect to the Euclidean distance or the cosine similarity has been extensively studied. However, the closely related problem of efficiently finding the best match with respect to the inner-product has never been explored in the general setting to the best of our knowledge. In this paper we consider this problem and contrast it with the previous problems considered. First, we propose a general branch-and-bound algorithm based on a (single) tree data structure. Subsequently, we present a dual-tree algorithm for the case where there are multiple queries. Our proposed branch-and-bound algorithms are based on novel inner-product bounds. Finally we present a new data structure, the cone tree, for increasing the efficiency of the dual-tree algorithm. We evaluate our proposed algorithms on a variety of data sets from various applications, and exhibit up to five orders of magnitude improvement in query time over the naive search technique in some cases.
2010634640;Probabilistic routing in intermittently connected networks;2003.0;[];We consider the problem of routing in intermittently connected networks. In such networks there is no guarantee that a fully connected path between source and destination exist at any time, rendering traditional routing protocols unable to deliver messages between hosts. We propose a probabilistic routing protocol for such networks.
2010659620;A new crossover operator for real coded genetic algorithms;2007.0;[];   In this paper, a new real coded crossover operator, called the Laplace Crossover (LX) is proposed. LX is used in conjunction with two well known mutation operators namely the Makinen, Periaux and Toivanen Mutation (MPTM) and Non-Uniform Mutation (NUM) to define two new generational genetic algorithms LX–MPTM and LX–NUM respectively. These two genetic algorithms are compared with two existing genetic algorithms (HX–MPTM and HX–NUM) which comprise of Heuristic Crossover operator and same two mutation operators. A set of 20 test problems available in the global optimization literature is used to test the performance of these four genetic algorithms. To judge the performance of the LX operator, two kinds of analysis is performed. Firstly a pair wise comparison is performed between LX–MPTM and HX–MPTM, and then between LX–NUM and HX–NUM. Secondly the overall comparison of performances of all the four genetic algorithms is carried out based on a performance index (PI). The comparative study shows that Laplace crossover (LX) performs quite well and one of the genetic algorithms defined (LX–MPTM) outperforms other genetic algorithms.
2010739835;Dependable Demand Response Management in the Smart Grid: A Stackelberg Game Approach;2013.0;[];Demand Response Management (DRM) is a key component in the smart grid to effectively reduce power generation costs and user bills. However, it has been an open issue to address the DRM problem in a network of multiple utility companies and consumers where every entity is concerned about maximizing its own benefit. In this paper, we propose a Stackelberg game between utility companies and end-users to maximize the revenue of each utility company and the payoff of each user. We derive analytical results for the Stackelberg equilibrium of the game and prove that a unique solution exists. We develop a distributed algorithm which converges to the equilibrium with only local information available for both utility companies and end-users. Though DRM helps to facilitate the reliability of power supply, the smart grid can be succeptible to privacy and security issues because of communication links between the utility companies and the consumers. We study the impact of an attacker who can manipulate the price information from the utility companies. We also propose a scheme based on the concept of shared reserve power to improve the grid reliability and ensure its dependability.
2010740403;A novel cross-entropy and entropy measures of IFSs and their applications;2013.0;[];"In this paper we discussed novel cross-entropy and entropy models on intuitionism and fuzziness of intuitionistic fuzzy sets (IFSs). In order to measure the discrimination uncertain information, cross-entropy and symmetric cross-entropy are defined based on intuitionistic factor and fuzzy factor. In particular, the constructive principle of entropy is refined; relationship between cross-entropy and entropy is investigated, so we establish a novel intuitionistic fuzzy entropy formula that simultaneously takes into account intuitionistic entropy and fuzzy entropy. We also propose concepts of marginal rate of substitution (MRS) and elasticity of substitution (ES) to describe substitution effect between intuitionism and fuzziness. Moreover, we extend the cross-entropy and entropy models to general version, which could be adjusted by different parameters, and comparative analysis with other existed cross-entropy and entropy measures are presented. Finally, we give applications of pattern recognition and decision making to demonstrate the efficiency of the cross-entropy and entropy models."
2010835019;Impulsivity in Internet Addiction: A Comparison with Pathological Gambling;2012.0;[]; Internet addiction has been considered to be associated with poor impulse control. The aim of this study is to compare the trait impulsivity of those suffering from Internet addiction with that of individuals suffering from pathological gambling. Twenty-seven patients diagnosed with Internet addiction (age: 24.78±4.37 years), 27 patients diagnosed with pathological gambling (age: 25.67±3.97 years), and 27 healthy controls (age: 25.33±2.79 years) were enrolled in this study. All patients were men seeking treatment. Trait impulsivity and the severity of the Internet addiction and pathological gambling were measured by the Barratt Impulsiveness Scale-11, the Youngu0027s Internet Addiction Test, and the South Oaks Gambling Screen, respectively. The Beck Depression Inventory and the Beck Anxiety Inventory were also administered to all subjects. Our results show that those suffering from Internet addiction showed increased levels of trait impulsivity which were comparable to those of patients diagnosed wit...
2011170389;Classification of EEG signals using a novel genetic programming approach;2014.0;[];In this paper, we present a new method for classification of electroencephalogram (EEG) signals using Genetic Programming (GP). The Empirical Mode Decomposition (EMD) is used to extract the features of EEG signals which served as an input for the GP. In this paper, new constructive crossover and mutation operations are also produced to improve GP. In these constructive crossover and mutation operators hill climbing search is integrated to remove the destructive nature of these operators. To improve GP, we apply constructive crossover on all the individuals which remain after reproduction. A new concept of selecting the global prime off-springs of the generation is also proposed. The constructive mutation approach is applied to poor individuals who are left after selecting globally prime off-springs. Improvement of the method is measured against classification accuracy, training time and the number of generations for EEG signal classification. As we show in the results section, the classification accuracy can be estimated to be 98.69% on the test cases, which is better than classification accuracy of Liang and coworkers method which was published in 2010.
2011390239;Fractional-order embedding canonical correlation analysis and its applications to multi-view dimensionality reduction and recognition;2014.0;[];Due to the noise disturbance and limited number of training samples, within-set and between-set sample covariance matrices in canonical correlation analysis (CCA) usually deviate from the true ones. In this paper, we re-estimate within-set and between-set covariance matrices to reduce the negative effect of this deviation. Specifically, we use the idea of fractional order to respectively correct the eigenvalues and singular values in the corresponding sample covariance matrices, and then construct fractional-order within-set and between-set scatter matrices which can obviously alleviate the problem of the deviation. On this basis, a new approach is proposed to reduce the dimensionality of multi-view data for classification tasks, called fractional-order embedding canonical correlation analysis (FECCA). The proposed method is evaluated on various handwritten numeral, face and object recognition problems. Extensive experimental results on the CENPARMI, UCI, ATu0026T, AR, and COIL-20 databases show that FECCA is very effective and obviously outperforms the existing joint dimensionality reduction or feature extraction methods in terms of classification accuracy. Moreover, its improvements for recognition rates are statistically significant on most cases below the significance level 0.05. A new method, namely FECCA, is presented for multi-view dimensionality reduction.Fractional-order within-set and between-set scatter matrices are constructed by sample spectrum modeling.The extracted features by FECCA have strong discriminant power for recognition.Experimental results show FECCA has better recognition rates than the existing joint feature extraction methods.
2012211082;Acoustic robot navigation using distributed microphone arrays;2004.0;[];   This paper presents a method for the navigation of a mobile robot using sound localization in the context of a robotic lab tour guide. Sound localization, which is achieved using an array of 24 microphones distributed on two walls of the lab, is performed whenever the robot speaks as part of the tour. The SRP-PHAT sound localization algorithm is used to estimate the current location of the robot using approximately 2 s of recorded signal. Navigation is achieved using several stops during which the estimated location of the robot is used to make course adjustments. Experiments using the acoustic robot navigation system illustrate the accuracy of the proposed technique, which resulted in an average localization error of about 7 cm close to the array and 30 cm far away from the array.
2012464934;Stable and Accurate Artificial Dissipation;2004.0;[];Stability for nonlinear convection problems using centered difference schemes require the addition of artificial dissipation. In this paper we present dissipation operators that preserve both stability and accuracy for high order finite difference approximations of initial boundary value problems.
2012833704;Similarity estimation techniques from rounding algorithms;2002.0;[];(MATH) A locality sensitive hashing scheme is a distribution on a family $\F$ of hash functions operating on a collection of objects, such that for two objects x,y, PrheF[h(x) = h(y)] = sim(x,y), where sim(x,y) e [0,1] is some similarity function defined on the collection of objects. Such a scheme leads to a compact representation of objects so that similarity of objects can be estimated from their compact sketches, and also leads to efficient algorithms for approximate nearest neighbor search and clustering. Min-wise independent permutations provide an elegant construction of such a locality sensitive hashing scheme for a collection of subsets with the set similarity measure sim(A,B) = \frac{|A P B|}{|A P Ehe\F [d(h(P),h(Q))] x O(log n log log n). EMD(P, Q).  .
2012927050;Opportunistic routing based on daily routines;2012.0;[];Opportunistic routing is being investigated to enable the proliferation of low-cost wireless applications. A recent trend is looking at social structures, inferred from the social nature of human mobility, to bring messages close to a destination. To have a better picture of social structures, social-based opportunistic routing solutions should consider the dynamism of usersu0027 behavior resulting from their daily routines. We address this challenge by presenting dLife, a routing algorithm able to capture the dynamics of the network represented by time-evolving social ties between pair of nodes. Experimental results based on synthetic mobility models and real human traces show that dLife has better delivery probability, latency, and cost than proposals based on social structures.
2013014665;The jMetal framework for multi-objective optimization: Design and architecture;2010.0;[];jMetal is a Java-based framework for multi-objective optimization using metaheuristics. It is a flexible, extensible, and easy-to-use software package that has been used in a wide range of applications. In this paper, we describe the design issues underlying jMetal, focusing mainly on its internal architecture, with the aim of offering a comprehensive view of its main features to interested researchers. Among the covered topics, we detail the basic components facilitating the implementation of multi-objective metaheuristics (solution representations, operators, problems, density estimators, archives), the included quality indicators to assess the performance of the algorithms, and jMetalu0027s support to carry out full experimental studies.
2013298510;A survey on engineering approaches for self-adaptive systems;2015.0;[];The complexity of information systems is increasing in recent years, leading to increased effort for maintenance and configuration. Self-adaptive systems (SASs) address this issue. Due to new computing trends, such as pervasive computing, miniaturization of IT leads to mobile devices with the emerging need for context adaptation. Therefore, it is beneficial that devices are able to adapt context. Hence, we propose to extend the definition of SASs and include context adaptation. This paper presents a taxonomy of self-adaptation and a survey on engineering SASs. Based on the taxonomy and the survey, we motivate a new perspective on SAS including context adaptation.
2013667086;Binary RDF representation for publication and exchange (HDT);2013.0;[];The current Web of Data is producing increasingly large RDF datasets. Massive publication efforts of RDF data driven by initiatives like the Linked Open Data movement, and the need to exchange large datasets has unveiled the drawbacks of traditional RDF representations, inspired and designed by a document-centric and human-readable Web. Among the main problems are high levels of verbosity/redundancy and weak machine-processable capabilities in the description of these datasets. This scenario calls for efficient formats for publication and exchange. This article presents a binary RDF representation addressing these issues. Based on a set of metrics that characterizes the skewed structure of real-world RDF data, we develop a proposal of an RDF representation that modularly partitions and efficiently represents three components of RDF datasets: Header information, a Dictionary, and the actual Triples structure (thus called HDT). Our experimental evaluation shows that datasets in HDT format can be compacted by more than fifteen times as compared to current naive representations, improving both parsing and processing while keeping a consistent publication scheme. Specific compression techniques over HDT further improve these compression rates and prove to outperform existing compression solutions for efficient RDF exchange.
2013908206;Intuitionistic Fuzzy Bonferroni Means;2011.0;[];The Bonferroni mean (BM) was originally introduced by Bonferroni and then more recently generalized by Yager. The desirable characteristic of the BM is its capability to capture the interrelationship between input arguments. Nevertheless, it seems that the existing literature only considers the BM for aggregating crisp numbers instead of any other types of arguments. In this paper, we investigate the BM under intuitionistic fuzzy environments. We develop an intuitionistic fuzzy BM (IFBM) and discuss its variety of special cases. Then, we apply the weighted IFBM to multicriteria decision making. Some numerical examples are given to illustrate our results.
2014124897;Improving neighborhood based Collaborative Filtering via integrated folksonomy information;2012.0;[];"Personalized recommender systems which can provide people with suggestions according to individual interests usually rely on Collaborative Filtering (CF). The neighborhood based model (NBM) is a common choice when implementing such recommenders due to the intuitive nature; however, the recommendation accuracy is a major concern. Current NBM based recommenders mostly address the accuracy issue based on the rating data alone, whereas research on hybrid recommender systems suggests that users enjoy specifying feedback about items across multiple dimensions. In this work we aim to improve the accuracy of NBM via integrating the folksonomy information. To achieve this objective, we first propose the folksonomy network (FN) to analyze the item relevance described by the folksonomy data. We subsequently integrate the obtained folksonomy information into the global-optimization based NBM for making multi-source based recommendations. Experiments on the MovieLens dataset suggest positive results, which prove the efficiency of our strategy."
2014374374;Network Similarity Decomposition (NSD): A Fast and Scalable Approach to Network Alignment;2012.0;[];As graph-structured data sets become commonplace, there is increasing need for efficient ways of analyzing such data sets. These analyses include conservation, alignment, differentiation, and discrimination, among others. When defined on general graphs, these problems are considerably harder than their well-studied counterparts on sets and sequences. In this paper, we study the problem of global alignment of large sparse graphs. Specifically, we investigate efficient methods for computing approximations to the state-of-the-art IsoRank solution for finding pairwise topological similarity between nodes in two networks (or within the same network). Pairs of nodes with high similarity can be used to seed global alignments. We present a novel approach to this computationally expensive problem based on uncoupling and decomposing ranking calculations associated with the computation of similarity scores. Uncoupling refers to independent preprocessing of each input graph. Decomposition implies that pairwise similarity scores can be explicitly broken down into contributions from different link patterns traced back to a low-rank approximation of the initial conditions for the computation. These two concepts result in significant improvements, in terms of computational cost, interpretability of similarity scores, and nature of supported queries. We show over two orders of magnitude improvement in performance over IsoRank/Random Walk formulations, and over an order of magnitude improvement over constrained matrix-triple-product formulations, in the context of real data sets.
2014406364;An Algorithm for the Dynamic Relocation of Fire Companies;1974.0;[];"When all the fire companies in a region are engaged in fighting fires, protection against a future fire is considerably reduced. It is standard practice in many urban fire departments to protect the exposed region by relocating outside fire companies temporarily to some of the vacant houses. In New York City, situations requiring such relocations arise ten times a day on the average. The Fire Department of the City of New York FDNY currently makes its relocations according to a system of preplanned moves. This system was designed at a time when alarm rates were low and is based on the assumption that only one fire is in progress at a time. Because of the high alarm rates currently being experienced in parts of New York City, this assumption is no longer valid, and the preplanned relocation system breaks down at the times when it is needed  :[147],""paper describes a computer-based method for determining relocations that overcomes the deficiencies of the existing method by utilizing the computeru0027s ability to 1 store up-to-date information about the status of all fires in progress and the location and activity of all fire companies, 2 generate and compare many alternative relocation plans quickly. The method, which will become part of the FDNYu0027s real-time Management Information and Control System MICS, is designed to be fast and to require little computer  :[226],""giving some background of the problem and the objectives of relocation, we give the problem a mathematical programming formulation and then describe the heuristic algorithm to be used for generating relocations in the MICS. The remainder of the :[147],""paper is devoted to a discussion of an example illustrating how the algorithm works, a rigorous test of the algorithm using a computer simulation model of Fire-Department operations, and a description of the current use of the computer algorithm by dispatchers in an interactive time-shared environment. The results of the testing indicate that the proposed algorithm is a significant improvement over existing methods, particularly in crisis situations. Although designed to solve a problem for the New York City Fire Department, the algorithm should be applicable to other cities."
2014417436;Entropy/cross entropy-based group decision making under intuitionistic fuzzy environment;2012.0;[];We study the group decision making problem under intuitionistic fuzzy environment. Based on entropy and cross entropy, we give two methods to determine the optimal weights of attributes, and develop two pairs of entropy and cross entropy measures for intuitionistic fuzzy values. Then, we discuss the properties of these measures and the relations between them and the existing ones. Furthermore, we introduce three new aggregation operators, which treat the membership and non-membership information fairly, to aggregate intuitionistic fuzzy information. Finally, several practical examples are presented to illustrate the developed methods.
2014500386;Soft computing approach for reliability optimization: State-of-the-art survey;2006.0;[];In the broadest sense, reliability is a measure of performance of systems. As systems have grown more complex, the consequences of their unreliable behavior have become severe in terms of cost, effort, lives, etc., and the interest in assessing system reliability and the need for improving the reliability of products and systems have become very important. Most solution methods for reliability optimization assume that systems have redundancy components in series and/or parallel systems and alternative designs are available. Reliability optimization problems concentrate on optimal allocation of redundancy components and optimal selection of alternative designs to meet system requirement. In the past two decades, numerous reliability optimization techniques have been proposed. Generally, these techniques can be classified as linear programming, dynamic programming, integer programming, geometric programming, heuristic method, Lagrangean multiplier method and so on. A Genetic Algorithm (GA), as a soft computing approach, is a powerful tool for solving various reliability optimization problems. In this paper, we briefly survey GA-based approach for various reliability optimization problems, such as reliability optimization of redundant system, reliability optimization with alternative design, reliability optimization with time-dependent reliability, reliability optimization with interval coefficients, bicriteria reliability optimization, and reliability optimization with fuzzy goals. We also introduce the hybrid approaches for combining GA with fuzzy logic, neural network and other conventional search techniques. Finally, we have some experiments with an example of various reliability optimization problems using hybrid GA approach.
2014522999;Towards Dynamic Evolution of Self-Adaptive Systems Based on Dynamic Updating of Control Loops;2012.0;[];Self-adaptive systems, which enable runtime adaptation, are promising ways of dealing with environmental changes, including system intrusions or faults. Such software systems must modify themselves to better fit their environment. One of the main approaches to constructing such systems is to introduce multiple control loops. Software evolution is an essential activity for expanding this adaptation capability, and dynamic evolution has been envisaged as a way of systems adapting themselves at runtime. In this paper, we establish a development process to deal with dynamic evolution. We devise a goal model compiler to generate models for designing dynamic evolutions and a programming framework that supports dynamic deployment of control loops. We experimentally applied our approach to a system and discuss how our compiler and framework support dynamic evolution of self-adaptive systems.
2014564484;The minimum degree threshold for perfect graph packings;2009.0;[];"Let H be any graph. We determine up to an additive constant the minimum degree of a graph G which ensures that G has a perfect H-packing (also called an H-factor). More precisely, let δ(H,n) denote the smallest integer k such that every graph G whose order n is divisible by |H| and with δ(G)≥k contains a perfect H-packing. We show that $$\delta (H,n) = \left( {1 - \frac{1} {{\chi ^ * (H)}}} \right)n +  :[75],""value of χ*(H) depends on the relative sizes of the colour classes in the optimal colourings of H and satisfies χ(H)−1u003cχ*(H)≤χ(H)."
2015191210;Linked Data - The Story So Far;2009.0;[];The term “Linked Data” refers to a set of best practices for publishing and connecting structured data on the Web. These best practices have been adopted by an increasing number of data providers over the last three years, leading to the creation of a global data space containing billions of assertions— the Web of Data. In this article, the authors present the concept and technical principles of Linked Data, and situate these within the broader context of related technological developments. They describe progress to date in publishing Linked Data on the Web, review applications that have been developed to exploit the Web of Data, and map out a research agenda for the Linked Data community as it moves forward.
2015776218;Classifying imbalanced data sets using similarity based hierarchical decomposition;2015.0;[];Classification of data is difficult if the data is imbalanced and classes are overlapping. In recent years, more research has started to focus on classification of imbalanced data since real world data is often skewed. Traditional methods are more successful with classifying the class that has the most samples (majority class) compared to the other classes (minority classes). For the classification of imbalanced data sets, different methods are available, although each has some advantages and shortcomings. In this study, we propose a new hierarchical decomposition method for imbalanced data sets which is different from previously proposed solutions to the class imbalance problem. Additionally, it does not require any data pre-processing step as many other solutions need. The new method is based on clustering and outlier detection. The hierarchy is constructed using the similarity of labeled data subsets at each level of the hierarchy with different levels being built by different data and feature subsets. Clustering is used to partition the data while outlier detection is utilized to detect minority class samples. The comparison of the proposed method with state of art the methods using 20 public imbalanced data sets and 181 synthetic data sets showed that the proposed method?s classification performance is better than the state of art methods. It is especially successful if the minority class is sparser than the majority class. It has accurate performance even when classes have sub-varieties and minority and majority classes are overlapping. Moreover, its performance is also good when the class imbalance ratio is low, i.e. classes are more imbalanced. A novel method for imbalanced dataset classification.A new hierarchical classifier which does not use a fixed feature/class hierarchy.Uses clustering and outlier detection to construct the hierarchy.Shows that different feature spaces can be used to build a hierarchy.Successful when the class imbalanced ratio is low, classes are highly overlapping.
2015777348;Testing structural change in partially linear single-index models with error-prone linear covariates;2013.0;[];Motivated by an analysis of a real data set from Duchenne Muscular Dystrophy (Andrews and Herzberg, 1985), we propose a new test of structural change for a class of partially linear single-index models with error-prone linear covariates. Based on the local linear estimation for the unknowns in these semiparametric models, we develop a new generalized F-test statistics for the nonparametric part in the partially linear single-index models with error-prone linear covariates. Asymptotic properties of the newly proposed test statistics are proved to follow asymptotically the chi-squared distribution. The new Wilksu0027 phenomenon is unveiled in a class of semiparametric measure error models. Simulations are conducted to examine the performance of our proposed method. The simulation results are consistent with our theoretical findings. Real data examples are used to illustrate the proposed methodology.
2016170913;Network Properties Revealed through Matrix Functions;2010.0;[];The emerging field of network science deals with the tasks of modeling, comparing, and summarizing large data sets that describe complex interactions. Because pairwise affinity data can be stored in a two-dimensional array, graph theory and applied linear algebra provide extremely useful tools. Here, we focus on the general concepts of centrality, communicability, and betweenness, each of which quantifies important features in a network. Some recent work in the mathematical physics literature has shown that the exponential of a networku0027s adjacency matrix can be used as the basis for defining and computing specific versions of these measures. We introduce here a general class of measures based on matrix functions, and show that a particular case involving a matrix resolvent arises naturally from graph-theoretic arguments. We also point out connections between these measures and the quantities typically computed when spectral methods are used for data mining tasks such as clustering and ordering. We finish with computational examples showing the new matrix resolvent version applied to real networks.
2017711125;Product location, allocation and server home base location for an order picking line with multiple servers;2004.0;[];In this paper, we are interested in several interrelated control issues for a u0027pick to orderu0027 (or, u0027strictu0027 order picking) picking line which stores N = nk types of products in n bins, each with k shelves. To fill each order, a container is transported past the various locations containing products, and the appropriate quantity of each product is removed from its respective storage location and put into the order container using an u0027out and backu0027 picking strategy. Each of several pickers is assigned a set or u0027zoneu0027 of products. We are interested in the concurrent problems of: (1) product location, (2) picker home base location, and (3) allocating products to each picker so that the expected order cycle time is minimized. We provide easily implemented algorithms to solve these problems and are able to show that the results apply for several alternate picking strategies. For fixed product locations, we develop an efficient dynamic programming algorithm which determines the optimal product allocation and server locations.
2017846705;Clustering algorithm for intuitionistic fuzzy sets;2008.0;[];The intuitionistic fuzzy set (IFS) theory, originated by Atanassov [K. Atanassov, Intuitionistic fuzzy sets, Fuzzy Sets and Systems 20 (1986) 87-96], has been used in a wide range of applications, such as logic programming, medical diagnosis, pattern recognition, and decision making, etc. However, so far there has been little investigation of the clustering techniques of IFSs. In this paper, we define the concepts of association matrix and equivalent association matrix, and introduce some methods for calculating the association coefficients of IFSs. Then, we propose a clustering algorithm for IFSs. The algorithm uses the association coefficients of IFSs to construct an association matrix, and utilizes a procedure to transform it into an equivalent association matrix. The @l-cutting matrix of the equivalent association matrix is used to cluster the given IFSs. Moreover, we extend the algorithm to cluster interval-valued intuitionistic fuzzy sets (IVIFSs), and finally, demonstrate the effectiveness of our clustering algorithm by experimental results.
2018188846;Herding behavior in online P2P lending: An empirical investigation;2012.0;[];We study lender behavior in the peer-to-peer (P2P) lending market, where individuals bid on unsecured microloans requested by other individual borrowers. Online P2P exchanges are growing, but lenders in this market are not professional investors. In addition, lenders have to take big risks because loans in P2P lending are granted without collateral. While the P2P lending market shares some characteristics of online markets with respect to herding behavior, it also has characteristics that may discourage it. This study empirically investigates herding behavior in the P2P lending market where seemingly conflicting conditions and features of herding are present. Using a large sample of daily data from one of the largest P2P lending platforms in Korea, we find strong evidence of herding and its diminishing marginal effect as bidding advances. We employ a multinomial logit market-share model in which relevant variables from prior studies on P2P lending are assessed.
2018634415;A Real-Time Information Based Demand-Side Management System in Smart Grid;2016.0;[];In this paper, we study a real-time information based demand-side management (DSM) system with advanced communication networks in smart grid. DSM can smooth peak-to-average ratio (PAR) of power usage in the grid, which in turn reduces the waste of fuel and the emission of greenhouse gas. We first target to minimize PAR with a centralized scheme. To motivate power suppliers, we further propose another centralized scheme targeting minimum power generation cost. However, customers may not be motivated by a centralized scheme since such a scheme requires total control and privacy from them. A centralized scheme also requires too much real-time data exchange for frequent DSM deployment. To tackle these issues, we propose game theoretical approaches so that most of the computation is performed locally. In the proposed game, all the customers are motivated by extra savings if participating. Moreover, we prove that all parties benefit from the DSM system to the same level because both the centralized schemes and the game theoretical approach minimize global PAR. Such an analysis is further demonstrated by the simulation results and discussions. Additionally, we evaluate the performance of several (partially) distributed approaches in order to find the best way to deploy DSM system.
2018682725;SOCRATES: A system for scheduling hydroelectric generation under uncertainty;1995.0;[];The Pacific Gas and Electric Company, the largest investor-owned energy utility in the United States, obtains a significant fraction of its electric energy and capacity from hydrogeneration. Although hydro provides valuable flexibility, it is subject to usage limits and must be carefully scheduled. In addition, the amount of energy available from hydro varies widely from year to year, depending on precipitation and streamflows. Optimal scheduling of hydrogeneration, in coordination with other energy sources, is a stochastic problem of practical significance to PGu0026E. SOCRATES is a system for the optimal scheduling of PGu0026Eu0027s various energy sources over a one- to two-year horizon. This paper concentrates on the component of SOCRATES that schedules hydro. The core is a stochastic optimization model, solved using Benders decomposition. Additional components are streamflow forecasting models and a database containing hydrological information. The stochastic hydro scheduling module of SOCRATES is undergoing testing in the useru0027s environment, and we expect PGu0026E hydrologists and hydro schedulers to place progressively more reliance upon it.
2018924502;A Longitudinal EEG Study of Alzheimer's Disease Progression Based on A Complex Network Approach;2015.0;[];A complex network approach is combined with time dynamics in order to conduct a space–time analysis applicable to longitudinal studies aimed to characterize the progression of Alzheimeru0027s disease (AD) in individual patients. The network analysis reveals how patient-specific patterns are associated with disease progression, also capturing the widespread effect of local disruptions. This longitudinal study is carried out on resting electroence phalography (EEGs) of seven AD patients. The test is repeated after a three monthsu0027 period. The proposed methodology allows to extract some averaged information and regularities on the patientsu0027 cohort and to quantify concisely the disease evolution. From the functional viewpoint, the progression of AD is shown to be characterized by a loss of connected areas here measured in terms of network parameters (characteristic path length, clustering coefficient, global efficiency, degree of connectivity and connectivity density). The differences found between baseline and at...
2019037190;Academic stress and Internet addiction from general strain theory framework;2015.0;[];Academic stress influenced negative emotions.Academic stress influenced Internet addiction.Negative emotions affected Internet addiction.Negative emotions fully mediated the associations. The goal of this study was to examine the mediating role of negative emotions in the link between academic stress and Internet addiction among Korean adolescents. We attempted to extend the general strain theory to Internet addiction by exploring psychological pathways from academic stress to Internet addiction using a national and longitudinal panel study. A total of 512 adolescents completed self-reported scales for academic stress, negative emotions, and Internet addiction. We found that academic stress was positively associated with negative emotions and Internet addiction, and negative emotions were positively associated with Internet addiction. Further, the results of structural equation modeling revealed that adolescentsu0027 academic stress had indirectly influenced Internet addiction through negative emotions. The results of this study suggest that adolescents who experience academic stress might be at risk for Internet addiction, particularly when accompanied with negative emotions. These findings provided significant implications for counselors and policymakers to prevent adolescentsu0027 Internet addiction, and extended the general strain theory to Internet addiction which is typically applicable to deviant behavior.
2019438060;SLA-Based Trust Model for Cloud Computing;2010.0;[];Cloud computing is a new form of technology, which infrastructure, developing platform, software, and storage can be delivered as a service in a pay as you use cost model. However, for critical business application and more sensitive information, cloud providers must be selected based on high level of trustworthiness. In this paper, we present a trust model to evaluate cloud services in order to help cloud users select the most reliable resources. We integrate our previous work “conceptual SLA framework for cloud computing” with the proposed trust management model to present a new solution of defining the reliable criteria for the selection process of cloud providers
2019482491;FESAS: Towards a Framework for Engineering Self-Adaptive Systems;2013.0;[];The complexity and size of information systems are growing, resulting in an increasing effort for maintenance. Self-adaptive systems (SAS) that autonomously adapt to changes in the environment or in the system itself (e.g. disfunction of components) can be a solution. So far, the development of SAS is frequently tailored to specific use case requirements. The creation of frameworks with reusable process elements and system components is often neglected. However, with such a framework developing SAS would become faster and less error prone. This work addresses this gap by providing a framework for engineering SAS.
2019836674;The Insight ToolKit image registration framework.;2014.0;[];"Publicly available scientific resources help establish evaluation standards, provide a platform for teaching and improve reproducibility. Version 4 of the Insight ToolKit ( ITK4 ) seeks to es- tablish new standards in publicly available image registration methodology. ITK4 makes severaladvances in comparison to previous versions of ITK. ITK4 supports both multivariate images and objective functions; it also unifies high-dimensional (deformation field) and low-dimensional (affine) transformations with metrics that are reusable across transform types and with com- posite transforms that allow arbitrary series of geometric mappings to be chained together seamlessly. Metrics and optimizers take advantage of multi-core resources, when available.Furthermore, ITK4 reduces the parameter optimization burden via principled heuristics that automatically set scaling across disparate parameter types (rotations versus translations). A related approach also constrains steps sizes for gradient-based optimizers. The result is that tuning for different metrics and/or image pairs is rarely necessary allowing the researcher tomore easily focus on design/comparison of registration strategies. In total, the ITK4 contribu- tion is intended as a structure to support reproducible research practices, will provide a more extensive foundation against which to evaluate new work in image registration and also enable application level programmers a broad suite of tools on which to build. Finally, we contextu- alize this work with a reference registration evaluation study with application to pediatric brainlabeling."
2020016035;Peat Mapping Associations of Airborne Radiometric Survey Data;2014.0;[];This study considers recent airborne radiometric (gamma ray) survey data, obtained at high-resolution, across various regions of the UK. The datasets all display a very evident attenuation of signal in association with peat, and intra-peat variations are observed. The geophysical response variations are examined in detail using example data sets across lowland areas (raised bogs, meres, fens and afforested peat) and upland areas of blanket bog, together with associated wetland zones. The radiometric data do not map soils per se. The bedrock (the radiogenic parent) provides a specific amplitude level. Attenuation of this signal level is then controlled by moisture content in conjunction with the density and porosity of the soil cover. Both soil and bedrock variations need to be jointly assessed. The attenuation theory, reviewed here, predicts that the behaviour of wet peat is distinct from most other soil types. Theory also predicts that the attenuation levels observed across wet peatlands cannot be generally used to map variations in peat thickness. Four survey areas at various scales, across England, Scotland, Wales and Ireland are used to demonstrate the ability of the airborne data to map peat zones. A 1:50 k national mapping of deep peat is used to provide control although variability in the definition of peat zones across existing databases is also demonstrated.
2020320008;Multiobjective evolutionary algorithms: A survey of the state of the art;2011.0;[];   A multiobjective optimization problem involves several conflicting objectives and has a set of Pareto optimal solutions. By evolving a population of solutions, multiobjective evolutionary algorithms (MOEAs) are able to approximate the Pareto optimal set in a single run. MOEAs have attracted a lot of research effort during the last 20 years, and they are still one of the hottest research areas in the field of evolutionary computation. This paper surveys the development of MOEAs primarily during the last eight years. It covers algorithmic frameworks such as decomposition-based MOEAs (MOEA/Ds), memetic MOEAs, coevolutionary MOEAs, selection and offspring reproduction operators, MOEAs with specific search methods, MOEAs for multimodal problems, constraint handling and MOEAs, computationally expensive multiobjective optimization problems (MOPs), dynamic MOPs, noisy MOPs, combinatorial and discrete MOPs, benchmark problems, performance indicators, and applications. In addition, some future research issues are also presented.
2020558201;A vacation from your mind: Problematic online gaming is a stress response;2014.0;[];We present ethnographically-informed survey and interview data suggesting that problematic online gaming in the World of Warcraft (WoW) can be conceptualized as a response to pre-existing life stress, which for highly stressed individuals magnifies rather than relieves their suffering. In particular, we explore how relaxing and arousing in-game experiences and activities provide forms of cognitive diversion that can lead to problematic play among more highly stressed individuals. Our research supports what has been called a u0027u0027rich get richeru0027u0027 model of problematic Internet use. In this instance, less stressed individuals manage to play WoW so as to enhance their offline lives. By contrast, more highly stressed players further magnify the stress and suffering in their lives by playing problematically the online game within which they sought refuge from their offline problems.
2021497904;The ONE simulator for DTN protocol evaluation;2009.0;[];Delay-tolerant Networking (DTN) enables communication in sparse mobile ad-hoc networks and other challenged environments where traditional networking fails and new routing and application protocols are required. Past experience with DTN routing and application protocols has shown that their performance is highly dependent on the underlying mobility and node characteristics. Evaluating DTN protocols across many scenarios requires suitable simulation tools. This paper presents the Opportunistic Networking Environment (ONE) simulator specifically designed for evaluating DTN routing and application protocols. It allows users to create scenarios based upon different synthetic movement models and real-world traces and offers a framework for implementing routing and application protocols (already including six well-known routing protocols). Interactive visualization and post-processing tools support evaluating experiments and an emulation mode allows the ONE simulator to become part of a real-world DTN testbed. We show sample simulations to demonstrate the simulatoru0027s flexible support for DTN protocol evaluation.
2021909147;The LTOPSIS: An alternative to TOPSIS decision-making approach for linguistic variables;2012.0;[];This paper develops an evaluation approach based on the Technique for Order Performance by Similarity to Ideal Solution (TOPSIS). When the input for a decision process is linguistic, it can be understood that the output should also be linguistic. For that reason, in this paper we propose a modification of the TOPSIS algorithm which develops the above idea and which can also be used as a linguistic classifier. In this new development, modifications to the classic algorithm have been considered which enable linguistic outputs and which can be checked through the inclusion of an applied example to demonstrate the goodness of the new model proposed.
2021941127;Towards radar-enabled sensor networks;2006.0;[];Ultra wideband radar-enabled wireless sensor networks have the potential to address key detection and classification requirements common to many surveillance and tracking applications. However, traditional radar signal processing techniques are mismatched with the limited computational and storage resources available on typical sensor nodes. The mismatch is exacerbated in noisy, cluttered environments or when the signals have corrupted spectra. To explore the compatibility of ultra wideband radar and mote-class sensor nodes, we designed and built a new platform called the radar mote. An early prototype of this platform was used to detect, classify, and track people and vehicles moving through an outdoor sensor network deployment. This paper describes the sensoru0027s theory of operation, discusses the design and implementation of the radar mote, and presents sample signal waveforms of people, vehicles, noise, and clutter. We demonstrate that radar sensors can be successfully integrated with mote-class devices and imbue them with an extraordinarily useful sensing modality.
2022123067;An exact algorithm for the reliability redundancy allocation problem;2015.0;[];The redundancy allocation problem is the problem of finding an optimal allocation of redundant components subject to a set of resource constraints. The problem studied in this paper refers to a series-parallel system configuration and allows for component mixing. We propose a new modeling/solution approach, in which the problem is transformed into a multiple choice knapsack problem and solved to optimality via a branch and cut algorithm. The algorithm is tested on well-known sets of benchmark instances. All instances have been solved to optimality in milliseconds or very few seconds on a normal workstation.
2022485595;An Evolutionary Many-Objective Optimization Algorithm Using Reference-Point-Based Nondominated Sorting Approach, Part I: Solving Problems With Box Constraints;2014.0;[];Having developed multiobjective optimization algorithms using evolutionary optimization methods and demonstrated their niche on various practical problems involving mostly two and three objectives, there is now a growing need for developing evolutionary multiobjective optimization (EMO) algorithms for handling many-objective (having four or more objectives) optimization problems. In this paper, we recognize a few recent efforts and discuss a number of viable directions for developing a potential EMO algorithm for solving many-objective optimization problems. Thereafter, we suggest a reference-point-based many-objective evolutionary algorithm following NSGA-II framework (we call it NSGA-III) that emphasizes population members that are nondominated, yet close to a set of supplied reference points. The proposed NSGA-III is applied to a number of many-objective test problems with three to 15 objectives and compared with two versions of a recently suggested EMO algorithm (MOEA/D). While each of the two MOEA/D methods works well on different classes of problems, the proposed NSGA-III is found to produce satisfactory results on all problems considered in this paper. This paper presents results on unconstrained problems, and the sequel paper considers constrained and other specialties in handling many-objective optimization problems.
2022580894;Meta-path based multi-network collective link prediction;2014.0;[];"Online social networks offering various services have become ubiquitous in our daily life. Meanwhile, users nowadays are usually involved in multiple online social networks simultaneously to enjoy specific services provided by different networks. Formally, social networks that share some common users are named as partially aligned networks. In this paper, we want to predict the formation of social links in multiple partially aligned social networks at the same time, which is formally defined as the multi-network link (formation) prediction problem. In multiple partially aligned social networks, users can be extensively correlated with each other by various connections. To categorize these diverse connections among users, 7 ""intra-network social meta paths"" and 4 categories of ""inter-network social meta paths"" are proposed in this paper. These ""social meta paths"" can cover a wide variety of connection information in the network, some of which can be helpful for solving the multi-network link prediction problem but some can be not. To utilize useful connection, a subset of the most informative ""social meta paths"" are picked, the process of which is formally defined as ""social meta path selection"" in this paper. An effective general link formation prediction framework, Mli (Multi-network Link Identifier), is proposed in this paper to solve the multi-network link (formation) prediction problem. Built with heterogenous topological features extracted based on the selected ""social meta paths"" in the multiple partially aligned social networks, Mli can help refine and disambiguate the prediction results reciprocally in all aligned networks. Extensive experiments conducted on real-world partially aligned heterogeneous networks, Foursquare and Twitter, demonstrate that Mli can solve the multi-network link prediction problem very well."
2022686119;Mean shift, mode seeking, and clustering;1995.0;[];"Mean shift, a simple interactive procedure that shifts each data point to the average of data points in its neighborhood is generalized and analyzed in the paper. This generalization makes some k-means like clustering algorithms its special cases. It is shown that mean shift is a mode-seeking process on the surface constructed with a ""shadow"" kernal. For Gaussian kernels, mean shift is a gradient mapping. Convergence is studied for mean shift iterations. Cluster analysis if treated as a deterministic problem of finding a fixed point of mean shift that characterizes the data. Applications in clustering and Hough transform are demonstrated. Mean shift is also considered as an evolutionary strategy that performs multistart global optimization. u003e"
2023261081;On personalizing Web search using social network analysis;2015.0;[];Most of the existing Web search solutions are built for satisfying broad set of users regardless whether naive or professionals. Further, with the emergence of high speed internet applications and advanced Web 2.0 based Rich Internet Applications (i.e. blogs, wikis, etc.), it has become much easier for users to publish data over the Web. This brings a challenge for Web search solutions to let individual users find the right information as per their preferences. Different users of the Web may have different preferences. Search results for the same query from different users may differ in priority for individual users. In this paper, we describe our approach of enabling personalized Web search for users based on their preferences. It is a challenge in itself to have the preferences of the users known to and considered by search engines. We have designed and developed our unique approach of finding the preferences of users from the relevant parts of their social networks and communities. We believe that the information related to the queries posed by users may have strong correlation with relevant information in their social networks. In order to find out the personal interests and the social-contexts, we find out (1) activities of users in their social-networks, and (2) relevant information from useru0027s social networks, based on our proposed trust and relevance matrices. We have developed a mechanism that extracts information from a useru0027s social network and uses it to re-rank the results from a search engine. We have also discussed the implementation and evaluation of our proposed solution by emphasizing how it improves the Web search.
2023354680;Near-field source localization with partly sensor gain and phase uncertainties;2014.0;[];In this paper, we consider the source localization for the multiple near-field narrowband signals impinging on a uniform linear array (ULA) with partly gain and phase uncertainties. By dividing the ULA into two overlapped symmetric subarrays and assuming that they have partly unknown gain and phase responses but correspondingly identical, a new modified generalized ESPRIT based method is presented, in which the direction of arrival (DOA) and range are estimated separately. Moreover, we prove that the proposed method is equivalent to the spectral Fresnel-region rank reduction (FR-RARE) [15] method, but more computationally efficient than it. The simulations demonstrate the effectiveness of the proposed method.
2023371922;Determining the weights of criteria in the ELECTRE type methods with a revised Simos' procedure;2002.0;[];"   In a decision aiding context, knowing the preferences of the Decision Maker (DM) and determining weights of criteria are very hard questions. Several methods can be used to give an appropriate value to the weights of criteria. J. Simos proposed a very simple procedure, using a set of cards, allowing to determine indirectly numerical values for weights. The purpose of this paper is first to explain why the above method needs to be revised, and second, the revised version we propose. This new version takes into account a new kind of information from the DM and changes certain computing rules of the former method. A software has been implemented based on the revised Simosu0027 procedure whose main features are presented in this paper. The new method has been applied to different real-life cases (public transportation problems, water resources problems, environment problems, etc); it proved to be successful."
2023462284;Evolutionary multiobjective optimization using an outranking-based dominance generalization;2010.0;[];One aspect that is often disregarded in the current research on evolutionary multiobjective optimization is the fact that the solution of a multiobjective optimization problem involves not only the search itself, but also a decision making process. Most current approaches concentrate on adapting an evolutionary algorithm to generate the Pareto frontier. In this work, we present a new idea to incorporate preferences into a multi-objective evolutionary algorithm (MOEA). We introduce a binary fuzzy preference relation that expresses the degree of truth of the predicate u0027u0027x is at least as good as yu0027u0027. On this basis, a strict preference relation with a reasonably high degree of credibility can be established on any population. An alternative x is not strictly outranked if and only if there does not exist an alternative y which is strictly preferred to x. It is easy to prove that the best solution is not strictly outranked. For validating our proposed approach, we used the non-dominated sorting genetic algorithm II (NSGA-II), but replacing Pareto dominance by the above non-outranked concept. So, we search for the non-strictly outranked frontier that is a subset of the Pareto frontier. In several instances of a nine-objective knapsack problem our proposal clearly outperforms the standard NSGA-II, achieving non-outranked solutions which are in an obviously privileged zone of the Pareto frontier.
2023634537;Robust Capon-based direction-of-arrival estimators in partly calibrated sensor array;2013.0;[];   In this paper a problem of direction-of-arrival (DOA) estimation in partly calibrated sensor arrays is considered. It is assumed that a sensor array consists of subarrays with full inner calibration and unknown intersubarray distortions. Capon-based methods are proposed for DOA estimation as suitable techniques have robustness to intersubarray distortions. These methods are an extension of robust Capon beamformer developed for partly calibrated array to the DOA estimation. The simulations validate efficiency of the proposed algorithms.
2024165284;Tensor Decompositions and Applications;2009.0;[];This survey provides an overview of higher-order tensor decompositions, their applications, and available software. A tensor is a multidimensional or $N$-way array. Decompositions of higher-order tensors (i.e., $N$-way arrays with $N \geq 3$) have applications in psycho-metrics, chemometrics, signal processing, numerical linear algebra, computer vision, numerical analysis, data mining, neuroscience, graph analysis, and elsewhere. Two particular tensor decompositions can be considered to be higher-order extensions of the matrix singular value decomposition: CANDECOMP/PARAFAC (CP) decomposes a tensor as a sum of rank-one tensors, and the Tucker decomposition is a higher-order form of principal component analysis. There are many other tensor decompositions, including INDSCAL, PARAFAC2, CANDELINC, DEDICOM, and PARATUCK2 as well as nonnegative variants of all of the above. The N-way Toolbox, Tensor Toolbox, and Multilinear Engine are examples of software packages for working with tensors.
2024816219;Residential Energy Management in Smart Grid: A Markov Decision Process-Based Approach;2013.0;[];The deployment of advanced information and communication technologies has helped in the transformation of the traditional power grids into smart grids by introducing demand side management in residential area. The use of demand side management in the residential area can successfully reduce customeru0027s energy consumption, and can also provide a well balanced energy demand throughout the day. Based on real-time pricing information, a customer can shift his/her energy demand to reduce the energy consumption cost. In this paper, we present a Markov Decision Process (MDP)-based scheduling mechanism for residential energy management (REM) in smart grid. The aim of the proposed work is to reduce the energy expenses of a customer. In this mechanism, the Home Energy Management Unit (HEMU) acts as one of the players, the Central Energy Management Unit (CEMU) acts as another player. The HEMU interacts with the CEMU to fulfill its energy request within its desired budget. The CEMU follows its own dynamic pricing mechanism to decide the price per unit energy for on-peak and off-peak hours. The proposed mechanism is able to reduce the energy expenses of the residential customers.
2025023024;Perfectly Matched Layers for the Convected Helmholtz Equation;2004.0;[];In this paper, we propose and analyze perfectly matched absorbing layers for a problem of time-harmonic acoustic waves propagating in a duct in the presence of a uniform flow. The absorbing layers are designed for the pressure field, satisfying the convected scalar Helmholtz equation. A difficulty, compared to the Helmholtz equation, comes from the presence of so-called inverse upstream modes which become unstable, instead of evanescent, with the classical Berengeru0027s perfectly matched layers (PMLs). We investigate here a PML model, recently introduced for time-dependent problems, which makes all outgoing waves evanescent. We then analyze the error due to the truncation of the domain and prove that the convergence is exponential with respect to the size of the layers for both the classical and the new PML models. Numerical validations are finally presented.
2025148441;A novel numerical optimization algorithm inspired from weed colonization;2006.0;[];   This paper introduces a novel numerical stochastic optimization algorithm inspired from colonizing weeds. Weeds are plants whose vigorous, invasive habits of growth pose a serious threat to desirable, cultivated plants making them a threat for agriculture. Weeds have shown to be very robust and adaptive to change in environment. Thus, capturing their properties would lead to a powerful optimization algorithm. It is tried to mimic robustness, adaptation and randomness of colonizing weeds in a simple but effective optimizing algorithm designated as Invasive Weed Optimization (IWO). The feasibility, the efficiency and the effectiveness of IWO are tested in details through a set of benchmark multi-dimensional functions, of which global and local minima are known. The reported results are compared with other recent evolutionary-based algorithms: genetic algorithms, memetic algorithms, particle swarm optimization, and shuffled frog leaping. The results are also compared with different versions of simulated annealing — a generic probabilistic meta-algorithm for the global optimization problem — which are simplex simulated annealing, and direct search simulated annealing. Additionally, IWO is employed for finding a solution for an engineering problem, which is optimization and tuning of a robust controller. The experimental results suggest that results from IWO are better than results from other methods. In conclusion, the performance of IWO has a reasonable performance for all the test functions.
2025705970;Reliability redundancy allocation: An improved realization for nonconvex nonlinear programming problems;2006.0;[];The redundancy allocation problem, which seeks to find the optimal allocation of redundancy that maximizes system reliability, is one of the representative problems in reliability optimization. The problem can be formulated as a nonconvex integer nonlinear programming problem. This paper presents an efficient branch-and-bound approach to solve this problem, where the considered system is coherent, that is, the objective and constraint functions have monotonic increasing properties. The proposed method is based primarily on a search space elimination of disjoint sets in a solution space that does not require any relaxation of branched subproblems. The main advantage of the proposed method is flexibility (i.e., it does not rely on any assumptions of linearity, separability, single constraint, or convexity) which make the method adaptive to various applications. Numerical experiments demonstrate that the method is superior to the existing exact algorithms for redundancy allocation problems in terms of computation time.
2026244788;Entropy for intuitionistic fuzzy sets;2001.0;[];A non-probabilistic-type entropy measure for intuitionistic fuzzy sets is proposed. It is a result of a geometric interpretation of intuitionistic fuzzy sets and uses a ratio of distances between them proposed in Szmidt and Kacprzyk (to appear). It is also shown that the proposed measure can be dened in terms of the ratio of intuitionistic fuzzy cardinalities: of F \F c and F [F c . c 2001 Elsevier Science B.V. All rights reserved.
2027886943;A 1.2V 30nm 1.6Gb/s/pin 4Gb LPDDR3 SDRAM with input skew calibration and enhanced control scheme;2012.0;[];Mobile DRAM is widely adopted in battery-powered portable devices because of its low power. Recently, in mobile devices such as smart phones and tablet PCs, higher performance is required to support 3D gaming mode and high-quality video. These trends lead to consideration of higher-performance DRAMs than LPDDR2, while the power budget for DRAMs for mobile devices cannot increase. DRAMs with wide I/O or serial I/O have been reviewed as candidates for over 6.4GB/s channel bandwidth. However, wide-I/O DRAMs [1] must solve issues such as stacking yield for higher density and failure analysis modeling of sys-tem-in-package (SiP), and most serial I/Os have worse I/O power efficiency than LPDDR2. For an evolutionary successor of LPDDR2, therefore, we design a 1.2V 1.6Gb/s/pin ×32 4Gb low-power DDR3 SDRAM (LPDDR3) with input skew calibration and enhanced refresh control schemes, achieving 6.4GB/s total data bandwidth. Most features of LPDDR3 are backward compatible with LPDDR2, except that channel termination, command-address (CA) training, and write leveling are adopted.
2028212620;Computing the Action of the Matrix Exponential, with an Application to Exponential Integrators;2011.0;[];A new algorithm is developed for computing $e^{tA}B$, where $A$ is an $n\times n$ matrix and $B$ is $n\times n_0$ with $n_0 \ll n$. The algorithm works for any $A$, its computational cost is dominated by the formation of products of $A$ with $n\times n_0$ matrices, and the only input parameter is a backward error tolerance. The algorithm can return a single matrix $e^{tA}B$ or a sequence $e^{t_kA}B$ on an equally spaced grid of points $t_k$. It uses the scaling part of the scaling and squaring method together with a truncated Taylor series approximation to the exponential. It determines the amount of scaling and the Taylor degree using the recent analysis of Al-Mohy and Higham [SIAM J. Matrix Anal. Appl., 31 (2009), pp. 970-989], which provides sharp truncation error bounds expressed in terms of the quantities $\|A^k\|^{1/k}$ for a few values of $k$, where the norms are estimated using a matrix norm estimator. Shifting and balancing are used as preprocessing steps to reduce the cost of the algorithm. Numerical experiments show that the algorithm performs in a numerically stable fashion across a wide range of problems, and analysis of rounding errors and of the conditioning of the problem provides theoretical support. Experimental comparisons with MATLAB codes based on Krylov subspace, Chebyshev polynomial, and Laguerre polynomial methods show the new algorithm to be sometimes much superior in terms of computational cost and accuracy. An important application of the algorithm is to exponential integrators for ordinary differential equations. It is shown that the sums of the form $\sum_{k=0}^p \varphi_k(A)u_k$ that arise in exponential integrators, where the $\varphi_k$ are related to the exponential function, can be expressed in terms of a single exponential of a matrix of dimension $n+p$ built by augmenting $A$ with additional rows and columns, and the algorithm of this paper can therefore be employed.
2028741709;MADServer: a server architecture for mobile advanced delivery;2012.0;[];Rapid increases in cellular data traffic demand creative alternative delivery vectors for data. Despite the conceptual attractiveness of mobile data offloading, no concrete web server architectures integrate intelligent offloading in a production-ready and easily deployable manner without relying on vast infrastructural changes to carriersu0027 networks. Delay-tolerant networking technology offers the means to do just this. We introduce MADServer, a novel DTN-based architecture for mobile data offloading that splits web content among multiple independent delivery vectors based on user and data context. It enables intelligent data offloading, caching, and querying solutions which can be incorporated in a manner that still satisfies user expectations for timely delivery. At the same time, it allows for users who have poor or expensive connections to the cellular network to leverage multi-hop opportunistic routing to send and receive data. We also present a preliminary implementation of MADServer and provide real-world performance evaluations.
2028837511;A novel two-stage model for cloud service trustworthiness evaluation;2014.0;[];In this paper, we address the cloud service trustworthiness evaluation problem, which in essence is a multi-attribute decision-making problem, by proposing a novel evaluation model based on the fuzzy gap measurement and the evidential reasoning approach. There are many sources of uncertainties in the process of cloud service trustworthiness evaluation. In addition to the intrinsic uncertainties, cloud service providers face the problem of discrepant evaluation information given by different users from different perspectives. To address these problems, we develop a novel fuzzy gap evaluation approach to assess cloud service trustworthiness and to provide evaluation values from different perspectives. From the evaluation values, the perception-importance, delivery-importance, and perception-delivery gaps are generated. These three gaps reflect the discrepancy evaluation of cloud service trustworthiness in terms of perception utility, delivery utility, and importance utility, respectively. Finally, the gap measurement of each perspective is represented by a belief structure and aggregated using the evidential reasoning approach to generate final evaluation results for informative and robust decision making. From this hybrid two-stage evaluation process, cloud service providers can get improvement suggestions from intermediate information derived from the gap measurement, which is the main advantage of this evaluation process.
2029074979;Entropy on intuitionistic fuzzy sets and on interval-valued fuzzy sets;1996.0;[];   We recall the definitions of intuitionistic fuzzy sets and interval-valued fuzzy sets with the relation between these sets established by K. Atanassov. We define the distance measure between intuitionistic fuzzy sets and we give an axiom definition of intuitionistic fuzzy entropy and a theorem which characterizes it. Finally, we study a very special entropy and recall that all we have done for intuitionistic fuzzy sets is also good for interval-valued fuzzy sets.
2029386095;Extended TODIM method for hybrid multiple attribute decision making problems;2013.0;[];TODIM (an acronym in Portuguese of interactive and multiple attribute decision making) is a method for solving the multiple attribute decision making (MADM) problem considering decision makeru0027s (DMu0027s) behavior, in which the attribute values are in the format of crisp numbers. It cannot be used to handle hybrid MADM problems with various formats of attribute values. In this paper, an extended TODIM method is proposed to solve the hybrid MADM problem. First, three formats of attribute values (crisp numbers, interval numbers and fuzzy numbers) are expressed in the format of random variables with cumulative distribution functions. Then, according to the concept of the classical TODIM method, the gain and loss matrices concerning each attribute are constructed by calculating the gain and loss of each alternative relative to the others. Further, by calculating the dominance degree of each alternative over the others, the overall value of each alternative can be obtained to rank the alternatives. Finally, two numerical examples are used to illustrate the use of the proposed method.
2029535444;On popularity prediction of videos shared in online social networks;2013.0;[];"Popularity prediction, with both technological and economic importance, has been extensively studied for conventional video sharing sites (VSSes), where the videos are mainly found via searching, browsing, or related links. Recent statistics however suggest that online social network (OSN) users regularly share video contents from VSSes, which has contributed to a significant portion of the accesses; yet the popularity prediction in this new context remains largely unexplored. In this paper, we present an initial study on the popularity prediction of videos propagated in OSNs along friendship links.   We conduct a large-scale measurement and analysis of viewing patterns of videos shared in one of largest OSNs in China, and examine the performance of typical views-based prediction models. We find that they are generally ineffective, if not totally fail, especially when predicting the early peaks and later bursts of accesses, which are common during video propagations in OSNs. To overcome these limits, we track the propagation process of videos shared in a Facebook-like OSN in China, and analyze the user viewing and sharing behaviors. We accordingly develop a novel propagation-based video popularity prediction solution, namely SoVP. Instead of relying solely on the early views for prediction, SoVP considers both the intrinsic attractiveness of a video and the influence from the underlying propagation structure. The effectiveness of SoVP, particularly for predicting the peaks and bursts, have been validated through our trace-driven experiments."
2030651378;Proof of the Alon—Yuster conjecture;2001.0;[];   In this paper we prove the following conjecture of Alon and Yuster. Let H be a graph with h vertices and chromatic number k. There exist constants  c ( H ) and  n  0 ( H ) such that if  n ⩾ n  0 ( H ) and G is a graph with  hn  vertices and minimum degree at least (1−1/ k ) hn + c ( H ), then G contains an H-factor. In fact, we show that if H has a k-coloring with color-class sizes  h  1 ⩽ h  2 ⩽⋯⩽ h   k  , then the conjecture is true with  c ( H )= h   k  + h   k −1 −1.
2030814745;Star-critical Ramsey numbers;2011.0;[];"The graph Ramsey numberR(G,H) is the smallest integer r such that every 2-coloring of the edges of K""r contains either a red copy of G or a blue copy of H. We find the largest star that can be removed from K""r such that the underlying graph is still forced to have a red G or a blue H. Thus, we introduce the star-critical Ramsey numberr""*(G,H) as the smallest integer k such that every 2-coloring of the edges of K""r-K""1"",""r""-""1""-""k contains either a red copy of G or a blue copy of H. We find the star-critical Ramsey number for trees versus complete graphs, multiple copies of K""2 and K""3, and paths versus a 4-cycle. In addition to finding the star-critical Ramsey numbers, the critical graphs are classified for R(T""n,K""m), R(nK""2,mK""2) and R(P""n,C""4)."
2031131447;An intelligent information access system assisting a case based learning methodology evaluated in higher education with medical students;2012.0;[];In recent years there has been a shift in educational methodologies toward a student-centered approach, one which increasingly emphasizes the integration of computer tools and intelligent systems adopting different roles. In this paper we describe in detail the development of an Intelligent Information Access system used as the basis for producing and assessing a constructivist learning methodology with undergraduate students. The system automatically detects significant concepts available within a given clinical case and facilitates an objective examination, following a proper selection process of the case in which is taken into account the studentsu0027 knowledge level. The learning methodology implemented is intimately related to concept-based, case-based and internet-based learning. In spite of growing theoretical research on the use of information technology in higher education, it is rare to find applications that measure learning and studentsu0027 perceptions and compare objective results with a free Internet search. Our work enables students to gain understanding of the concepts in a case through Web browser interaction with our computer system identifying these concepts and providing direct access to enriched related information from Medlineplus, Freebase and PubMed. In order to evaluate the learning activity outcomes, we have done a trial run with volunteer students from a 2nd year undergraduate Medicine course, dividing the volunteers into two groups. During the activity all students were provided with a clinical case history and a multiple choice test with medical questions relevant to the case. This test could be done in two different ways: learners in one group were allowed to freely seek information on the Internet, while the other group could only search for information using the newly developed computer tool. In the latter group, we measured how students perceived the toolu0027s support for solving the activity and the Web interface usability, supplying them with a Likert questionnaire for anonymous completion. The particular case selected was a female with a medical history of heart pathology, from which the system derived medical terms closely associated with her condition description, her clinical evolution and treatment.
2031183907;Multi-Verse Optimizer: a nature-inspired algorithm for global optimization;2016.0;[];This paper proposes a novel nature-inspired algorithm called Multi-Verse Optimizer (MVO). The main inspirations of this algorithm are based on three concepts in cosmology: white hole, black hole, and wormhole. The mathematical models of these three concepts are developed to perform exploration, exploitation, and local search, respectively. The MVO algorithm is first benchmarked on 19 challenging test problems. It is then applied to five real engineering problems to further confirm its performance. To validate the results, MVO is compared with four well-known algorithms: Grey Wolf Optimizer, Particle Swarm Optimization, Genetic Algorithm, and Gravitational Search Algorithm. The results prove that the proposed algorithm is able to provide very competitive results and outperforms the best algorithms in the literature on the majority of the test beds. The results of the real case studies also demonstrate the potential of MVO in solving real problems with unknown search spaces. Note that the source codes of the proposed MVO algorithm are publicly available at http://www.alimirjalili.com/MVO.html.
2031489346;The Pascal Visual Object Classes (VOC) Challenge;2010.0;[];"The Pascal Visual Object Classes (VOC) challenge is a benchmark in visual object category recognition and detection, providing the vision and machine learning communities with a standard dataset of images and annotation, and standard evaluation procedures. Organised annually from 2005 to present, the challenge and its associated dataset has become accepted as the benchmark for object  :[56],""paper describes the dataset and evaluation procedure. We review the state-of-the-art in evaluated methods for both classification and detection, analyse whether the methods are statistically different, what they are learning from the images (e.g. the object or its context), and what the methods find easy or confuse. The :[56],""paper concludes with lessons learnt in the three year history of the challenge, and proposes directions for future improvement and extension."
2031621632;Priority dispatching strategies for EMS systems;2014.0;[];Emergency medical service (EMS) systems provide urgent medical care and transport. In this study we implement dispatching policies for EMS systems that incorporate the severity of the call in order to increase the survival probability of patients. A simulation model is developed to evaluate the performance of EMS systems. Performance is measured in terms of patients’ survival probability, since survival probability more directly mirrors patient outcomes. Different response strategies are evaluated utilizing several examples to study the nature of the optimal dispatching policy. The results show that dispatching the closest vehicle is not always optimal and dispatching vehicles considering priority of the call leads to an increase in the average survival probability of patients. A heuristic algorithm, that is easy to implement, is developed to dispatch ambulances for large-scale EMS systems. Computational examples show that the dispatching algorithm is valuable in increasing the patients’ survival probability.
2031999643;Item recommendation in collaborative tagging systems via heuristic data fusion;2015.0;[];"Collaborative tagging systems have been popular on the Web. However, information overload results in the increasing need for recommender services from users, and thus item recommendation has been one of the key issues in such systems. In this paper, we examine if data fusion can be helpful for improving effectiveness of item recommendation in these systems. For this, we first summarize the state-of-the-art recommendation methods which are classified into several categories according to their algorithmic principles. Then, we experiment with about 40 recommending components against the datasets from three social tagging systems-Delicious, Lastfm and CiteULike. Based on these, several heuristic data fusion models including rank-based and score-based are used to combine selected components. We also put forward a hybrid linear combination (HLC) model for fusing item recommendation. We use four kinds of evaluation metrics, which respectively consider accuracy, inner-diversity, inter-diversity and novelty, to systematically assess quality of recommendations obtained by various components or fusion models. Depending on experimental results, combining evidence from separate components can lead to performance improvement in the accuracy of recommendations, with a little or without loss of recommendation diversity and novelty, if separate components can suggest similar sets of relevant items but recommend different sets of non-relevant items. Particularly, fusing recommendation sets formed from different combinations of profile representations and similarity functions in user-based and item-based collaborative filtering can significantly improve recommendation accuracy. In addition, some other useful findings are also drawn: (i) Using the tag to represent users profiles or items profiles maybe not as good as profiling users with the item or profiling items with the user, however, exploiting tags in the topic models and random walks can notably improve the accuracy, diversity and novelty of recommendations; (ii) Generally, user-based collaborative filtering, item-based collaborative filtering and random walks methods are robust for the task of item recommendation in social tagging systems, thus can be chosen as the basic components of data fusion process; and (iii) The proposed method (HLC) is more flexible and robust than traditional data fusion models."
2032338312;Lion pride optimizer: An optimization algorithm inspired by lion pride behavior;2012.0;[];"In this paper, we report a novel optimization algorithm, lion pride optimizer (LPO), which is inspired by lion pride behavior. The framework is mainly based on lion prides’ evolution process and group living theory. In a lion pride, brutal competition of individuals happens among male lions both within and among prides; on the other hand, each member plays an important role in the persistence of a lion pride. Based on this framework, concepts from lion prides behavior, e.g., the strongest males occupy nearly all mating resources, and if a new cohort of males is able to take over a pride, they will seek to kill young cubs sired by their predecessors, are employed metaphorically to design optimum searching strategies for solving continuous optimization problems. From the studies of the algorithm property, it is found that the LPO algorithm is not sensitive to most parameters, which shows the robustness of the algorithm and the parameters are not problemdependent. Central tendency of the algorithm is not found. It is found that the pride update strategy and brutal competition of individuals are two main factors that contribute to the performance of LPO. According to the test results on 23 famous benchmark functions, the LPO algorithm has better performance than the other seven state-of-the-art algorithms on both unimodal and multimodal benchmark functions; in the test of high-dimensional multimodal problems, LPO outperforms the other five algorithms on all benchmark functions."
2032784298;WebCT - The quasimoderating effect of perceived affective quality on an extending Technology Acceptance Model;2010.0;[];"Perceived affective quality is an attractive area of research in Information System. Specifically, understanding the intrinsic and extrinsic individual factors and interaction effects that influence Information and Communications Technology (ICT) acceptance and adoption - in higher education - continues to be a focal interest in learning research. In this regard, one type of affective reactions toward ICT (in our study, the WebCT), perceived affective quality, is an essential dimension in user technology acceptance. A structural equation modelling, specifically partial least square (PLS), is proposed to assess the relationships between the constructs together with the predictive power of the research model. The results demonstrate that the research model significantly predicts the intention to use the WebCT. The results provide strong support for the proposals that (a) perceived usefulness, ease of use and flow lead the learners towards developing high intention to use the WebCT; and (b) perceived affective quality exhibits a relevant interaction effect on the model. This study, therefore, represents a u0027u0027crucial testu0027u0027 of non-utilitarian influences on use of Web-based applications. The model and results can thus be used to assess motivational design aspects during electronic learning process."
2032829364;The impact of trust, risk and optimism bias on E-file adoption;2010.0;[];One of congressu0027 goals for 2007 was for 80% of all tax and informational returns to be filed electronically. However, to date that lofty goal has fallen well short. This research proposes a model of e-filing adoption. To test the model a survey is administered to 260 participants. The instrument assesses citizen perceptions of trust, risk and optimism bias. Structural equation modeling is used to evaluate the relationships between these three concepts and intention to use. The results indicate that trust of the Internet, trust of the e-filer, perceived risk and optimism bias all have an impact on intention to use e-filing. Implications for practice and research are discussed.
2033238208;A unified framework for risk and vulnerability analysis covering both safety and security;2007.0;[];Recently, we have seen several attempts to establish adequate risk and vulnerability analyses tools and related management frameworks dealing not only with accidental events but also security problems. These attempts have been based on different analysis approaches and using alternative building blocks. In this paper, we discuss some of these and show how a unified framework for such analyses and management tasks can be developed. The framework is based on the use of probability as a measure of uncertainty, as seen through the eyes of the assessor, and define risk as the combination of possible consequences and related uncertainties. Risk and vulnerability characterizations are introduced incorporating ideas both from vulnerability analyses literature as well as from the risk classification scheme introduced by Renn and Klinke.
2033300564;LIVE: Lightweight Integrity Verification and Content Access Control for Named Data Networking;2015.0;[];Named data networking (NDN) is a new paradigm for the future Internet wherein interest and data packets carry content names rather than the current IP paradigm of source and destination addresses. Security is built into NDN by embedding a public key signature in each data packet to enable verification of authenticity and integrity of the content. However, existing heavyweight signature generation and verification algorithms prevent universal integrity verification among NDN nodes, which may result in content pollution and denial of service attacks. Furthermore, caching and location-independent content access disables the capability of a content provider to control content access, e.g., who can cache a content and which end user or device can access it. We propose a lightweight integrity verification (LIVE) architecture, an extension to the NDN protocol, to address these two issues seamlessly. LIVE enables universal content signature verification in NDN with lightweight signature generation and verification algorithms. Furthermore, it allows a content provider to control content access in NDN nodes by selectively distributing integrity verification tokens to authorized nodes. We evaluate the effectiveness of LIVE with open source CCNx project. Our paper shows that LIVE only incurs average 10% delay in accessing contents. Compared with traditional public key signature schemes, the verification delay is reduced by over 20 times in LIVE.
2033406822;Software Engineering for Self-Adaptive Systems: A Research Roadmap;2009.0;[];"The goal of this roadmap paper is to summarize the state-of-the-art and to identify critical challenges for the systematic software engineering of self-adaptive systems. The paper is partitioned into four parts, one for each of the identified essential views of self-adaptation: modelling dimensions, requirements, engineering, and assurances. For each view, we present the state-of-the-art and the challenges that our community must address. This roadmap paper is a result of the Dagstuhl Seminar 08031 on ""Software Engineering for Self-Adaptive Systems,"" which took place in January 2008."
2033411413;PeopleRank: Social Opportunistic Forwarding;2010.0;[];In opportunistic networks, end-to-end paths between two communicating nodes are rarely available. In such situations, the nodes might still copy and forward messages to nodes that are more likely to meet the destination. The question is which forwarding algorithm offers the best trade off between cost (number of message replicas) and rate of successful message delivery. We address this challenge by developing the PeopleRank approach in which nodes are ranked using a tunable weighted social information. Similar to the PageRank idea, PeopleRank gives higher weight to nodes if they are socially connected to important other nodes of the network. We develop centralized and distributed variants for the computation of PeopleRank. We present an evaluation using real mobility traces of nodes and their social interactions to show that PeopleRank manages to deliver messages with near optimal success rate (close to Epidemic Routing) while reducing the number of message retransmissions by 50% compared to Epidemic Routing.
2033724226;Standby redundancy optimization problems with fuzzy lifetimes;2005.0;[];Three types of system performances--the expected system lifetime, α-system lifetime, and system reliability--characterized in the context of credibility are investigated in this paper. Some fuzzy simulations are designed to estimate these system performances. In order to formulate general standby redundancy optimization problems with fuzzy lifetimes, a spectrum of standby redundancy fuzzy programming models are proposed. Fuzzy simulation, neural network, and genetic algorithm are also integrated to produce a hybrid intelligent algorithm for solving those models. Finally, some numerical experiments on multi-stage system and network system are provided.
2033917481;A unified model between the weighted average and the induced OWA operator;2011.0;[];We present a new model that uses the weighted average (WA) and the induced ordered weighted averaging (IOWA) operator in the same formulation. We call it the induced ordered weighted averaging-weighted average (IOWAWA) operator. We study some of its main properties and we see that it has a lot of particular cases such as the WA and the OWA operator. The main advantage of the IOWAWA operator is that it unifies the IOWA operator with the WA in the same formulation considering the degree of importance that each concept has in the aggregation. We analyze the applicability of this new approach and we see that it is very broad because it can be applied in a wide range of fields such as statistics, economics, decision theory and engineering. Theoretically, we could state that all the previous models and applications based on the WA and the IOWA can be revised and improved with this new approach because they will be included in this framework as a particular case. We focus on an application in a multi-person decision-making in political management.
2034563113;Induced uncertain linguistic OWA operators applied to group decision making;2006.0;[];The ordered weighted averaging (OWA) operator was developed by Yager [IEEE Trans. Syst., Man, Cybernet. 18 (1998) 183]. Later, Yager and Filev [IEEE Trans. Syst., Man, Cybernet.--Part B 29 (1999) 141] introduced a more general class of OWA operators called the induced ordered weighted averaging (IOWA) operators, which take as their argument pairs, called OWA pairs, in which one component is used to induce an ordering over the second components which are exact numerical values and then aggregated. The aim of this paper is to develop some induced uncertain linguistic OWA (IULOWA) operators, in which the second components are uncertain linguistic variables. Some desirable properties of the IULOWA operators are studied, and then, the IULOWA operators are applied to group decision making with uncertain linguistic information.
2034912561;Sindice.com: a document-oriented lookup index for open linked data;2008.0;[];Data discovery on the Semantic Web requires crawling and indexing of statements, in addition to the u0027linked-datau0027 approach of de-referencing resource URIs. Existing Semantic Web search engines are focused on database-like functionality, compromising on index size, query performance and live updates. We present Sindice, a lookup index over Semantic Web resources. Our index allows applications to automatically locate documents containing information about a given resource. In addition, we allow resource retrieval through inverse-functional properties, offer a full-text search and index SPARQL endpoints. Finally, we extend the sitemap protocol to efficiently index large datasets with minimal impact on data providers.
2036318837;Ten Simple Rules for Reproducible Computational Research;2013.0;[];"Replication is the cornerstone of a cumulative science [1]. However, new tools and technologies, massive amounts of data, interdisciplinary approaches, and the complexity of the questions being asked are complicating replication efforts, as are increased pressures on scientists to advance their research [2]. As full replication of studies on independently collected data is often not feasible, there has recently been a call for reproducible research as an attainable minimum standard for assessing the value of scientific claims [3]. This requires that papers in experimental science describe the results and provide a sufficiently clear protocol to allow successful repetition and extension of analyses based on original data  :[106],""importance of replication and reproducibility has recently been exemplified through studies showing that scientific papers commonly leave out experimental details essential for reproduction [5], studies showing difficulties with replicating published experimental results [6], an increase in retracted papers [7], and through a high number of failing clinical trials [8], [9]. This has led to discussions on how individual researchers, institutions, funding bodies, and journals can establish routines that increase transparency and reproducibility. In order to foster such aspects, it has been suggested that the scientific community needs to develop a “culture of reproducibility” for computational science, and to require it for published claims  :[210],""want to emphasize that reproducibility is not only a moral responsibility with respect to the scientific field, but that a lack of reproducibility can also be a burden for you as an individual researcher. As an example, a good practice of reproducibility is necessary in order to allow previously developed methodology to be effectively applied on new data, or to allow reuse of code and results for new projects. In other words, good habits of reproducibility may actually turn out to be a time-saver in the longer  :[298],""further note that reproducibility is just as much about the habits that ensure reproducible research as the technologies that can make these processes efficient and realistic. Each of the following ten rules captures a specific aspect of reproducibility, and discusses what is needed in terms of information handling and tracking of procedures. If you are taking a bare-bones approach to bioinformatics analysis, i.e., running various custom scripts from the command line, you will probably need to handle each rule explicitly. If you are instead performing your analyses through an integrated framework (such as GenePattern [10], Galaxy [11], LONI pipeline [12], or Taverna [13]), the system may already provide full or partial support for most of the rules. What is needed on your part is then merely the knowledge of how to exploit these existing  a :[433],""pragmatic setting, with publication pressure and deadlines, one may face the need to make a trade-off between the ideals of reproducibility and the need to get the research out while it is still relevant. This trade-off becomes more important when considering that a large part of the analyses being tried out never end up yielding any results. However, frequently one will, with the wisdom of hindsight, contemplate the missed opportunity to ensure reproducibility, as it may already be too late to take the necessary notes from memory (or at least much more difficult than to do it while underway). We believe that the rewards of reproducibility will compensate for the risk of having spent valuable time developing an annotated catalog of analyses that turned out as blind  a :[562],""minimal requirement, you should at least be able to reproduce the results yourself. This would satisfy the most basic requirements of sound research, allowing any substantial future questioning of the research to be met with a precise explanation. Although it may sound like a very weak requirement, even this level of reproducibility will often require a certain level of care in order to be met. There will for a given analysis be an exponential number of possible combinations of software versions, parameter values, pre-processing steps, and so on, meaning that a failure to take notes may make exact reproduction essentially  this basic level of reproducibility in :[664],""place, there is much more that can be wished for. An obvious extension is to go from a level where you can reproduce results in case of a critical situation to a level where you can practically and routinely reuse your previous work and increase your productivity. A second extension is to ensure that peers have a practical possibility of reproducing your results, which can lead to increased trust in, interest for, and citations of your work [6],  :[749],""here present ten simple rules for reproducibility of computational research. These rules can be at your disposal for whenever you :[210],""want to make your research more accessible—be it for peers or for your future self."
2036821962;MDP routing for multi-rate loss networks;2000.0;[];   Markov decision theory has been successfully used in adaptive routing in traditional circuit-switched networks. When extending Markov decision-based routing algorithms to future Broadband Integrated Service Digital Networks (B-ISDNs), the required computational complexity becomes extremely high. In this paper, we propose an approach towards adaptive routing in multi-rate networks using a Markov decision theoretic framework which maintains low computational complexity while still providing quite good routing information. In this approach, each link, based on PASCAL distribution, is modeled as a one-dimensional birth–death process to reduce the state space size and a policy iteration method from Markov decision theory is iteratively applied to achieve better network performance. Our results show that routing algorithms based on this approach yield better performance than least-load path routing (LLP) without incurring any significant increase in computational complexity.
2037007846;Optimizing Talbot’s Contours for the Inversion of the Laplace Transform;2006.0;[];Talbot’s method for the numerical inversion of the Laplace transform consists of numerically integrating the Bromwich integral on a special contour by means of the trapezoidal or midpoint rules. In this paper we address the issue of parameter selection in the method, for the particular situation when parabolic PDEs are solved. In the process the well-known subgeometric convergence rate $O(\exp(-c \sqrt{N}))$ of this method is improved to the geometric rate $O(\exp(-c N))$, with $N$ the number of nodes in the integration rule. The value of the maximum decay rate $c$ is explicitly determined. Numerical results for two versions of the heat equation are presented. With the choice of parameters derived here, the rule of thumb is that to achieve an accuracy of $10^{-\ell}$ at any given time, the associated elliptic problem has to be solved no more than $\ell$ times.
2037137187;Matching users and items across domains to improve the recommendation quality;2014.0;[];Given two homogeneous rating matrices with some overlapped users/items whose mappings are unknown, this paper aims at answering two questions. First, can we identify the unknown mapping between the users and/or items? Second, can we further utilize the identified mappings to improve the quality of recommendation in either domain? Our solution integrates a latent space matching procedure and a refining process based on the optimization of prediction to identify the matching. Then, we further design a transfer-based method to improve the recommendation performance. Using both synthetic and real data, we have done extensive experiments given different real life scenarios to verify the effectiveness of our models. The code and other materials are available at http://www.csie.ntu.edu.tw/~r00922051/matching/
2037554805;A NOVEL SIMILARITY MEASURE BETWEEN INTUITIONISTIC FUZZY SETS AND ITS APPLICATIONS;2013.0;[];In this paper, we propose a novel similarity measure between intuitionistic fuzzy sets and apply it to deal with pattern recognition problems and medical diagnosis problems. First, we propose a new similarity measure between intuitionistic fuzzy values based on the medians of intervals, the Hausdorff distance, and the ratio of the uncertainty degrees of intuitionistic fuzzy values. We also prove some properties of the proposed similarity measure between intuitionistic fuzzy values. Then, based on the proposed similarity measure between intuitionistic fuzzy values, we propose a novel similarity measure between intuitionistic fuzzy sets. It can overcome the drawbacks of existing methods for measuring the degree of similarity between intuitionistic fuzzy sets. We also prove some properties of the proposed similarity measure between intuitionistic fuzzy sets. Finally, we apply the proposed similarity measure between intuitionistic fuzzy sets to deal with pattern recognition problems and medical diagnosis prob...
2037701101;Interpolation of diffusion weighted imaging datasets;2014.0;[];   Diffusion weighted imaging (DWI) is used to study white-matter fibre organisation, orientation and structural connectivity by means of fibre reconstruction algorithms and tractography. For clinical settings, limited scan time compromises the possibilities to achieve high image resolution for finer anatomical details and signal-to-noise-ratio for reliable fibre reconstruction. We assessed the potential benefits of interpolating DWI datasets to a higher image resolution before fibre reconstruction using a diffusion tensor model. Simulations of straight and curved crossing tracts smaller than or equal to the voxel size showed that conventional higher-order interpolation methods improved the geometrical representation of white-matter tracts with reduced partial-volume-effect (PVE), except at tract boundaries. Simulations and interpolation of ex-vivo monkey brain DWI datasets revealed that conventional interpolation methods fail to disentangle fine anatomical details if PVE is too pronounced in the original data. As for validation we used ex-vivo DWI datasets acquired at various image resolutions as well as Nissl-stained sections. Increasing the image resolution by a factor of eight yielded finer geometrical resolution and more anatomical details in complex regions such as tract boundaries and cortical layers, which are normally only visualized at higher image resolutions. Similar results were found with typical clinical human DWI dataset. However, a possible bias in quantitative values imposed by the interpolation method used should be considered. The results indicate that conventional interpolation methods can be successfully applied to DWI datasets for mining anatomical details that are normally seen only at higher resolutions, which will aid in tractography and microstructural mapping of tissue compartments.
2037716689;Exploiting intercontact time for routing in delay tolerant networks;2013.0;[];Because of the dynamic nature of delay tolerant networks (DTNs), many replication-based routing schemes were proposed to increase the probability of delivery by making multiple copies of each message. In such schemes, one concern is how many replicas of a message should be distributed in the network. In this paper, we propose a routing scheme for DTNs, called adaptive spraying based on the intercontact time (ASBIT). The scheme is based on the idea of that each node dynamically chooses the right number of message copies disseminated to respond to the current conditions of the network. When forwarding, ASBIT selects the node with a higher centrality as the next hop, and utilises the multi-attribute decision making theory for the division of the replication number between two nodes. Simulation results show that ASBIT achieves comparable delivery ratio and delivery delay while maintaining lower overhead compared with some well-known routing schemes in sparse scenarios. Copyright © 2012 John Wiley u0026 Sons, Ltd.
2037754816;Online allocation of virtual machines in a distributed cloud;2014.0;[];
2039074122;Expressive, Efficient, and Revocable Data Access Control for Multi-Authority Cloud Storage;2014.0;[];Data access control is an effective way to ensure the data security in the cloud. Due to data outsourcing and untrusted cloud servers, the data access control becomes a challenging issue in cloud storage systems. Ciphertext-Policy Attribute-based Encryption (CP-ABE) is regarded as one of the most suitable technologies for data access control in cloud storage, because it gives data owners more direct control on access policies. However, it is difficult to directly apply existing CP-ABE schemes to data access control for cloud storage systems because of the attribute revocation problem. In this paper, we design an expressive, efficient and revocable data access control scheme for multi-authority cloud storage systems, where there are multiple authorities co-exist and each authority is able to issue attributes independently. Specifically, we propose a revocable multi-authority CP-ABE scheme, and apply it as the underlying techniques to design the data access control scheme. Our attribute revocation method can efficiently achieve both forward security and backward security. The analysis and simulation results show that our proposed data access control scheme is secure in the random oracle model and is more efficient than previous works.
2039157284;BUBBLE Rap: Social-Based Forwarding in Delay-Tolerant Networks;2011.0;[];The increasing penetration of smart devices with networking capability form novel networks. Such networks, also referred as pocket switched networks (PSNs), are intermittently connected and represent a paradigm shift of forwarding data in an ad hoc manner. The social structure and interaction of users of such devices dictate the performance of routing protocols in PSNs. To that end, social information is an essential metric for designing forwarding algorithms for such types of networks. Previous methods relied on building and updating routing tables to cope with dynamic network conditions. On the downside, it has been shown that such approaches end up being cost ineffective due to the partial capture of the transient network behavior. A more promising approach would be to capture the intrinsic characteristics of such networks and utilize them in the design of routing algorithms. In this paper, we exploit two social and structural metrics, namely centrality and community, using real human mobility traces. The contributions of this paper are two-fold. First, we design and evaluate BUBBLE, a novel social-based forwarding algorithm, that utilizes the aforementioned metrics to enhance delivery performance. Second, we empirically show that BUBBLE can substantially improve forwarding performance compared to a number of previously proposed algorithms including the benchmarking history-based PROPHET algorithm, and social-based forwarding SimBet algorithm.
2039637952;A Trust Evaluation Model for Cloud Computing;2013.0;[];   Trust has attracted extensive attention in social science and computer science as a solution to enhance the security of the system. This paper proposes a trust evaluation model based on D-S evidence theory and sliding windows for cloud computing. The timeliness of the interaction evidence as the first-hand evidence is reflected by introducing sliding windows. The direct trust of entities is computed based on the interaction evidence by D-S evidence theory. The conflict of the recommendation trust as the second-hand evidence is eliminated with a help of an improved fusion approach as far as possible. Finally, the combination of the recommendation trust exposes the credibility of entities. Experimental results show that the proposed model is effective and extensible.
2040121225;A definition of a nonprobabilistic entropy in the setting of fuzzy sets theory;1972.0;[];A functional defined on the class of generalized characteristic functions (fuzzy sets), called “entropy”, is introduced using no probabilistic concepts in order to obtain a global measure of the  indefiniteness  connected with the situations described by fuzzy sets. This “entropy” may be regarded as a measure of a quantity of information which is not necessarily related to random experiments.  Some mathematical properties of this functional are analyzed and some considerations on its applicability to pattern analysis are made.
2040256447;Utilizing user tag-based interests in recommender systems for social resource sharing websites;2014.0;[];"Tag frequency, recency, and duration were combined to model the personalized preference.The social network was utilized to find similar users in the collaborative filtering.The incorporated system was applied to the social resource sharing systems. Recently collaborative tagging, also known as ""folksonomy"" in Web 2.0, allows users to collaboratively create and manage tags to classify and categorize dynamic content for searching and sharing. A useru0027s interest in social resources usually changes with time in such a dynamic and information rich environment. Additionally, a social network is one innovative characteristic in social resource sharing websites. The information from a social network provides an inference of a certain useru0027s interests based on the interests of this useru0027s network neighbors.To handle the problem of personalized interests changing gradually with time, and to utilize the benefit of the social network, this study models a personalized user interest, incorporating frequency, recency, and duration of tag-based information, and performs collaborative recommendations using the useru0027s social network in social resource sharing websites. The proposed method includes finding neighbors from the ""social friends"" network by using collaborative filtering and recommending similar resource items to the users by using content-based filtering.This study examines the proposed systemu0027s performance using an experimental dataset collected from a social bookmarking website. The experimental results show that the hybridization of useru0027s preferences with frequency, recency, and duration plays an important role, and provides better performances than traditional collaborative recommendation systems. The experimental results also reveal that the friend network information can successfully collaborate, thus improving the collaborative recommendation process."
2040459121;Spray and Focus: Efficient Mobility-Assisted Routing for Heterogeneous and Correlated Mobility;2007.0;[];"Intermittently connected mobile networks are wireless networks where most of the time there does not exist a complete path from the source to the destination. There are many real networks that follow this model, for example, wildlife tracking sensor networks, military networks, vehicular ad hoc networks (VANETs), etc. To deal with such networks researchers have suggested to use controlled replication or ""spraying "" methods that can reduce the overhead of flooding-based schemes by distributing a small number of copies to only a few relays. These relays then ""look"" for the destination in parallel as they move into the network. Although such schemes can perform well in scenarios with high mobility (e.g. VANETs), they struggle in situations were mobility is slow and correlated in space and/or time. To route messages efficiently in such networks, we propose a scheme that also distributes a small number of copies to few relays. However, each relay can then forward its copy further using a single-copy utility-based scheme, instead of naively waiting to deliver it to the destination itself. This scheme exploits all the advantages of controlled replication, but is also able to identify appropriate forwarding opportunities that could deliver the message faster. Simulation results for traditional mobility models, as well as for a more realistic ""community-based"" model, indicate that our scheme can reduce the delay of existing spraying techniques up to 20 times in some scenarios"
2040882071;On the Convergence of Krylov Subspace Methods for Matrix Mittag-Leffler Functions;2011.0;[];In this paper we analyze the convergence of some commonly used Krylov subspace methods for computing the action of matrix Mittag-Leffler functions. As is well known, such functions find application in the solution of fractional differential equations. We illustrate the theoretical results by some numerical experiments.
2040986566;Some new information measures for fuzzy sets;1993.0;[];   After reviewing some existing measures for fuzzy sets, we introduce a new informative measure for discrimination between two fuzzy sets. This discriminating measure reduces to the nonprobabilistic entropy of Deluca and Termini [7] under a special condition. The divergence measure between two sets has been defined along with a large set of properties. It has also been used to define an ambiguity (fuzziness) measure. Renyiu0027s [17] probabilistic entropy of order α has been extended to define nonprobabilistic entropy of a fuzzy set. Various properties of this definition have also been proved. Applications of these measures to clustering, image processing, vision, etc., are highlighted.
2041455711;Fuzzy decision-making method based on the weighted correlation coefficient under intuitionistic fuzzy environment;2010.0;[];A multicriteria fuzzy decision-making method based on weighted correlation coefficients using entropy weights is proposed under intuitionistic fuzzy environment for some situations where the information about criteria weights for alternatives is completely unknown. To determine the entropy weights with respect to a set of criteria represented by intuitionistic fuzzy sets (IFSs), we establish an entropy weight model, which can be used to get the criteria weights, and then propose an evaluation formula of weighted correlation coefficient between an alternative and the ideal alternative. The alternatives can be ranked and the most desirable one(s) can be selected according to the weighted correlation coefficients. Finally, two illustrative examples demonstrate the practicality and effectiveness of the proposed method.
2042308234;Low-Rank Explicit QTT Representation of the Laplace Operator and Its Inverse;2012.0;[];We focus on the construction of explicit low-rank representations of matrices in the tensor train (TT) and quantized tensor train (QTT) formats which have been proposed recently for the low-parametric structured representation of large-scale tensors. The matrices under consideration are discretizations of the Laplace operator in a hypercube (in one and many dimensions) and their inverses (in one dimension). For these matrices we derive explicit and exact QTT representations of low QTT ranks independent of the numbers $d$ and $n^{2}$ of dimensions and entries in each dimension. This implies that for the matrices considered the storage cost is $\mathcal{O}\left(d\,\log n\right)$, i.e., logarithmic with respect to the total number $n^{2d}$ of entries. The same applies to the computational complexity of the QTT-structured operations with these matrices, which now depends on the QTT ranks of the other operands. The general result of the paper is the notation and technique we introduce in order to examine the Q...
2042316011;Local Invariant Feature Detectors: A Survey;2008.0;[];In this survey, we give an overview of invariant interest point detectors, how they evolvd over time, how they work, and what their respective strengths and weaknesses are. We begin with defining the properties of the ideal local feature detector. This is followed by an overview of the literature over the past four decades organized in different categories of feature extraction methods. We then provide a more detailed analysis of a selection of methods which had a particularly significant impact on the research field. We conclude with a summary and promising future research directions.
2042601312;Experiences of Time Loss among Videogame Players: An Empirical Study;2007.0;[];Playing videogames is now a major leisure pursuit, yet research in the area is comparatively sparse. Previous correlational evidence suggests that subjective time loss occurs during playing videogames. This study examined experiences of time loss among a relatively large group of gamers (n = 280). Quantitative and qualitative data were collected through an online survey. Results showed that time loss occurred irrespective of gender, age, or frequency of play, but was associated with particular structural characteristics of games such as their complexity, the presence of multi-levels, missions and/or high scores, multiplayer interactions, and plot. Results also demonstrated that time loss could have both positive and negative outcomes for players. Positive aspects of time loss included helping players to relax and temporarily escape from reality. Negative aspects included the sacrificing of other things in their lives, guilty feelings about wasted time, and social conflict. It is concluded that for many ga...
2042731036;Social interactions in P2P lending;2009.0;[];"Access to capital in the form of credit through money lending requires that the lender to be able to measure the risk of repayment for a given return. In ancient times money lending needed to occur between known parties or required collateral to secure the loan. In the modern era of banking institutions provide loans to individuals who meet a qualification test. Grameen Bank in Bangladesh has demonstrated that small poor communities benefited from the ""microcredit"" financial innovation, which allowed a priori non-bankable entrepreneurs to engage in self-employment projects. Online P2P (Peer to Peer) lending is considered an evolution of the microcredit concept, and reflects the application of its principles into internet communities. Internet ventures like Prosper.com, Zopa or Lendingclub.com, provide the means for lenders and borrowers to meet, interact and define relationships as part of social groups. This paper measures the influence of social interactions in the risk evaluation of a money request; with special focus on the impact of one-to-one and one-to-many relationships. The results showed that fostering social features increases the chances of getting a loan fully funded, when financial features are not enough to construct a differentiating successful credit request. For this task, a model-based clustering method was applied on actual P2P Lending data provided by Prosper.com."
2043833509;The Role of Preparedness in Ambulance Dispatching;2011.0;[];Response time in the emergency medical service is an important performance measure and ambulance dispatching is one of the most important factors affecting the response time. The most commonly used dispatching rule is to send the closest available unit to the call site. However, though dispatching the closest unit enables the service to achieve the minimal response time for the current call, the response times for the next incoming calls may increase if the area where the closest ambulance is currently located has a high call rate, that is the area becomes ill-prepared. A dispatching algorithm based on the preparedness concept was recently proposed. Rather than greedily minimizing each current response time, the dispatching algorithm takes account of future calls by a quantitative definition of preparedness. This study investigates the role of preparedness by examining the performance of the preparedness-based dispatching algorithm as well as by evolving the algorithm in several ways in order to magnify the effectiveness of preparedness consideration. As a result of these efforts, it is found that the consideration of preparedness in ambulance dispatching can provide significant benefits in reducing response time but only when appropriately used.
2043902857;Interval-valued hesitant fuzzy linguistic sets and their applications in multi-criteria decision-making problems;2014.0;[];"An interval-valued hesitant fuzzy linguistic set (IVHFLS) can serve as an extension of both a linguistic term set and an interval-valued hesitant fuzzy set. This new set combines quantitative evaluation with qualitative evaluation; these can describe the real preferences of decision-makers and reflect their uncertainty, hesitancy, and inconsistency. This work focuses on multi-criteria decision-making (MCDM) problems in which the criteria are in different priority levels and the criteria values take the form of interval-valued hesitant fuzzy linguistic numbers (IVHFLNs). The new approach to solving these problems is based on the prioritized aggregation operators of IVHFLNs. Having reviewed the relevant literature, we provide interval-valued hesitant fuzzy linguistic operations and apply some linguistic scale functions, which have been improved on the basis of psychological theory and prospect theory. Ultimately, two kinds of prioritized aggregation operators of IVHFLNs are developed, which extend to a grouping prioritized situation and are applied to MCDM problems. Finally, an example is provided to illustrate and verify the proposed approach in two separate situations, which are then compared to other representative methods."
2044465660;Textural Features for Image Classification;1973.0;[];Texture is one of the important characteristics used in identifying objects or regions of interest in an image, whether the image be a photomicrograph, an aerial photograph, or a satellite image. This paper describes some easily computable textural features based on gray-tone spatial dependancies, and illustrates their application in category-identification tasks of three different kinds of image data: photomicrographs of five kinds of sandstones, 1:20 000 panchromatic aerial photographs of eight land-use categories, and Earth Resources Technology Satellite (ERTS) multispecial imagery containing seven land-use categories. We use two kinds of decision rules: one for which the decision regions are convex polyhedra (a piecewise linear decision rule), and one for which the decision regions are rectangular parallelpipeds (a min-max decision rule). In each experiment the data set was divided into two parts, a training set and a test set. Test set identification accuracy is 89 percent for the photomicrographs, 82 percent for the aerial photographic imagery, and 83 percent for the satellite imagery. These results indicate that the easily computable textural features probably have a general applicability for a wide variety of image-classification applications.
2044591816;Incorporating sentiment into tag-based user profiles and resource profiles for personalized search in folksonomy;2016.0;[];We present a framework SenticRank to incorporate sentiment for personalized search.Content-based and collaborative sentiment ranking methods are proposed.We compare the proposed sentiment-based search with baselines experimentally.We study the influence of sentiment corpora by using some sentiment dictionaries.Sentiment-based information can significantly improve performance in folksonomy. In recent years, there has been a rapid growth of user-generated data in collaborative tagging (a.k.a. folksonomy-based) systems due to the prevailing of Web 2.0 communities. To effectively assist users to find their desired resources, it is critical to understand user behaviors and preferences. Tag-based profile techniques, which model users and resources by a vector of relevant tags, are widely employed in folksonomy-based systems. This is mainly because that personalized search and recommendations can be facilitated by measuring relevance between user profiles and resource profiles. However, conventional measurements neglect the sentiment aspect of user-generated tags. In fact, tags can be very emotional and subjective, as users usually express their perceptions and feelings about the resources by tags. Therefore, it is necessary to take sentiment relevance into account into measurements. In this paper, we present a novel generic framework SenticRank to incorporate various sentiment information to various sentiment-based information for personalized search by user profiles and resource profiles. In this framework, content-based sentiment ranking and collaborative sentiment ranking methods are proposed to obtain sentiment-based personalized ranking. To the best of our knowledge, this is the first work of integrating sentiment information to address the problem of the personalized tag-based search in collaborative tagging systems. Moreover, we compare the proposed sentiment-based personalized search with baselines in the experiments, the results of which have verified the effectiveness of the proposed framework. In addition, we study the influences by popular sentiment dictionaries, and SenticNet is the most prominent knowledge base to boost the performance of personalized search in folksonomy.
2044675702;Obfuscated malicious javascript detection using classification techniques;2009.0;[];As the World Wide Web expands and more users join, it becomes an increasingly attractive means of distributing malware. Malicious javascript frequently serves as the initial infection vector for malware. We train several classifiers to detect malicious javascript and evaluate their performance. We propose features focused on detecting obfuscation, a common technique to bypass traditional malware detectors. As the classifiers show a high detection rate and a low false alarm rate, we propose several uses for the classifiers, including selectively suppressing potentially malicious javascript based on the classifieru0027s recommendations, achieving a compromise between usability and security.
2044936152;Parallel data processing with MapReduce: a survey;2012.0;[];A prominent parallel data processing tool MapReduce is gaining significant momentum from both industry and academia as the volume of data to analyze grows rapidly. While MapReduce is used in many areas where massive data analysis is required, there are still debates on its performance, efficiency per node, and simple abstraction. This survey intends to assist the database and open source communities in understanding various technical aspects of the MapReduce framework. In this survey, we characterize the MapReduce framework and discuss its inherent pros and cons. We then introduce its optimization strategies reported in the recent literature. We also discuss the open issues and challenges raised on parallel data analysis with MapReduce.
2045345923;PLASMA: a plan-based layered architecture for software model-driven adaptation;2010.0;[];Modern software-intensive systems are expected to adapt, often while the system is executing, to changing requirements, failures, and new operational contexts. This paper describes an approach to dynamic system adaptation that utilizes plan-based and architecture-based mechanisms. Our approach utilizes an architecture description language (ADL) and a planning-as-model-checking technology to enable dynamic replanning. The ability to automatically generate adaptation plans based solely on ADL models and an application problem description simplifies the specification and use of adaptation mechanisms for system architects. The approach uses a three-layer architecture that, while similar to previous work, provides several significant improvements. We apply our approach within the context of a mobile robotics case study.
2045403087;Real-Time Semantic Search Using Approximate Methodology for Large-Scale Storage Systems;2016.0;[];The challenges of handling the explosive growth in data volume and complexity cause the increasing needs for semantic queries. The semantic queries can be interpreted as the correlation-aware retrieval, while containing approximate results. Existing cloud storage systems mainly fail to offer an adequate capability for the semantic queries. Since the true value or worth of data heavily depends on how efficiently semantic search can be carried out on the data in (near-) real-time, large fractions of data end up with their values being lost or significantly reduced due to the data staleness. To address this problem, we propose a near-real-time and cost-effective semantic queries based methodology, called FAST. The idea behind FAST is to explore and exploit the semantic correlation within and among datasets via correlation-aware hashing and manageable flat-structured addressing to significantly reduce the processing latency, while incurring acceptably small loss of data-search accuracy. The near-real-time property of FAST enables rapid identification of correlated files and the significant narrowing of the scope of data to be processed. FAST supports several types of data analytics, which can be implemented in existing searchable storage systems. We conduct a real-world use case in which children reported missing in an extremely crowded environment (e.g., a highly popular scenic spot on a peak tourist day) are identified in a timely fashion by analyzing 60 million images using FAST. FAST is further improved by using semantic-aware namespace to provide dynamic and adaptive namespace management for ultra-large storage systems. Extensive experimental results demonstrate the efficiency and efficacy of FAST in the performance improvements.
2045591981;A language and environment for architecture-based software development and evolution;1999.0;[];Software architectures have the potential to substantially improve the development and evolution of large, complex, multi-lingual, multi-platform, long-running systems. However, in order to achieve this potential, specific techniques for architecture-based modeling, analysis, and evolution must be provided. Furthermore, one cannot fully benefit from such techniques unless support for mapping an architecture to an implementation also exists. This paper motivates and presents one such approach, which is an outgrowth of our experience with systems developed and evolved according to the C2 architectural style. We describe an architecture description language (ADL) specifically designed to support architecture-based evolution and discuss the kinds of evolution the language supports. We then describe a component-based environment that enables modeling, analysis, and evolution of architectures expressed in the ADL, as well as mapping of architectural models to an implementation infrastructure. The architecture of the environment itself can be evolved easily to support multiple ADLs, kinds of analyses, architectural styles, and implementation platforms. Our approach is fully reflexive: the environment can be used to describe, analyze, evolve, and (partially) implement itself, using the very ADL it supports. An existing architecture is used throughout the paper to provide illustrations and examples.
2045610944;Some reflections on the anniversary of Fuzzy Sets and Systems;1999.0;[];
2046229586;Trust in the Cloud;2011.0;[];Cloud infrastructure is expected to be able to support Internet scale critical applications (e.g. hospital systems and smart grid systems). Critical infrastructure services and organizations alike will not outsource their critical applications to a public Cloud without strong assurances that their requirements will be enforced. Central to this concern is that the user should be provided with evidence of the trustworthiness of the elements of the Cloud. Establishing Cloudu0027s trust model is important but the Cloudu0027s infrastructure complexity and dynamism makes it difficult to address. Establishing trust in the Cloud is one of the key objectives of the EU funded TClouds (Trustworthy Clouds) project. In TClouds we focus on building trust models that provide various levels of transparency in the context of technical complexities and trust establishment. These trust models are not only beneficial to a Cloudu0027s users, but also to Cloud providers, collaborating Clouds-of-Clouds, and external auditors. In this paper we explore this problem, and summarize some of the recent results from the TClouds project in context of trust establishment.
2046353767;Developing artificial neural networks for safety critical systems;2006.0;[];There are many performance based techniques that aim to improve the safety of neural networks for safety critical applications. However, many of these approaches provide inadequate forms of safety assurance required for certification. As a result, neural networks are typically restricted to advisory roles in safety-related applications. Neural networks have the ability to operate in unpredictable and changing environments. It is therefore desirable to certify them for highly-dependable roles in safety critical systems. This paper outlines the safety criteria which are safety requirements for the behaviour of neural networks. If enforced, the criteria can contribute to justifying the safety of ANN functional properties. Characteristics of potential neural network models are also outlined and are based upon representing knowledge in interpretable and understandable forms. The paper also presents a safety lifecycle for artificial neural networks. This lifecycle focuses on managing behaviour represented by neural networks and contributes to providing acceptable forms of safety assurance.
2047162235;A Learning-Based Framework for Engineering Feature-Oriented Self-Adaptive Software Systems;2013.0;[];Self-adaptive software systems are capable of adjusting their behavior at runtime to achieve certain functional or quality-of-service goals. Often a representation that reflects the internal structure of the managed system is used to reason about its characteristics and make the appropriate adaptation decisions. However, runtime conditions can radically change the internal structure in ways that were not accounted for during their design. As a result, unanticipated changes at runtime that violate the assumptions made about the internal structure of the system could degrade the accuracy of the adaptation decisions. We present an approach for engineering self-adaptive software systems that brings about two innovations: 1) a feature-oriented approach for representing engineersu0027 knowledge of adaptation choices that are deemed practical, and 2) an online learning-based approach for assessing and reasoning about adaptation decisions that does not require an explicit representation of the internal structure of the managed software system. Engineersu0027 knowledge, represented in feature-models, adds structure to learning, which in turn makes online learning feasible. We present an empirical evaluation of the framework using a real-world self-adaptive software system. Results demonstrate the frameworku0027s ability to accurately learn the changing dynamics of the system while achieving efficient analysis and adaptation.
2047756776;Opinion spam and analysis;2008.0;[];Evaluative texts on the Web have become a valuable source of opinions on products, services, events, individuals, etc. Recently, many researchers have studied such opinion sources as product reviews, forum posts, and blogs. However, existing research has been focused on classification and summarization of opinions using natural language processing and data mining techniques. An important issue that has been neglected so far is opinion spam or trustworthiness of online opinions. In this paper, we study this issue in the context of product reviews, which are opinion rich and are widely used by consumers and product manufacturers. In the past two years, several startup companies also appeared which aggregate opinions from product reviews. It is thus high time to study spam in reviews. To the best of our knowledge, there is still no published study on this topic, although Web spam and email spam have been investigated extensively. We will see that opinion spam is quite different from Web spam and email spam, and thus requires different detection techniques. Based on the analysis of 5.8 million reviews and 2.14 million reviewers from amazon.com, we show that opinion spam in reviews is widespread. This paper analyzes such spam activities and presents some novel techniques to detect them
2048562911;Searching and browsing Linked Data with SWSE: The Semantic Web Search Engine;2011.0;[];"In this paper, we discuss the architecture and implementation of the Semantic Web Search Engine (SWSE). Following traditional search engine architecture, SWSE consists of crawling, data enhancing, indexing and a user interface for search, browsing and retrieval of information; unlike traditional search engines, SWSE operates over RDF Web data - loosely also known as Linked Data - which implies unique challenges for the system design, architecture, algorithms, implementation and user interface. In particular, many challenges exist in adopting Semantic Web technologies for Web data: the unique challenges of the Web - in terms of scale, unreliability, inconsistency and noise - are largely overlooked by the current Semantic Web standards. Herein, we describe the current SWSE system, initially detailing the architecture and later elaborating upon the function, design, implementation and performance of each individual component. In so doing, we also give an insight into how current Semantic Web standards can be tailored, in a best-effort manner, for use on Web data. Throughout, we offer evaluation and complementary argumentation to support our design choices, and also offer discussion on future directions and open research questions. Later, we also provide candid discussion relating to the difficulties currently faced in bringing such a search engine into the mainstream, and lessons learnt from roughly six years working on the Semantic Web Search Engine project."
2048932320;Probabilistic routing based on two-hop information in delay/disruption tolerant networks;2015.0;[];We investigate an opportunistic routing protocol in delay/disruption tolerant networks (DTNs) where the end-to-end path between source and destination nodes may not exist for most of the time. Probabilistic routing protocol using history of encounters and transitivity (PRoPHET) is an efficient history-based routing protocol specifically proposed for DTNs, which only utilizes the delivery predictability of one-hop neighbors to make a decision for message forwarding. In order to further improve the message delivery rate and to reduce the average overhead of PRoPHET, in this paperwe propose an improved probabilistic routing algorithm (IPRA), where the history information of contacts for the immediate encounter and two-hop neighbors has been jointly used to make an informed decision for message forwarding. Based on the Opportunistic Networking Environment (ONE) simulator, the performance of IPRA has been evaluated via extensive simulations.The results showthat IPRA can significantly improve the average delivery rate while achieving a better or comparable performance with respect to average overhead, average delay, and total energy consumption compared with the existing algorithms.
2048957655;Influence Functions of the Spearman and Kendall Correlation Measures;2010.0;[];Nonparametric correlation estimators as the Kendall and Spearman correlation are widely used in the applied sciences. They are often said to be robust, in the sense of being resistant to outlying observations. In this paper we formally study their robustness by means of their influence functions and gross-error sensitivities. Since robustness of an estimator often comes at the price of an increased variance, we also compute statistical efficiencies at the normal model. We conclude that both the Spearman and Kendall correlation estimators combine a bounded and smooth influence function with a high efficiency. In a simulation experiment we compare these nonparametric estimators with correlations based on a robust covariance matrix estimator.
2049080106;Gamification. using game-design elements in non-gaming contexts;2011.0;[];"""Gamification"" is an informal umbrella term for the use of video game elements in non-gaming systems to improve user experience (UX) and user engagement. The recent introduction of u0027gamifiedu0027 applications to large audiences promises new additions to the existing rich and diverse research on the heuristics, design patterns and dynamics of games and the positive UX they provide. However, what is lacking for a next step forward is the integration of this precise diversity of research endeavors. Therefore, this workshop brings together practitioners and researchers to develop a shared understanding of existing approaches and findings around the gamification of information systems, and identify key synergies, opportunities, and questions for future research."
2049211729;Sentiment analysis in financial markets A framework to utilize the human ability of word association for analyzing stock market news reports;2014.0;[];
2049309847;Students' perceptions of Facebook for academic purposes;2014.0;[];"Facebook is the most popular Social Network Site (SNS) among college students. Despite the popularity and extensive use of Facebook by students, its use has not made significant inroads into classroom usage.In this study, we seek to examine why this is the case and whether it would be worthwhile for faculty to invest the time to integrate Facebook into their teaching. To this end, we decided to undertake a study with a sample of 214 undergraduate students at the University of Huelva (Spain). We applied the structural equation model specifically designed by Mazman and Usluel (2010) to identify the factors that may motivate these students to adopt and use social network tools, specifically Facebook, for educational purposes.According to our results, Social Influence is the most important factor in predicting the adoption of Facebook; students are influenced to adopt it to establish or maintain contact with other people with whom they share interests. Regarding the purposes of Facebook usage, Social Relations is perceived as the most important factor among all of the purposes collected. Our findings also revealed that the educational use of Facebook is explained directly by its purposes of usage and indirectly by its adoption. Structural model to explain the educational usage of Facebook.Direct effect of purposes of Facebook usage on educational usage of Facebook.Indirect effect of Facebook adoption on educational usage of Facebook.Social Influence is the most important factor in predicting the adoption of Facebook.Social Relations is the most important factor among all of the purposes collected."
2049365101;Multiple view semi-supervised dimensionality reduction;2010.0;[];Multiple view data, together with some domain knowledge in the form of pairwise constraints, arise in various data mining applications. How to learn a hidden consensus pattern in the low dimensional space is a challenging problem. In this paper, we propose a new method for multiple view semi-supervised dimensionality reduction. The pairwise constraints are used to derive embedding in each view and simultaneously, the linear transformation is introduced to make different embeddings from different pattern spaces comparable. Hence, the consensus pattern can be learned from multiple embeddings of multiple representations. We derive an iterating algorithm to solve the above problem. Some theoretical analyses and out-of-sample extensions are also provided. Promising experiments on various data sets, together with some important discussions, are also presented to demonstrate the effectiveness of the proposed algorithm.
2051247433;Genetic algorithms in reliability engineering;2006.0;[];
2051336598;Asymptotic lower bounds for Ramsey functions;1977.0;[];   A probability theorem, due to Lovasz, is used to derive lower bounds for various Ramsey functions. A short proof of the known result    R(3, t) ⩾   ct     2   (  ln   t)     2     is given.
2053900989;Approximating the Nondominated Front Using the Pareto Archived Evolution Strategy;2000.0;[];We introduce a simple evolution scheme for multiobjective optimization problems, called the Pareto Archived Evolution Strategy (PAES). We argue that PAES may represent the simplest possible nontrivial algorithm capable of generating diverse solutions in the Pareto optimal set. The algorithm, in its simplest form, is a (1 + 1) evolution strategy employing local search but using a reference archive of previously found solutions in order to identify the approximate dominance ranking of the current and candidate solution vectors. (1 + 1)-PAES is intended to be a baseline approach against which more involved methods may be compared. It may also serve well in some real-world applications when local search seems superior to or competitive with population-based methods. We introduce (1 + λ) and (μ | λ) variants of PAES as extensions to the basic algorithm. Six variants of PAES are compared to variants of the Niched Pareto Genetic Algorithm and the Nondominated Sorting Genetic Algorithm over a diverse suite of six test functions. Results are analyzed and presented using techniques that reduce the attainment surfaces generated from several optimization runs into a set of univariate distributions. This allows standard statistical analysis to be carried out for comparative purposes. Our results provide strong evidence that PAES performs consistently well on a range of multiobjective optimization tasks.
2054249991;Blow-up Lemma;1997.0;[];Regular pairs behave like complete bipartite graphs from the point of view of bounded degree subgraphs.
2055022211;Robust Online Multi-object Tracking Based on Tracklet Confidence and Online Discriminative Appearance Learning;2014.0;[];Online multi-object tracking aims at producing complete tracks of multiple objects using the information accumulated up to the present moment. It still remains a difficult problem in complex scenes, because of frequent occlusion by clutter or other objects, similar appearances of different objects, and other factors. In this paper, we propose a robust online multi-object tracking method that can handle these difficulties effectively. We first propose the tracklet confidence using the detectability and continuity of a tracklet, and formulate a multi-object tracking problem based on the tracklet confidence. The multi-object tracking problem is then solved by associating tracklets in different ways according to their confidence values. Based on this strategy, tracklets sequentially grow with online-provided detections, and fragmented tracklets are linked up with others without any iterative and expensive associations. Here, for reliable association between tracklets and detections, we also propose a novel online learning method using an incremental linear discriminant analysis for discriminating the appearances of objects. By exploiting the proposed learning method, tracklet association can be successfully achieved even under severe occlusion. Experiments with challenging public datasets show distinct performance improvement over other batch and online tracking methods.
2055084468;Some star-critical Ramsey numbers;2015.0;[];Let S k = K 1 , k be a star of k edges, and let K n ? S k be a graph obtained from K n and an additional vertex v by joining v and k vertices of K n . For graphs G and H , let r = r ( G , H ) be the Ramsey number of G and H . The star-critical Ramsey number r ? ( G , H ) is the smallest k such that every red/blue edge-coloring of K r - 1 ? S k contains a red G or a blue H . In this note, we show that r ? ( K n , m K 2 ) = n + 2 m - 3 for m ? 1 and n 2 , r ? ( F n , K 3 ) = 2 n + 2 for n ? 2 , where F n is an n -fan. We also show that r ? ( n K 4 , m K 3 ) = 4 n + 2 m for n ? m ? 1 and n ? 2 , and r ? ( n K 4 , m K 3 ) = 3 n + 3 m for m ? n ? 2 .
2055906872;FUSION: a framework for engineering self-tuning self-adaptive software systems;2010.0;[];Self-adaptive software systems are capable of adjusting their behavior at run-time to achieve certain objectives. Such systems typically employ analytical models specified at design-time to assess their characteristics at run-time and make the appropriate adaptation decisions. However, prior to systemu0027s deployment, engineers often cannot foresee the changes in the environment, requirements, and systemu0027s operational profile. Therefore, any analytical model used in this setting relies on underlying assumptions that if not held at run-time make the analysis and hence the adaptation decisions inaccurate. We present and evaluate FeatUre-oriented Self-adaptatION (FUSION) framework, which aims to solve this problem by learning the impact of adaptation decisions on the systemu0027s goals. The framework (1) allows for automatic online fine-tuning of the adaptation logic to unanticipated conditions, (2) reduces the upfront effort required for building such systems, and (3) makes the run-time analysis of such systems very efficient.
2056069438;Graphlet-based measures are suitable for biological network comparison;2013.0;[];"Motivation: Large amounts of biological network data exist for many species. Analogous to sequence comparison, network comparison aims to provide biological insight. Graphlet-based methods are proving to be useful in this respect. Recently some doubt has arisen concerning the applicability of graphlet-based measures to low edge density networks—in particular that the methods are ‘unstable’—and further that no existing network model matches the structure found in real biological  :[67],""We demonstrate that it is the model networks themselves that are ‘unstable’ at low edge density and that graphlet-based measures correctly reflect this instability. Furthermore, while model network topology is unstable at low edge density, biological network topology is stable. In particular, one must distinguish between average density and local density. While model networks of low average edge densities also have low local edge density, that is not the case with protein–protein interaction (PPI) networks: real PPI networks have low average edge density, but high local edge densities, and hence, they (and thus graphlet-based measures) are stable on these networks. Finally, we use a recently devised non-parametric statistical test to demonstrate that PPI networks of many species are well-fit by several models not previously tested. In addition, we model several viral PPI networks for the first time and demonstrate an exceptionally good fit between the data and theoretical  :[216],""natasha@imperial.ac.uk"
2056124433;Global alignment of multiple protein interaction networks with application to functional orthology detection;2008.0;[];Protein–protein interactions (PPIs) and their networks play a central role in all biological processes. Akin to the complete sequencing of genomes and their comparative analysis, complete descriptions of interactomes and their comparative analysis is fundamental to a deeper understanding of biological processes. A first step in such an analysis is to align two or more PPI networks. Here, we introduce an algorithm, IsoRank, for global alignment of multiple PPI networks. The guiding intuition here is that a protein in one PPI network is a good match for a protein in another network if their respective sequences and neighborhood topologies are a good match. We encode this intuition as an eigenvalue problem in a manner analogous to Googleu0027s PageRank method. Using IsoRank, we compute a global alignment of the Saccharomyces cerevisiae, Drosophila melanogaster, Caenorhabditis elegans, Mus musculus, and Homo sapiens PPI networks. We demonstrate that incorporating PPI data in ortholog prediction results in improvements over existing sequence-only approaches and over predictions from local alignments of the yeast and fly networks. Previous methods have been effective at identifying conserved, localized network patterns across pairs of networks. This work takes the further step of performing a global alignment of multiple PPI networks. It simultaneously uses sequence similarity and network data and, unlike previous approaches, explicitly models the tradeoff inherent in combining them. We expect IsoRank—with its simultaneous handling of node similarity and network similarity—to be applicable across many scientific domains.
2056137745;Breast cancer diagnosis using least square support vector machine;2007.0;[];The use of machine learning tools in medical diagnosis is increasing gradually. This is mainly because the effectiveness of classification and recognition systems has improved in a great deal to help medical experts in diagnosing diseases. Such a disease is breast cancer, which is a very common type of cancer among woman. In this paper, breast cancer diagnosis was conducted using least square support vector machine (LS-SVM) classifier algorithm. The robustness of the LS-SVM is examined using classification accuracy, analysis of sensitivity and specificity, k-fold cross-validation method and confusion matrix. The obtained classification accuracy is 98.53% and it is very promising compared to the previously reported classification techniques. Consequently, by LS-SVM, the obtained results show that the used method can make an effective interpretation and point out the ability of design of a new intelligent assistance diagnosis system.
2056155664;Understanding the intention to use interactive whiteboards: model development and testing;2015.0;[];The effective use of an interactive whiteboard (IWB) in teacher-education institutions depends strongly on student teachersu0027 intention of using it. Despite the recent surge in published research on the widespread applications for IWBs in teaching and learning, few have developed a model to elucidate the factors which influence student teachersu0027 behavioural intentions (BIs) regarding the use of IWBs. The aim of this study was to develop a model which demonstrates the variables that affect student teachersu0027 intentions and which also explain their interactions. The proposed research model is based on previous models of technology acceptance. Six variables (technology self-efficacy (TSE), performance expectancy (PE), effort expectancy (EE), social influence (SI), facilitating condition, and BI) were selected to build a model for this study. Structural equation modelling was used as the main technique for data analysis. The research model was found reliable and valid, the findings being based on a self-reporte...
2056231502;An enriched social behavioural information diffusion model in social networks;2015.0;[];"Online social networks have recently become an innovative and effective method for spreading information among people around the world. Information diffusion, rumour spreading and diseases infection are all instances of stochastic processes that occur over the edges of social networks. Many prior works have carried out empirical studies and diffusion models to understand how information propagates in online social networks; however they suffer from problems. In this paper, we propose an information diffusion model inspired by information propagation among people. Our proposed Social Behavioural Information Diffusion Model, abbreviated as SBIDM, considers the effect of mainstream media like TV and radio, as well as interaction with the neighbours. The advantages of our approach are four-fold. First, it models information diffusion in social networks inspired by social life, which considers the effect of aggregate social behaviour to diffuse information; second, it allows partial knowledge to be held in each individual; third, it considers the effects of social media in propagating information as well as the effects of interacting with neighbours; and last but not least, it is applicable to different types of data including synthetic and well-known real social networks like Facebook, Amazon, Epinions and DBLP. To explore the advantages of our approach, many experiments with different settings and specifications were conducted. The obtained results are very promising."
2056307179;Function Field Sieve Method for Discrete Logarithms over Finite Fields;1999.0;[];We present a function field sieve method for discrete logarithms over finite fields. This method is an analog of the number field sieve method originally developed for factoring integers. It is asymptotically faster than the previously known algorithms when applied to finite fields Fpn, where p6?n.
2056510622;On Exploiting Dynamic Execution Patterns for Workload Offloading in Mobile Cloud Applications;2014.0;[];Mobile Cloud Computing (MCC) bridges the gap between limited capabilities of mobile devices and the increasing usersu0027 demand of mobile multimedia applications, by offloading the computational workloads from local devices to the remote cloud. Current MCC research focuses on making offloading decisions over different methods of a MCC application, but may inappropriately increase the energy consumption if having transmitted a large amount of program states over expensive wireless channels. Limited research has been done on avoiding such energy waste by exploiting the dynamic patterns of applicationsu0027 run-time execution for workload offloading. In this paper, we adaptively offload the local computational workload with respect to the run-time application dynamics. Our basic idea is to formulate the dynamic executions of user applications using a semi-Markov model, and to further make offloading decisions based on probabilistic estimations of the offloading operationu0027s energy saving. Such estimation is motivated by experimental investigations over practical smart phone applications, and then builds on analytical modeling of methodsu0027 execution times and offloading expenses. Systematic evaluations show that our scheme significantly improves the efficiency of workload offloading compared to existing schemes over various smart phone applications.
2056788985;A note on information entropy measures for vague sets and its applications;2008.0;[];A new nonprobabilistic entropy of a vague set is proposed by means of the intersection and union of the membership degree and nonmembership degree of the vague set. The concept called vague cross-entropy of vague sets will also be discussed and its definition is given by analogy with the cross-entropy of probability distributions. Finally, two numeric examples are presented to illustrate the applications of vague cross-entropy to pattern recognition and medical diagnosis.
2056835025;User identification for cross-system personalisation;2009.0;[];Currently, there is an increasing demand for user-adaptive systems for various purposes in many different domains. Typically, personalisation in information systems occurs separately within each system. The recent trends in user modeling rely on cross-system personalisation, i.e., the opportunity to share information across multiple information systems in order to improve user adaptation. Cooperation among systems in order to exchange user model knowledge is a complex task. This paper addresses a key challenge for cross-system personalisation which is often taken as a starting assumption, i.e., user identification. In this paper, we describe the conceptualization and implementation of a framework that provides a common base for user identification for cross-system personalisation among web-based user-adaptive systems. However, the framework can be easily adopted in different working environments and for different purposes. The framework represents a hybrid approach which draws parallels both from centralized and decentralized solutions for user modeling. To perform user identification, we propose to exploit a set of identification properties that are combined using an identification algorithm.
2057012437;Information Systems Success: The Quest for the Dependent Variable;1992.0;[];A large number of studies have been conducted during the last decade and a half attempting to identify those factors that contribute to information systems success. However, the dependent variable in these studies-I/S success-has been an elusive one to define. Different researchers have addressed different aspects of success, making comparisons difficult and the prospect of building a cumulative tradition for I/S research similarly elusive. To organize this diverse research, as well as to present a more integrated view of the concept of I/S success, a comprehensive taxonomy is introduced. This taxonomy posits six major dimensions or categories of I/S success-SYSTEM QUALITY, INFORMATION QUALITY, USE, USER SATISFACTION, INDIVIDUAL IMPACT, and ORGANIZATIONAL IMPACT. Using these dimensions, both conceptual and empirical studies are then reviewed a total of 180 articles are cited and organized according to the dimensions of the taxonomy. Finally, the many aspects of I/S success are drawn together into a descriptive model and its implications for future I/S research are discussed.
2057124872;FlashMob: distributed adaptive self-assembly;2011.0;[];Autonomous systems need to support dynamic software adaptation in order to handle the complexity and unpredictability of the execution environment, and the changing needs of the end user. Although a number of approaches have been proposed, few address a key issue: that of distribution.   In this paper we seek to overcome the limitations of centralised approaches. We build on our previous work on adaptive self-assembly within the three-layer model for autonomous systems to provide a decentralised technique for self-assembly. To achieve this in a fault-tolerant and scalable manner, we use a gossip protocol as a basis. While no central or leader node is aware of the full space of solutions, gossip ensures that agreement on a particular solution - in this case a component configuration - is reached in a logarithmic number of steps with respect to the size of the network.
2058036501;COSNET: Connecting Heterogeneous Social Networks with Local and Global Consistency;2015.0;[];More often than not, people are active in more than one social network. Identifying users from multiple heterogeneous social networks and integrating the different networks is a fundamental issue in many applications. The existing methods tackle this problem by estimating pairwise similarity between users in two networks. However, those methods suffer from potential inconsistency of matchings between multiple networks.     In this paper, we propose COSNET (COnnecting heterogeneous Social NETworks with local and global consistency), a novel energy-based model, to address this problem by considering both local and global consistency among multiple networks. An efficient subgradient algorithm is developed to train the model by converting the original energy-based objective function into its dual form.     We evaluate the proposed model on two different genres of data collections: SNS and Academia, each consisting of multiple heterogeneous social networks. Our experimental results validate the effectiveness and efficiency of the proposed model. On both data collections, the proposed COSNET method significantly outperforms several alternative methods by up to 10-30% (p
2058205491;Cross-cultural analysis of users' attitudes toward the use of mobile devices in second and foreign language learning in higher education: A case from Sweden and China;2013.0;[];The present study examined the current state of studentsu0027 attitudes toward mobile technology use in and for second and foreign language learning in higher education. Moreover, the study investigated if age, gender or cultural factors affect these attitudes. A total of 345 students from two in many aspects different countries, China (Yunnan University) and Sweden (Dalarna University) participated in this study. To access learnersu0027 perceptions toward mobile technology use, we employed Kearneyu0027s pedagogical framework to mobile learning from a socio-cultural perspective (Kearney, Schuck, Burden, u0026 Aubusson, 2012). Hofstedeu0027s cultural dimensions were used to approach studentsu0027 cultural views, as these dimensions represent some values - aspects of culture - that may affect attitudes toward technology and learning individually as well as in combination. The findings show the respondentsu0027 attitudes toward mobile learning are very positive with individualization being most positive (83%) followed by collaboration (74%), and authenticity (73%). The statistical analysis indicates that Hofstedeu0027s factors cannot explain the differences in mobile-assisted language learning (MALL) attitudes in the chosen sample. Among the personal factors, gender is identified to be a predictor to explain the differences in studentsu0027 attitudes toward MALL. This study shows that technology itself seems to be the most important culture-shaping factor, more important than culture inherited from the physical environment, and more important than age.
2058208920;Why are faculty members not teaching blended courses? Insights from faculty members;2011.0;[];This paper describes the findings of an exploratory, qualitative case study and examines problems and impediments faculty members encountered in blended learning environments in Turkish Higher Education system. A total of 117 faculty members from 4 universities responded to 8 interview questions. Findings were based on content analyses of interview transcripts. The results show that faculty membersu0027 problems with blended teaching resulted in the identification of three inductive categories: instructional processes, community concerns and technical issues. The eight themes emerged from these three categories include the following: (1) complexity of the instruction, (2) lack of planning and organization, (3) lack of effective communication, (4) need for more time, (5) lack of institutional support, (6) changing roles, (7) difficulty of adoption to new technologies and (8) lack of electronic means. This study indicates that teaching blended courses can be highly complex and have different teaching patterns, which, in turn, impacts successful implementation of the blended college courses.
2058890866;Calculation of the Quality Parameter of Digital Nets and Application to Their Construction;2001.0;[];"In quasi-Monte Carlo methods, point sets of low discrepancy are crucial for accurate results. A class of point sets with low theoretic upper bounds of discrepancy are the digital point sets known as digital (t, m, s)-nets which can be implemented very efficiently. The parameter t is indicative of the quality; i.e., small values of t lead to small upper bounds of the discrepancy. We introduce an effective way to establish this quality parameter t for digital nets constructed over arbitrary finite fields and give an application to the construction of digital nets of high quality."
2059164916;Dynamically evolving the structural variability of dynamic software product lines;2015.0;[];A Dynamic Software Product Line ( dspl ) is a widely used approach to handle variability at runtime, e.g., by activating or deactivating features to adapt the running configuration. With the emergence of highly configurable and evolvable systems,  dspl s have to cope with the evolution of their structural variability, i.e., the Feature Model ( fm ) used to derive the configuration. So far, little is known about the evolution of the  fm  while a configuration derived from this  fm  is running. In particular, such a dynamic evolution changes the  dspl  configuration space, which is thus unsynchronized with the running configuration and its adaptation capabilities. In this position paper, we propose and describe an initial architecture to manage the dynamic evolution of  dspl s and their synchronization. In particular, we explain how this architecture supports the evolution of  dspl s based on  fm s extended with cardinality and attributes, which, to the best of our knowledge, has never been addressed yet.
2059407171;Trust management of services in cloud environments: Obstacles and solutions;2013.0;[];Trust management is one of the most challenging issues in the emerging cloud computing area. Over the past few years, many studies have proposed different techniques to address trust management issues. However, despite these past efforts, several trust management issues such as identification, privacy, personalization, integration, security, and scalability have been mostly neglected and need to be addressed before cloud computing can be fully embraced. In this article, we present an overview of the cloud service models and we survey the main techniques and research prototypes that efficiently support trust management of services in cloud environments. We present a generic analytical framework that assesses existing trust management research prototypes in cloud computing and relevant areas using a set of assessment criteria. Open research issues for trust management in cloud environments are also discussed.
2059415317;Guest Editors' Introduction: In Cloud Computing We Trust - But Should We?;2010.0;[];The Guest Editors Introduction to the Special Issue on Cloud Computing Security presents the economic benefits of cloud computing as well as the security and privacy risks inherent to cloud computing and inherited from traditional computing models. The opportunity to build security into the design of cloud computing services starting from a clean sheet is discussed with game-changing cyber security themes.
2059456505;Hypergraphs, Quasi-randomness, and Conditions for Regularity;2002.0;[];Haviland and Thomason and Chung and Graham were the first to investigate systematically some properties of quasi-random hypergraphs. In particular, in a series of articles, Chung and Graham considered several quite disparate properties of random-like hypergraphs of density 1/2 and proved that they are in fact equivalent. The central concept in their work turned out to be the so called deviation of a hypergraph. They proved that having small deviation is equivalent to a variety of other properties that describe quasi-randomness. In this paper, we consider the concept of discrepancy for k-uniform hypergraphs with an arbitrary constant density d (0
2060013459;Scenario tree modeling for multistage stochastic programs;2009.0;[];An important issue for solving multistage stochastic programs consists in the approximate representation of the (multivariate) stochastic input process in the form of a scenario tree. In this paper, we develop (stability) theory-based heuristics for generating scenario trees out of an initial set of scenarios. They are based on forward or backward algorithms for tree generation consisting of recursive scenario reduction and bundling steps. Conditions are established implying closeness of optimal values of the original process and its tree approximation, respectively, by relying on a recent stability result in Heitsch, Romisch and Strugarek (SIAM J Optim 17:511–525, 2006) for multistage stochastic programs. Numerical experience is reported for constructing multivariate scenario trees in electricity portfolio management.
2060059578;Video object matching across multiple non-overlapping camera views based on multi-feature fusion and incremental learning;2014.0;[];   Matching objects across multiple cameras with non-overlapping views is a necessary but difficult task in the wide area video surveillance. Owing to the lack of spatio-temporal information, only the visual information can be used in some scenarios, especially when the cameras are widely separated. This paper proposes a novel framework based on multi-feature fusion and incremental learning to match the objects across disjoint views in the absence of space–time cues. We first develop a competitive major feature histogram fusion representation (CMFH  1  ) to formulate the appearance model for characterizing the potentially matching objects. The appearances of the objects can change over time and hence the models should be continuously updated. We then adopt an improved incremental general multicategory support vector machine algorithm (IGMSVM  2  ) to update the appearance models online and match the objects based on a classification method. Only a small amount of samples are needed for building an accurate classification model in our method. Several tests are performed on CAVIAR, ISCAPS and VIPeR databases where the objects change significantly due to variations in the viewpoint, illumination and poses. Experimental results demonstrate the advantages of the proposed methodology in terms of computational efficiency, computation storage, and matching accuracy over that of other state-of-the-art classification-based matching approaches. The system developed in this research can be used in real-time video surveillance applications.
2060334710;Human mobility models for opportunistic networks;2011.0;[];Mobile ad hoc networks enable communications between clouds of mobile devices without the need for a preexisting infrastructure. One of their most interesting evolutions are opportunistic networks, whose goal is to also enable communication in disconnected environments, where the general absence of an end-to-end path between the sender and the receiver impairs communication when legacy MANET networking protocols are used. The key idea of OppNets is that the mobility of nodes helps the delivery of messages, because it may connect, asynchronously in time, otherwise disconnected subnetworks. This is especially true for networks whose nodes are mobile devices (e.g., smartphones and tablets) carried by human users, which is the typical OppNets scenario. In such a network where the movements of the communicating devices mirror those of their owners, finding a route between two disconnected devices implies uncovering habits in human movements and patterns in their connectivity (frequencies of meetings, average duration of a contact, etc.), and exploiting them to predict future encounters. Therefore, there is a challenge in studying human mobility, specifically in its application to OppNets research. In this article we review the state of the art in the field of human mobility analysis and present a survey of mobility models. We start by reviewing the most considerable findings regarding the nature of human movements, which we classify along the spatial, temporal, and social dimensions of mobility. We discuss the shortcomings of the existing knowledge about human movements and extend it with the notion of predictability and patterns. We then survey existing approaches to mobility modeling and fit them into a taxonomy that provides the basis for a discussion on open problems and further directions for research on modeling human mobility.
2060505035;Crowdsourced time-sync video tagging using temporal and personalized topic modeling;2014.0;[];Time-sync video tagging aims to automatically generate tags for each video shot. It can improve the useru0027s experience in previewing a videou0027s timeline structure compared to traditional schemes that tag an entire video clip. In this paper, we propose a new application which extracts time-sync video tags by automatically exploiting crowdsourced comments from video websites such as Nico Nico Douga, where videos are commented on by online crowd users in a time-sync manner. The challenge of the proposed application is that users with bias interact with one another frequently and bring noise into the data, while the comments are too sparse to compensate for the noise. Previous techniques are unable to handle this task well as they consider video semantics independently, which may overfit the sparse comments in each shot and thus fail to provide accurate modeling. To resolve these issues, we propose a novel temporal and personalized topic model that jointly considers temporal dependencies between video semantics, usersu0027 interaction in commenting, and usersu0027 preferences as prior knowledge. Our proposed model shares knowledge across video shots via users to enrich the short comments, and peels off user interaction and user bias to solve the noisy-comment problem. Log-likelihood analyses and user studies on large datasets show that the proposed model outperforms several state-of-the-art baselines in video tagging quality. Case studies also demonstrate our modelu0027s capability of extracting tags from the crowdsourced short and noisy comments.
2060907774;On ordered weighted averaging aggregation operators in multicriteria decisionmaking;1988.0;[];The author is primarily concerned with the problem of aggregating multicriteria to form an overall decision function. He introduces a type of operator for aggregation called an ordered weighted aggregation (OWA) operator and investigates the properties of this operator. The OWAu0027s performance is found to be between those obtained using the AND operator, which requires all criteria to be satisfied, and the OR operator, which requires at least one criteria to be satisfied. u003e
2061193494;The iterative solutions of nonlinear fractional differential equations;2013.0;[];By means of monotone iterative technique, the existence and uniqueness of the positive solution for a fractional differential equation with derivatives are established, and the iterative sequence of the solution, an error estimation and the convergence rate of the positive solution are also given.
2061438946;Grey Wolf Optimizer;2014.0;[];This work proposes a new meta-heuristic called Grey Wolf Optimizer (GWO) inspired by grey wolves (Canis lupus). The GWO algorithm mimics the leadership hierarchy and hunting mechanism of grey wolves in nature. Four types of grey wolves such as alpha, beta, delta, and omega are employed for simulating the leadership hierarchy. In addition, the three main steps of hunting, searching for prey, encircling prey, and attacking prey, are implemented. The algorithm is then benchmarked on 29 well-known test functions, and the results are verified by a comparative study with Particle Swarm Optimization (PSO), Gravitational Search Algorithm (GSA), Differential Evolution (DE), Evolutionary Programming (EP), and Evolution Strategy (ES). The results show that the GWO algorithm is able to provide very competitive results compared to these well-known meta-heuristics. The paper also considers solving three classical engineering design problems (tension/compression spring, welded beam, and pressure vessel designs) and presents a real application of the proposed method in the field of optical engineering. The results of the classical engineering design problems and real application prove that the proposed algorithm is applicable to challenging problems with unknown search spaces.
2061997902;Age-related degeneration of corpus callosum measured with diffusion tensor imaging.;2006.0;[];   The corpus callosum is the major commissure connecting the cerebral hemispheres, and there is evidence of its change with aging. The sub-regions of the corpus callosum (genu, rostral body, anterior midbody, posterior midbody, isthmus, splenium) respectively comprise fibers connecting heteromodal- and unimodal-associated cortical regions, and it is known that abnormalities of the corpus callosum are correlated with abnormalities in cognition and behavior. Yet, little is known about changes in the tissue characteristics of its sub-regions. We assessed age-related changes in fractional anisotropy and mean diffusivity in the sub-regions of the corpus callosum using diffusion tensor imaging. We studied 42 healthy right-handed individuals aged 21–73 years. There were no significant interactions of sex × region. Age has significant negative correlation with fractional anisotropy in the genu (P
2062429067;Supervised and unsupervised learning of multidimensionally varying non-native speech categories;2008.0;[];"The acquisition of novel phonetic categories is hypothesized to be affected by the distributional properties of the input, the relation of the new categories to the native phonology, and the availability of supervision (feedback). These factors were examined in four experiments in which listeners were presented with novel categories based on vowels of Dutch. Distribution was varied such that the categorization depended on the single dimension duration, the single dimension frequency, or both dimensions at once. Listeners were clearly sensitive to the distributional information, but unidimensional contrasts proved easier to learn than multidimensional. The native phonology was varied by comparing Spanish versus American English listeners. Spanish listeners found categorization by frequency easier than categorization by duration, but this was not true of American listeners, whose native vowel system makes more use of duration-based distinctions. Finally, feedback was either available or not; this comparison showed supervised learning to be significantly superior to unsupervised learning."
2062670301;Imbalances in k-colorations;1971.0;[];
2063738313;On intuitionistic fuzzy entropy of order-α;2014.0;[];Using the idea of Renyiu0027s entropy, intuitionistic fuzzy entropy of order-α is proposed in the setting of intuitionistic fuzzy sets theory. This measure is a generalized version of fuzzy entropy of order-α proposed by Bhandari and Pal and intuitionistic fuzzy entropy defined by Vlachos and Sergiadis. Our study of the four essential and some other properties of the proposed measure clearly establishes the validity of the measure as intuitionistic fuzzy entropy. Finally, a numerical example is given to show that the proposed entropy measure for intuitionistic fuzzy set is reasonable by comparing it with other existing entropies.
2063903336;DOA Estimation and Tracking of ULAs with Mutual Coupling;2012.0;[];A class of subspace-based methods for direction-of-arrival (DOA) estimation and tracking in the case of uniform linear arrays (ULAs) with mutual coupling is proposed. By treating the angularly-independent mutual coupling as angularly-dependent complex array gains, the middle subarray is found to have the same complex array gains. Using this property, a new way for parameterizing the steering vector is proposed and the corresponding method for joint estimation of DOAs and mutual coupling matrix (MCM) using the whole array data is derived based on subspace principle. Simulation results show that the proposed algorithm has a better performance than the conventional subarray-based method especially for weak signals. Furthermore, to achieve low computational complexity for online and time-varying DOA estimation, three subspace tracking algorithms with different arithmetic complexities and tracking abilities are developed. More precisely, by introducing a better estimate of the subspace to the conventional tracking algorithms, two modified methods, namely modified projection approximate subspace tracking (PAST) (MPAST) and modified orthonormal PAST (MOPAST), are developed for slowly changing subspace, whereas a Kalman filter with a variable number of measurements (KFVM) method for rapidly changing subspace is introduced. Simulation results demonstrate that these algorithms offer high flexibility and effectiveness for tracking DOAs in the presence of mutual coupling.
2064173066;Personalized recommendation in social tagging systems using hierarchical clustering;2008.0;[];Collaborative tagging applications allow Internet users to annotate resources with personalized tags. The complex network created by many annotations, often called a folksonomy, permits users the freedom to explore tags, resources or even other useru0027s profiles unbound from a rigid predefined conceptual hierarchy. However, the freedom afforded users comes at a cost: an uncontrolled vocabulary can result in tag redundancy and ambiguity hindering navigation. Data mining techniques, such as clustering, provide a means to remedy these problems by identifying trends and reducing noise. Tag clusters can also be used as the basis for effective personalized recommendation assisting users in navigation. We present a personalization algorithm for recommendation in folksonomies which relies on hierarchical tag clusters. Our basic recommendation framework is independent of the clustering method, but we use a context-dependent variant of hierarchical agglomerative clustering which takes into account the useru0027s current navigation context in cluster selection. We present extensive experimental results on two real world dataset. While the personalization algorithm is successful in both cases, our results suggest that folksonomies encompassing only one topic domain, rather than many topics, present an easier target for recommendation, perhaps because they are more focused and often less sparse. Furthermore, context dependent cluster selection, an integral step in our personalization algorithm, demonstrates more utility for recommendation in multi-topic folksonomies than in single-topic folksonomies. This observation suggests that topic selection is an important strategy for recommendation in multi-topic folksonomies.
2064213435;Topic-Driven SocialRank: Personalized search result ranking by identifying similar, credible users in a social network;2013.0;[];A Social Network Service (SNS) is a type of popular, lifestyle Web service to connect a user with friends, and a useru0027s interest in Web search can affect her friends who have similar interests. If these usersu0027 preferences can be tracked, we can show more relevant information following a useru0027s interests. In this paper, we propose the Topic-Driven SocialRank algorithm to show interest-driven search results with relevant Web content from friends using social contacts online by identifying similar, credible users. Our assumption is that credible users issue more relevant information. We observe that a user has certain common interest with her similar friends in the SN, and focus on identifying similar users who have high credibility and sharing their search experiences. Experimental validation shows that our method significantly outperforms the baseline method. Our method is potentially effective to find more relevant search results by implicit help of familiar, credible users.
2064241518;Toward a secure system engineering methodolgy;1998.0;[];This paper presents a methodology for enumerating the vuinerabilities of a system, and determining what countermeasures can best close those vulnerabilities. We first describe how to characterize possible adversaries in terms of their resources, access, and risk tolerance, then we show how to map vulnerabilities to the system throughout its life cycle, and finally we demonstrate how to correlate the attackeru0027s characteristics with the characteristics of the vulnerability to see if an actual threat exists. Countermeasures need to be considered only for the attacks that meet the adversariesu0027 resources and objectives. Viable countermeasures must meet user needs for cost, ease of use, compatibility, performance, and availability. 1998 NSPW 9/96 Chodottsville, VA, USA 1-,58113-168-2/99/0007... * This paper is based on research done by a working group sponsored by the National Security Agency.
2064263554;The effectiveness of m-learning in the form of podcast revision lectures in higher education;2008.0;[];In this paper we describe a study of the effectiveness of mobile learning (m-learning) in the form of podcasting, for teaching undergraduate students in Higher Education. Podcasting involves downloading a series of audio or video broadcasts (files) onto a digital media player, via a computer, over a period of weeks. These can then be watched or listened to when, where and as often as students choose. The use of digital media players, popularised by Appleu0027s iPod(TM), is widespread amongst undergraduate students. A pilot survey of Business and Management students indicated that over 74% owned some form of digital media player, with a further 7% indicating that they intended to purchase one in the next six months. Whilst podcasting is being utilized as a teaching tool by some educators in the secondary sector, its use in higher education, and its effectiveness as a learning tool for adults, remains to be established. In our study, a separate group of just under 200 first-level students were given a series of revision podcasts after completing a course in Information and Communications Technology (and prior to their examination). As part of the subscription process, they had to complete an online questionnaire about their experience. The questionnaire utilized a five-point Likert scale comparing their attitudes to lectures, podcasts, notes, textbooks and multimedia e-learning systems. Statistical analysis of the results of the study indicates that students believe that podcasts are more effective revision tools than their textbooks and they are more efficient than their own notes in helping them to learn. They also indicate that they are more receptive to the learning material in the form of a podcast than a traditional lecture or textbook. The study suggests that the use of podcasts as a revision tool has clear benefits as perceived by undergraduate students in terms of the time they take to revise and how much they feel they can learn. Coupled with the advantages of flexibility in when, where and how it is used, podcasting appears to have significant potential as an innovative learning tool for adult learners in Higher Education.
2064491739;A genetic algorithm approach to multiobjective land use planning;2004.0;[];This paper describes a class of spatial planning problems in which different land uses have to be allocated across a geographical region, subject to a variety of constraints and conflicting management objectives. A goal programming/reference point approach to the problem is formulated, which leads however to a difficult nonlinear combinatorial optimization problem. A special purpose genetic algorithm is developed for the solution of this problem, and is extensively tested numerically. The model and algorithm is then applied to a specific land use planning problem in The Netherlands. The ultimate goal is to integrate the algorithm into a complete land use planning decision support system.
2064622432;Seeker Optimization Algorithm;2006.0;[];A novel algorithm called seeker optimization algorithm (SOA) for the real-parameter optimization is proposed in this paper. SOA is based on the concept of simulating the act of human randomized search. In the SOA, after given center point, search direction, search radius, and trust degree, every seeker moves to a new position (next solution) from his current position based on his historical and social experience. In this process, the update formula is like Y-conditional cloud generator. The algorithmu0027s performance was studied using several typically complex functions. In all cases studied, SOA is superior to continuous genetic algorithm (CGA) and particle swarm optimization (PSO) greatly in terms of optimization quality, robustness and efficiency.
2065043952;Qualitative decision making with correlation coefficients of hesitant fuzzy linguistic term sets;2015.0;[];The hesitant fuzzy linguistic term set (HFLTS) is a new and flexible tool in representing hesitant qualitative information in decision making. Correlation measures and correlation coefficients have been applied widely in many research domains and practical fields. This paper focuses on the correlation measures and correlation coefficients of HFLTSs. To start the investigation, the definition of HFLTS is improved and the concept of hesitant fuzzy linguistic element (HFLE) is introduced. Motivated by the idea of traditional correlation coefficients of fuzzy sets, intuitionistic fuzzy sets and hesitant fuzzy sets, several different types of correlation coefficients for HFLTSs are proposed. The prominent properties of these correlation coefficients are then investigated. In addition, considering that different HFLEs may have different weights, the weighted correlation coefficients and ordered weighted correlation coefficients are further investigated. Finally, an application example concerning the traditional Chinese medical diagnosis is given to illustrate the applicability and validation of the proposed correlation coefficients of HFLTSs in the process of qualitative decision making.
2065071081;Gamification: designing for motivation;2012.0;[];Social Mediator is a forum exploring the ways that HCI research and principles interact---or might interact---with practices in the social media world. Joe McCarthy, Editor
2065261777;A note on full intuitionistic linear logic;1996.0;[];   This short note considers the formulation of Full Intuitionistic Linear Logic (FILL) given by Hyland and de Paiva (1993). Unfortunately the formulation is not closed under the process of cut elimination. This note proposes an alternative formulation based on the notion of patterns.
2065267227;Interacting meaningfully with machine learning systems: Three experiments;2009.0;[];Although machine learning is becoming commonly used in todayu0027s software, there has been little research into how end users might interact with machine learning systems, beyond communicating simple u0027u0027right/wrongu0027u0027 judgments. If the users themselves could work hand-in-hand with machine learning systems, the usersu0027 understanding and trust of the system could improve and the accuracy of learning systems could be improved as well. We conducted three experiments to understand the potential for rich interactions between users and machine learning systems. The first experiment was a think-aloud study that investigated usersu0027 willingness to interact with machine learning reasoning, and what kinds of feedback users might give to machine learning systems. We then investigated the viability of introducing such feedback into machine learning systems, specifically, how to incorporate some of these types of user feedback into machine learning systems, and what their impact was on the accuracy of the system. Taken together, the results of our experiments show that supporting rich interactions between users and machine learning systems is feasible for both user and machine. This shows the potential of rich human-computer collaboration via on-the-spot interactions as a promising direction for machine learning systems and users to collaboratively share intelligence.
2065557289;Car Detection in High-Resolution Urban Scenes Using Multiple Image Descriptors;2014.0;[];Robust and efficient detection of cars in urban scenes has many useful applications. This paper introduces a framework for car detection from high-resolution satellite images, wherein a novel extended image descriptor is used to depict the geometric, spectral and colour distribution properties of cars. The proposed framework is based on a sliding-window detection approach and it begins with a pre-prepossessing stage, which discards detection windows that are very unlikely to contain cars, e.g., plain areas and vegetation, followed by the computation of a concatenated feature vector of Histogram of Oriented Gradients, Fourier and truncated Pyramid Colour Self-Similarity image descriptors that is then fed to a pre-trained linear Support Vector Machine classifier to discriminate between the feature and non-feature subspaces. For post-processing, a non-maximum suppression technique is used to eliminate multiple detections. The performance of the proposed framework has been assessed on the Vaihingen dataset and results show that it exceeds the performance of the current state-of-the-art car detection algorithms.
2066260470;MediaScope: selective on-demand media retrieval from mobile devices;2013.0;[];Motivated by an availability gap for visual media, where images and videos are uploaded from mobile devices well after they are generated, we explore the selective, timely retrieval of media content from a collection of mobile devices. We envision this capability being driven by similarity-based queries posed to a cloud search front-end, which in turn dynamically retrieves media objects from mobile devices that best match the respective queries within a given time limit. Building upon a crowd-sensing framework, we have designed and implemented a system called MediaScope that provides this capability. MediaScope is an extensible framework that supports nearest-neighbor and other geometric queries on the feature space (e.g. clusters, spanners), and contains novel retrieval algorithms that attempt to maximize the retrieval of relevant information. From experiments on a prototype, MediaScope is shown to achieve near-optimal query completeness and low to moderate overhead on mobile devices.
2066393173;A fuzzy-genetic approach to breast cancer diagnosis;1999.0;[];"The automatic diagnosis of breast cancer is an important, real-world medical problem. In this paper we focus on the Wisconsin breast cancer diagnosis (WBCD) problem, combining two methodologies—fuzzy systems and evolutionary algorithms—so as to automatically produce diagnostic systems. We find that our fuzzy-genetic approach produces systems exhibiting two prime characteristics: first, they attain high classification performance (the best shown to date), with the possibility of attributing a confidence measure to the output diagnosis; second, the resulting systems involve a few simple rules, and are therefore (human-) interpretable. © 1999 Elsevier Science B.V. All rights reserved."
2066426987;The Role of Flow Experience in Cyber-Game Addiction;2003.0;[];Consumer habit, an important key to repetitive consumption, is an interesting yet puzzling phenomenon. Sometimes this consumption becomes obsessive—consumers will continue to act a certain way even when they feel it is not in their best interests. However, not all consumers develop such addictions. This study uses cyber-game addiction syndrome as an analogue to trace the possible causes of consumer addiction. Results from structure equation modeling show that repetition of favorite activities has a moderate effect upon addiction, which is in line with the assertion of rational addiction theory. However, flow experience—the emotional state embracing perceptional distortion and enjoyment—shows a much stronger impact on addiction. This suggests that consumers who have experienced flow are more likely to be addicted.
2066653708;What is the academic efficacy of podcasting;2010.0;[];Podcasting may be an answer to some of the challenges to higher education to modernize, to open up, and to develop a competitive edge. However, over the years there have been many high claims for new technology, and not all of them have been redeemed. In terms of academic performance, it may therefore be asked if podcasting really is worth the investment? Looking for at least a tentative answer, the present paper reviews an extensive body of scholarly literature published 2004-2009 on experiences with podcasting in higher education. It is concluded that purely in terms of assessing student performance, indications of the efficacy of podcasting are as yet fairly weak - admitting for a general lack of longitudinal studies. Still, podcasting does seem to have a general positive impact on the academic environment. One such effect is opening up for experimentation with known forms of teaching. Another effect is that many students experience podcasts as a genuine improvement to the study environment, and that they use the new tool rationally as a supplement to their study activities.
2066783542;Analysis and classification of speech imagery EEG for BCI;2013.0;[];   Electroencephalogram (EEG) is generally used in brain–computer interface (BCI), including motor imagery, mental task, steady-state evoked potentials (SSEPs) and P300. In order to complement existing motor-based control paradigms, this paper proposed a novel imagery mode: speech imagery. Chinese characters are monosyllabic and one Chinese character can express one meaning. Thus, eight Chinese subjects were required to read two Chinese characters in mind in this experiment. There were different shapes, pronunciations and meanings between two Chinese characters. Feature vectors of EEG signals were extracted by common spatial patterns (CSP), and then these vectors were classified by support vector machine (SVM). The accuracy between two characters was not superior. However, it was still effective to distinguish whether subjects were reading one character in mind, and the accuracies were between 73.65% and 95.76%. The results were better than vowel speech imagery, and they were suitable for asynchronous BCI. BCI systems will be also extended from motor imagery to combine motor imagery and speech imagery in the future.
2067528438;Watch the Story Unfold with TextWheel: Visualization of Large-Scale News Streams;2012.0;[];Keyword-based searching and clustering of news articles have been widely used for news analysis. However, news articles usually have other attributes such as source, author, date and time, length, and sentiment which should be taken into account. In addition, news articles and keywords have complicated macro/micro relations, which include relations between news articles (i.e., macro relation), relations between keywords (i.e., micro relation), and relations between news articles and keywords (i.e., macro-micro relation). These macro/micro relations are time varying and pose special challenges for news analysis.   In this article we present a visual analytics system for news streams which can bring multiple attributes of the news articles and the macro/micro relations between news streams and keywords into one coherent analytical context, all the while conveying the dynamic natures of news streams. We introduce a new visualization primitive called TextWheel which consists of one or multiple keyword wheels, a document transportation belt, and a dynamic system which connects the wheels and belt. By observing the TextWheel and its content changes, some interesting patterns can be detected. We use our system to analyze several news corpora related to some major companies and the results demonstrate the high potential of our method.
2067708377;Redundancy optimization problems with uncertainty of combining randomness and fuzziness;2004.0;[];By using random fuzzy lifetimes as basic parameters, three types of system performance––expected system lifetime, (α,β)-system lifetime and system reliability––are presented. Some random fuzzy simulations are employed to estimate the system performance. Furthermore, a spectrum of random fuzzy models are constructed for redundancy optimization problems. In order to solve these models, a hybrid intelligent algorithm is used. Finally, some numerical examples are provided for the sake of illustration.
2067875401;Are personal innovativeness and social influence critical to continue with mobile commerce;2014.0;[];Purpose – The purpose of this paper is to report a study investigating the impact of personal innovativeness in information technology (PIIT) and social influence on user continuance intention toward mobile commerce (m-commerce) in the USA. Design/methodology/approach – A survey was conducted among undergraduate and graduate mobile users in a regional university. Structural equation modeling procedures were deployed to analyse 323 valid data points. Findings – The study found that among well-educated m-commerce users, user personal innovativeness as measured by PIIT and perceived usefulness, the determinants of initial adoption, remain as strong determinants of user continuance intention. PIIT also remains as the antecedent of perceived ease of use. Social influence has changed the pattern of influence on continuance intention. Research limitations/implications – This study is unable to investigate m-commerce user expectations and satisfaction levels. The small and convenient sample does not offer guarant...
2067939568;Gaussian Spectral Rules for the Three-Point Second Differences: I. A Two-Point Positive Definite Problem in a Semi-Infinite Domain;1999.0;[];We suggest an approach to grid optimization for a second order finite-difference scheme for elliptic equations. A model problem corresponding to the three-point finite-difference semidiscretization of the Laplace equation on a semi-infinite strip is considered. We relate the approximate boundary Neumann-to-Dirichlet map to a rational function and calculate steps of our finite-difference grid using the Pade--Chebyshev approximation of the inverse square root. It increases the convergence order of the Neumann-to-Dirichlet map from second to exponential without increasing the stencil of the finite-difference scheme and losing stability.
2068273572;An optimal probabilistic forwarding protocolin delay tolerant networks;2009.0;[];Due to uncertainty in nodal mobility, DTN routing usually employs multi-copy forwarding schemes. To avoid the cost associated with flooding, much effort has been focused on probabilistic forwarding, which aims to reduce the cost of forwarding while retaining a high performance rate by forwarding messages only to nodes that have high delivery probabilities. This paper aims to provide an optimal forwarding protocol which maximizes the expected delivery rate while satisfying a certain constant on the number of forwardings per message. In our proposed optimal probabilistic forwarding (OPF) protocol, we use an optimal probabilistic forwarding metric derived by modeling each forwarding as an optimal stopping rule problem. We also present several extensions to allow OPF to use only partial routing information and work with other probabilistic forwarding schemes such as ticket-based forwarding. We implement OPF and several other protocols and perform trace-driven simulations. Simulation results show that the delivery rate of OPF is only 5% lower than epidemic, and 20% greater than the state-of-the-art delegation forwarding while generating 5% more copies and 5% longer delay.
2068297245;A Generalized Transfer-Function Based Array Calibration Technique for Direction Finding;2008.0;[];This correspondence studies direction finding on sources of electromagnetic energy using a passive sensor array whose manifold is only nominally known. The problem of direction finding is studied in the context of an array that can observe a ground-based source from multiple angles, such as an airborne system. An array calibration technique utilizing multiple calibration matrices with angle dependent corrections is developed. Results on experimental data and comparisons with other array calibration techniques are shown to verify the efficacy of the proposed technique.
2068611653;Dense trajectories and motion boundary descriptors for action recognition;2013.0;[];This paper introduces a video representation based on dense trajectories and motion boundary descriptors. Trajectories capture the local motion information of the video. A dense representation guarantees a good coverage of foreground motion as well as of the surrounding context. A state-of-the-art optical flow algorithm enables a robust and efficient extraction of dense trajectories. As descriptors we extract features aligned with the trajectories to characterize shape (point coordinates), appearance (histograms of oriented gradients) and motion (histograms of optical flow). Additionally, we introduce a descriptor based on motion boundary histograms (MBH) which rely on differential optical flow. The MBH descriptor shows to consistently outperform other state-of-the-art descriptors, in particular on real-world videos that contain a significant amount of camera motion. We evaluate our video representation in the context of action classification on nine datasets, namely KTH, YouTube, Hollywood2, UCF sports, IXMAS, UIUC, Olympic Sports, UCF50 and HMDB51. On all datasets our approach outperforms current state-of-the-art results.
2069280737;A heuristic for solving the redundancy allocation problem for multi-state series-parallel systems;2004.0;[];   The redundancy allocation problem is formulated with the objective of minimizing design cost, when the system exhibits a multi-state reliability behavior, given system-level performance constraints. When the multi-state nature of the system is considered, traditional solution methodologies are no longer valid. This study considers a multi-state series-parallel system (MSPS) with capacitated binary components that can provide different multi-state system performance levels. The different demand levels, which must be supplied during the system-operating period, result in the multi-state nature of the system. The new solution methodology offers several distinct benefits compared to traditional formulations of the MSPS redundancy allocation problem. For some systems, recognizing that different component versions yield different system performance is critical so that the overall system reliability estimation and associated design models the true system reliability behavior more realistically. The MSPS design problem, solved in this study, has been previously analyzed using genetic algorithms (GAs) and the universal generating function. The specific problem being addressed is one where there are multiple component choices, but once a component selection is made, only the same component type can be used to provide redundancy. This is the first time that the MSPS design problem has been addressed without using GAs. The heuristic offers more efficient and straightforward analyses. Solutions to three different problem types are obtained illustrating the simplicity and ease of application of the heuristic without compromising the intended optimization needs.
2070511479;Compact Packings of the Plane with Two Sizes of Discs;2006.0;[];We consider packings of the plane using discs of radius 1 and r. A packing is compact if every disc D is tangent to a sequence of discs D1, D2, ..., Dn such that Di is tangent to Di+1. We prove that there are only nine values of r with r u003c 1 for which such packings are possible. For each of the nine values we describe the possible compact packings.
2071384456;The Declining Infrastructure of the Aging Brain;2011.0;[]; Great effort has been dedicated to mapping the functional architecture of the brain in health and disease. The neural centers that support cognition and behavior are the “hubs” defining the salient geographic landmarks of the cerebral topography. Similar to urban cartography, however, the functionality of these hubs is critically dependent on the infrastructure permitting the transfer of relevant information from site to site, and this infrastructure is susceptible to deterioration. The groundwork of the brain lies in the form of the complexly organized myelinated nerve fibers responsible for the inter-regional transmission of electrical impulses among distinct neural areas. Damage to the myelin sheath and reduction in the total number of nerve fibers with aging are thought to result in a degradation in the efficiency of communication among neural regions and to contribute to the decline of function in older adults. This article describes selected studies that are relevant to understanding the de...
2071529495;WBCD breast cancer database classification applying artificial metaplasticity neural network;2011.0;[];The correct diagnosis of breast cancer is one of the major problems in the medical field. From the literature it has been found that different pattern recognition techniques can help them to improve in this domain. These techniques can help doctors form a second opinion and make a better diagnosis. In this paper we present a novel improvement in neural network training for pattern classification. The proposed training algorithm is inspired by the biological metaplasticity property of neurons and Shannonu0027s information theory. During the training phase the Artificial metaplasticity Multilayer Perceptron (AMMLP) algorithm gives priority to updating the weights for the less frequent activations over the more frequent ones. In this way metaplasticity is modeled artificially. AMMLP achieves a more effcient training, while maintaining MLP performance. To test the proposed algorithm we used the Wisconsin Breast Cancer Database (WBCD). AMMLP performance is tested using classification accuracy, sensitivity and specificity analysis, and confusion matrix. The obtained AMMLP classification accuracy of 99.26%, a very promising result compared to the Backpropagation Algorithm (BPA) and recent classification techniques applied to the same database.
2072704264;Evolution in dynamic software product lines: challenges and perspectives;2015.0;[];In many domains systems need to run continuously and cannot be shut down for reconfiguration or maintenance tasks. Cyber-physical or cloud-based systems, for instance, thus often provide means to support their adaptation at runtime. The required flexibility and adaptability of systems suggests the application of Software Product Line (spl) principles to manage their variability and to support their reconfiguration. Specifically, Dynamic Software Product Lines (dspl) have been proposed to support the management and binding of variability at runtime. While spl evolution has been widely studied, it has so far not been investigated in detail in a dspl context. Variability models that are used in a dspl have to co-evolve and be kept consistent with the systems they represent to support reconfiguration even after changes to the systems at runtime. In this short paper we present a classification of the required operations for jointly evolving problem and solution space in a dspl. We analyze the impact of such operations on the consistency of a dspl and propose an approach to deal with the described issues. We describe a runtime monitoring system used in the domain of industrial automation software as an example of a dspl evolving at runtime to motivate and explain our work.
2072890208;Ordinal regression revisited: Multiple criteria ranking using a set of additive value functions;2008.0;[];We present a new method, called UTAGMS, for multiple criteria ranking of alternatives from set A using a set of additive value functions which result from an ordinal regression. The preference information provided by the decision maker is a set of pairwise comparisons on a subset of alternatives ARÂ [subset, double equals]Â A, called reference alternatives. The preference model built via ordinal regression is the set of all additive value functions compatible with the preference information. Using this model, one can define two relations in the set A: the necessary weak preference relation which holds for any two alternatives a, b from set A if and only if for all compatible value functions a is preferred to b, and the possible weak preference relation which holds for this pair if and only if for at least one compatible value function a is preferred to b. These relations establish a necessary and a possible ranking of alternatives from A, being, respectively, a partial preorder and a strongly complete relation. The UTAGMS method is intended to be used interactively, with an increasing subset AR and a progressive statement of pairwise comparisons. When no preference information is provided, the necessary weak preference relation is a weak dominance relation, and the possible weak preference relation is a complete relation. Every new pairwise comparison of reference alternatives, for which the dominance relation does not hold, is enriching the necessary relation and it is impoverishing the possible relation, so that they converge with the growth of the preference information. Distinguishing necessary and possible consequences of preference information on the complete set of actions, UTAGMS answers questions of robustness analysis. Moreover, the method can support the decision maker when his/her preference statements cannot be represented in terms of an additive value function. The method is illustrated by an example solved using the UTAGMS software. Some extensions of the method are also presented.
2072955302;GSA: A Gravitational Search Algorithm;2009.0;[];In recent years, various heuristic optimization methods have been developed. Many of these methods are inspired by swarm behaviors in nature. In this paper, a new optimization algorithm based on the law of gravity and mass interactions is introduced. In the proposed algorithm, the searcher agents are a collection of masses which interact with each other based on the Newtonian gravity and the laws of motion. The proposed method has been compared with some well-known heuristic search methods. The obtained results confirm the high performance of the proposed method in solving various nonlinear functions.
2073083495;Hybrid index structures for location-based web search;2005.0;[];"There is more and more commercial and research interest in location-based web search, i.e. finding web content whose topic is related to a particular place or region. In this type of search, location information should be indexed as well as text information. However, the index of conventional text search engine is set-oriented, while location information is two-dimensional and in Euclidean space. This brings new research problems on how to efficiently represent the location attributes of web pages and how to combine two types of indexes. In this paper, we propose to use a hybrid index structure, which integrates inverted files and R*-trees, to handle both textual and location aware queries. Three different combining schemes are studied: (1) inverted file and R*-tree double index, (2) first inverted file then R*-tree, (3) first R*-tree then inverted file. To validate the performance of proposed index structures, we design and implement a complete location-based web search engine which mainly consists of four parts: (1) an extractor which detects geographical scopes of web pages and represents geographical scopes as multiple MBRs based on geographical coordinates; (2) an indexer which builds hybrid index structures to integrate text and location information; (3) a ranker which ranks results by geographical relevance as well as non-geographical relevance; (4) an interface which is friendly for users to input location-based search queries and to obtain geographical and textual relevant results. Experiments on large real-world web dataset show that both the second and the third structures are superior in query time and the second is slightly better than the third. Additionally, indexes based on R*-trees are proven to be more efficient than indexes based on grid structures."
2075364954;Relation Collection for the Function Field Sieve;2013.0;[];"In this paper, we focus on the relation collection step of the Function Field Sieve (FFS), which is to date the best algorithm known for computing discrete logarithms in small-characteristic finite fields of cryptographic sizes. Denoting such a finite field by Fpn, where p is much smaller than n, the main idea behind this step is to find polynomials of the form a(t)-b(t)x in Fp[t][x] which, when considered as principal ideals in carefully selected function fields, can be factored into products of low-degree prime ideals. Such polynomials are called ""relations"", and current record-sized discrete-logarithm computations need billions of those. Collecting relations is therefore a crucial and extremely expensive step in FFS, and a practical implementation thereof requires heavy use of cache-aware sieving algorithms, along with efficient polynomial arithmetic over Fp[t]. This paper presents the algorithmic and arithmetic techniques which were put together as part of a new public implementation of FFS, aimed at medium-to record-sized computations."
2075633077;What's in a name?: an unsupervised approach to link users across communities;2013.0;[];"In this paper, we consider the problem of linking users across multiple online communities. Specifically, we focus on the alias-disambiguation step of this user linking task, which is meant to differentiate users with the same usernames. We start quantitatively analyzing the importance of the alias-disambiguation step by conducting a survey on 153 volunteers and an experimental analysis on a large dataset of About.me (75,472 users). The analysis shows that the alias-disambiguation solution can address a major part of the user linking problem in terms of the coverage of true pairwise decisions (46.8%). To the best of our knowledge, this is the first study on human behaviors with regards to the usages of online usernames. We then cast the alias-disambiguation step as a pairwise classification problem and propose a novel unsupervised approach. The key idea of our approach is to automatically label training instances based on two observations: (a) rare usernames are likely owned by a single natural person, e.g. pennystar88 as a positive instance; (b) common usernames are likely owned by different natural persons, e.g. tank as a negative instance. We propose using the n-gram probabilities of usernames to estimate the rareness or commonness of usernames. Moreover, these two observations are verified by using the dataset of Yahoo! Answers. The empirical evaluations on 53 forums verify: (a) the effectiveness of the classifiers with the automatically generated training data and (b) that the rareness and commonness of usernames can help user linking. We also analyze the cases where the classifiers fail."
2075809784;Induced intuitionistic fuzzy ordered weighted averaging: Weighted average operator and its application to business decision-making;2014.0;[];"We present the induced intuitionistic fuzzy ordered    :[7],""averaging-weighted average (I-IFOWAWA) operator. It is a new    :[18],""operator that uses the intuitionistic fuzzy weighted average (IFWA) and    induced intuitionistic fuzzy ordered weighted :[31],""averaging (I-IFOWA) :[18],""operator    the :[42],""same formulation. We study some of its main properties and we have    that :[58],""it has a lot of particular cases such as the IFWA and the    fuzzy ordered weighted :[31],""averaging :[75],""(IFOWA) operator. We also study    :[87],""applicability in a decision-making problem concerning strategic selection    :[98],""investments. We see that depending on the particular type of    :[18],""operator :[111],""used, the results may lead to different decisions."
2076329543;Direction finding and mutual coupling estimation for uniform rectangular arrays;2015.0;[];A novel two-dimensional (2-D) direct-of-arrival (DOA) and mutual coupling coefficients estimation algorithm for uniform rectangular arrays (URAs) is proposed. A general mutual coupling model is first built based on banded symmetric Toeplitz matrices, and then it is proved that the steering vector of a URA in the presence of mutual coupling has a similar form to that of a uniform linear array (ULA). The 2-D DOA estimation problem can be solved using the rank-reduction method. With the obtained DOA information, we can further estimate the mutual coupling coefficients. A better performance is achieved by our proposed algorithm than those auxiliary sensor-based ones, as verified by simulation results. HighlightsWe create a general mutual coupling model for uniform rectangular array (URA).The steering vector of a URA with mutual coupling is similar to that of a ULA.An array calibration algorithm without auxiliary sensors for URAs is proposed.Our algorithm has a better performance than auxiliary sensor based algorithms.
2076365336;A Reconstruction Error Based Framework for Multi-Label and Multi-View Learning;2015.0;[];A significant challenge to make learning techniques more suitable for general purpose use is to move beyond i) complete supervision, ii) low dimensional data, iii) a single label and single view per instance. Solving these challenges allows working with complex learning problems that are typically high dimensional with multiple (but possibly incomplete) labelings and views. While other work has addressed each of these problems separately, in this paper we show how to address them together, namely  semi-supervised dimension reduction for multi-label and multi-view learning  (SSDR-MML), which performs optimization for dimension reduction and label inference in semi-supervised setting. The proposed framework is designed to handle both multi-label and multi-view learningsettings, and can be easily extended to many useful applications. Our formulation has a number of advantages. We explicitly model the information combining mechanism as a data structure (a weight/nearest-neighbor matrix) which allows investigating fundamentalquestions in multi-label and multi-view learning. We address one such question by presenting a general measure to quantify thesuccess of simultaneous learning of multiple labels or views. We empirically demonstrate the usefulness of our SSDR-MML approach, and show that it can outperform many state-of-the-art baseline methods.
2076545077;Khan academy gamifies computer science;2014.0;[];Gamification is the buzzword for adding gaming elements such as points or badges to learning experiences to make them more engaging and to increase motivation. In this paper we explore how Khan Academy has incorporated gaming elements into its CS learning platform. By mapping the literature on motivational processes to popular games we critically analyze how successful Khan Academy is at gamifying their site.
2076627362;Symmetric positive solutions to singular system with multi-point coupled boundary conditions;2013.0;[];In this paper, we study the existence and multiplicity of symmetric positive solutions for a nonlinear system with multi-point coupled boundary conditions. The arguments are based upon a specially constructed cone and the fixed point index theorem in cones. An example is then given to demonstrate the applicability of our results.
2076793873;Exploiting fast carry-chains of FPGAs for designing compressor trees;2009.0;[];"Fast carry chains featuring dedicated adder circuitry is a distinctive feature of modern FPGAs. The carry chains bypass the general routing network and are embedded in the logic blocks of FPGAs for fast addition. Conventional intuition is that such carry chains can be used only for implementing carry-propagate addition; state-of-the-art FPGA synthesizers can only exploit the carry chains for these specific circuits. This paper demonstrates that the carry chains can be used to build compressor trees, i.e., multi-input addition circuits used for parallel accumulation and partial product reduction for parallel multipliers implemented in FPGA logic. The key to our technique is to program the lookup tables (LUTs) in the logic blocks to stop the propagation of carry bits along the carry chain at appropriate points. This approach improves the area of compressor trees significantly compared to previous methods that synthesized compressor trees solely on LUTs, without compromising the performance gain over trees built from ternary carry-propagate adders."
2077240096;Symmetric diffeomorphic registration of fibre orientation distributions;2011.0;[];"Registration of diffusion-weighted images is an important step in comparing white matter fibre bundles across subjects, or in the same subject at different time points. Using diffusion-weighted imaging, Spherical Deconvolution enables multiple fibre populations within a voxel to be resolved by computing the fibre orientation distribution (FOD). In this paper, we present a novel method that employs FODs for the registration of diffusion-weighted images. Registration was performed by optimising a symmetric diffeomorphic non-linear transformation model, using image metrics based on the mean squared difference, and cross-correlation of the FOD spherical harmonic coefficients. The proposed method was validated by recovering known displacement fields using FODs represented with maximum harmonic degrees (l max) of 2, 4 and 6. Results demonstrate a benefit in using FODs at l max=4 compared to l max=2. However, a decrease in registration accuracy was observed when l max=6 was used; this was likely caused by noise in higher harmonic degrees. We compared our proposed method to fractional anisotropy driven registration using an identical code base and parameters. FOD registration was observed to perform significantly better than FA in all experiments. The cross-correlation metric performed significantly better than the mean squared difference. Finally, we demonstrated the utility of this method by computing an unbiased group average FOD template that was used for probabilistic fibre tractography. This work suggests that using crossing fibre information aids in the alignment of white matter and could therefore benefit several methods for investigating population differences in white matter, including voxel based analysis, tensor based morphometry, atlas based segmentation and labelling, and group average fibre tractography"
2077313028;On the cycle-transitivity of the mutual rank probability relation of a poset;2010.0;[];The mutual rank probability relation associated with a finite poset is a reciprocal relation expressing the probability that a given element succeeds another one in a random linear extension of that poset. We contribute to the characterization of the transitivity of this mutual rank probability relation, also known as proportional probabilistic transitivity, by situating it between strong stochastic transitivity and moderate product transitivity. The methodology used draws upon the cycle-transitivity framework, which is tailor-made for describing the transitivity of reciprocal relations.
2078014196;DEMORS: A hybrid multi-objective optimization algorithm using differential evolution and rough set theory for constrained problems;2010.0;[];"The aim of this paper is to show how the hybridization of a multi-objective evolutionary algorithm (MOEA) and a local search method based on the use of rough set theory is a viable alternative to obtain a robust algorithm able to solve difficult constrained multi-objective optimization problems at a moderate computational cost. This paper extends a previously published MOEA [Hernandez-Diaz AG, Santana-Quintero LV, Coello Coello C, Caballero R, Molina J. A new proposal for multi-objective optimization using differential evolution and rough set theory. In: 2006 genetic and evolutionary computation conference (GECCOu00272006). Seattle, Washington, USA: ACM Press; July 2006], which was limited to unconstrained multi-objective optimization problems. Here, the main idea is to use this sort of hybrid approach to approximate the Pareto front of a constrained multi-objective optimization problem while performing a relatively low number of fitness function evaluations. Since in real-world problems the cost of evaluating the objective functions is the most significant, our underlying assumption is that, by aiming to minimize the number of such evaluations, our MOEA can be considered efficient. As in its previous version, our hybrid approach operates in two stages: in the first one, a multi-objective version of differential evolution is used to generate an initial approximation of the Pareto front. Then, in the second stage, rough set theory is used to improve the spread and quality of this initial approximation. To assess the performance of our proposed approach, we adopt, on the one hand, a set of standard bi-objective constrained test problems and, on the other hand, a large real-world problem with eight objective functions and 160 decision variables. The first set of problems are solved performing 10,000 fitness function evaluations, which is a competitive value compared to the number of evaluations previously reported in the specialized literature for such problems. The real-world problem is solved performing 250,000 fitness function evaluations, mainly because of its high dimensionality. Our results are compared with respect to those generated by NSGA-II, which is a MOEA representative of the state-of-the-art in the area."
2078730256;Resource modalities in tensor logic;2010.0;[];   The description of resources in game semantics has never achieved the simplicity and precision of linear logic, because of the misleading conception that linear logic is more primitive than game semantics. Here, we defend the opposite view, and thus advocate that game semantics is conceptually more primitive than linear logic. This revised point of view leads us to introduce tensor logic, a primitive variant of linear logic where negation is not involutive. After formulating its categorical semantics, we interpret tensor logic in a model based on Conway games equipped with a notion of payoff, in order to reflect the various resource policies of the logic: linear, affine, relevant or exponential.
2079276401;Data muling with mobile phones for sensornets;2011.0;[];Sensors are all around us, in buildings, vehicles and public places, from commodity thermostats to custom sensornets. Yet today these sensors are often disconnected from the world, either because they are distant from infrastructure, and wide-area networking (by 3G cellular, satellite, or other approaches) is too expensive to justify. Data muling makes communication cost-effective by leveraging short-range wireless and mobility, perhaps by zebras, buses or farmworkers. In this paper we propose that human-carried mobile phones can serve as data mules for sensornet deployments, exploiting ubiquity of mobile phones and human mobility to bring low-cost communication to sensors. We use two mobile phone datasets to show that Bluetooth can serve as a viable muling network, and humans already see many potential sensors regularly. We have implemented a mobile-phone-based data muling system, and used it in four sensornet deployments totaling ten months operation. We find that muling can be the only cost-effective option for rural deployments, where it is critical to monitoring remote sensor networks. We also show opportunistic mobility can collect data without any extra effort in residential and office environments. Finally, we systematically evaluate our deployments to understand how contact duration and data size interact, and to evaluate the effect of muling on phone batteries.
2079366547;Liberating the weights;2004.0;[];"A partial answer to why quasi-Monte Carlo (QMC) algorithms work well for multivariate integration was given in Sloan and Woźniakowski (J. Complexity 14 (1998) 1-33) by introducing weighted spaces. In these spaces the importance of successive coordinate directions is quantified by a sequence of weights. However, to be able to make use of weighted spaces for a particular application one has to make a choice of the  :[67,215],""this work, we take a more general view of the weights by allowing them to depend arbitrarily not only on the coordinates but also on the number of variables. Liberating the weights in :[67,215],""this way allows us to give a recommendation for how to choose the weights in practice. This recommendation results from choosing the weights so as to minimize the error bound. We also consider how best to choose the underlying weighted Sobolev space within which to carry out the  :[149],""revisit also lower bounds on the worst-case error, which change in many minor ways now, since the weights are allowed to depend on the number of variables, and we do not assume that the weights are uniformly bounded as has been assumed in previous papers. Necessary and sufficient conditions for QMC tractability and strong QMC tractability are obtained for the weighted Sobolev spaces with general  the final section, we show that the analysis of variance decomposition of functions from one of the Sobolev spaces is equivalent to the decomposition of functions with respect to an orthogonal decomposition of :[67,215],""this space."
2079457935;Pythagorean fuzzy subsets;2013.0;[];We introduce a new class of non-standard fuzzy subsets called Pythagorean fuzzy subsets and the related idea of Pythagorean membership grades. We focus on the negation operation and its relationship to the Pythagorean theorem. We compare Pythagorean fuzzy subsets with intuitionistic fuzzy subsets. We look at the basic set operations for the Pythagorean fuzzy subsets.
2079492342;Support vector machines for default prediction of SMEs based on technology credit;2010.0;[];In Korea, many forms of credit guarantees have been issued to fund small and medium enterprises (SMEs) with a high degree of growth potential in technology. However, a high default rate among funded SMEs has been reported. In order to effectively manage such governmental funds, it is important to develop an accurate scoring model for selecting promising SMEs. This paper provides a support vector machines (SVM) model to predict the default of funded SMEs, considering various input variables such as financial ratios, economic indicators, and technology evaluation factors. The results show that the accuracy performance of the SVM model is better than that of back-propagation neural networks (BPNs) and logistic regression. It is expected that the proposed model can be applied to a wide range of technology evaluation and loan or investment decisions for technology-based SMEs.
2079607477;Using folksonomies for building user interest profile;2011.0;[];This work exploits folksonomy for building User Interest Profile (UIP) based on useru0027s search history. UIP is an indispensable source of knowledge which can be exploited by intelligent systems for query recommendation, personalized search, and web search result ranking etc. A UIP consist of a clustered list of concepts and their weights. We show how to design, implement, and visualize such a system, in practice, which aids in finding interesting relationships between concepts and detect outliers, if any. The experiment reveals that UIP not only captures user interests but also its context and results are very promising.
2080133951;Wikidata: a free collaborative knowledgebase;2014.0;[];This collaboratively edited knowledgebase provides a common source of data for Wikipedia, and everyone else.
2080550005;Success factors for deploying cloud computing;2012.0;[];Trust between client organization and cloud provider is a strong predictor of successful cloud deployment.
2080753128;A case study of applying data mining to sensor data for contextual requirements analysis;2014.0;[];Determining the context situations specific to contextual requirements is challenging, particularly for environments that are largely unobservable by system designers (e.g., dangerous system contexts of use and mobile applications). In this paper, we describe the application of data mining techniques in a case study of identifying contextual requirements for a context-aware mobile application to be used by a team of four long-distance rowers. The context of use for this application was dangerous and isolated, making it unobservable by the developers. The context situations for five mobile application requirements were defined by using a data mining algorithm applied to historical sensor data passively collected by the users while they crossed the Atlantic Ocean in a rowboat. The performance of the resulting classifiers is analyzed over time with promising results demonstrating that the data mining approach is feasible with implications for requirements engineering, context-aware mobile applications, and group-context-aware mobile applications.
2081344723;The eigenvalue for a class of singular p-laplacian fractional differential equations involving the Riemann-Stieltjes integral boundary condition;2014.0;[];In this paper, we are concerned with the eigenvalue problem of a class of singular p-Laplacian fractional differential equations involving the Riemann–Stieltjes integral boundary condition. The conditions for the existence of at least one positive solution is established together with the estimates of the lower and upper bounds of the solution at any instant of time. Our results are derived based on the method of upper and lower solutions and the Schauder fixed point theorem.
2082148511;Characterizing the spatial structure of Mangrove features for optimizing image-based mangrove mapping;2014.0;[];Understanding the relationship between the size of mangrove structural features and the optimum image pixel size is essential to support effective mapping activities in mangrove environments. This study developed a method to estimate the optimum image pixel size for accurately mapping mangrove features (canopy types and features (gaps, tree crown), community, and cover types) and tested the applicability of the results. Semi-variograms were used to characterize the spatial structure of mangrove vegetation by estimating the size of dominant image features in WorldView-2 imagery resampled over a range of pixel sizes at several mangrove areas in Moreton Bay, Australia. The results show that semi-variograms detected the variations in the structural properties of mangroves in the study area and its forms were controlled by the image pixel size, the spectral-band used, and the spatial characteristics of the scene object, e.g., tree or gap. This information was synthesized to derive the optimum image pixel size for mapping mangrove structural and compositional features at specific spatial scales. Interpretation of semi-variograms combined with field data and visual image interpretation confirms that certain vegetation structural features are detectable at specific scales and can be optimally detected using a specific image pixel size. The analysis results provide a basis for multi-scale mangrove mapping using high spatial resolution image datasets.
2082839354;A bi-objective model to optimize reliability and cost of system with a choice of redundancy strategies;2012.0;[];Reliability problems are an important type of optimization problems that are motivated by different needs of real-world applications such as telecommunication systems, transformation systems, and electrical systems, so on. This paper studies a special type of these problems which is called redundancy allocation problem (RAP) and develops a bi-objective RAP (BORAP). The model includes non-repairable series-parallel systems in which the redundancy strategy is considered as a decision variable for individual subsystems. The objective functions of the model are (1) maximizing system reliability and (2) minimizing the system cost. Meanwhile, subject to system-level constraint, the best redundancy strategy among active or cold-standby, component type, and the redundancy level for each subsystem should be determined. To have a more practical model, we have also considered non-constant component hazard functions and imperfect switching of cold-standby redundant component. To solve the model, since RAP belong to the NP-hard class of the optimization problems, two effective multi-objective metaheuristic algorithms named non-dominated sorting genetic algorithms (NSGA-II) and multi-objective particle swarm optimization (MOPSO) are proposed. Finally, the performance of the algorithms is analyzed on a typical case and conclusions are demonstrated.
2083044905;Integrating Millimeter Wave Radar with a Monocular Vision Sensor for On-Road Obstacle Detection Applications;2011.0;[];This paper presents a systematic scheme for fusing millimeter wave (MMW) radar and a monocular vision sensor for on-road obstacle detection. As a whole, a three-level fusion strategy based on visual attention mechanism and driver’s visual consciousness is provided for MMW radar and monocular vision fusion so as to obtain better comprehensive performance. Then an experimental method for radar-vision point alignment for easy operation with no reflection intensity of radar and special tool requirements is put forward. Furthermore, a region searching approach for potential target detection is derived in order to decrease the image processing time. An adaptive thresholding algorithm based on a new understanding of shadows in the image is adopted for obstacle detection, and edge detection is used to assist in determining the boundary of obstacles. The proposed fusion approach is verified through real experimental examples of on-road vehicle/pedestrian detection. In the end, the experimental results show that the proposed method is simple and feasible.
2083147990;Some Syntactical Observations on Linear Logic;1991.0;[];
2083253722;Constructing Sobol Sequences with Better Two-Dimensional Projections;2008.0;[];Direction numbers for generating Sobol$u0027$ sequences that satisfy the so-called Property A in up to 1111 dimensions have previously been given in Joe and Kuo [ACM Trans. Math. Software, 29 (2003), pp. 49-57]. However, these Sobol$u0027$ sequences may have poor two-dimensional projections. Here we provide a new set of direction numbers alleviating this problem. These are obtained by treating Sobol$u0027$ sequences in $d$ dimensions as $(t,d)$-sequences and then optimizing the $t$-values of the two-dimensional projections. Our target dimension is 21201.
2083780116;Machine learning for medical diagnosis: history, state of the art and perspective;2001.0;[];The paper provides an overview of the development of intelligent data analysis in medicine from a machine learning perspective: a historical view, a state-of-the-art view, and a view on some future trends in this subfield of applied artificial intelligence. The paper is not intended to provide a comprehensive overview but rather describes some subareas and directions which from my personal point of view seem to be important for applying machine learning in medical diagnosis. In the historical overview, I emphasize the naive Bayesian classifier, neural networks and decision trees. I present a comparison of some state-of-the-art systems, representatives from each branch of machine learning, when applied to several medical diagnostic tasks. The future trends are illustrated by two case studies. The first describes a recently developed method for dealing with reliability of decisions of classifiers, which seems to be promising for intelligent data analysis in medicine. The second describes an approach to using machine learning in order to verify some unexplained phenomena from complementary medicine, which is not (yet) approved by the orthodox medical community but could in the future play an important role in overall medical diagnosis and treatment.
2083830795;Throughput analysis for order picking system with multiple pickers and aisle congestion considerations;2012.0;[];Most of previous studies in picker-to-parts warehousing systems investigated only single-picker operations and are therefore adequate to evaluate order picking efficiency by travel distance as aisle congestion never takes place in such systems. In real world applications, the congestion inevitably occurs when a system has multiple pickers working together within the same region. This paper presents an approximation method based on a GI/G/1 closed queueing network by using self-correcting approximation technique algorithm to evaluate the throughput time of an order picking system with multiple pickers and aisle congestion considerations for different routing policies. The results generated by the proposed method are compared and validated via simulation model using eM-plant simulator for different sizes of warehouses. The results indicate that the approximation method appears to be sufficiently accurate for practical purposes. The sensitivity analysis of the throughput time with respect to order sizes, number of pickers and number of aisles are conducted and the performance of different item storage policies are also evaluated using the proposed approximation model.
2084160675;Robust ordinal regression in preference learning and ranking;2013.0;[];Multiple Criteria Decision Aiding (MCDA) offers a diversity of approaches designed for providing the decision maker (DM) with a recommendation concerning a set of alternatives (items, actions) evaluated from multiple points of view, called criteria. This paper aims at drawing attention of the Machine Learning (ML) community upon recent advances in a representative MCDA methodology, called Robust Ordinal Regression (ROR). ROR learns by examples in order to rank a set of alternatives, thus considering a similar problem as Preference Learning (ML-PL) does. However, ROR implements the interactive preference construction paradigm, which should be perceived as a mutual learning of the model and the DM. The paper clarifies the specific interpretation of the concept of preference learning adopted in ROR and MCDA, comparing it to the usual concept of preference learning considered within ML. This comparison concerns a structure of the considered problem, types of admitted preference information, a character of the employed preference models, ways of exploiting them, and techniques to arrive at a final ranking.
2085144121;Self-adaptation of mobile systems driven by the Common Variability Language;2015.0;[];The execution context in which pervasive systems or mobile computing run changes continually. Hence, applications for these systems require support for self-adaptation to the continual context changes. Most of the approaches for self-adaptive systems implement a reconfiguration service that receives as input the list of all possible configurations and the plans to switch between them. In this paper we present an alternative approach for the automatic generation of application configurations and the reconfiguration plans at runtime. With our approach, the generated configurations are optimal as regards different criteria, such as functionality or resource consumption (e.g. battery or memory). This is achieved by: (1) modelling architectural variability at design-time using the Common Variability Language (CVL), and (2) using a genetic algorithm that finds nearly-optimal configurations at run-time using the information provided by the variability model. We also specify a case study and we use it to evaluate our approach, showing that it is efficient and suitable for devices with scarce resources. We specify an approach for the dynamic reconfiguration of mobile applications.We model a mobile application with variability which can be reconfigured at runtime.We simulate the execution of the mobile application when our dynamic reconfiguration service is applied and not applied, respectively.We measure the battery life as well as the overall utility of the application perceived by the user.Applying our dynamic reconfiguration, the battery life is incremented by 45.9% and the utility is incremented by 10.31%.
2085422494;The Financing of Innovative SMEs: a multicriteria credit rating model;2015.0;[];Small and Medium-sized Enterprises (SMEs) face many obstacles when they try to access the credit market. These obstacles increase if the SMEs are innovative. In this case, financial data are insufficient or even unreliable. Thus, building a judgmental rating model, mainly based on qualitative criteria (soft information), is very important to finance SMEs’ activities. Till now, there has not been a multicriteria credit risk model based on soft information for innovative SMEs. In this paper, we try to fill this gap by presenting a multicriteria credit risk model named ELECTRE-TRI. A SMAA-TRI analysis is also implemented to obtain robust SMEs’ assignments to the risk classes. SMAA-TRI incorporates ELECTRE-TRI by considering different sets of preference parameters and uncertainty in the data via Monte Carlo simulations. Finally, we carry out a real case study with the aim of illustrating the multicriteria credit risk model.
2085925999;LAICOS: an open source platform for personalized social web search;2013.0;[];In this paper, we introduce LAICOS, a social Web search engine as a contribution to the growing area of Social Information Retrieval (SIR). Social information and personalization are at the heart of LAICOS. On the one hand, the social context of documents is added as a layer to their textual content traditionally used for indexing to provide Personalized Social Document Representations. On the other hand, the social context of users is used for the query expansion process using the Personalized Social Query Expansion framework (PSQE) proposed in our earlier works. We describe the different components of the system while relying on social bookmarking systems as a source of social information for personalizing and enhancing the IR process. We show how the internal structure of indexes as well as the query expansion process operated using social information.
2086726476;Combining QoS prediction and customer satisfaction estimation to solve cloud service trustworthiness evaluation problems;2014.0;[];The collection and combination of assessment data in trustworthiness evaluation of cloud service is challenging, notably because QoS value may be missing in offline evaluation situation due to the time-consuming and costly cloud service invocation. Considering the fact that many trustworthiness evaluation problems require not only objective measurement but also subjective perception, this paper designs a novel framework named CSTrust for conducting cloud service trustworthiness evaluation by combining QoS prediction and customer satisfaction estimation. The proposed framework considers how to improve the accuracy of QoS value prediction on quantitative trustworthy attributes, as well as how to estimate the customer satisfaction of target cloud service by taking advantages of the perception ratings on qualitative attributes. The proposed methods are validated through simulations, demonstrating that CSTrust can effectively predict assessment data and release evaluation results of trustworthiness.
2086869454;Two effective measures of intuitionistic fuzzy entropy;2010.0;[];Based on the concept of fuzzy entropy, two effective measures of intuitionistic fuzzy entropy are proposed in intuitionistic fuzzy information, and then the essential properties of these measures are introduced. These measures are a generalized version of the fuzzy entropy and a complementarity of existing entropy for intuitionistic fuzzy sets. Based on this generalization, a connection between the concepts of the fuzzy entropy and the intuitionistic fuzzy entropy is established. Finally, a numeral example is given to show that the information measures of the proposed intuitionistic fuzzy entropy are reasonable and effective by the comparison of the proposed entropy and existing entropy.
2086871667;Fuzzy fault tree analysis: a review of concept and application;2013.0;[];Fault tree analysis (FTA) is a widely used method for analyzing a system’s failure logic and calculating overall reliability. However, application of conventional FTA has some shortcomings, e.g. in handling the uncertainties, allowing the use of linguistic variables, and integrating human error in failure logic model. Hence, Fuzzy set theory has been proposed to overcome the limitation of conventional FTA. Fuzzy logic provides a framework whereby basic notions such as similarity, uncertainty and preference can be modeled effectively. The aim of this paper is to present a review of the concept of fuzzy theory with fault tree analysis and their applications since 1981, to reflect the current status of Fuzzy fault tree analysis (FFTA) methodologies, their strengths, weaknesses, and their applications. This paper explains the fundamentals of fuzzy theory and describes application of fuzzy importance for using FFTA. The concept of the failure possibility and uncertainty analysis by using FFTA is discussed, and concludes with discussion on the application of FFTA in different fields. The review reveals the effectiveness of the FFTA in comparison with conventional FTA, when there is inadequate amount of accurate reliability oriented information.
2087757910;Direction finding in partly calibrated uniform linear arrays with unknown gains and phases;2015.0;[];Recently, we considered the problem of direction finding with partly calibrated uniform linear arrays (ULAs) with unknown gains and phases and proposed an ESPRIT-like method for direction-of-arrival (DOA) estimation. It was shown that the DOAs, together with unknown sensor gains and phases in the uncalibrated portion of the array, can be estimated in closed form. However, the identifiability of DOA estimation has not yet been addressed. Moreover, though the proposed method performs better than existing ones, it uses the overlapping subarrays only. Thus it is possible to further improve the performance if the whole array aperture is employed. To fill this gap, two main issues are addressed in this paper. First, the ESPRIT-like algorithm is reinvestigated and conditions ensuring the uniqueness of DOA estimates and identifiability are derived. Second, by exploiting the subspace principle, a refining scheme is proposed that is able to improve the performance of the ESPRIT-like algorithm. Numerical examples are carried out to demonstrate the identifiability issue and performance of the refinement.
2088049833;Selective Search for Object Recognition;2013.0;[];This paper addresses the problem of generating possible object locations for use in object recognition. We introduce selective search which combines the strength of both an exhaustive search and segmentation. Like segmentation, we use the image structure to guide our sampling process. Like exhaustive search, we aim to capture all possible object locations. Instead of a single technique to generate possible object locations, we diversify our search and use a variety of complementary image partitionings to deal with as many image conditions as possible. Our selective search results in a small set of data-driven, class-independent, high quality locations, yielding 99 % recall and a Mean Average Best Overlap of 0.879 at 10,097 locations. The reduced number of locations compared to an exhaustive search enables the use of stronger machine learning techniques and stronger appearance models for object recognition. In this paper we show that our selective search enables the use of the powerful Bag-of-Words model for recognition. The selective search software is made publicly available (Software: http://disi.unitn.it/~uijlings/SelectiveSearch.html ).
2088363515;Approximate Dynamic Programming for Ambulance Redeployment;2010.0;[];We present an approximate dynamic programming approach for making ambulance redeployment decisions in an emergency medical service system. The primary decision is where we should redeploy idle ambulances so as to maximize the number of calls reached within a delay threshold. We begin by formulating this problem as a dynamic program. To deal with the high-dimensional and uncountable state space in the dynamic program, we construct approximations to the value function that are parameterized by a small number of parameters. We tune the parameters using simulated cost trajectories of the system. Computational experiments demonstrate the performance of the approach on emergency medical service systems in two metropolitan areas. We report practically significant improvements in performance relative to benchmark static policies.
2088440111;Promoting congestion control in opportunistic networks;2010.0;[];This paper is concerned with congestion aware forwarding algorithms within opportunistic networks. We remove the reoccurring assumption of unlimited storage, and make it evident that congestion is a prominent problem that needs to be addressed. We propose a distributed congestion control algorithm that adaptively chooses the next hop based on contact history and statistics, as well as storage statistics. We aim to distribute the load away from the storage hotspots in order to spread the traffic around. We perform an extensive set of trace driven simulations for “several-to-many” communication patterns in opportunistic networks. We show that congestion control is an essential component in the transfer of data in opportunistic networks, and can be achieved in a fully open loop manner and by only local dissemination of statistics of nodes availability and connectivity. Our results with real connectivity traces show that by using novel availability heuristic we achieve higher levels of sent and delivered packets and outperform current opportunistic forwarding protocols, such as SimBetTS and FairRoute.
2088615732;Positive solutions of an abstract fractional semipositone differential system model for bioprocesses of HIV infection;2015.0;[];The  fractional dynamics model is based on a class of bioprocesses of HIV infection.The nonlinear terms and boundary conditions all depend on fractional derivatives of unknown functions.The system is singular and semipositone.The system involves some uncertain parametrical variations λ . Fractional order derivative is nonlocal which exhibits a long time memory behavior. With advantage of these, fractional order dynamic system models are more accurate than integer order ones in understanding the dynamic behavior of bioprocesses such as HIV infection. In this paper, we systematically study the existence of positive solutions of an  fractional semipositone differential system involving integral boundary conditions arising from the study of HIV infection models. By using the fixed point theorem in cone, some new results are established and an example is given to demonstrate the application of our main results.
2088692353;ThinkAir: Dynamic resource allocation and parallel execution in the cloud for mobile code offloading;2012.0;[];Smartphones have exploded in popularity in recent years, becoming ever more sophisticated and capable. As a result, developers worldwide are building increasingly complex applications that require ever increasing amounts of computational power and energy. In this paper we propose ThinkAir, a framework that makes it simple for developers to migrate their smartphone applications to the cloud. ThinkAir exploits the concept of smartphone virtualization in the cloud and provides method-level computation offloading. Advancing on previous work, it focuses on the elasticity and scalability of the cloud and enhances the power of mobile cloud computing by parallelizing method execution using multiple virtual machine (VM) images. We implement ThinkAir and evaluate it with a range of benchmarks starting from simple micro-benchmarks to more complex applications. First, we show that the execution time and energy consumption decrease two orders of magnitude for a N-queens puzzle application and one order of magnitude for a face detection and a virus scan application. We then show that a parallelizable application can invoke multiple VMs to execute in the cloud in a seamless and on-demand manner such as to achieve greater reduction on execution time and energy consumption. We finally use a memory-hungry image combiner tool to demonstrate that applications can dynamically request VMs with more computational power in order to meet their computational requirements.
2088979095;A robust trust model for service-oriented systems;2013.0;[];In service-oriented computing applications, service consumers and providers need to evaluate the trust levels of potential partners before engaging in interactions. The accuracy of trust evaluation greatly affects the success rate of the interaction. Trust evaluation is a challenging problem in open and dynamic environment as there is no central mediator to manage standardized evaluation criteria or reputation records. In this paper, a novel trust model, called the priority-based trust model, is presented. The model derives the trustworthiness of a service provider from designated referees and its historical performance. In addition, consumers can specify their preferred priorities which will affect the result of trust evaluations. The experimental results show that the proposed model has better performance than other trust models, especially in open and dynamic environments. Highlights? A robust trust model for service-oriented systems. ? Trust evaluation in distributed and dynamic environments. ? Derivation of more objective trust information from different aspects.
2089160254;Fuzzy Goals for Requirements-Driven Adaptation;2010.0;[];"Self-adaptation is imposing as a key characteristic of many modern software systems to tackle their complexity and cope with the many environments in which they can operate. Self-adaptation is a requirement per-se, but it also impacts the other (conventional) requirements of the system; all these new and old requirements must be elicited and represented in a coherent and homogenous way. This paper presents FLAGS, an innovative goal model that generalizes the KAOS model, adds adaptive goals to embed adaptation countermeasures, and fosters self-adaptation by considering requirements as live, runtime entities. FLAGS also distinguishes between crisp goals, whose satisfaction is boolean, and fuzzy goals, whose satisfaction is represented through fuzzy constraints. Adaptation countermeasures are triggered by violated goals and the goal model is modified accordingly to maintain a coherent view of the system and enforce adaptation directives on the running system. The main elements of the approach are demonstrated through an example application."
2089677243;Decision support tools for ambulance dispatch and relocation;2007.0;[];In this paper, the development of decision support tools for dynamic ambulance relocation and automatic ambulance dispatching is described. The ambulance dispatch problem is to choose which ambulance to send to a patient. The dynamic ambulance relocation problem occurs in the operational control of ambulances. The objective is to find new locations for some of the ambulances, to increase the preparedness in the area of responsibility. Preparedness is a way of evaluating the ability to serve potential patients with ambulances now and in the future. Computational tests using a simulation model show that the tools are beneficial in reducing the waiting periods for the patients.
2090072113;A framework for ranking of cloud computing services;2013.0;[];Cloud computing is revolutionizing the IT industry by enabling them to offer access to their infrastructure and application services on a subscription basis. As a result, several enterprises including IBM, Microsoft, Google, and Amazon have started to offer different Cloud services to their customers. Due to the vast diversity in the available Cloud services, from the customeru0027s point of view, it has become difficult to decide whose services they should use and what is the basis for their selection. Currently, there is no framework that can allow customers to evaluate Cloud offerings and rank them based on their ability to meet the useru0027s Quality of Service (QoS) requirements. In this work, we propose a framework and a mechanism that measure the quality and prioritize Cloud services. Such a framework can make a significant impact and will create healthy competition among Cloud providers to satisfy their Service Level Agreement (SLA) and improve their QoS. We have shown the applicability of the ranking framework using a case study.
2090226449;Multiple positive solutions of a singular fractional differential equation with negatively perturbed term;2012.0;[];   Let      D    0  +    α      be the standard Riemann–Liouville derivative. We discuss the existence of multiple positive solutions for the following fractional differential equation with a negatively perturbed term       {     −    D    0  +    α    u   (  t  )   =  p   (  t  )   f   (  t  ,  u   (  t  )   )   −  q   (  t  )   ,    0    t    1  ,      u   (  0  )   =    u    ′     (  0  )   =  u   (  1  )   =  0  ,            where    2    α  ≤  3    is a real number, the perturbed term    q  :   (  0  ,  1  )   →   [  0  ,  +  ∞  )     is Lebesgue integrable and may be singular at some zero measures set of [0,1], which implies the nonlinear term may change sign.
2090515334;Helmholtz Equation with Artificial Boundary Conditions in a Two-Dimensional Waveguide;2012.0;[];We consider a time-harmonic acoustic wave propagation problem in a two-dimensional water waveguide confined between a horizontal surface and a locally varying bottom. We formulate a model based on the Helmholtz equation coupled with nonlocal Dirichlet-to-Neumann boundary conditions imposed on two artificial boundaries. We establish the well-posedness of the associated variational problem, under the assumption of a downsloping bottom, by showing stability estimates in appropriate function spaces. The outcome of some numerical experiments with a code implementing a standard/Galerkin finite element approximation of the variational formulation of the model are also presented.
2091015960;Excluded permutation matrices and the Stanley-Wilf conjecture;2004.0;[];This paper examines the extremal problem of how many 1-entries an n × n 0-1 matrix can have that avoids a certain fixed submatrix P. For any permutation matrix P we prove a linear bound, settling a conjecture of Zoltan Furedi and Peter Hajnal (Discrete Math. 103(1992) 233). Due to the work of Martin Klazar (D. Krob, A.A. Mikhalev, A.V. Mikhalev (Eds.), Formal Power Series and Algebraics Combinatorics, Springer, Berlin, 2000, pp. 250-255), this also settles the conjecture of Stanley and Wilf on the number of n -permutations avoiding a fixed permutation and a related conjecture of Alon and Friedgut (J. Combin Theory Ser A 89(2000) 133).
2091118421;Metalearning: a survey of trends and technologies;2015.0;[];Metalearning attracted considerable interest in the machine learning community in the last years. Yet, some disagreement remains on what does or what does not constitute a metalearning problem and in which contexts the term is used in. This survey aims at giving an all-encompassing overview of the research directions pursued under the umbrella of metalearning, reconciling different definitions given in scientific literature, listing the choices involved when designing a metalearning system and identifying some of the future research challenges in this domain.
2091224289;Linear SVM classification using boosting HOG features for vehicle detection in low-altitude airborne videos;2011.0;[];Visual surveillance from low-altitude airborne platforms has been widely addressed in recent years. Moving vehicle detection is an important component of such a system, which is a very challenging task due to illumination variance and scene complexity. Therefore, a boosting Histogram Orientation Gradients (boosting HOG) feature is proposed in this paper. This feature is not sensitive to illumination change and shows better performance in characterizing object shape and appearance. Each of the boosting HOG feature is an output of an adaboost classifier, which is trained using all bins upon a cell in traditional HOG features. All boosting HOG features are combined to establish the final feature vector to train a linear SVM classifier for vehicle classification. Compared with classical approaches, the proposed method achieved better performance in higher detection rate, lower false positive rate and faster detection speed.
2091260227;Fast evaluation of logarithms in fields of characteristic two;1984.0;[];A method for determining logarithms in GF (2^{n}) is presented. Its asymptotic running time is O(\exp (cn^{1/3} \log^{2/3} n)) for a small constant c , while, by comparison, Adlemanu0027s scheme runs in time O(\exp (c^{u0027}n^{1/2} \log^{1/2} n )) . The ideas give a dramatic improvement even for moderate-sized fields such as GF (2^{127}) , and make (barely) possible computations in fields of size around 2^{400} . The method is not applicable to GF (q) for a large prime q .
2091275864;Intuitionistic fuzzy information - Applications to pattern recognition;2007.0;[];This paper addresses the issue of information-theoretic discrimination measures for intuitionistic fuzzy sets (IFSs). Although many measures of distance, similarity, dissimilarity, and correlation between IFSs have been proposed, there is no reference regarding information-driven measures used for comparison between sets. In this work we introduce the concepts of discrimination information and cross-entropy in the intuitionistic fuzzy setting and we derive an extension of the De Luca-Termini nonprobabilistic entropy for IFSs. Based on this entropy, we reveal an intuitive and mathematical connection between the notions of entropy for fuzzy sets (FSs) and IFSs in terms of fuzziness and intuitionism. Finally, we demonstrate the efficiency of the proposed discrimination information measure for pattern recognition, medical diagnosis, and image segmentation.
2091973517;Attribute Decoration of Attack-Defense Trees;2012.0;[];Attack-defense trees can be used as part of threat and risk analysis for system development and maintenance. They are an extension of attack trees with defense measures. Moreover, tree nodes can be decorated with attributes, such as probability, impact, and penalty, to increase the expressiveness of the model. Attribute values are typically assigned based on cognitive estimations and historically recorded events. This paper presents a practical case study with attack-defense trees. First, the authors create an attack-defense tree for an RFID-based goods management system for a warehouse. Then, they explore how to use a rich set of attributes for attack and defense nodes and assign and aggregate values to obtain condensed information, such as performance indicators or other key security figures. The authors discuss different modeling choices and tradeoffs. The case study led them to define concrete guidelines that can be used by software developers, security analysts, and system owners when performing similar assessments.
2092361432;Melting the Boundaries Between Fantasy and Reality;2009.0;[];Technology goes beyond visual effects to produce alternative realities for immersive entertainment experiences. The art and science of immersive entertainment illusions engage all the senses in every direction and dimension. The theme park is an extreme example of immersive entertainment, wherein experiential entertainment designers and scenario authors project the audience into an experienced reality, whether actual or imagined. It is unparalleled by other forms of entertainment because it can sustain this illusion for thousands of people for a few minutes, a full day, or even a week. With that amount of scrutiny, theme parks present a challenge in integrating computer-generated content for experiential entertainment. Seamlessly blending the physical reality and the authoru0027s virtual fantasy is critical in creating a successful audience experience. This illusion is only effective when the author can engage and direct the audienceu0027s imaginary reality.
2092542745;Variational structure and multiple solutions for a fractional advection-dispersion equation;2014.0;[];By establishing a variational structure and using the critical point theory, we investigate the existence of multiple solutions for a class of fractional advection-dispersion equations arising from a symmetric transition of the mass flux. Several criteria for the existence of multiple nonzero solutions are established under certain assumptions.
2092718852;From the wisdom of crowds to my own judgment in microfinance through online peer-to-peer lending platforms;2012.0;[];Information asymmetry is one of the fundamental problems that online peer-to-peer (P2P) lending platforms face. This problem becomes more acute when platforms are used for microfinance, where the targeted customers are mostly economically under-privileged people. Most of the prior empirical studies have been based on data from Prosper.com or similar sites that compete in traditional consumer loan markets. Our study examines P2P lending in microfinance for which borrowers are unbankable so that signals on creditworthiness of new borrowers are very limited. In addition, microfinance customers have more incentive to repeatedly seek loans from the market. Under this microfinance setting, we examine how lenders change their decisions as creditworthiness inference becomes increasingly possible through the accumulation of transaction history. Our findings confirm that lenders seek the wisdom of crowds when information on creditworthiness is extremely limited but switch to their own judgment when more signals are transmitted through the market. Different information sets are utilized according to the structures of decisions. Due to the possibility of a repeated game, it is also shown that borrowers try to maintain a good reputation, and direct communication with lenders may adjust incorrect inference from hard data when their creditworthiness is questioned.
2092752818;Separable routing: a scheme for state-dependent routing of circuit switched telephone traffic;1992.0;[];Separable routing is the first of a number of routing schemes for circuit switched telephone traffic invented at Bellcore. These routing schemes are state dependent, in the sense that, for each call attempt, a routing decision is made on the basis of the state of the network (defined in terms of the numbers of busy and idle trunks in the various trunk groups at the moment of the call attempt). In this paper, we describe separable routing and its mathematical background. Simulation results we have presented elsewhere show that the family of state-dependent routing schemes, of which separable routing is a member, is very attractive in terms of blocking rate, built-in network management features, and behavior in the presence of traffic forecast error.
2093228967;A hypercube queuing model for facility location and redistricting in urban emergency services;1974.0;[];   This paper develops computationally efficient algorithms for studying the analytical behavior of a multi-server queuing system with distinguishable servers. The model is intended for analyzing problems of vehicle location and response district design in urban emergency services, includes interdistrict as well as intradistrict responses, and allows computation of several pointspecific as well as area-specific performance measures.
2093446084;Extensions of Results on Rainbow Hamilton Cycles in Uniform Hypergraphs;2015.0;[];Let $${K_n^{(k)}}$$ K n ( k ) be the complete k-uniform hypergraph, $${k\ge3}$$ k ? 3 , and let l be an integer such that 1 ≤ l ≤ k?1 and k?l divides n. An l-overlapping Hamilton cycle in $${K_n^{(k)}}$$ K n ( k ) is a spanning subhypergraph C of $${K_n^{(k)}}$$ K n ( k ) with n/(k?l) edges and such that for some cyclic ordering of the vertices each edge of C consists of k consecutive vertices and every pair of consecutive edges in C intersects in precisely l vertices. An edge-coloring of $${K_n^{(k)}}$$ K n ( k ) is (a, r)-bounded if every subset of a vertices of $${K_n^{(k)}}$$ K n ( k ) is contained in at most r edges of the same color. In this paper, we refine recent results of the first author, Frieze and Rucinski by proving that there is a constant c = c(k, l) such that every $${(\ell, cn^{k-\ell})}$$ ( l , c n k - l ) -bounded edge-colored $${K_n^{(k)}}$$ K n ( k ) in which no color appears more that cn k-1 times contains a rainbow l-overlapping Hamilton cycle. We also show that there is a constant c? = c?(k, l) such that every (l, c?n k-l)-bounded edge-colored $${K_n^{(k)}}$$ K n ( k ) contains a properly colored l-overlapping Hamilton cycle.
2093523923;The Robust Cold Standby Redundancy Allocation in Series-Parallel Systems With Budgeted Uncertainty;2015.0;[];"This paper studies a redundancy allocation problem (RAP) with cold standby strategy in non-repairable series-parallel systems. We assume that the components’ reliabilities are uncertain values in a budgeted uncertainty set, with unknown probability distributions. Because the system reliability is a nonlinear function of the components’ reliabilities, classical robust optimization approaches cannot be directly applied to construct the robust counterpart of this problem. Therefore, this paper for the first time proposes linear mixed integer programming (MIP) and binary equivalent models for the cold standby RAP; and by exploiting the problem structure, robust counterparts are developed to deal with budgeted uncertainty in this problem. Then, two exact solution methods are proposed: one of them solves a MIP model iteratively in a Bendersu0027 decomposition framework, and the other one solves a single binary linear model. The validity and the performance of the proposed approach are tested through a Monte Carlo simulation, and computational results."
2093697352;Automatic highlights extraction for drama video using music emotion and human face features;2013.0;[];This paper describes a novel system that uses music emotion and human face as features for automatic highlights extraction for drama video. These high-level audiovisual features are used because music evokes emotion response from the viewer and characters express emotion on their faces. In addition, a novel scheme is developed to improve the accuracy of music emotion recognition in drama video. Specifically, emotion recognition is performed not on the input audio signal but on the noise-free music available from the album of the incidental music, with the presence of incidental music detected by an audio fingerprint technique. Besides the conventional subjective evaluation, we propose a new metric for quantitative performance evaluation of highlights extraction. Experiments conducted over four different types of drama videos demonstrate that the proposed system significantly outperforms baseline ones in terms of both subjective and objective measures.
2094056275;Texture classification using texture spectrum;1990.0;[];   Pursuing our previous study where the Texture Spectrum method has been proposed for texture analysis, the purpose of this paper is to demonstrate the usefulness of the Texture Spectrum for texture classification. Promising results are obtained when applying the Texture Spectrum to classify four of Brodatzu0027s natural images.
2094563761;Distinct MEG correlates of conscious experience, perceptual reversals and stabilization during binocular rivalry.;2014.0;[];During binocular rivalry, visual perception alternates spontaneously between two different monocular images. Such perceptual reversals are slowed or halted if stimuli are presented intermittently with inter-stimulus intervals larger than ~ 400 ms — a phenomenon called stabilization. Often, the neural correlates of reversal and stabilization are studied separately, and both phenomena in turn are studied separately from the neural correlates of conscious perception. To distinguish the neural correlates of perceptual content, stabilization and reversal, we recorded MEG signals associated with each in the same group of healthy humans observing repeated trials of intermittent presentation of a dichoptic stimulus. Perceptual content correlated mainly with modulation of stimulus-specific activity in occipital/temporal areas 150–270 ms after stimulus onset, possibly reflecting inhibition of the neural populations representing the suppressed image. Stability of perception reflected a gradual build-up of this modulation across at least 10 trials and was also, to some extent, associated with parietal activity 40–90 ms and 220–270 ms after stimulus onset. Perceptual reversals, in contrast, were associated with parietal (150–270 ms) and temporal (150–210 ms) activity on the trial before the reversal and a gradual change in perception-specific activity in occipital (150–270 ms) and temporal (220–420 ms) areas across at least 10 trials leading up to a reversal. Mechanistically, these findings suggest that stability of perception during rivalry is maintained by modulation of activity related to the two monocular images, and gradual adaptation of neuronal populations leads to instability that is eventually resolved by signals from parietal and late sensory cortices.
2094834818;Inactive learning?: difficulties employing active learning in practice;2011.0;[];Despite the tremendous level of adoption of machine learning techniques in real-world settings, and the large volume of research on active learning, active learning techniques have been slow to gain substantial traction in practical applications. This reluctance of adoption is contrary to active learningu0027s promise of reduced model-development costs and increased performance on a model-development budget. This essay presents several important and under-discussed challenges to using active learning well in practice. We hope this paper can serve as a call to arms for researchers in active learning--an encouragement to focus even more attention on how practitioners might actually use active learning.
2094864959;CitNetExplorer: A new software tool for analyzing and visualizing citation networks;2014.0;[];We present CitNetExplorer, a new software tool for analyzing and visualizing citation networks of scientific publications. CitNetExplorer can for instance be used to study the development of a research field, to delineate the literature on a research topic, and to support literature reviewing. We first introduce the main concepts that need to be understood when working with CitNetExplorer. We then demonstrate CitNetExplorer by using the tool to analyze the scientometric literature and the literature on community detection in networks. Finally, we discuss some technical details on the construction, visualization, and analysis of citation networks in CitNetExplorer.
2095223629;Practical Data Prediction for Real-World Wireless Sensor Networks;2015.0;[];Data prediction is proposed in wireless sensor networks (WSNs) to extend the system lifetime by enabling the sink to determine the data sampled, within some accuracy bounds, with only minimal communication from source nodes. Several theoretical studies clearly demonstrate the tremendous potential of this approach, able to suppress the vast majority of data reports at the source nodes. Nevertheless, the techniques employed are relatively complex, and their feasibility on resource-scarce WSN devices is often not ascertained. More generally, the literature lacks reports from real-world deployments, quantifying the overall system-wide lifetime improvements determined by the interplay of data prediction with the underlying network. These two aspects, feasibility and system-wide gains, are key in determining the  practical  usefulness of data prediction in real-world WSN applications. In this paper, we describe derivative-based prediction (DBP), a novel data prediction technique much simpler than those found in the literature. Evaluation with real data sets from diverse WSN deployments shows that DBP often performs better than the competition, with data suppression rates up to 99 percent and good prediction accuracy. However, experiments with a real WSN in a road tunnel show that, when the network stack is taken into consideration, DBP only  triples   lifetime—a remarkable result per se, but a far cry from the data suppression rates above. To fully achieve the energy savings enabled by data prediction, the data and network layers must be jointly optimized. In our testbed experiments, a simple tuning of the MAC and routing stack, taking into account the operation of DBP, yields a remarkable  seven-fold  lifetime improvement w.r.t. the mainstream periodic reporting.
2095328651;Non-reflecting boundary conditions for waveguides;1999.0;[];New non-reflecting boundary conditions are introduced for the solution of the Helmholtz equation in a waveguide. These boundary conditions are perfectly transparent for all propagating modes. They do not require the determination of these propagating modes but only their propagation constants. A quasi-local form of these boundary conditions is well suited as terminating boundary condition beyond finite element meshes. Related convergence properties to the exact solution and optimal error estimates are established.
2095658739;A 10-Bit 300-MS/s Pipelined ADC With Digital Calibration and Digital Bias Generation;2013.0;[];A 10-bit pipelined ADC was fabricated using a 65 nm CMOS technology. To reduce power consumption, switching opamps are used. These switching opamps are designed to have a short turn-on time. Digital background calibration is employed to correct the A/D conversion error caused by the low dc gain of the opamps. The biasing voltages in each opamp are automatically generated using digital circuits. This bias scheme can maintain the settling behavior of the opamp against process-voltage-temperature variations. At 300 MS/s sampling rate, the ADC consumes 26.6 mW from a 1 V supply. Its measured DNL and INL are + 0.52/-0.4 LSB and +0.99/-1.65 LSB respectively. Its measured SNDR and SFDR are 55.4 dB and 67.2 dB respectively. The chip active area is 0.36 mm2 .
2095976990;Adaptive web search based on user profile constructed without any effort from users;2004.0;[];Web search engines help users find useful information on the World Wide Web (WWW). However, when the same query is submitted by different users, typical search engines return the same result regardless of who submitted the query. Generally, each user has different information needs for his/her query. Therefore, the search result should be adapted to users with different information needs. In this paper, we first propose several approaches to adapting search results according to each useru0027s need for relevant information without any user effort, and then verify the effectiveness of our proposed approaches. Experimental results show that search systems that adapt to each useru0027s preferences can be achieved by constructing user profiles based on modified collaborative filtering with detailed analysis of useru0027s browsing history in one day.
2096349415;Quantitative Attack Tree Analysis via Priced Timed Automata;2015.0;[];"The success of a security attack crucially depends on the resources available to an attacker: time, budget, skill level, and risk appetite. Insight in these dependencies and the most vulnerable system parts is key to providing effective counter  :[38],""paper considers attack trees, one of the most prominent security formalisms for threat analysis. We provide an effective way to compute the resources needed for a successful attack, as well as the associated attack paths. These paths provide the optimal ways, from the perspective of the attacker, to attack the system, and provide a ranking of the most vulnerable system  :[99],""exploiting the priced timed automaton model checker Uppaal CORA, we realize important advantages over earlier attack tree analysis methods: we can handle more complex gates, temporal dependencies between attack steps, shared subtrees, and realistic, multi-parametric cost structures. Furthermore, due to its compositionality, our approach is flexible and easy to  :[149],""illustrate our approach with several standard case studies from the literature, showing that our method agrees with existing analyses of these cases, and can incorporate additional data, leading to more informative results."
2096939802;Feature matching by searching maximum clique on high order association graph;1999.0;[];In this work we consider the most important problem in computer vision of performing matching among elementary features of objects when observed from different points of view. We formulate the problem in terms of subgraph isomorphism between relational graphs characterized by nodes representing interested object features and linking edges weighted by projective invariant values. The matching involves determination of all nodes in the association graph mutually compatible according to the similarity of the imposed invariant relations encoded on the edges. The solution requires us to determine subsets of nodes totally interconnected by edges with highest weights. Moreover, in most contexts, relations among more than two features can be involved, giving rise to association graphs of higher order. Recently it has been recognized that a particular class of dynamical equations are able to solve the maximum clique problem in an optimal manner. In our work, we have extended and applied such results to solve the most general problem of searching for the maximum edge-weighted clique on high-order association graphs. Moreover we have applied the method to a classical problem in computer vision of planar 3D surface reconstruction which is of fundamental importance for an autonomous moving vehicle in order to accomplish some elementary tasks, such as detection of ground floor obstacles or independent moving objects.
2097117768;Going deeper with convolutions;2015.0;[];We propose a deep convolutional neural network architecture codenamed Inception that achieves the new state of the art for classification and detection in the ImageNet Large-Scale Visual Recognition Challenge 2014 (ILSVRC14). The main hallmark of this architecture is the improved utilization of the computing resources inside the network. By a carefully crafted design, we increased the depth and width of the network while keeping the computational budget constant. To optimize quality, the architectural decisions were based on the Hebbian principle and the intuition of multi-scale processing. One particular incarnation used in our submission for ILSVRC14 is called GoogLeNet, a 22 layers deep network, the quality of which is assessed in the context of classification and detection.
2098363469;Local resilience of graphs;2008.0;[];In this article, we initiate a systematic study of graph resilience. The (local) resilience of a graph G with respect to a property $\cal {P}$ measures how much one has to change G (locally) to destroy $\cal {P}$. Estimating the resilience leads to many new and challenging problems. Here we focus on random and pseudorandom graphs and prove several sharp results. © 2008 Wiley Periodicals, Inc. Random Struct. Alg., 2008
2098755033;Designing for Performative Tangible Interaction;2008.0;[];We propose that designing tangibles for public interaction requires an understanding of both functional and non-functional aspects informed by Live Art theories. In this paper, we outline design requirements for performative tangible interaction, propose a framework for assessing performative interaction and demonstrate its use through four case studies of the iterative redesign of a highly portable, tangible exertion interface. By reflecting on our experience of designing for performative interaction we develop guidelines for developing multi-participant Digital Live Art.
2099271358;A Simulation Model of Fire Department Operations: Design and Preliminary Results;1970.0;[];"A simulation model designed to compare different policies for locating, relocating, and dispatching fire-fighting units is described. Issues treated include: the use of internal measures of performance as proxies for global ones; the use of analytical models for various subproblems to yield policies to be tested; the handling of loss of life and other important but rare events. The SIMSCRIPT 1.5 simulator and input and post-simulation analysis programs are described. Results that have been used by the Fire Department of the City of New York are presented and analyzed."
2099375630;Constructions of (t ,m,s)-nets and (t,s)-sequences;2005.0;[];We present a survey of constructions of (t,m,s)-nets and (t,s)-sequences. The emphasis is on work since about 1998 when a previous survey on the same topic was written.
2099419573;A survey on concept drift adaptation;2014.0;[];"Concept drift primarily refers to an online supervised learning scenario when the relation between the input data and the target variable changes over time. Assuming a general knowledge of supervised learning in this article, we characterize adaptive learning processes; categorize existing strategies for handling concept drift; overview the most representative, distinct, and popular techniques and algorithms; discuss evaluation methodology of adaptive algorithms; and present a set of illustrative applications. The survey covers the different facets of concept drift in an integrated way to reflect on the existing scattered state of the art. Thus, it aims at providing a comprehensive introduction to the concept drift adaptation for researchers, industry analysts, and practitioners."
2099530916;Sensor Network with Delay Tolerance (SeNDT);2007.0;[];As the technology underlying sensor networks becomes more advanced and reliable, ubiquitous computing will come closer to being a reality. This paper details the design and deployment of several sensor nodes that use the delay tolerant network approach in order to meet the relevant application requirements. Two applications are presented, lake water quality monitoring in rural lakes and noise level logging in urban areas and along motorways. The installations are discussed and initial results presented.
2099808581;Robust color histogram descriptors for video segment retrieval and identification;2002.0;[];Effective and efficient representation of color features of multiple video frames or pictures is an important yet challenging task for visual information management systems. Key frame-based methods to represent the color features of a group of frames (GoF) are highly dependent on the selection criterion of the representative frame(s), and may lead to unreliable results. We present various histogram-based color descriptors to reliably capture and represent the color properties of multiple images or a GoF. One family of such descriptors, called alpha-trimmed average histograms, combine individual frame or image histograms using a specific filtering operation to generate robust color histograms that can eliminate the adverse effects of brightness/color variations, occlusion, and edit effects on the color representation. We show the efficacy of the alpha-trimmed average histograms for video segment retrieval applications, and illustrate how they consistently outperform key frame-based methods. Another color histogram descriptor that we introduce, called the intersection histogram, reflects the number of pixels of a given color that is common to all the frames in the GoF. We employ the intersection histogram to develop a fast and efficient algorithm for identification of the video segment to which a query frame belongs. The proposed color histogram descriptors have been included in the ISO standard MPEG-7 after extensive evaluation experiments.
2099813784;VADER: A Parsimonious Rule-Based Model for Sentiment Analysis of Social Media Text;2014.0;[];The inherent nature of social media content poses serious challenges to practical applications of sentiment analysis. We present VADER, a simple rule-based model for general sentiment analysis, and compare its effectiveness to eleven typical state-of-practice benchmarks including LIWC, ANEW, the General Inquirer, SentiWordNet, and machine learning oriented techniques relying on Naive Bayes, Maximum Entropy, and Support Vector Machine (SVM) algorithms. Using a combination of qualitative and quantitative methods, we first construct and empirically validate a goldstandard list of lexical features (along with their associated sentiment intensity measures) which are specifically attuned to sentiment in microblog-like contexts. We then combine these lexical features with consideration for five general rules that embody grammatical and syntactical conventions for expressing and emphasizing sentiment intensity. Interestingly, using our parsimonious rule-based model to assess the sentiment of tweets, we find that VADER outperforms individual human raters (F1 Classification Accuracy = 0.96 and 0.84, respectively), and generalizes more favorably across contexts than any of our benchmarks.
2099989374;A Dirac-Type Theorem for 3-Uniform Hypergraphs;2006.0;[];A Hamiltonian cycle in a 3-uniform hypergraph is a cyclic ordering of the vertices in which every three consecutive vertices form an edge. In this paper we prove an approximate and asymptotic version of an analogue of Diracu0027s celebrated theorem for graphs: for each γu003e0 there exists n0 such that every 3-uniform hypergraph on $n\geq n_0$ vertices, in which each pair of vertices belongs to at least $(1/2+\gamma)n$ edges, contains a Hamiltonian cycle.
2093848495;MORPH: a reference architecture for configuration and behaviour self-adaptation;2015.0;[];An architectural approach to self-adaptive systems involves runtime change of system configuration (i.e., the systemu0027s components, their bindings and operational parameters) and behaviour update (i.e., component orchestration). Thus, dynamic reconfiguration and discrete event control theory are at the heart of architectural adaptation. Although controlling configuration and behaviour at runtime has been discussed and applied to architectural adaptation, architectures for self-adaptive systems often compound these two aspects reducing the potential for adaptability. In this paper we propose a reference architecture that allows for coordinated yet transparent and independent adaptation of system configuration and behaviour.
2064675550;Long short-term memory;1997.0;[];"Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiteru0027s (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O. 1. Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms."
2100025715;Understanding continued information technology usage behavior: a comparison of three models in the context of mobile internet;2006.0;[];"This study examines the utility of three prospective models for understanding the continued IT usage behavior. The three models include: Expectation-Confirmation Model in IT Domain (ECM-IT), Technology Acceptance Model (TAM), and a hybrid model integrating TAM and ECM-IT (extended ECM-IT). Based on a survey of 1826 mobile Internet users, the LISREL analysis shows that all three models meet the various goodness-of-fit criteria. When compared using special indices for differentiating among alternative good models, TAM has the best fit to the data followed by ECM-IT, and the extended ECM-IT. In terms of variance explained for intention to continue IT usage, the extended ECM-IT has the highest R2 (67%) followed by TAM (63%), and ECM-IT (50%). We conclude that TAM is the most parsimonious and generic model that can be used to study both initial and continued IT adoption; the extended ECM-IT explains continued IT usage behavior as well as TAM; and both the ECM-IT and extended ECM-IT models provide additional information to increase our understanding of continued IT usage."
2100096506;A probability-based unified framework for semantic search and recommendation;2013.0;[];The objective of search and recommendation is to provide users with documents that are relevant to their needs. Keyword-based search and recommendation approaches suffer from sparsity and semantic ambiguity problems because they correlate usersu0027 needs with documents only via keywords. Thus, for a given query, some documents that are semantically relevant to a useru0027s needs are not provided if they do not include specific keywords. To address this, some search approaches have used the authority of documents, which is commonly represented using hyperlinks within documents. However, if there are no hyperlinks, it is difficult to exploit the authority for ranking documents. As the links of documents are determined by their owners, the authority derived from links does not consider usersu0027 current needs. In order to resolve these problems, we propose a unified framework for semantic search and recommendation to enrich the semantics of usersu0027 needs and documents with their corresponding concepts and to use personalized authority derived from recommendation approaches. The proposed approach makes it possible to retrieve documents with a high degree of semantic relevance as well as high authority. Through extensive experiments, we show that our approach outperforms conventional search and recommendation approaches.
2100131857;A Review of Production Scheduling;1981.0;[];Production scheduling can be defined as the allocation of available production resources over time to best satisfy some set of criteria. Typically, the scheduling problem involves a set of tasks to be performed, and the criteria may involve both tradeoffs between early and late completion of a task, and between holding inventory for the task and frequent production changeovers. The intent of this paper is to present a broad classification for various scheduling problems, to review important theoretical developments for these problem classes, and to contrast the currently available theory with the practice of production scheduling. This paper will highlight problem areas for which there is both a significant discrepancy between practice and theory, and for which the practice corresponds closely to the theory.
2100170800;On blank nodes;2011.0;[];Blank nodes are defined in RDF as u0027existential variablesu0027 in the same way that has been used before in mathematical logic. However, evidence suggests that actual usage of RDF does not follow this definition. In this paper we thoroughly cover the issue of blank nodes, from incomplete information in database theory, over different treatments of blank nodes across the W3C stack of RDFrelated standards, to empirical analysis of RDF data publicly available on the Web. We then summarize alternative approaches to the problem, weighing up advantages and disadvantages, also discussing proposals for Skolemization.
2100199701;Brain tissue MR-image segmentation via optimum-path forest clustering;2012.0;[];We present an accurate and fast approach for MR-image segmentation of brain tissues, that is robust to anatomical variations and takes an average of less than 1min for completion on modern PCs. The method first corrects voxel values in the brain based on local estimations of the white-matter intensities. This strategy is inspired by other works, but it is simple, fast, and very effective. Tissue classification exploits a recent clustering approach based on the motion of optimum-path forest (OPF), which can find natural groups such that the absolute majority of voxels in each group belongs to the same class. First, a small random set of brain voxels is used for OPF clustering. Cluster labels are propagated to the remaining voxels, and then class labels are assigned to each group. The experiments used several datasets from three protocols (involving normal subjects, phantoms, and patients), two state-of-the-art approaches, and a novel methodology which finds the best choice of parameters for each method within the operational range of these parameters using a training dataset. The proposed method outperformed the compared approaches in speed, accuracy, and robustness.
2100453679;Non-negative Laplacian Embedding;2009.0;[];Laplacian embedding provides a low dimensional representation for a matrix of pairwise similarity data using the eigenvectors of the Laplacian matrix. The true power of Laplacian embedding is that it provides an approximation of the Ratio Cut clustering. However, Ratio Cut clustering requires the solution to be {\it nonnegative}. In this paper, we propose a new approach, nonnegative Laplacian embedding, which approximates Ratio Cut clustering in a more direct way than traditional approaches. From the solution of our approach, clustering structures can be read off directly. We also propose an efficient algorithm to optimize the objective function utilized in our approach. Empirical studies on many real world datasets show that our approach leads to more accurate Ratio Cut solution and improves clustering accuracy at the same time.
2100683571;Transitivity of fuzzy preference relations—an empirical study;2001.0;[];Based on an experiment we show that human preferences, when represented by fuzzy relations, may violate transitivity. We define an index of transitivity and compute the degree to which the transitivity is violated.
2100755716;A survey of collaborative filtering based social recommender systems;2014.0;[];Recommendation plays an increasingly important role in our daily lives. Recommender systems automatically suggest to a user items that might be of interest to her. Recent studies demonstrate that information from social networks can be exploited to improve accuracy of recommendations. In this paper, we present a survey of collaborative filtering (CF) based social recommender systems. We provide a brief overview over the task of recommender systems and traditional approaches that do not use social network information. We then present how social network information can be adopted by recommender systems as additional input for improved accuracy. We classify CF-based social recommender systems into two categories: matrix factorization based social recommendation approaches and neighborhood based social recommendation approaches. For each category, we survey and compare several representative algorithms.
2100960835;Fairness through awareness;2012.0;[];"We study fairness in classification, where individuals are classified, e.g., admitted to a university, and the goal is to prevent discrimination against individuals based on their membership in some group, while maintaining utility for the classifier (the university). The main conceptual contribution of this paper is a framework for fair classification comprising (1) a (hypothetical) task-specific metric for determining the degree to which individuals are similar with respect to the classification task at hand; (2) an algorithm for maximizing utility subject to the fairness constraint, that similar individuals are treated similarly. We also present an adaptation of our approach to achieve the complementary goal of ""fair affirmative action,"" which guarantees statistical parity (i.e., the demographics of the set of individuals receiving any classification are the same as the demographics of the underlying population), while treating similar individuals as similarly as possible. Finally, we discuss the relationship of fairness to privacy: when fairness implies privacy, and how tools developed in the context of differential privacy may be applied to fairness."
2101391769;Trust Brokering and Its Use for Resource Matchmaking in Public-Resource Grids;2006.0;[];This paper presents a trust brokering system that operates in a peer-to-peer manner. The network of trust brokers operate by providing peer reviews in the form of recommendations regarding potential resource targets. One of the distinguishing features of our work is that it separately models the accuracy and honesty concepts. By separately modeling these concepts, our model is able to significantly improve the performance. We apply the trust brokering system to a resource manager to illustrate its utility in a public-resource Grid environment. The simulations performed to evaluate the trust-aware resource matchmaking strategies indicate that high levels of ‘robustness’ can be attained by considering trust while matchmaking and allocating resources.
2101668304;How well do multi-objective evolutionary algorithms scale to large problems;2007.0;[];In spite of large amount of research work in multi- objective evolutionary algorithms, most have evaluated their algorithms on problems with only two to four objectives. Little has been done to understand the performance of the multi- objective evolutionary algorithms on problems with a larger number of objectives. It is unclear whether the conclusions drawn from the experiments on problems with a small number of objectives could be generalised to those with a large number of objectives. In fact, some of our preliminary work [1] has indicated that such generalisation may not be possible. This paper first presents a comprehensive set of experimental studies, which show that the performance of multi-objective evolutionary algorithms, such as NSGA-II and SPEA2, deteriorates substantially as the number of objectives increases. NSGA-II, for example, did not even converge for problems with six or more objectives. This paper analyses why this happens and proposes several new methods to improve the convergence of NSGA-II for problems with a large number of objectives. The proposed methods categorise members of an archive into small groups (non-dominated solutions with or without domination), using dominance relationship between the new and existing members in the archive. New removal strategies are introduced. Our experimental results show that the proposed methods clearly outperform NSGA-II in terms of convergence.
2102295719;deal.II—A general-purpose object-oriented finite element library;2007.0;[];An overview of the software design and data abstraction decisions chosen for deal.II, a general purpose finite element library written in Cpp, is given. The library uses advanced object-oriented and data encapsulation techniques to break finite element implementations into smaller blocks that can be arranged to fit users requirements. Through this approach, deal.II supports a large number of different applications covering a wide range of scientific areas, programming methodologies, and application-specific algorithms, without imposing a rigid framework into which they have to fit. A judicious use of programming techniques allows us to avoid the computational costs frequently associated with  object-oriented class libraries.   The paper presents a detailed description of the abstractions chosen for defining geometric information of meshes and the handling of degrees of freedom associated with finite element spaces, as well as of linear algebra, input/output capabilities and of interfaces to other software, such as visualization tools. Finally, some results obtained with applications built atop deal.II are shown to demonstrate the powerful capabilities of this toolbox.
2102500637;Arithmetic Networks and Their Minimization Using a New Line of Elementary Units;1975.0;[];"A family of switching networks, called ""arithmetic networks,"" is investigated. The elementary units of these networks are generalizations of full adders that can process input signals of different weights."
2103496339;Approximation by superpositions of a sigmoidal function;1989.0;[];"In this paper we demonstrate that finite linear combinations of compositions of a fixed, univariate function and a set of affine functionals can uniformly approximate any continuous function ofn real variables with support in the unit hypercube; only mild conditions are imposed on the univariate function. Our results settle an open question about representability in the class of single hidden layer neural networks. In particular, we show that arbitrary decision regions can be arbitrarily well approximated by continuous feedforward neural networks with only a single internal, hidden layer and any continuous sigmoidal nonlinearity. The paper discusses approximation properties of other possible types of nonlinearities that might be implemented by artificial neural networks."
2103729454;Brain mediators of cardiovascular responses to social threat: part I: Reciprocal dorsal and ventral sub-regions of the medial prefrontal cortex and heart-rate reactivity.;2009.0;[];Social threat is a key component of mental “stress” and a potent generator of negative emotions and physiological responses in the body. How the human brain processes social context and drives peripheral physiology, however, is relatively poorly understood. Human neuroimaging and animal studies implicate the dorsal medial prefrontal cortex (MPFC), though this heterogeneous region is likely to contain multiple sub-regions with diverse relationships with physiological reactivity and regulation. We used fMRI combined with a novel multi-level path analysis approach to identify brain mediators of the effects of a public speech preparation task (social evaluative threat, SET) on heart rate (HR). This model provides tests of functional pathways linking experimentally manipulated threat, regional fMRI activity, and physiological output, both across time (within person) and across individuals (between persons). It thus integrates time series connectivity and individual difference analyses in the same path model. The results provide evidence for two dissociable, inversely coupled sub-regions of MPFC that independently mediated HR responses. SET caused activity increases in a more dorsal pregenual cingulate region, whose activity was coupled with HR increases. Conversely, SET caused activity decreases in a right ventromedial/medial orbital region, which were coupled with HR increases. Individual differences in coupling strength in each pathway independently predicted individual differences in HR reactivity. These results underscore both the importance and heterogeneity of MPFC in generating physiological responses to threat.
2104195067;Assuring system goals under uncertainty with active formal models of self-adaptation;2014.0;[];Designing software systems with uncertainties, such as incomplete knowledge about changing system goals, is challenging. One approach to handle uncertainties is self-adaptation, where a system consists of a managed system and a managing system that realizes a feedback loop. The promise of self-adaptation is to enable a system to adapt itself realizing the system goals, regarding uncertainties. To realize this promise it is critical to provide assurances for the self-adaptive behaviours. Several approaches have been proposed that exploit formal methods to provide these assurances. However, an integrated approach that combines: (1) seamless integration of offline and online verification (to deal with inherent limitations of verification), with (2) support for runtime evolution of the system (to deal with new or changing goals) is lacking. In this paper, we outline a new approach named Active FORmal Models of Self-adaptation (ActivFORMS) that aims to deal with these challenges. In ActivFORMS, the formal models of the managing system are directly deployed and executed to realize self-adaptation, guaranteeing the verified properties. Having the formal models readily available at runtime paves the way for: (1) incremental verification during system execution, and (2) runtime evolution of the self-adaptive system. Experiences with a robotic system show promising results.
2104266030;Cython: The Best of Both Worlds;2011.0;[];Cython is a Python language extension that allows explicit type declarations and is compiled directly to C. As such, it addresses Pythonu0027s large overhead for numerical loops and the difficulty of efficiently using existing C and Fortran code, which Cython can interact with natively.
2104812688;Biological network comparison using graphlet degree distribution;2007.0;[];"Motivation: Analogous to biological sequence comparison, comparing cellular networks is an important problem that could provide insight into biological understanding and therapeutics. For technical reasons, comparing large networks is computationally infeasible, and thus heuristics, such as the degree distribution, clustering coefficient, diameter, and relative graphlet frequency distribution have been sought. It is easy to demonstrate that two networks are different by simply showing a short list of properties in which they differ. It is much harder to show that two networks are similar, as it requires demonstrating their similarity in all of their exponentially many properties. Clearly, it is computationally prohibitive to analyze all network properties, but the larger the number of constraints we impose in determining network similarity, the more likely it is that the networks will truly be  :[130],""We introduce a new systematic measure of a networku0027s local structure that imposes a large number of similarity constraints on networks being compared. In particular, we generalize the degree distribution, which measures the number of nodes u0027touchingu0027 k edges, into distributions measuring the number of nodes u0027touchingu0027 k graphlets, where graphlets are small connected non-isomorphic subgraphs of a large network. Our new measure of network local structure consists of 73 graphlet degree distributions of graphlets with 2--5 nodes, but it is easily extendible to a greater number of constraints (i.e. graphlets), if necessary, and the extensions are limited only by the available CPU. Furthermore, we show a way to combine the 73 graphlet degree distributions into a network u0027agreementu0027 measure which is a number between 0 and 1, where 1 means that networks have identical distributions and 0 means that they are far apart. Based on this new network agreement measure, we show that almost all of the 14 eukaryotic PPI networks, including human, resulting from various high-throughput experimental techniques, as well as from curated databases, are better modeled by geometric random graphs than by Erdos--Reny, random scale-free, or Barabasi--Albert scale-free  :[322],""Software executables are available upon  :[328],""natasha@ics.uci.edu"
2104828970;Discrete-continuous optimization for multi-target tracking;2012.0;[];"The problem of multi-target tracking is comprised of two distinct, but tightly coupled challenges: (i) the naturally discrete problem of data association, i.e. assigning image observations to the appropriate target; (ii) the naturally continuous problem of trajectory estimation, i.e. recovering the trajectories of all targets. To go beyond simple greedy solutions for data association, recent approaches often perform multi-target tracking using discrete optimization. This has the disadvantage that trajectories need to be pre-computed or represented discretely, thus limiting accuracy. In this paper we instead formulate multi-target tracking as a discrete-continuous optimization problem that handles each aspect in its natural domain and allows leveraging powerful methods for multi-model fitting. Data association is performed using discrete optimization with label costs, yielding near optimality. Trajectory estimation is posed as a continuous fitting problem with a simple closed-form solution, which is used in turn to update the label costs. We demonstrate the accuracy and robustness of our approach with state-of-the-art performance on several standard datasets."
2105101328;Action Recognition with Improved Trajectories;2013.0;[];Recently dense trajectories were shown to be an efficient video representation for action recognition and achieved state-of-the-art results on a variety of datasets. This paper improves their performance by taking into account camera motion to correct them. To estimate camera motion, we match feature points between frames using SURF descriptors and dense optical flow, which are shown to be complementary. These matches are, then, used to robustly estimate a homography with RANSAC. Human motion is in general different from camera motion and generates inconsistent matches. To improve the estimation, a human detector is employed to remove these matches. Given the estimated camera motion, we remove trajectories consistent with it. We also use this estimation to cancel out camera motion from the optical flow. This significantly improves motion-based descriptors, such as HOF and MBH. Experimental results on four challenging action datasets (i.e., Hollywood2, HMDB51, Olympic Sports and UCF50) significantly outperform the current state of the art.
2105101597;Power consumption and energy efficiency in the internet;2011.0;[];This article provides an overview of a network-based model of power consumption in Internet infrastructure. This model provides insight into how different parts of the Internet will contribute to network power as Internet access increase over time. The model shows that today the access network dominates the Internetu0027s power consumption and, as access speeds grow, the core network routers will dominate power consumption. The power consumption of data centers and content distribution networks is dominated by the power consumption of data storage for material that is infrequently downloaded and by the transport of the data for material that is frequently downloaded. Based on the model several strategies to improve the energy efficiency of the Internet are presented.
2105108073;Direction finding in partly calibrated sensor arrays composed of multiple subarrays;2002.0;[];We consider the direction-finding problem in partly calibrated arrays composed of several calibrated and identically oriented (but possibly nonidentical) subarrays that are displaced by unknown (and possibly time-varying) vector translations. A new search-free eigenstructure-based direction-finding approach is proposed for such class of sensor arrays. It is referred to as the rank-reduction (RARE) estimator and enjoys simple implementation that entails computing the eigendecomposition of the sample array covariance matrix and polynomial rooting. Closed-form expressions for the deterministic Cramer-Rao bounds (CRBs) on direction-of-arrival (DOA) estimation for the considered class of sensor arrays are derived. Comparison of these expressions with simulation results show that the finite-sample performance of RARE algorithms in both time-invariant and time-varying array cases is close to the corresponding bounds. Moreover, comparisons of the derived CRBs with the well-known bounds for the fully calibrated time-invariant array case help to discover several interesting properties of DOA estimation in partly calibrated and time-varying arrays.
2105457218;Regularity lemmas for hypergraphs and quasi-randomness;2007.0;[];We give a simple proof for Szemerediu0027s Regularity Lemma and its generalization for k-uniform hypergraphs. For fixed k, there are altogether k -1 different versions of the regularity lemma for k-uniform hypergraphs. The connection between regularity lemmas for hypergraphs and quasi-random classes of hypergraphs is also investigated.
2105981469;Context-sensitive information retrieval using implicit feedback;2005.0;[];"A major limitation of most existing retrieval models and systems is that the retrieval decision is made based solely on the query and document collection; information about the actual user and search context is largely ignored. In this paper, we study how to exploit implicit feedback information, including previous queries and clickthrough information, to improve retrieval accuracy in an interactive information retrieval setting. We propose several context-sensitive retrieval algorithms based on statistical language models to combine the preceding queries and clicked document summaries with the current query for better ranking of documents. We use the TREC AP data to create a test collection with search context information, and quantitatively evaluate our models using this test set. Experiment results show that using implicit feedback, especially the clicked document summaries, can improve retrieval performance substantially."
2106327281;Risk assessment in practice: A real case study;2008.0;[];The aim of this work is to evaluate the risk of an external attack to the network of our Department in the University. Thus, this work wants to complement the results in [M. Benini, S. Sicari, A mathematical framework for risk assessment, in: H. Labiod, M. Badra (Eds.), New Technologies, Mobility and Security, Signals and Communication, Springer-Verlag, May 2007, pp. 459-469] where a mathematical framework justifying our risk assessment method has been presented. Hence, this article describes a detailed account of our experience where the instruments, the techniques and the results are described and evaluated.
2106334424;Multiobjective evolutionary algorithms: a comparative case study and the strength Pareto approach;1999.0;[];Evolutionary algorithms (EAs) are often well-suited for optimization problems involving several, often conflicting objectives. Since 1985, various evolutionary approaches to multiobjective optimization have been developed that are capable of searching for multiple solutions concurrently in a single run. However, the few comparative studies of different methods presented up to now remain mostly qualitative and are often restricted to a few approaches. In this paper, four multiobjective EAs are compared quantitatively where an extended 0/1 knapsack problem is taken as a basis. Furthermore, we introduce a new evolutionary approach to multicriteria optimization, the strength Pareto EA (SPEA), that combines several features of previous multiobjective EAs in a unique manner. It is characterized by (a) storing nondominated solutions externally in a second, continuously updated population, (b) evaluating an individualu0027s fitness dependent on the number of external nondominated points that dominate it, (c) preserving population diversity using the Pareto dominance relationship, and (d) incorporating a clustering procedure in order to reduce the nondominated set without destroying its characteristics. The proof-of-principle results obtained on two artificial problems as well as a larger problem, the synthesis of a digital hardware-software multiprocessor system, suggest that SPEA can be very effective in sampling from along the entire Pareto-optimal front and distributing the generated solutions over the tradeoff surface. Moreover, SPEA clearly outperforms the other four multiobjective EAs on the 0/1 knapsack problem.
2106565812;Nineteen dubious ways to compute the exponential of a matrix, twenty-five years later;2003.0;[];In principle, the exponential of a matrix could be computed in many ways. Methods involv- ing approximation theory, dierential equations, the matrix eigenvalues, and the matrix characteristic polynomial have been proposed. In practice, consideration of computational stability and eciency indicates that some of the methods are preferable to others, but that none are completely satisfactory. Most of this paper was originally published in 1978. An update, with a separate bibliog- raphy, describes a few recent developments.
2107878631;Learning long-term dependencies with gradient descent is difficult;1994.0;[];Recurrent neural networks can be used to map input sequences to output sequences, such as for recognition, production or prediction problems. However, practical difficulties have been reported in training recurrent neural networks to perform tasks in which the temporal contingencies present in the input/output sequences span long intervals. We show why gradient based learning algorithms face an increasingly difficult problem as the duration of the dependencies to be captured increases. These results expose a trade-off between efficient learning by gradient descent and latching on information for long periods. Based on an understanding of this problem, alternatives to standard gradient descent are considered. u003e
2108013467;Multiview Vector-Valued Manifold Regularization for Multilabel Image Classification;2013.0;[];In computer vision, image datasets used for classification are naturally associated with multiple labels and comprised of multiple views, because each image may contain several objects (e.g., pedestrian, bicycle, and tree) and is properly characterized by multiple visual features (e.g., color, texture, and shape). Currently, available tools ignore either the label relationship or the view complementarily. Motivated by the success of the vector-valued function that constructs matrix-valued kernels to explore the multilabel structure in the output space, we introduce multiview vector-valued manifold regularization   $({\rm MV}^{3}{\rm MR})$   to integrate multiple features.   ${\rm MV}^{3}{\rm MR}$   exploits the complementary property of different features and discovers the intrinsic local geometry of the compact support shared by different features under the theme of manifold regularization. We conduct extensive experiments on two challenging, but popular, datasets, PASCAL VOCu0027 07 and MIR Flickr, and validate the effectiveness of the proposed   ${\rm MV}^{3}{\rm MR}$   for image classification.
2109255472;Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition;2015.0;[];Existing deep convolutional neural networks (CNNs) require a fixed-size (e.g., 224  $\times$      224) input image. This requirement is “artificial” and may reduce the recognition accuracy for the images or sub-images of an arbitrary size/scale. In this work, we equip the networks with another pooling strategy, “spatial pyramid pooling”, to eliminate the above requirement. The new network structure, called SPP-net, can generate a fixed-length representation regardless of image size/scale. Pyramid pooling is also robust to object deformations. With these advantages, SPP-net should in general improve all CNN-based image classification methods. On the ImageNet 2012 dataset, we demonstrate that SPP-net boosts the accuracy of a variety of CNN architectures despite their different designs. On the Pascal VOC 2007 and Caltech101 datasets, SPP-net achieves state-of-the-art classification results using a single full-image representation and no fine-tuning. The power of SPP-net is also significant in object detection. Using SPP-net, we compute the feature maps from the entire image only once, and then pool features in arbitrary regions (sub-images) to generate fixed-length representations for training the detectors. This method avoids repeatedly computing the convolutional features. In processing test images, our method is 24-102   $\times$       faster than the R-CNN method, while achieving better or comparable accuracy on Pascal VOC 2007. In ImageNet Large Scale Visual Recognition Challenge (ILSVRC) 2014, our methods rank #2 in object detection and #3 in image classification among all 38 teams. This manuscript also introduces the improvement made for this competition.
2109418048;ConnectomeExplorer: Query-Guided Visual Analysis of Large Volumetric Neuroscience Data;2013.0;[];This paper presents ConnectomeExplorer, an application for the interactive exploration and query-guided visual analysis of large volumetric electron microscopy (EM) data sets in connectomics research. Our system incorporates a knowledge-based query algebra that supports the interactive specification of dynamically evaluated queries, which enable neuroscientists to pose and answer domain-specific questions in an intuitive manner. Queries are built step by step in a visual query builder, building more complex queries from combinations of simpler queries. Our application is based on a scalable volume visualization framework that scales to multiple volumes of several teravoxels each, enabling the concurrent visualization and querying of the original EM volume, additional segmentation volumes, neuronal connectivity, and additional meta data comprising a variety of neuronal data attributes. We evaluate our application on a data set of roughly one terabyte of EM data and 750 GB of segmentation data, containing over 4,000 segmented structures and 1,000 synapses. We demonstrate typical use-case scenarios of our collaborators in neuroscience, where our system has enabled them to answer specific scientific questions using interactive querying and analysis on the full-size data for the first time.
2109436662;Collaborative Vision-Integrated Pseudorange Error Removal: Team-Estimated Differential GNSS Corrections with no Stationary Reference Receiver;2012.0;[];This paper presents an approach for generating Global Navigation Satellite System (GNSS) differential corrections by distributing GNSS and georeferenced vision measurements through a vehicle-to-vehicle (V2V) communications network. Conventionally, high-quality differential GNSS corrections are generated from a stationary reference receiver in close proximity to a set of mobile users. The proposed method, which is called Collaborative Vision Integrated Pseudorange Error Removal (C-VIPER), instead generates differential corrections using data from moving vehicles, thus eliminating the need for an infrastructure of stationary receivers. An important feature of the proposed algorithm is that individual differential corrections are computed for each satellite, so that corrections can be shared among users with different satellites in view. As demonstrated in simulation, measurement sharing significantly improves positioning accuracy in both the cross-track direction, where the quality of visual lane-boundary measurements is high, and the along-track direction, where the quality is low. Furthermore, because measurements are shared among many vehicles, the networked solution is robust to vision-sensor dropouts that may occur for individual vehicles.
2109528718;Performance modeling of epidemic routing;2007.0;[];In this paper, we develop a rigorous, unified framework based on ordinary differential equations (ODEs) to study epidemic routing and its variations. These ODEs can be derived as limits of Markovian models under a natural scaling as the number of nodes increases. While an analytical study of Markovian models is quite complex and numerical solution impractical for large networks, the corresponding ODE models yield closed-form expressions for several performance metrics of interest, and a numerical solution complexity that does not increase with the number of nodes. Using this ODE approach, we investigate how resources such as buffer space and the number of copies made for a packet can be traded for faster delivery, illustrating the differences among various forwarding and recovery schemes considered. We perform model validations through simulation studies. Finally we consider the effect of buffer management by complementing the forwarding models with Markovian and fluid buffer models.
2109680381;The dark side of smartphone usage: Psychological traits, compulsive behavior and technostress;2014.0;[];Smartphones have become necessities in peopleu0027 lives. Along with its obvious benefits, however, the smartphone has other effects that are not all that glorious. This study investigates the dark side of the smartphone trend. We examine the link between psychological traits and the compulsive behaviors of smartphone users, and look further into the stress caused by those compulsive behaviors. We conducted an empirical study consisting of 325 participants and compared Structural Equation Modeling with competing models. The results suggest that compulsive usage of smartphone and technostress are positively related to psychological traits including locus of control, social interaction anxiety, materialism and the need for touch. Gender differences are also found in the aforementioned relationships. The results have practical implications to user-oriented smartphone design and operation companies as well as government agencies as they combat the social ills brought on by smartphones.
2109707919;A Note on Extension of TOPSIS to Multiple Criteria Decision Making with Pythagorean Fuzzy Sets;2016.0;[];"In this note, we point out an error to the proof of Theorem 3.4 in Zhang and Xu Int J Intell Syst 2014;2912:1061-1078 by a counterexample. We find that the inequality i.e., |πβ12-πβ22|i¾?|πβ12-πβ32| with respect to the degrees of indeterminacy of any three Pythagorean fuzzy numbers in the proof of Theorem 3.4 in Zhang and Xuu0027s paper is not valid. A new proof is provided in this note."
2109722477;Map-Reduce for Machine Learning on Multicore;2006.0;[];"We are at the beginning of the multicore era. Computers will have increasingly many cores (processors), but there is still no good programming framework for these architectures, and thus no simple and unified way for machine learning to take advantage of the potential speed up. In this paper, we develop a broadly applicable parallel programming method, one that is easily applied to many different learning algorithms. Our work is in distinct contrast to the tradition in machine learning of designing (often ingenious) ways to speed up a single algorithm at a time. Specifically, we show that algorithms that fit the Statistical Query model [15] can be written in a certain ""summation form,"" which allows them to be easily parallelized on multicore computers. We adapt Googleu0027s map-reduce [7] paradigm to demonstrate this parallel speed up technique on a variety of learning algorithms including locally weighted linear regression (LWLR), k-means, logistic regression (LR), naive Bayes (NB), SVM, ICA, PCA, gaussian discriminant analysis (GDA), EM, and backpropagation (NN). Our experimental results show basically linear speedup with an increasing number of processors."
2110114082;Python for Scientific Computing;2007.0;[];"Python is an excellent ""steering"" language for scientific codes written in other languages. However, with additional basic tools, Python transforms into a high-level language suited for scientific and engineering code thatu0027s often fast enough to be immediately useful but also flexible enough to be sped up with additional extensions."
2110787567;Powers of Hamilton Cycles in Pseudorandom Graphs;2017.0;[];We study the appearance of powers of Hamilton cycles in pseudorandom graphs, using the following comparatively weak pseudorandomness notion. A graph G is (e,p,k,l)-pseudorandom if for all disjoint X and Y ⊂ V(G) with |X|≥ep k n and |Y|≥ep l n we have e(X,Y)=(1±e)p|X||Y|. We prove that for all βu003e0 there is an eu003e0 such that an (e,p,1,2)-pseudorandom graph on n vertices with minimum degree at least βpn contains the square of a Hamilton cycle. In particular, this implies that (n,d,λ)-graphs with λ≪d 5/2 n -3/2 contain the square of a Hamilton cycle, and thus a triangle factor if n is a multiple of 3. This improves on a result of Krivelevich, Sudakov and Szabo [27].
2111006384;Optimized Product Quantization for Approximate Nearest Neighbor Search;2013.0;[];Product quantization is an effective vector quantization approach to compactly encode high-dimensional vectors for fast approximate nearest neighbor (ANN) search. The essence of product quantization is to decompose the original high-dimensional space into the Cartesian product of a finite number of low-dimensional subspaces that are then quantized separately. Optimal space decomposition is important for the performance of ANN search, but still remains unaddressed. In this paper, we optimize product quantization by minimizing quantization distortions w.r.t. the space decomposition and the quantization codebooks. We present two novel methods for optimization: a non-parametric method that alternatively solves two smaller sub-problems, and a parametric method that is guaranteed to achieve the optimal solution if the input data follows some Gaussian distribution. We show by experiments that our optimized approach substantially improves the accuracy of product quantization for ANN search.
2111145992;Model-based Security Metrics Using ADversary VIew Security Evaluation (ADVISE);2011.0;[];System architects need quantitative security metrics to make informed trade-off decisions involving system security. The security metrics need to provide insight on weak points in the system defense, considering characteristics of both the system and its adversaries. To provide such metrics, we formally define the ADversary View Security Evaluation (ADVISE) method. Our approach is to create an executable state-based security model of a system and an adversary that represents how the adversary is likely to attack the system and the results of such an attack. The attack decision function uses information about adversary attack preferences and possible attacks against the system to mimic how the adversary selects the most attractive next attack step. The adversaryu0027s decision involves looking ahead some number of attack steps. System architects can use ADVISE to compare the security strength of system architecture variants and analyze the threats posed by different adversaries. We demonstrate the feasibility and benefits of ADVISE using a case study. To produce quantitative model-based security metrics, we have implemented the ADVISE method in a tool that facilitates user input of system and adversary data and automatically generates executable models.
2111200615;Energy characterization and optimization of image sensing toward continuous mobile vision;2013.0;[];"A major hurdle to frequently performing mobile computer vision tasks is the high power consumption of image sensing. In this work, we report the first publicly known experimental and analytical characterization of CMOS image sensors. We find that modern image sensors are not energy-proportional: energy per pixel is in fact inversely proportional to frame rate and resolution of image capture, and thus image sensor systems fail to provide an important principle of energy-aware system design: trading quality for energy efficiency. We reveal two energy-proportional mechanisms, supported by current image sensors but unused by mobile systems: (i) using an optimal clock frequency reduces the power up to 50% or 30% for low-quality single frame (photo) and sequential frame (video) capturing, respectively; (ii) by entering low-power standby mode between frames, an image sensor achieves almost constant energy per pixel for video capture at low frame rates, resulting in an additional 40% power reduction. We also propose architectural modifications to the image sensor that would further improve operational efficiency. Finally, we use computer vision benchmarks to show the performance and efficiency tradeoffs that can be achieved with existing image sensors. For image registration, a key primitive for image mosaicking and depth estimation, we can achieve a 96% success rate at 3 FPS and 0.1 MP resolution. At these quality metrics, an optimal clock frequency reduces image sensor power consumption by 36% and aggressive standby mode reduces power consumption by 95%."
2111296615;Introduction to Statistical Learning Theory;2004.0;[];The goal of statistical learning theory is to study, in a statistical framework, the properties of learning algorithms. In particular, most results take the form of so-called error bounds. This tutorial introduces the techniques that are used to obtain such results.
2111393363;A study of particle swarm optimization particle trajectories;2006.0;[];Particle swarm optimization (PSO) has shown to be an efficient, robust and simple optimization algorithm. Most of the PSO studies are empirical, with only a few theoretical analyses that concentrate on understanding particle trajectories. These theoretical studies concentrate mainly on simplified PSO systems. This paper overviews current theoretical studies, and extend these studies to investigate particle trajectories for general swarms to include the influence of the inertia term. The paper also provides a formal proof that each particle converges to a stable point. An empirical analysis of multi-dimensional stochastic particles is also presented. Experimental results are provided to support the conclusions drawn from the theoretical findings.
2112036188;Completely Derandomized Self-Adaptation in Evolution Strategies;2001.0;[];This paper puts forward two useful methods for self-adaptation of the mutation distribution - the concepts of derandomization and cumulation. Principle shortcomings of the concept of mutative strategy parameter control and two levels of derandomization are reviewed. Basic demands on the self-adaptation of arbitrary (normal) mutation distributions are developed. Applying arbitrary, normal mutation distributions is equivalent to applying a general, linear problem encoding. The underlying objective of mutative strategy parameter control is roughly to favor previously selected mutation steps in the future. If this objective is pursued rigorously, a completely derandomized self-adaptation scheme results, which adapts arbitrary normal mutation distributions. This scheme, called covariance matrix adaptation (CMA), meets the previously stated demands. It can still be considerably improved by cumulation - utilizing an evolution path rather than single search steps. Simulations on various test functions reveal local and global search properties of the evolution strategy with and without covariance matrix adaptation. Their performances are comparable only on perfectly scaled functions. On badly scaled, non-separable functions usually a speed up factor of several orders of magnitude is observed. On moderately mis-scaled functions a speed up factor of three to ten can be expected.
2112088608;ANNIVERSARY ARTICLE: Improving Emergency Responsiveness with Management Science;2004.0;[];While the goal of OR/MS is to aid decision makers, implementation of published models occurs less frequently than one might hope. However, one area that has been significantly impacted by management science is emergency response systems. Dozens of papers on emergency service management appeared in the OR/MS literature in the 1970s alone, many of which were published inManagement Science. Three of these papers won major prizes. More importantly, many of these papers led to the implementation of substantially new policies and practices, particularly in policing and firefighting. Much of this work originated in New York City, though many other cities subsequently adopted the resulting models and strategies. In this paper, we look at the context, content, and nature of the research and the factors that led to these early implementation successes. We then track the extent to which these original models are still affecting decision making in emergency response systems. We also examine the pace of development of new OR/MS models and applications in the area. Finally, we look at issues in emergency responsiveness that have emerged recently as a result of the national focus on terrorism and discuss the potential for future OR/MS modeling and application.
2112565255;Projection-Based Augmented Reality in Disney Theme Parks;2012.0;[];Walt Disney Imagineering and Disney Research Zurich are building a projector- camera toolbox to help create spatially augmented 3D objects and dynamic, interactive spaces that enhance the theme park experience by immersing guests in magical worlds. A related video can be seen here: http://youtu.be/wjrylXl0tTk. It shows examples of projection-based augmented reality techniques being employed in Disney theme parks.
2112850401;New Efficient Utility Upper Bounds for the Fully Adaptive Model of Attack Trees;2013.0;[];We present a new fully adaptive computational model for attack trees that allows attackers to repeat atomic attacks if they fail and to play on if they are caught and have to pay penalties. The new model allows safer conclusions about the security of real-life systems and is somewhat (computationally) easier to analyze. We show that in the new model optimal strategies always exist and finding the optimal strategy is (just) an np-complete problem. We also present methods to compute adversarial utility estimation and utility upper bound approximated estimation using a bottom-up approach.
2112970627;Full intuitionistic linear logic (extended abstract);1993.0;[];"   In this paper we give a brief treatment of a theory of proofs for a system of Full Intuitionistic Linear Logic. This system is distinct from Classical Linear Logic, but unlike the standard Intuitionistic Linear Logic of Girard and Lafont includes the multiplicative disjunction par. This connective does have an entirely natural interpretation in a variety of (non-classical) categorical models of Intuitionistic Linear Logic. The main proof-theoretic problem arises from the observation of Schellinx that cut elimination fails outright for an intuitive formulation of Full Intuitionistic Linear Logic; the nub of the problem is the interaction between par and linear implication. We present here a term assignment system which gives an interpretation of proofs as some kind of non-deterministic function. In this way we find a system which does enjoy cut elimination. The system is a direct result of an analysis of the categorical semantics, though we make an effort to present the system as if it were purely a proof-theoretic construction."
2113310608;The EBI RDF platform: linked open data for the life sciences;2014.0;[];Motivation: Resource description framework (RDF) is an emerging technology for describing, publishing and linking life science data. As a major provider of bioinformatics data and services, the European Bioinformatics Institute (EBI) is committed to making data readily accessible to the community in ways that meet existing demand. The EBI RDF platform has been developed to meet an increasing demand to coordinate RDF activities across the institute and provides a new entry point to querying and exploring integrated resources available at the EBI. Availability: http://www.ebi.ac.uk/rdf Contact: jupp@ebi.ac.uk
2113407082;Triangle Factors In Sparse Pseudo-Random Graphs;2004.0;[];The goal of the paper is to initiate research towards a general, Blow-up Lemma type embedding statement for pseudo-random graphs with sublinear degrees. In particular, we show that if the second eigenvalue λ of a d-regular graph G on 3n vertices is at most cd3/n2 log n, for some sufficiently small constant c u003e 0, then G contains a triangle factor. We also show that a fractional triangle factor already exists if λ u003c 0.1d2/n. The latter result is seen to be best possible up to a constant factor, for various values of the degree d = d(n).
2114025269;Affective video content representation and modeling;2005.0;[];This paper looks into a new direction in video content analysis - the representation and modeling of affective video content . The affective content of a given video clip can be defined as the intensity and type of feeling or emotion (both are referred to as affect) that are expected to arise in the user while watching that clip. The availability of methodologies for automatically extracting this type of video content will extend the current scope of possibilities for video indexing and retrieval. For instance, we will be able to search for the funniest or the most thrilling parts of a movie, or the most exciting events of a sport program. Furthermore, as the user may want to select a movie not only based on its genre, cast, director and story content, but also on its prevailing mood, the affective content analysis is also likely to contribute to enhancing the quality of personalizing the video delivery to the user. We propose in this paper a computational framework for affective video content representation and modeling. This framework is based on the dimensional approach to affect that is known from the field of psychophysiology. According to this approach, the affective video content can be represented as a set of points in the two-dimensional (2-D) emotion space that is characterized by the dimensions of arousal (intensity of affect) and valence (type of affect). We map the affective video content onto the 2-D emotion space by using the models that link the arousal and valence dimensions to low-level features extracted from video data. This results in the arousal and valence time curves that, either considered separately or combined into the so-called affect curve, are introduced as reliable representations of expected transitions from one feeling to another along a video, as perceived by a viewer.
2114292603;OWL: Yet to arrive on the Web of Data?;2012.0;[];Seven years on from OWL becoming a W3C recommendation, and two years on from the more recent OWL 2 W3C recommendation, OWL has still experienced only patchy uptake on the Web. Although certain OWL features (like owl:sameAs) are very popular, other features of OWL are largely neglected by publishers in the Linked Data world. This may suggest that despite the promise of easy implementations and the proposal of tractable profiles suggested in OWL’s second version, there is still no “right” standard fragment for the Linked Data community. In this paper, we (1) analyse uptake of OWL on the Web of Data, (2) gain insights into the OWL fragment that is actually used/usable on the Web, where we arrive at the conclusion that this fragment is likely to be a simplified profile based on OWL RL, (3) propose and discuss such a new fragment, which we call OWL LD (for Linked Data).
2114686809;Semantic Multidimensional Scaling for Open-Domain Sentiment Analysis;2014.0;[];The ability to understand natural language text is far from being emulated in machines. One of the main hurdles to overcome is that computers lack both the common and common-sense knowledge that humans normally acquire during the formative years of their lives. To really understand natural language, a machine should be able to comprehend this type of knowledge, rather than merely relying on the valence of keywords and word co-occurrence frequencies. In this article, the largest existing taxonomy of common knowledge is blended with a natural-language-based semantic network of common-sense knowledge. Multidimensional scaling is applied on the resulting knowledge base for open-domain opinion mining and sentiment analysis.
2114690005;Exploratory evaluations of a computer game supporting cognitive behavioural therapy for adolescents;2011.0;[];"The need to provide effective mental health treatments for adolescents has been described as a global public health challenge [27]. In this paper we discuss the exploratory evaluations of the first adolescent intervention to fully integrate a computer game implementing Cognitive Behavioural Therapy. Three distinct studies are presented: a detailed evaluation in which therapists independent of the design team used the game with 6 adolescents experiencing clinical anxiety disorders; a study in which a member of the design team used the game with 15 adolescents; and finally a study assessing the acceptability of the game and intervention with 216 practicing therapists. Findings are presented within the context of a framework for the design and evaluation of complex health interventions. The paper provides an in-depth insight into the use of therapeutic games to support adolescent interventions and provides stronger evidence than previously available for both their effectiveness and acceptability to stakeholders."
2114884316;Cyber Security Risks Assessment with Bayesian Defense Graphs and Architectural Models;2009.0;[];To facilitate rational decision making regarding cyber security investments, decision makers need to be able to assess expected losses before and after potential investments. This paper presents a model based assessment framework for analyzing the cyber security provided by different architectural scenarios. The framework uses the Bayesian statistics based Extended Influence Diagrams to express attack graphs and related countermeasures. In this paper it is demonstrated how this structure can be captured in an
2115023510;Mining the peanut gallery: opinion extraction and semantic classification of product reviews;2003.0;[];The web contains a wealth of product reviews, but sifting through them is a daunting task. Ideally, an opinion mining tool would process a set of search results for a given item, generating a list of product attributes (quality, features, etc.) and aggregating opinions about each of them (poor, mixed, good). We begin by identifying the unique properties of this problem and develop a method for automatically distinguishing between positive and negative reviews. Our classifier draws on information retrieval techniques for feature extraction and scoring, and the results for various metrics and heuristics vary depending on the testing situation. The best methods work as well as or better than traditional machine learning. When operating on individual sentences collected from web searches, performance is limited due to noise and ambiguity. But in the context of a complete web-based tool and aided by a simple method for grouping sentences into attributes, the results are qualitatively quite useful.
2115164934;On the Number of Complete Subgraphs Contained in Certain Graphs;1981.0;[];
2115739848;The Way They Move: Tracking Multiple Targets with Similar Appearance;2013.0;[];We introduce a computationally efficient algorithm for multi-object tracking by detection that addresses four main challenges: appearance similarity among targets, missing data due to targets being out of the field of view or occluded behind other objects, crossing trajectories, and camera motion. The proposed method uses motion dynamics as a cue to distinguish targets with similar appearance, minimize target mis-identification and recover missing data. Computational efficiency is achieved by using a Generalized Linear Assignment (GLA) coupled with efficient procedures to recover missing data and estimate the complexity of the underlying dynamics. The proposed approach works with track lets of arbitrary length and does not assume a dynamical model a priori, yet it captures the overall motion dynamics of the targets. Experiments using challenging videos show that this framework can handle complex target motions, non-stationary cameras and long occlusions, on scenarios where appearance cues are not available or poor.
2115752676;Learning cross-modality similarity for multinomial data;2011.0;[];Many applications involve multiple-modalities such as text and images that describe the problem of interest. In order to leverage the information present in all the modalities, one must model the relationships between them. While some techniques have been proposed to tackle this problem, they either are restricted to words describing visual objects only, or require full correspondences between the different modalities. As a consequence, they are unable to tackle more realistic scenarios where a narrative text is only loosely related to an image, and where only a few image-text pairs are available. In this paper, we propose a model that addresses both these challenges. Our model can be seen as a Markov random field of topic models, which connects the documents based on their similarity. As a consequence, the topics learned with our model are shared across connected documents, thus encoding the relations between different modalities. We demonstrate the effectiveness of our model for image retrieval from a loosely related text.
2116373735;Semantic Annotation and Retrieval of Music and Sound Effects;2008.0;[];We present a computer audition system that can both annotate novel audio tracks with semantically meaningful words and retrieve relevant tracks from a database of unlabeled audio content given a text-based query. We consider the related tasks of content-based audio annotation and retrieval as one supervised multiclass, multilabel problem in which we model the joint probability of acoustic features and words. We collect a data set of 1700 human-generated annotations that describe 500 Western popular music tracks. For each word in a vocabulary, we use this data to train a Gaussian mixture model (GMM) over an audio feature space. We estimate the parameters of the model using the weighted mixture hierarchies expectation maximization algorithm. This algorithm is more scalable to large data sets and produces better density estimates than standard parameter estimation techniques. The quality of the music annotations produced by our system is comparable with the performance of humans on the same task. Our ldquoquery-by-textrdquo system can retrieve appropriate songs for a large number of musically relevant words. We also show that our audition system is general by learning a model that can annotate and retrieve sound effects.
2116827549;Reliability optimization of series-parallel systems using a genetic algorithm;1996.0;[];A problem-specific genetic algorithm (GA) is developed and demonstrated to analyze series-parallel systems and to determine the optimal design configuration when there are multiple component choices available for each of several k-out-of-n:G subsystems. The problem is to select components and redundancy-levels to optimize some objective function, given system-level constraints on reliability, cost, and/or weight. Previous formulations of the problem have implicit restrictions concerning the type of redundancy allowed, the number of available component choices, and whether mixing of components is allowed. GA is a robust evolutionary optimization search technique with very few restrictions concerning the type or size of the design problem. The solution approach was to solve the dual of a nonlinear optimization problem by using a dynamic penalty function. GA performs very well on two types of problems: (1) redundancy allocation originally proposed by Fyffe, Hines, Lee, and (2) randomly generated problem with more complex k-out-of-n:G configurations.
2117345021;An Eigenstructure Method for Estimating DOA and Sensor Gain-Phase Errors;2011.0;[];In this paper, we consider the problem of direction of arrival (DOA) estimation in the presence of sensor gain-phase errors. Under some mild assumptions, we propose a new DOA estimation method based on the eigendecomposition of a covariance matrix which is constructed by the dot product of the array output vector and its conjugate. By combining the new DOA estimation with the conventional gain-phase error estimation, a method is proposed to simultaneously estimate the DOA and gain-phase errors without joint iteration. Theoretical analysis shows that the proposed method performs independently of phase errors and thus behaves well regardless of phase errors. However, the resolution capability of the proposed method is lower than that of the method in [A. J. Weiss and B. Friedlander, “Eigenstructure methods for direction finding with sensor gain and phase uncertainties,” Circuits Systems Signal Process., vol. 9, no. 3, pp. 271-300, 1990], named as the WF method. In order to improve the resolution capability and maintain phase error independence, a combined strategy is developed using the proposed and WF methods. The advantage of the proposed methods is that they are independent of phase errors, leading to the cancellation of phase error calibration during the operation life of an array. Moreover, the proposed methods avoid the problem of suboptimal convergence which occurs in the WF method. The drawbacks of the proposed methods are their high computational complexity and their requirement for the condition that at least two signals are spatially far from each other, and they are not applicable to a linear array. Simulation results verify the effectiveness of the proposed methods.
2118313703;Dynamic demand fulfillment in spare parts networks with multiple customer classes;2013.0;[];We study real-time demand fulfillment for networks consisting of multiple local warehouses, where spare parts of expensive technical systems are kept on stock for customers with different service contracts. Each service contract specifies a maximum response time in case of a failure and hourly penalty costs for contract violations. Part requests can be fulfilled from multiple local warehouses via a regular delivery, or from an external source with ample capacity via an expensive emergency delivery. The objective is to minimize delivery cost and penalty cost by smartly allocating items from the available network stock to arriving part requests. We propose a dynamic allocation rule that belongs to the class of one-step lookahead policies. To approximate the optimal relative cost, we develop an iterative calculation scheme that estimates the expected total cost over an infinite time horizon, assuming that future demands are fulfilled according to a simple static allocation rule. In a series of numerical experiments, we compare our dynamic allocation rule with the optimal allocation rule, and a simple but widely used static allocation rule. We show that the dynamic allocation rule has a small optimality gap and that it achieves an average cost reduction of 7.9% compared to the static allocation rule on a large test bed containing problem instances of real-life size.
2118674552;Personalized recommendation via cross-domain triadic factorization;2013.0;[];Collaborative filtering (CF) is a major technique in recommender systems to help users find their potentially desired items. Since the data sparsity problem is quite commonly encountered in real-world scenarios, Cross-Domain Collaborative Filtering (CDCF) hence is becoming an emerging research topic in recent years. However, due to the lack of sufficient dense explicit feedbacks and even no feedback available in usersu0027 uninvolved domains, current CDCF approaches may not perform satisfactorily in user preference prediction. In this paper, we propose a generalized Cross Domain Triadic Factorization (CDTF) model over the triadic relation user-item-domain, which can better capture the interactions between domain-specific user factors and item factors. In particular, we devise two CDTF algorithms to leverage user explicit and implicit feedbacks respectively, along with a genetic algorithm based weight parameters tuning algorithm to trade off influence among domains optimally. Finally, we conduct experiments to evaluate our models and compare with other state-of-the-art models by using two real world datasets. The results show the superiority of our models against other comparative models.
2118794032;Incorporating Workflow Interference in Facility Layout Design: The Quartic Assignment Problem;2002.0;[];Although many authors have noted the importance of minimizing workflow interference in facility layout design, traditional layout research tends to focus on minimizing the distance-based transportation cost. This paper formalizes the concept of workflow interference from a facility layout perspective. A model, formulated as a quartic assignment problem, is developed that explicitly considers the interference of workflow. Optimal and heuristic solution methodologies are developed and evaluated.
2119565742;The Google file system;2003.0;[];We have designed and implemented the Google File System, a scalable distributed file system for large distributed data-intensive applications. It provides fault tolerance while running on inexpensive commodity hardware, and it delivers high aggregate performance to a large number of clients. While sharing many of the same goals as previous distributed file systems, our design has been driven by observations of our application workloads and technological environment, both current and anticipated, that reflect a marked departure from some earlier file system assumptions. This has led us to reexamine traditional choices and explore radically different design points. The file system has successfully met our storage needs. It is widely deployed within Google as the storage platform for the generation and processing of data used by our service as well as research and development efforts that require large data sets. The largest cluster to date provides hundreds of terabytes of storage across thousands of disks on over a thousand machines, and it is concurrently accessed by hundreds of clients. In this paper, we present file system interface extensions designed to support distributed applications, discuss many aspects of our design, and report measurements from both micro-benchmarks and real world use.
2119605622;Speeded-Up Robust Features (SURF);2008.0;[];"This article presents a novel scale- and rotation-invariant detector and descriptor, coined SURF (Speeded-Up Robust Features). SURF approximates or even outperforms previously proposed schemes with respect to repeatability, distinctiveness, and robustness, yet can be computed and compared much faster. This is achieved by relying on integral images for image convolutions; by building on the strengths of the leading existing detectors and descriptors (specifically, using a Hessian matrix-based measure for the detector, and a distribution-based descriptor); and by simplifying these methods to the essential. This leads to a combination of novel detection, description, and matching steps. The paper encompasses a detailed description of the detector and descriptor and then explores the effects of the most important parameters. We conclude the article with SURFu0027s application to two challenging, yet converse goals: camera calibration as a special case of image registration, and object recognition. Our experiments underline SURFu0027s usefulness in a broad range of topics in computer vision."
2120474114;Compressor tree synthesis on commercial high-performance FPGAs;2011.0;[];"Compressor trees are a class of circuits that generalizes multioperand addition and the partial product reduction trees of parallel multipliers using carry-save arithmetic. Compressor trees naturally occur in many DSP applications, such as FIR filters, and, in the more general case, their use can be maximized through the application of high-level transformations to arithmetically intensive data flow graphs. Due to the presence of carry-chains, it has long been thought that trees of 2- or 3-input carry-propagate adders are more efficient than compressor trees for FPGA synthesis; however, this is not the case. This article presents a heuristic for FPGA synthesis of compressor trees that outperforms adder trees and exploits carry-chains when possible. The experimental results show that, on average, the use of compressor trees can reduce critical path delay by 33p and 45p respectively, compared to adder trees synthesized on the Xilinx Virtex-5 and Altera Stratix III FPGAs."
2120578579;Existence results for multiple positive solutions of nonlinear higher order perturbed fractional differential equations with derivatives;2012.0;[];"In this work, we establish the existence of multiple positive solutions for a general higher order fractional differential equation with derivatives and a sign-changing Caratheodory perturbed  :[26],""n-1u003cα⩽n,n∈Nn-1u003cα⩽n,n∈N and n⩾3n⩾3 with 0u003cμ1u003cμ2u003c⋯u003cμn-2u003cμn-10u003cμ1u003cμ2u003c⋯u003cμn-2u003cμn-1 and n-3u003cμn-1u003cα-2,aj∈R,0u003cξ1u003cξ2u003c…u003cξm-2u003c1 satisfying 0u003c∑j=1m-2ajξjα-μn-1-1u003c1, DαDα is the standard Riemann–Liouville derivative, f∈C([0,1]×Rn,[0,+∞))f∈C([0,1]×Rn,[0,+∞)). This equation is viewed as a perturbation of a general higher order fractional differential equation, where the perturbed term g:[0,1]×Rn→(-∞,+∞)g:[0,1]×Rn→(-∞,+∞) only satisfies the global Caratheodory conditions, which implies that the perturbed effect of g on f is quite large so that the nonlinearity can tend to negative infinity at some singular points."
2120749565;The Complete Analysis of a Polynomial Factorization Algorithm over Finite Fields;2001.0;[];A unified treatment of parameters relevant to factoring polynomials over finite fields is given. The framework is based on generating functions for describing parameters of interest and on singularity analysis for extracting asymptotic values. An outcome is a complete analysis of the standard polynomial factorization chain that is based on the elimination of repeated factors, distinct degree factorization, and equal degree separation. Several basic statistics on polynomials over finite fields are obtained in the course of the analysis.
2120782266;A hypercube queueing model embedded into a genetic algorithm for ambulance deployment on highways;2007.0;[];The hypercube is a well-known descriptive model for planning server-to-customer systems. In the present study we adapt this model to analyze Emergency Medical Systems on highways involving partial backup and multiple dispatching of ambulances. The modified model is then embedded into a genetic algorithm to optimize the configuration and operation of the system. By embedding the hypercube into a genetic algorithm, we can support decisions, such as, determining the optimal districts for the system in order to optimize the mean performance measures. Computational results are analyzed applying the approach to the case study of an EMS operating on Brazilian highways.
2120873112;Podcasting: A new technological tool to facilitate good practice in higher education;2009.0;[];"The literature has frequently highlighted the usefulness of podcasting in higher education; however, there is an important gap between the theory on good practice in higher education and empirical studies about podcasting. With this in mind, we carried out an empirical study on an undergraduate degree course in Information Systems Management. The study consisted of the creation and broadcast of 13 podcasts, distributed over four months in which ninety distance students took part. The analysis follows the suggestions proposed in previous literature about the evaluation of technologies in a university learning environment. The findings, discussed within the framework of principles for good practice in higher education, suggest some interesting issues in distance courses, such as: (1) podcasting is a powerful tool as a complement to the traditional resources on a course, but not a substitute for them; (2) the characteristics of podcasting increase the impression of permanent contact between students and teachers, increasing studentsu0027 motivation; (3) the use of podcasting allows for a diverse range of student skills and learning methods. Other secondary findings are discussed and some suggestions for future research are proposed at the end of this paper."
2121214036;Defining Mobile Learning in the Higher Education Landscape.;2010.0;[];"Introduction The evolution of handheld portable devices and wireless technology has resulted in radical changes in the social and economic lifestyles of modern people. Today, many technological devices are produced in portable form and people have become accustomed to them. These devices are reshaping users daily lives in different ways. But the development of digital technologies has so far been limited to social communication and few people have regarded mobile learning as a core pedagogical activity in higher institutions of learning. Although this model has been used as a minor adjunct to learning activities such as lectures and assignments, it is still not the primary mode of delivery in higher education. Currently, the instructional technology transmitted by means of mobile technology is mainly social and, to a lesser extent, economic. Advanced mobile devices such as ""smart"" cellular telephones are very popular among people primarily because they are wireless and portable. These functionalities enable users to communicate while on the move. The popularity of these devices is therefore a consequent of their ability to function at multiple levels. Moreover, the intense commercial competitiveness in the mobile device industry is forcing manufacturers to be very innovative, constantly striving to introduce new features that can give them a competitive edge. Against this backdrop, visionary educators, designers and developers should begin to consider the implications of these devices for the modern teaching and learning environment. In such an environment, contents and services can be relayed to a university student by personal wireless mobile devices. This will add another layer to the personal computer-based model of teaching and learning. This also means e-learning will take place in conditions that will be radically different from those educators and learners are familiar with. Providing university students with services, content instruction and information outside the traditional learning space is becoming more acceptable among education providers who predicate their services on the routine use of advanced information and communication technologies. This article seeks to provide a comprehensive definition of mobile learning and attempts to understand why actual learning practices are changing very rapidly while the learning theories that support educational practices are not. To find viable answers, the article will describe the different components of mobile learning that reflect on the increasing mobility of learners, learning and learner technology. The emergence of revolutionary technologies has had a significant impact on educational technology. It has increased the potential of e-learning as a mode of delivery in education. By definition, mobile learning (or ""m- learning"") is learning by means of wireless technological devices that can be pocketed and utilised wherever the learneru0027s device is able to receive unbroken transmission signals (Attewell u0026 Savill-Smith, 2005). For example, Laouris and Eteokleous (2005) have reiterated the need for a definition of mobile learning that takes into account all the aspects of the mobile learning process Nyir (2002) has also contributed to a philosophy of mobile learning that relies on Deweyu0027s insights into democracy and education. Nyir and his contemporaries argue that mobile devices are responsible for undermining and, in many cases, eliminating the fixity of traditional classrooms such as lecture halls, laboratories and all the paraphernalia of traditional education. For decades, these traditional spaces have depended on static models of communication and devices for subject delivery. Significantly, mobile devices are revolutionary because they transcend the boundaries of the structural stasis of classrooms and lecture halls and their associated modes of communication--they do not have to be confined to one particular place in order to be effective. …"
2122111042;Nearest neighbor pattern classification;1967.0;[];The nearest neighbor decision rule assigns to an unclassified sample point the classification of the nearest of a set of previously classified points. This rule is independent of the underlying joint distribution on the sample points and their classifications, and hence the probability of error R of such a rule must be at least as great as the Bayes probability of error R^{\ast} --the minimum probability of error over all decision rules taking underlying probability structure into account. However, in a large sample analysis, we will show in the M -category case that R^{\ast} \leq R \leq R^{\ast}(2 --MR^{\ast}/(M-1)) , where these bounds are the tightest possible, for all suitably smooth underlying distributions. Thus for any number of categories, the probability of error of the nearest neighbor rule is bounded above by twice the Bayes probability of error. In this sense, it may be said that half the classification information in an infinite sample set is contained in the nearest neighbor.
2122129277;SMICloud: A Framework for Comparing and Ranking Cloud Services;2011.0;[];With the growth of Cloud Computing, more and more companies are offering different cloud services. From the customeru0027s point of view, it is always difficult to decide whose services they should use, based on usersu0027 requirements. Currently there is no software framework which can automatically index cloud providers based on their needs. In this work, we propose a framework and a mechanism, which measure the quality and prioritize Cloud services. Such framework can make significant impact and will create healthy competition among Cloud providers to satisfy their Service Level Agreement (SLA) and improve their Quality of Services (QoS).
2122642161;A Survey of Information-Centric Networking Research;2014.0;[];The current Internet architecture was founded upon a host-centric communication model, which was appropriate for coping with the needs of the early Internet users. Internet usage has evolved however, with most users mainly interested in accessing (vast amounts of) information, irrespective of its physical location. This paradigm shift in the usage model of the Internet, along with the pressing needs for, among others, better security and mobility support, has led researchers into considering a radical change to the Internet architecture. In this direction, we have witnessed many research efforts investigating Information-Centric Networking (ICN) as a foundation upon which the Future Internet can be built. Our main aims in this survey are: (a) to identify the core functionalities of ICN architectures, (b) to describe the key ICN proposals in a tutorial manner, highlighting the similarities and differences among them with respect to those core functionalities, and (c) to identify the key weaknesses of ICN proposals and to outline the main unresolved research challenges in this area of networking research.
2122841972;Personalizing search via automated analysis of interests and activities;2005.0;[];We formulate and study search algorithms that consider a useru0027s prior interactions with a wide variety of content to personalize that useru0027s current Web search. Rather than relying on the unrealistic assumption that people will precisely specify their intent when searching, we pursue techniques that leverage implicit information about the useru0027s interests. This information is used to re-rank Web search results within a relevance feedback framework. We explore rich models of user interests, built from both search-related information, such as previously issued queries and previously visited Web pages, and other information about the user such as documents and email the user has read and created. Our research suggests that rich representations of the user and the corpus are important for personalization, but that it is possible to approximate these representations and provide efficient client-side algorithms for personalizing search. We show that such personalization algorithms can significantly improve on current Web search.
2122887675;Evaluating Mobility Pattern Space Routing for DTNs;2006.0;[];Because a delay tolerant network (DTN) can often be partitioned, routing is a challenge. However, routing benefits considerably if one can take advantage of knowledge concerning node mobility. This paper addresses this problem with a generic algorithm based on the use of a high-dimensional Euclidean space, that we call MobySpace, constructed upon nodesu0027 mobility patterns. We provide here an analysis and a large scale evaluation of this routing scheme in the context of ambient networking by replaying real mobility traces. The specific MobySpace evaluated is based on the frequency of visits of nodes to each possible location. We show that routing based on MobySpace can achieve good performance compared to that of a number of standard algorithms, especially for nodes that are present in the network a large portion of the time. We determine that the degree of homogeneity of node mobility patterns has a high impact on routing. And finally, we study the ability of nodes to learn their own mobility patterns.
2123235204;Link Prediction Across Multiple Social Networks;2010.0;[];The problem of link prediction has been studied extensively in literature. There are various versions of the link prediction problem \textit{e.g.,} link existence problem, link removal problem, predicting edge weights over time etc. In this paper we describe a new type of link prediction problem called the Inter-network link-prediction problem where the task is to predict links \textit{across} different networks. Thus given a set of nodes which participate in multiple networks the task is to determine if one can predict the edges that occur in one network by only using node attribute and edge information from other networks. We use insights from theories of evolution of social communication networks and the MTML framework to derive models which can be used to make link predictions across networks. For the experiments data from different \textit{types} of social networks from a Massively Multiplayer Online Role Playing Game (MMORPG) is used.
2123358761;Data-Flow Transformations to Maximize the Use of Carry-Save Representation in Arithmetic Circuits;2008.0;[];The increasing importance of datapath circuits in complex systems-on-chip calls for special arithmetic optimizations. The goal is to automatically achieve the handcrafted results which escape classic logic optimizations. Some work has been done in the recent years to infer the use of the carry-save representation in the synthesis of arithmetic circuits. Yet, many cases of practical interest cannot be handled due to the scattering of logic operations among the arithmetic ones - particularly in arithmetic computations which are originally described at the bit level in high-level languages such as C. We therefore introduce an algorithm to restructure dataflow graphs so that they can be synthesized as high-quality arithmetic circuits, close to those that an expert designer would conceive. On typical embedded software benchmarks which could be advantageously implemented with hardware accelerators, our technique always reduces tangibly the critical path by up to 46% and generally achieves the quality of manual implementations. In many cases, our algorithm also manages to reduce the cell area by up to 10%-20%.
2123682012;Opposition-Based Differential Evolution;2008.0;[];Evolutionary algorithms (EAs) are well-known optimization approaches to deal with nonlinear and complex problems. However, these population-based algorithms are computationally expensive due to the slow nature of the evolutionary process. This paper presents a novel algorithm to accelerate the differential evolution (DE). The proposed opposition-based DE (ODE) employs opposition-based learning (OBL) for population initialization and also for generation jumping. In this work, opposite numbers have been utilized to improve the convergence rate of DE. A comprehensive set of 58 complex benchmark functions including a wide range of dimensions is employed for experimental verification. The influence of dimensionality, population size, jumping rate, and various mutation strategies are also investigated. Additionally, the contribution of opposite numbers is empirically verified. We also provide a comparison of ODE to fuzzy adaptive DE (FADE). Experimental results confirm that the ODE outperforms the original DE and FADE in terms of convergence speed and solution accuracy.
2124849328;Organic Computing Addressing Complexity by Controlled Self-Organization;2006.0;[];In the past, the focus of the computer industry has been to improve hardware performance and add more and more features to the software. As a result, more and more appliances surrounding us are equipped with embedded computational power and wireless communication. As such, they become ever more flexible and multifunctional, and almost indispensable in daily life. On the other hand, the resulting systems become increasingly complex and unreliable, posing new challenges to designer and user. Organic Computing (OC) has the vision to address the challenges of complex distributed systems by making them more life-like (organic), i.e. endowing them with abilities such as self- organization, self-configuration, self-repair, or adaptation. The designeru0027s task is simplified, because it is no longer necessary to exactly specify the low-level system behavior in all possible situations that might occur, but instead leaving the system with a certain degree of freedom which allows it to react in an intelligent way to new situations. Also, use of such systems is simplified, as they can be controlled by setting few high-level goals, rather than having to manipulate many low-level parameters with unclear influence. In this paper, we give a general introduction to OC, and propose a generic observer-controller architecture as a framework for designing OC systems. Then, it is shown how to use this architecture at the example of a traffic light controller. The paper concludes with a summary and a discussion of future challenges.
2124903139;Early visual responses predict conscious face perception within and between subjects during binocular rivalry;2013.0;[];Previous studies indicate that conscious face perception may be related to neural activity in a large time window around 170-800 msec after stimulus presentation, yet in the majority of these studies changes in conscious experience are confounded with changes in physical stimulation. Using multivariate classification on MEG data recorded when participants reported changes in conscious perception evoked by binocular rivalry between a face and a grating, we showed that only MEG signals in the 120-320 msec time range, peaking at the M170 around 180 msec and the P2m at around 260 msec, reliably predicted conscious experience. Conscious perception could not only be decoded significantly better than chance from the sensors that showed the largest average difference, as previous studies suggest, but also from patterns of activity across groups of occipital sensors that individually were unable to predict perception better than chance. In addition, source space analyses showed that sources in the early and late visual system predicted conscious perception more accurately than frontal and parietal sites, although conscious perception could also be decoded there. Finally, the patterns of neural activity associated with conscious face perception generalized from one participant to another around the times of maximum prediction accuracy. Our work thus demonstrates that the neural correlates of particular conscious contents here, faces are highly consistent in time and space within individuals and that these correlates are shared to some extent between individuals.
2125031621;Neural Word Embedding as Implicit Matrix Factorization;2014.0;[];We analyze skip-gram with negative-sampling (SGNS), a word embedding method introduced by Mikolov et al., and show that it is implicitly factorizing a word-context matrix, whose cells are the pointwise mutual information (PMI) of the respective word and context pairs, shifted by a global constant. We find that another embedding method, NCE, is implicitly factorizing a similar matrix, where each cell is the (shifted) log conditional probability of a word given its context. We show that using a sparse Shifted Positive PMI word-context matrix to represent words improves results on two word similarity tasks and one of two analogy tasks. When dense low-dimensional vectors are preferred, exact factorization with SVD can achieve solutions that are at least as good as SGNSu0027s solutions for word similarity tasks. On analogy questions SGNS remains superior to SVD. We conjecture that this stems from the weighted nature of SGNSu0027s factorization.
2125119150;An Empirical Examination of the Antecedents and Consequences of Contribution Patterns in Crowd-Funded Markets;2013.0;[];Crowd-funded markets have recently emerged as a novel source of capital for entrepreneurs. As the economic potential of these markets is now being realized, they are beginning to go mainstream, a trend reflected by the explicit attention crowdfunding has received in the American Jobs Act as a potential avenue for economic growth, as well as the recent focus that regulators such as the U.S. Securities and Exchange Commission have placed upon it. Although the formulation of regulation and policy surrounding crowd-funded markets is becoming increasingly important, the behavior of crowdfunders, an important aspect that must be considered in this formulation effort, is not yet well understood. A key factor that can influence the behavior of crowd funders is information on prior contribution behavior, including the amount and timing of others’ contributions, which is published for general consumption. With that in mind, in this study, we empirically examine social influence in a crowd-funded marketplace for online journalism projects, employing a unique data set that incorporates contribution events and Web traffic statistics for approximately 100 story pitches. This data set allows us to examine both the antecedents and consequences of the contribution process. First, noting that digital journalism is a form of public good, we evaluate the applicability of two competing classes of economic models that explain private contribution toward public goods in the presence of social information: substitution models and reinforcement models. We also propose a new measure that captures both the amount and the timing of others’ contribution behavior: contribution frequency (dollars per unit time). We find evidence in support of a substitution model, which suggests a partial crowding-out effect, where contributors may experience a decrease in their marginal utility from making a contribution as it becomes less important to the recipient. Further, we find that the duration of funding and, more importantly, the degree of exposure that a pitch receives over the course of the funding process, are positively associated with readership upon the story’s publication. This appears to validate the widely held belief that a key benefit of the crowdfunding model is the potential it offers for awareness and attention-building around causes and ventures. This last aspect is a major contribution of the study, as it demonstrates a clear linkage between marketing effort and the success of crowd-funded projects.
2125239197;A DTN wireless sensor network for wildlife habitat monitoring;2010.0;[];This paper reports on a preliminary study of applying a wireless sensor network using DTN (Delay Tolerant Network) to sample the current status of a certain wildlife mammal with the name of White Tail Deer (Odocoileus virginianus) in the WMU Area 47 Ontario, Canada North of Parry Sound district. System architecture is proposed to obtain the highest monitoring range achievable with the least amount of sensors. With this method we can obtain the status of white tail deer, and detect other kinds of animals as well. The current method for surveying the white tail deer needs to be reviewed in order to obtain a most frequent status report of this important animal. By using DTN we expect to obtain disruption support capabilities in our sensor network. Furthermore, we simulated the wildlife habitat monitoring system on the Planet Lab environment. Our future work will develop sensors, deploy them in the field, and create a DTN network for the monitoring of wildlife habitat.
2125268144;The Properties of Continuous Pythagorean Fuzzy Information;2016.0;[];In practical decision-making processes, we can utilize various types of fuzzy sets to express the uncertain and ambiguous information. However, we may encounter such the situations: the sum of the support membership degree and the against nonmembership degree to which an alternative satisfies a criterion provided by the decision maker may be bigger than 1 but their square sum is equal to or less than 1. The Pythagorean fuzzy sets PFS, as the generalization of the fuzzy sets, can be used to effectively deal with this issue. Therefore, to enrich the theory of PFS, it is very necessary to investigate the fundamental properties of Pythagorean fuzzy information. In this paper, we first describe the change values of Pythagorean fuzzy numbers PFNs, which are the basic components of PFSs, when considering them as variables. Then we divide all the change values into the eight regions by using the basic operations of PFNs. Finally, we develop several Pythagorean fuzzy functions and study their fundamental properties such as continuity, derivability, and differentiability in detail.
2125281549;Group Search Optimizer: An Optimization Algorithm Inspired by Animal Searching Behavior;2009.0;[];Nature-inspired optimization algorithms, notably evolutionary algorithms (EAs), have been widely used to solve various scientific and engineering problems because of to their simplicity and flexibility. Here we report a novel optimization algorithm, group search optimizer (GSO), which is inspired by animal behavior, especially animal searching behavior. The framework is mainly based on the producer-scrounger model, which assumes that group members search either for ldquofindingrdquo (producer) or for ldquojoiningrdquo (scrounger) opportunities. Based on this framework, concepts from animal searching behavior, e.g., animal scanning mechanisms, are employed metaphorically to design optimum searching strategies for solving continuous optimization problems. When tested against benchmark functions, in low and high dimensions, the GSO algorithm has competitive performance to other EAs in terms of accuracy and convergence speed, especially on high-dimensional multimodal problems. The GSO algorithm is also applied to train artificial neural networks. The promising results on three real-world benchmark problems show the applicability of GSO for problem solving.
2125844878;Network analysis of massively collaborative creation of multimedia contents: case study of hatsune miku videos on nico nico douga;2008.0;[];The World Wide Web supports new styles of creative activities. For this study, we investigate massively collaborative creation via the Web, by which numerous people gather to evolve their works collaboratively. Nico Nico Douga is a video sharing website, where many videos are created collaboratively. We specifically examine Hatsune Miku, a version of singing synthesizer application software that has inspired not only song creation but also songwriting, illustration, and video editing. As described herein, different types of creators interact mutually to create new contents though their social network. Using tags, we classified videos and creators on Nico Nico Douga automatically into four basic categories. Thereby, we produced a social network from relationships among videos and creators by analyzing videosu0027 descriptions. The social network reveals interesting features. Different categories of creators serve different roles in evolving the network, e.g., songwriters gather more links than other categories, implying that they are triggers to network evolution. We also extracted communities from the network and observed different community structures. One is a centralized network in which a single songwriter is central and others are peripheral. The other is a messier network, in which some illustrators are central, but the centrality is weak.
2125894968;Evolutionary development: a model for the design, implementation, and evaluation of ICT for education programmes;2012.0;[];"The impact of information and communication technology (ICT) in primary and secondary education is still an open question. Following review of the available literature, we classify the causes of the lack of impact on studentsu0027 attainment in four dimensions: (1) the design and implementation of ICT in educational settings; (2) the evaluation of its impact; (3) the scaling up of these kinds of innovations; and (4) the cost-effectiveness of technology-enhanced learning environments. Based on this evidence, we proposed the evolutionary development model (EDM), which aims to produce a cost-effective and sustainable ICT for education (ICT4E) programme in three steps: efficacy, effectiveness, and efficiency. In each step, one component of the programme is built and validated in real educational settings. Therefore, the resultant ICT4E programme is ready to be replicated across the school system. We also show how the EDM guided the development of a programme based on mobile computer supported collaborative learning, known as Eduinnova. Finally, we discuss how EDM can serve as an analysis tool for researchers and policy makers. © 2012 Wiley Periodicals, Inc."
2125994488;Collision Sensing by Stereo Vision and Radar Sensor Fusion;2009.0;[];To take advantage of both stereo cameras and radar, this paper proposes a fusion approach to accurately estimate the location, size, pose, and motion information of a threat vehicle with respect to a host one from observations that are obtained by both sensors. To do that, we first fit the contour of a threat vehicle from stereo depth information and find the closest point on the contour from the vision sensor. Then, the fused closest point is obtained by fusing radar observations and the vision closest point. Next, by translating the fitted contour to the fused closest point, the fused contour is obtained. Finally, the fused contour is tracked by using rigid body constraints to estimate the location, size, pose, and motion of the threat vehicle. Experimental results from both synthetic data and real-world road test data demonstrate the success of the proposed algorithm.
2126087318;Energy-efficient link adaptation in frequency-selective channels;2010.0;[];"Energy efficiency is becoming increasingly important for small form factor mobile devices, as battery technology has not kept up with the growing requirements stemming from ubiquitous multimedia applications. This paper addresses link adaptive transmission for maximizing energy efficiency, as measured by the ""throughput per Joule"" metric. In contrast to the existing water-filling power allocation schemes that maximize throughput subject to a fixed overall transmit power constraint, our scheme maximizes energy efficiency by adapting both overall transmit power and its allocation, according to the channel states and the circuit power consumed. We demonstrate the existence of a unique globally optimal link adaptation solution and develop iterative algorithms to obtain it. We further consider the special case of flat-fading channels to develop an upper bound on energy efficiency and to characterize its variation with bandwidth, channel gain and circuit power. Our results for OFDM systems demonstrate improved energy savings with energy optimal link adaptation as well as illustrate the fundamental tradeoff between energy-efficient and spectrum-efficient transmission."
2126105956;A fast and elitist multiobjective genetic algorithm: NSGA-II;2002.0;[];"Multi-objective evolutionary algorithms (MOEAs) that use non-dominated sorting and sharing have been criticized mainly for: (1) their O(MN/sup 3/) computational complexity (where M is the number of objectives and N is the population size); (2) their non-elitism approach; and (3) the need to specify a sharing parameter. In this paper, we suggest a non-dominated sorting-based MOEA, called NSGA-II (Non-dominated Sorting Genetic Algorithm II), which alleviates all of the above three difficulties. Specifically, a fast non-dominated sorting approach with O(MN/sup 2/) computational complexity is presented. Also, a selection operator is presented that creates a mating pool by combining the parent and offspring populations and selecting the best N solutions (with respect to fitness and spread). Simulation results on difficult test problems show that NSGA-II is able, for most problems, to find a much better spread of solutions and better convergence near the true Pareto-optimal front compared to the Pareto-archived evolution strategy and the strength-Pareto evolutionary algorithm - two other elitist MOEAs that pay special attention to creating a diverse Pareto-optimal front. Moreover, we modify the definition of dominance in order to solve constrained multi-objective problems efficiently. Simulation results of the constrained NSGA-II on a number of test problems, including a five-objective, seven-constraint nonlinear problem, are compared with another constrained multi-objective optimizer, and the much better performance of NSGA-II is observed."
2126236168;Remote Sensing of Mangrove Ecosystems: A Review;2011.0;[];Mangrove ecosystems dominate the coastal wetlands of tropical and subtropical regions throughout the world. They provide various ecological and economical ecosystem services contributing to coastal erosion protection, water filtration, provision of areas for fish and shrimp breeding, provision of building material and medicinal ingredients, and the attraction of tourists, amongst many other factors. At the same time, mangroves belong to the most threatened and vulnerable ecosystems worldwide and experienced a dramatic decline during the last half century. International programs, such as the Ramsar Convention on Wetlands or the Kyoto Protocol, underscore the importance of immediate protection measures and conservation activities to prevent the further loss of mangroves. In this context, remote sensing is the tool of choice to provide spatio-temporal information on mangrove ecosystem distribution, species differentiation, health status, and ongoing changes of mangrove populations. Such studies can be based on various sensors, ranging from aerial photography to high- and medium-resolution optical imagery and from hyperspectral data to active microwave (SAR) data. Remote-sensing techniques have demonstrated a high potential to detect, identify, map, and monitor mangrove conditions and changes during the last two decades, which is reflected by the large number of scientific papers published on this topic. To our knowledge, a recent review paper on the remote sensing of mangroves does not exist, although mangrove ecosystems have become the focus of attention in the context of current climate change and discussions of the services provided by these ecosystems. Also, climate change-related remote-sensing studies in coastal zones have increased drastically in recent years. The aim of this review paper is to provide a comprehensive overview and sound summary of all of the work undertaken, addressing the variety of remotely sensed data applied for mangrove ecosystem mapping, as well as the numerous methods and techniques used for data analyses, and to further discuss their potential and limitations.
2126441872;On Additive Bases and Harmonious Graphs;1980.0;[];This paper first considers several types of additive bases. A typical problem is to find $n_\gamma ( k )$, the largest n for which there exists a set $\{ 0 = a_1 u003c a_2 u003c \cdots u003c a_k \}$ of distinct integers modulo n such that each r in the range $0\leqq r \leqq n - 1$ can be written at least once as $r \equiv a_i + a_j $ (modulo n) with $i u003c j$. For example, $n_\gamma ( 8 ) = 24$, as illustrated by the set {0, 1, 2, 4, 8, 13, 18, 22}. The other problems arise if at least is changed to at most, or $i u003c j$ to $i\leqq j$, or if the words modulo n are omitted. Tables and bounds are given for each of these problems. Then a closely related graph labeling problem is studied. A connected graph with n edges is called harmonious if it is possible to label the vertices with distinct numbers (modulo n) in such a way that the edge sums are also distinct (modulo n). Some infinite families of graphs (odd cycles, ladders, wheels, $ \cdots $) are shown to be harmonious while others (even cycles, most complete or complete...
2126602679;Impact of Ambulance Dispatch Policies on Performance of Emergency Medical Services;2011.0;[];In ambulance location models, fleet size and ambulance location sites are two critical factors that emergency medical service (EMS) managers can control to ensure efficient delivery of the system. The ambulance relocation and dispatch policies that are studied in dynamic ambulance relocation models also significantly contribute to improving the response time of EMS. In this paper, we review dynamic ambulance relocation models from the perspective of dispatch policies. The connection between the reviewed ambulance dispatch policies and real-life policies is highlighted. Our ambulance model is based on the modified maximal covering location problem (MCLP). It is used to examine the commonly used dispatch policy and the proposed method of free-ambulance exploitation to further improve urgent call response time. Simulation results show that the proposed method can reduce the response time of urgent calls, especially during low-ambulance-supply period. We also compared the performance of EMS with and without reroute-enabled dispatch.
2126648306;Dirac's theorem for random graphs;2012.0;[];"A classical theorem of Dirac from 1952 asserts that every graph on n vertices with minimum degree at least  \documentclass{article} \usepackage{mathrsfs} \usepackage{amsmath} \pagestyle{empty} \begin{document} \begin{align*}\left\lceil n/2 \right\rceil\end{align*} \end{document} **image** is Hamiltonian. In this paper we extend this result to random graphs. Motivated by the study of resilience of random graph properties we prove that if p ≫ log n/n, then a.a.s. every subgraph of G(n,p) with minimum degree at least (1/2 + o (1))np is Hamiltonian. Our result improves on previously known bounds, and answers an open problem of Sudakov and Vu. Both, the range of edge probability p and the value of the constant 1/2 are asymptotically best possible. © 2012 Wiley Periodicals, Inc. Random Struct. Alg.,  by :[119],""NSF CAREER grant, :[119],""NSF grant DMS-1101185 and USA-Israel BSF grant.)"
2126693856;Permutation inference for the general linear model.;2014.0;[];Permutation methods can provide exact control of false positives and allow the use of non-standard statistics, making only weak assumptions about the data. With the availability of fast and inexpensive computing, their main limitation would be some lack of flexibility to work with arbitrary experimental designs. In this paper we report on results on approximate permutation methods that are more flexible with respect to the experimental design and nuisance variables, and conduct detailed simulations to identify the best method for settings that are typical for imaging research scenarios. We present a generic framework for permutation inference for complex general linear models (glms) when the errors are exchangeable and/or have a symmetric distribution, and show that, even in the presence of nuisance effects, these permutation inferences are powerful while providing excellent control of false positives in a wide range of common and relevant imaging research scenarios. We also demonstrate how the inference on glm parameters, originally intended for independent data, can be used in certain special but useful cases in which independence is violated. Detailed examples of common neuroimaging applications are provided, as well as a complete algorithm – the “randomise” algorithm – for permutation inference with the glm.
2126768050;Subnets of Proof-nets in multiplicative linear logic with MIX;1997.0;[];"This paper studies the properties of the subnets of a      :[10],""for     first-order Multiplicative     Linear Logic without propositional constants ( MLL  − ),     extended with the rule of Mix: from [vdash   ]Γ and [vdash   ]Δ infer     [vdash   ]Γ, Δ. Aspertiu0027s correctness criterion and its interpretation     in terms of concurrent processes are extended to the first-order case.     The notions of  kingdom  and  empire  of a formula are extended from      MLL  −  to  MLL  − + MIX .     A new proof of the sequentialization theorem is given. As a      a :[139],""system     of proof-nets is given :[10],""for De     Paiva and Hylandu0027s Full Intuitionistic Linear Logic with Mix; this     result gives a general     method :[10],""for translating Abramsky-style term assignments into proof-nets,     and  vice versa ."
2126779549;Pipelined compressor tree optimization using integer linear programming;2014.0;[];Compressor trees offer an effective realization of the multiple input addition needed by many arithmetic operations. However, mapping the commonly used carry save adders (CSA) of classical compressor trees to FPGAs suffers from a poor resource utilization. This can be enhanced by using generalized performance counters (GPCs). Prior work has shown that high efficient GPCs can be constructed by exploiting the low-level structure of the FPGA. However, due to their irregular shape, the selection of those is not straight forward. Furthermore, the compressor tree has to be pipelined to achieve the potential FPGA performance. Then, a selection between registered GPCs or flip-flops has to be done to balance the pipeline. This work defines the pipelined compressor tree synthesis as an optimization problem and proposes a (resource) optimal method using integer linear programming (ILP). Besides that, two new GPC mappings with high efficiency are proposed for Xilinx FPGAs.
2127309075;An integrated approach to correction for off-resonance effects and subject movement in diffusion MR imaging.;2016.0;[];"In this paper we describe a method for retrospective estimation and correction of eddy current (EC)-induced distortions and subject movement in diffusion imaging. In addition a susceptibility-induced field can be supplied and will be incorporated into the calculations in a way that accurately reflects that the two fields (susceptibility- and EC-induced) behave differently in the presence of subject movement. The method is based on registering the individual volumes to a model free prediction of what each volume should look like, thereby enabling its use on high b-value data where the contrast is vastly different in different volumes. In addition we show that the linear EC-model commonly used is insufficient for the data used in the present paper (high spatial and angular resolution data acquired with Stejskal–Tanner gradients on a 3 T Siemens Verio, a 3 T Siemens Connectome Skyra or a 7 T Siemens Magnetome scanner) and that a higher order model performs significantly  method is :[151],""already in extensive practical use and is used by four major projects (the WU-UMinn HCP, the MGH HCP, the UK Biobank and the Whitehall studies) to correct for distortions and subject movement."
2127473562;Automics: souvenir generating photoware for theme parks;2011.0;[];Automics is a photo-souvenir service which utilises mobile devices to support the capture, sharing and annotation of digital images amongst groups of visitors to theme parks. The prototype service mixes individual and group photo-capture with existing in-park, on-ride photo services, to allow users to create printed photo-stories. Herein we discuss initial fieldwork in theme parks that grounded the design of Automics, our development of the service prototype, and its real-world evaluation with theme park visitors. We relate our findings on user experience of the service to a literature on mobile photoware, finding implications for the design of souvenir services.
2127503699;Analysis and Comparison of Queues with Different Levels of Delay Information;2007.0;[];Information about delays can enhance service quality in many industries. Delay information can take many forms, with different degrees of precision. Different levels of information have different effects on customers and therefore on the overall system. To explore these effects, we consider a queue with balking under three levels of delay information: no information, partial information (the system occupancy), and full information (the exact waiting time). We assume Poisson arrivals, independent exponential service times, and a single server. Customers decide whether to stay or balk based on their expected waiting costs, conditional on the information provided. We show how to compute the key performance measures in the three systems, obtaining closed-form solutions for special cases. We then compare the three systems. We identify some important cases where more accurate delay information improves performance. In other cases, however, information can actually hurt the provider or the customers.
2127607093;Inside the atoms: ranking on a network of networks;2014.0;[];"Networks are prevalent and have posed many fascinating research questions. How can we spot similar users, e.g., virtual identical twins, in Cleveland for a New Yorker? Given a query disease, how can we prioritize its candidate genes by incorporating the tissue-specific protein interaction networks of those similar diseases? In most, if not all, of the existing network ranking methods, the nodes are the ranking objects with the finest granularity. In this paper, we propose a new network data model, a Network of Networks (NoN), where each node of the main network itself can be further represented as another (domain-specific) network. This new data model enables to compare the nodes in a broader context and rank them at a finer granularity. Moreover, such an NoN model enables much more efficient search when the ranking targets reside in a certain domain-specific network. We formulate ranking on NoN as a regularized optimization problem; propose efficient algorithms and provide theoretical analysis, such as optimality, convergence, complexity and equivalence. Extensive experimental evaluations demonstrate the effectiveness and the efficiency of our methods."
2127832194;Improving FPGA Performance for Carry-Save Arithmetic;2010.0;[];The selective use of carry-save arithmetic, where appropriate, can accelerate a variety of arithmetic-dominated circuits. Carry-save arithmetic occurs naturally in a variety of DSP applications, and further opportunities to exploit it can be exposed through systematic data flow transformations that can be applied by a hardware compiler. Field-programmable gate arrays (FPGAs), however, are not particularly well suited to carry-save arithmetic. To address this concern, we introduce the ?field programmable counter array? (FPCA), an accelerator for carry-save arithmetic intended for integration into an FPGA as an alternative to DSP blocks. In addition to multiplication and multiply accumulation, the FPCA can accelerate more general carry-save operations, such as multi-input addition (e.g., add k u003e 2 integers) and multipliers that have been fused with other adders. Our experiments show that the FPCA accelerates a wider variety of applications than DSP blocks and improves performance, area utilization, and energy consumption compared with soft FPGA logic.
2127931254;Bare bones particle swarms;2003.0;[];"The particle swarm algorithm has just enough moving parts to make it hard to understand. The formula is very simple, it is even easy to describe the working of the algorithm verbally, yet it is very difficult to grasp in oneu0027s mind how the particles oscillate around centers that are constantly changing; how they influence one another; how the various parameters affect the trajectory of the particle; how the topology of the swarm affects its performance; and so on. This paper strips away some traditional features of the particle swarm in the search for the properties that make it work. The particle swarm algorithm is modified by eliminating the velocity formula. Variations are compared. In the process some of the mysteries of the algorithm are revealed, we discover its similarity to other stochastic population-based problem solving methods, and new avenues of investigation are suggested or implied."
2128027813;Optimal circuits for parallel multipliers;1998.0;[];We present new design and analysis techniques for the synthesis of parallel multiplier circuits that have smaller predicted delay than the best current multipliers. V.G. Oklobdzija et al. (1996) suggested a new approach, the Three-Dimensional Method (TDM), for Partial Product Reduction Tree (PPRT) design that produces multipliers that outperform the current best designs. The goal of TDM is to produce a minimum delay PPRT using full adders. This is done by carefully modeling the relationship of the output delays to the input delays in an adder and, then, interconnecting the adders in a globally optimal way. Oklobdzija et al. suggested a good heuristic for finding the optimal PPRT, but no proofs about the performance of this heuristic were given. We provide a formal characterization of optimal PPRT circuits and prove a number of properties about them. For the problem of summing a set of input bits within the minimum delay, we present an algorithm that produces a minimum delay circuit in time linear in the size of the inputs. Our techniques allow us to prove tight lower bounds on multiplier circuit delays. These results are combined to create a program that finds optimal TDM multiplier designs. Using this program, we can show that, while the heuristic used by Oklobdzija et al. does not always find the optimal TDM circuit, it performs very well in terms of overall PPRT circuit delay. However, our search algorithms find better PPRT circuits for reducing the delay of the entire multiplier.
2128678576;The small-world phenomenon: an algorithmic perspective;2000.0;[];"Long a matter of folklore, the ``small-world phenomenonu0027u0027u0027u0027 --the principle that we are all linked by short chains of acquaintances --was inaugurated as an area of experimental study in the social sciences through the pioneering work of Stanley Milgram in the 1960u0027u0027s. This work was among the first to make the phenomenon quantitative, allowing people to speak of the ``six degrees of separationu0027u0027u0027u0027 between any two people in the United States. Since then, a number of network models have been proposed as frameworks in which to study the problem analytically. One of the most refined of these models was formulated in recent work of Watts and Strogatz; their framework provided compelling evidence that the small-world phenomenon is pervasive in a range of networks arising in nature and technology, and a fundamental ingredient in the evolution of the World Wide Web. But existing models are insufficient to explain the striking algorithmic component of Milgramu0027u0027s original findings: that individuals using local information are collectively very effective at actually constructing short paths between two points in a social network. Although recently proposed network models are rich in short paths, we prove that no decentralized algorithm, operating with local information only, can construct short paths in these networks with non-negligible probability. We then define an infinite family of network models that naturally generalizes the Watts-Strogatz model, and show that for one of these models, there is a decentralized algorithm capable of finding short paths with high probability. More generally, we provide a strong characterization of this family of network models, showing that there is in fact a unique model within the family for which decentralized algorithms are effective."
2128970394;Design and control of warehouse order picking: A literature review;2007.0;[];"Order picking has long been identified as the most labour-intensive and costly activity for almost every warehouse; the cost of order picking is estimated to be as much as 55% of the total warehouse operating expense. Any underperformance in order picking can lead to unsatisfactory service and high operational cost for the warehouse, and consequently for the whole supply chain. In order to operate efficiently, the order-picking process needs to be robustly designed and optimally controlled. This paper gives a literature overview on typical decision problems in design and control of manual order-picking processes. We focus on optimal (internal) layout design, storage assignment methods, routing methods, order batching and zoning. The research in this area has grown rapidly recently. Still, combinations of the above areas have hardly been explored. Order-picking system developments in practice lead to promising new research directions."
2129414564;Parametric Mixture Models for Multi-Labeled Text;2002.0;[];We propose probabilistic generative models, called parametric mixture models (PMMs), for multiclass, multi-labeled text categorization problem. Conventionally, the binary classification approach has been employed, in which whether or not text belongs to a category is judged by the binary classifier for every category. In contrast, our approach can simultaneously detect multiple categories of text using PMMs. We derive efficient learning and prediction algorithms for PMMs. We also empirically show that our method could significantly outperform the conventional binary methods when applied to multi-labeled text categorization using real World Wide Web pages.
2129526018;An investigation of the textural characteristics associated with gray level cooccurrence matrix statistical parameters;1995.0;[];The aim of this study was to investigate the statistical meaning of six GLCM (Gray Level Cooccurrence Matrix) parameters. This objective was mainly pursued by means of a selfcorrsistent, theoretical assessment in order to remain independent from test image. The six statistical parameters are energy, contrast, variance, correlation, entropy and inverse dzfference moment, which are considered the most relevant among the 14 originally proposed by Haralick et al.. The functional analysis supporting theoretical considerations was based on natural clustering in the feature space of segment texture values. The results show that among the six GLCM statistical parameters, five different sets can be identified, each set featuring a specific textural meaning. The first set contains energy and entropy, while the four remaining parameters can be regarded as belonging to four different sets. Two parameters, energy and contrust, are considered to be the most efficient for discriminating different textural patterns. A new GLCM statistical parameter, recursivity, is presented in order to replace energy which presents some degree of correlation with contrast. It is demonstrated that in some cases it may be reasonable to replace the computation of GLCM with that of GLDH (Gray Level Difference Histogram), in order to benefit by a better compromise between texture measurement accuracy, computer storage and computation time.
2129861682;Odessa: enabling interactive perception applications on mobile devices;2011.0;[];Resource constrained mobile devices need to leverage computation on nearby servers to run responsive applications that recognize objects, people, or gestures from real-time video. The two key questions that impact performance are what computation to offload, and how to structure the parallelism across the mobile device and server. To answer these questions, we develop and evaluate three interactive perceptual applications. We find that offloading and parallelism choices should be dynamic, even for a given application, as performance depends on scene complexity as well as environmental factors such as the network and device capabilities. To this end we develop Odessa, a novel, lightweight, runtime that automatically and adaptively makes offloading and parallelism decisions for mobile interactive perception applications. Our evaluation shows that the incremental greedy strategy of Odessa converges to an operating point that is close to an ideal offline partitioning. It provides more than a 3x improvement in application performance over partitioning suggested by domain experts. Odessa works well across a variety of execution environments, and is agile to changes in the network, device and application inputs.
2130196654;Summarizing local context to personalize global web search;2006.0;[];The PC Desktop is a very rich repository of personal information, efficiently capturing useru0027s interests. In this paper we propose a new approach towards an automatic personalization of web search in which the user specific information is extracted from such local desktops, thus allowing for an increased quality of user profiling, while sharing less private information with the search engine. More specifically, we investigate the opportunities to select personalized query expansion terms for web search using three different desktop oriented approaches: summarizing the entire desktop data, summarizing only the desktop documents relevant to each user query, and applying natural language processing techniques to extract dispersive lexical compounds from relevant desktop resources. Our experiments with the Google API showed at least the latter two techniques to produce a very strong improvement over current web search.
2130548493;Why do we need algorithmic historiography;2003.0;[];This article discusses the rationale for creating historiographs of scholarly topics using a new program called HistCiteTM, which produces a variety of analyses to aid the historian identify key events (papers), people (authors), and journals in a field. By creating a genealogic profile of the evolution, the program aids the scholar in evaluating the paradigm involved.
2130727150;Expokit: a software package for computing matrix exponentials;1998.0;[];Expokit provides a set of routines aimed at computing matrix exponentials. More precisely, it computes either a small matrix exponential in full, the action of a large sparse matrix exponential on an operand vector, or the solution of a system of linear OBEs with  constant inhomogeneity. The backbone of the sparse routines consists of matrix-free Krylov subspace projection methods (Arnoldi and Lanczos processes), and that is why the toolkit is capable of coping with sparse matrices of large dimension. The software handles real and complex matrices and provides specific routines for symmetric and Hermitian matrices. The computation of matrix exponentials is a numerical issue of critical importance in the area of Markov chains and furthermore, the computed solution is subject to  probabilistic constraints. In addition to addressing general matrix exponentials, a distinct attention is assigned to the computation of transient states of Markov chains.
2130834086;Challenges and Opportunities in Applied Machine Learning;2012.0;[];Machine learning research is often conducted in vitro, divorced from motivating practical applications. A researcher might develop a new method for the general task of classification, then assess its utility by comparing its performance (such as accuracy or AUC) to that of existing classification models on publicly available datasets. In terms of advancing machine learning as an academic discipline, this approach has thus far proven quite fruitful. However, it is our view that the most interesting open problems in machine learning are those that arise during its application to real-world problems. We illustrate this point by reviewing two of our interdisciplinary collaborations, both of which have posed unique machine learning problems, providing fertile ground for novel research.
2130942839;Sequence to Sequence Learning with Neural Networks;2014.0;[];Deep Neural Networks (DNNs) are powerful models that have achieved excellent performance on difficult learning tasks. Although DNNs work well whenever large labeled training sets are available, they cannot be used to map sequences to sequences. In this paper, we present a general end-to-end approach to sequence learning that makes minimal assumptions on the sequence structure. Our method uses a multilayered Long Short-Term Memory (LSTM) to map the input sequence to a vector of a fixed dimensionality, and then another deep LSTM to decode the target sequence from the vector. Our main result is that on an English to French translation task from the WMT-14 dataset, the translations produced by the LSTM achieve a BLEU score of 34.8 on the entire test set, where the LSTMu0027s BLEU score was penalized on out-of-vocabulary words. Additionally, the LSTM did not have difficulty on long sentences. For comparison, a phrase-based SMT system achieves a BLEU score of 33.3 on the same dataset. When we used the LSTM to rerank the 1000 hypotheses produced by the aforementioned SMT system, its BLEU score increases to 36.5, which is close to the previous state of the art. The LSTM also learned sensible phrase and sentence representations that are sensitive to word order and are relatively invariant to the active and the passive voice. Finally, we found that reversing the order of the words in all source sentences (but not target sentences) improved the LSTMu0027s performance markedly, because doing so introduced many short term dependencies between the source and the target sentence which made the optimization problem easier.
2131601980;Exploiting Rush Hours for Energy-Efficient Contact Probing in Opportunistic Data Collection;2011.0;[];In many potential wireless sensor network applications, the cost of the base station infrastructure can be prohibitive. Instead, we consider the use of mobile devices carried by people in their daily life to collect sensor data opportunistically. As the movement of these mobile nodes is, by definition, uncontrolled, contact probing becomes a challenging task, particularly for sensor nodes which need to be aggressively duty-cycled to achieve long life. It has been reported that when the duty-cycle of a sensor node is fixed, SNIP, a sensor node-initiated probing mechanism, performs much better than mobile node-initiated probing mechanisms. Considering that the intended applications are delay-tolerant, mobile nodes tend to follow some repeated mobility patterns, and contacts are distributed unevenly in temporal, SNIP-RH is proposed in this paper to further improve the performance of contact probing through exploiting Rush Hours during which contacts arrive more frequently. In SNIP-RH, SNIP is activated only when the time is within Rush Hours and there are enough data to be uploaded in the next probed contact. As for the duty-cycle, it is selected based on the mean of contact length that is learned on line. Both analysis and simulation results indicate that under a typical simulated road-side wireless sensor network scenario, SNIP-RH can significantly reduce the energy consumed for probing the contacts, that are necessary for uploading the sensed data, or significantly increase the probed contact capacity under a sensor nodeu0027s energy budget for contact probing.
2131613989;Comprehensive learning particle swarm optimizer for global optimization of multimodal functions;2006.0;[];This paper presents a variant of particle swarm optimizers (PSOs) that we call the comprehensive learning particle swarm optimizer (CLPSO), which uses a novel learning strategy whereby all other particlesu0027 historical best information is used to update a particleu0027s velocity. This strategy enables the diversity of the swarm to be preserved to discourage premature convergence. Experiments were conducted (using codes available from http://www.ntu.edu.sg/home/epnsugan) on multimodal test functions such as Rosenbrock, Griewank, Rastrigin, Ackley, and Schwefel and composition functions both with and without coordinate rotation. The results demonstrate good performance of the CLPSO in solving multimodal problems when compared with eight other recent variants of the PSO.
2131747574;Large Displacement Optical Flow: Descriptor Matching in Variational Motion Estimation;2011.0;[];Optical flow estimation is classically marked by the requirement of dense sampling in time. While coarse-to-fine warping schemes have somehow relaxed this constraint, there is an inherent dependency between the scale of structures and the velocity that can be estimated. This particularly renders the estimation of detailed human motion problematic, as small body parts can move very fast. In this paper, we present a way to approach this problem by integrating rich descriptors into the variational optical flow setting. This way we can estimate a dense optical flow field with almost the same high accuracy as known from variational optical flow, while reaching out to new domains of motion analysis where the requirement of dense sampling in time is no longer satisfied.
2131876387;A Latent Semantic Model with Convolutional-Pooling Structure for Information Retrieval;2014.0;[];In this paper, we propose a new latent semantic model that incorporates a convolutional-pooling structure over word sequences to learn low-dimensional, semantic vector representations for search queries and Web documents. In order to capture the rich contextual structures in a query or a document, we start with each word within a temporal context window in a word sequence to directly capture contextual features at the word n-gram level. Next, the salient word n-gram features in the word sequence are discovered by the model and are then aggregated to form a sentence-level feature vector. Finally, a non-linear transformation is applied to extract high-level semantic information to generate a continuous vector representation for the full text string. The proposed convolutional latent semantic model (CLSM) is trained on clickthrough data and is evaluated on a Web document ranking task using a large-scale, real-world data set. Results show that the proposed model effectively captures salient semantic information in queries and documents for the task while significantly outperforming previous state-of-the-art semantic models.
2132307560;Policy-Centric Integration and Dynamic Composition of Autonomic Computing Techniques;2007.0;[];This paper presents innovative work in the development of policy-based autonomic computing. The core of the work is a powerful and flexible policy-expression language AGILE, which facilitates run-time adaptable policy configuration of autonomic systems. AGILE also serves as an integrating platform for other self-management technologies including signal processing, automated trend analysis and utility functions. Each of these technologies has specific advantages and applicability to different types of dynamic adaptation. The AGILE platform enables seamless interoperability of the different technologies to each perform various aspects of self-management within a single application. The various technologies are implemented as object components. Self-management behaviour is specified using the policy language semantics to bind the various components together as required. Since the policy semantics support run-time re-configuration, the self-management architecture is dynamically composable. Additional benefits include the standardisation of the application programmer interface, terminology and semantics, and only a single point of embedding is required.
2132360759;Filter Bank Common Spatial Pattern (FBCSP) in Brain-Computer Interface;2008.0;[];In motor imagery-based brain computer interfaces (BCI), discriminative patterns can be extracted from the electroencephalogram (EEG) using the common spatial pattern (CSP) algorithm. However, the performance of this spatial filter depends on the operational frequency band of the EEG. Thus, setting a broad frequency range, or manually selecting a subject-specific frequency range, are commonly used with the CSP algorithm. To address this problem, this paper proposes a novel filter bank common spatial pattern (FBCSP) to perform autonomous selection of key temporal-spatial discriminative EEG characteristics. After the EEG measurements have been bandpass-filtered into multiple frequency bands, CSP features are extracted from each of these bands. A feature selection algorithm is then used to automatically select discriminative pairs of frequency bands and corresponding CSP features. A classification algorithm is subsequently used to classify the CSP features. A study is conducted to assess the performance of a selection of feature selection and classification algorithms for use with the FBCSP. Extensive experimental results are presented on a publicly available dataset as well as data collected from healthy subjects and unilaterally paralyzed stroke patients. The results show that FBCSP, using a particular combination feature selection and classification algorithm, yields relatively higher cross-validation accuracies compared to prevailing approaches.
2132943370;A 4.35Gb/s/pin LPDDR4 I/O interface with multi-VOH level, equalization scheme, and duty-training circuit for mobile applications;2015.0;[];A 4.35Gb/s/pin LPDDR4 I/O interface with multi-VOH level, equalization scheme and Duty-Training Circuit (DTC) is presented. A Low Voltage-Swing Terminated Logic (LVSTL) driver using 4-to-1 multiplexer is implemented to the transmitter. A DTC to adjust the CK duty is implemented to the receiver. In addition, a ZQ calibration scheme for Multi-VOH level is also presented. Designed schemes are compatible with the LPDDR4 standard. Power efficiency for the I/O interface is about 2.3mW/Gb/s/pin with 1.1V supply in 2y-nm DRAM process, which is 31% lower than that of LPDDR3.
2132966425;Using a mixed research method to evaluate the effectiveness of formative assessment in supporting student teachers' wiki authoring;2014.0;[];This study aims to investigate whether for preservice early childhood teachers, integrating assessment for learning (AfL) is a viable pedagogy to improve the quality of their wiki-based projects. A total of 76 student teachers who were in their first year of study at a teacher training institute in Hong Kong participated in the study. The student teachers were required to apply the skills and knowledge they had learned about ICT skills and concepts of ICT in education to create digital learning materials for young children in a wiki environment and to peer assess their projects prior to formal submission using an assessment rubric created by the author. The data were triangulated from the responses collected from a discussion forum, a questionnaire, and focus group meetings. The content and number of comments made in the discussion forum indicated that the student teachers not only actively contributed ideas to their peers but also took their peersu0027 comments seriously. Their comments were mainly related to project design, followed by content, organization, and credibility. The questionnaire findings suggested that although the students felt that feedback from their peers could facilitate their own learning, they valued their teacheru0027s comments the most. Seven students participated in the focus group interviews to substantiate the opinions they gave in the questionnaire. The interviewees believed that even though their peers provided comments from different perspectives, their teacheru0027s comments were the most important because she graded them. It was concluded that integrating AfL from the teacher and peers could improve the quality of wiki projects.
2133079113;ActivFORMS: active formal models for self-adaptation;2014.0;[];Self-adaptation enables a software system to deal autonomously with uncertainties, such as dynamic operating conditions that are difficult to predict or changing goals. A common approach to realize self-adaptation is with a MAPE-K feedback loop that consists of four adaptation components: Monitor, Analyze, Plan, and Execute. These components share Knowledge models of the managed system, its goals and environment. To provide guarantees of the adaptation goals, state of the art approaches propose using formal models of the knowledge. However, less attention is given to the formalization of the adaptation components themselves, which is important to provide guarantees of correctness of the adaptation behavior (e.g., does the execute component execute the plan correctly?). We propose Active FORmal Models for Self-adaptation (ActivFORMS) that uses an integrated formal model of the adaptation components and knowledge models. The formal model is directly executed by a virtual machine to realize adaptation, hence active model. The contributions of ActivFORMS are: (1) the approach assures that the adaptation goals that are verified offline are guaranteed at runtime, and (2) it supports dynamic adaptation of the active model to support changing goals. We show how we have applied ActivFORMS for a small-scale robotic system.
2133109597;Toward principles for the design of ontologies used for knowledge sharing;1995.0;[];Recent work in Artificial Intelligence is exploring the use of formal ontologies as a way of specifying content-specific agreements for the sharing and reuse of knowledge among software entities. We take an engineering perspective on the development of such ontologies. Formal ontologies are viewed as designed artifacts, formulated for specific purposes and evaluated against objective design criteria. We describe the role of ontologies in supporting knowledge sharing activities, and then present a set of criteria to guide the development of ontologies for these purposes. We show how these criteria are applied in case studies from the design of ontologies for engineering mathematics and bibliographic data. Selected design decisions are discussed, and alternative representation choices and evaluated against the design criteria.
2133286823;On the levy-walk nature of human mobility;2011.0;[];We report that human walk patterns contain statistically similar features observed in Levy walks. These features include heavy-tail flight and pause-time distributions and the super-diffusive nature of mobility. Human walks are not random walks, but it is surprising that the patterns of human walks and Levy walks contain some statistical similarity. Our study is based on 226 daily GPS traces collected from 101 volunteers in five different outdoor sites. The heavy-tail flight distribution of human mobility induces the super-diffusivity of travel, but up to 30 min to 1 h due to the boundary effect of peopleu0027s daily movement, which is caused by the tendency of people to move within a predefined (also confined) area of daily activities. These tendencies are not captured in common mobility models such as random way point (RWP). To evaluate the impact of these tendencies on the performance of mobile networks, we construct a simple truncated Levy walk mobility (TLW) model that emulates the statistical features observed in our analysis and under which we measure the performance of routing protocols in delay-tolerant networks (DTNs) and mobile ad hoc networks (MANETs). The results indicate the following. Higher diffusivity induces shorter intercontact times in DTN and shorter path durations with higher success probability in MANET. The diffusivity of TLW is in between those of RWP and Brownian motion (BM). Therefore, the routing performance under RWP as commonly used in mobile network studies and tends to be overestimated for DTNs and underestimated for MANETs compared to the performance under TLW.
2133341045;Learning Subjective Language;2004.0;[];Subjectivity in natural language refers to aspects of language used to express opinions, evaluations, and speculations. There are numerous natural language processing applications for which subjectivity analysis is relevant, including information extraction and text categorization. The goal of this work is learning subjective language from corpora. Clues of subjectivity are generated and tested, including low-frequency words, collocations, and adjectives and verbs identified using distributional similarity. The features are also examined working together in concert. The features, generated from different data sets using different procedures, exhibit consistency in performance in that they all do better and worse on the same data sets. In addition, this article shows that the density of subjectivity clues in the surrounding context strongly affects how likely it is that a word is subjective, and it provides the results of an annotation study assessing the subjectivity of sentences with high-density features. Finally, the clues are used to perform opinion piece recognition (a type of text categorization and genre detection) to demonstrate the utility of the knowledge acquired in this article.
2133540137;Routing in pocket switched networks;2012.0;[];Pocket switched networks (PSNs) provide a new networking paradigm that takes advantage of human mobility to distribute data. Due to the frequent and long-duration disruptions of network links, routing in PSNs is nontrivial. In this article, we first outline the challenges of PSN routing. After that, we summarize the behavioral traits of human beings employed by existing PSN routing schemes and give a brief survey on the state-of-the- art PSN routing techniques. Finally, we analyze the characteristics of existing PSN routing protocols and present some open problems that may foster future research on PSN routing.
2133555750;Group decision making based on induced uncertain linguistic OWA operators;2013.0;[];"Based on the extended triangular conorm, some new operational laws of linguistic variables are defined. Then a class of new linguistic aggregation operators is developed by extending the OWA operator to the linguistic environment, the uncertain linguistic ordered weighted averaging (ULOWA""f) operator, the induced uncertain linguistic ordered weighted averaging (IULOWA""f) operator and the uncertain linguistic weighted averaging (ULWA""f) operator. Later some desirable properties of the extended operators are studied and a practical group decision making methodology for an uncertain linguistic environment is proposed based on the ULWA""f and the IULOWA""f operators. A numerical example is given to illustrate the new approach for a financial decision making problem."
2133795641;Defense trees for economic evaluation of security investments;2006.0;[];In this paper we present a mixed qualitative and quantitative approach for evaluation of information technology (IT) security investments. For this purpose, we model security scenarios by using defense trees, an extension of attack trees with attack countermeasures and we use economic quantitative indexes for computing the defenderu0027s return on security investment and the attackeru0027s return on attack. We show how our approach can be used to evaluate effectiveness and economic profitability of countermeasures as well as their deterrent effect on attackers, thus providing decision makers with a useful tool for performing better evaluation of IT security investments during the risk management process.
2133859873;Model evolution by run-time parameter adaptation;2009.0;[];Models can help software engineers to reason about design-time decisions before implementing a system. This paper focuses on models that deal with non-functional properties, such as reliability and performance. To build such models, one must rely on numerical estimates of various parameters provided by domain experts or extracted by other similar systems. Unfortunately, estimates are seldom correct. In addition, in dynamic environments, the value of parameters may change over time. We discuss an approach that addresses these issues by keeping models alive at run time and feeding a Bayesian estimator with data collected from the running system, which produces updated parameters. The updated model provides an increasingly better representation of the system. By analyzing the updated model at run time, it is possible to detect or predict if a desired property is, or will be, violated by the running implementation. Requirement violations may trigger automatic reconfigurations or recovery actions aimed at guaranteeing the desired goals. We illustrate a working framework supporting our methodology and apply it to an example in which a Web service orchestrated composition is modeled through a Discrete Time Markov Chain. Numerical simulations show the effectiveness of the approach.
2134752223;Fair Routing in Delay Tolerant Networks;2009.0;[];"The typical state-of-the-art routing algorithms for delay tolerant networks are based on best next hop hill-climbing heuristics in order to achieve throughput and efficiency. The combination of these heuristics and the social network structure leads the routing to direct most of the traffic through a small subset of good users. For instance, in the SimBet algorithm, the top 10% of users carry out 54% of all the forwards and 85% of all the handovers. This unfair load distribution is not sustainable as it can quickly deplete constraint resources in heavily utilized mobile devices (e.g. storage, battery, budget, etc.). Moreover, because a small number of users carry a significant amount of the traffic, the system is not robust to random failures and attacks. To overcome these inefficiencies, this paper introduces Fair-Route, a routing algorithm for delay tolerant networks inspired by the social processes of perceived interaction strength, where messages are preferably forwarded to users that have a stronger social relation with the target of the message; and assortativity, that limits the exchange of messages to those users with similar ""social status"". We compare the performance of FairRoute to the state-of-the-art algorithms by extensive simulations on the MIT reality mining dataset. The results show that our algorithm outperforms existing algorithms in the de facto benchmark of throughput vs. forwards. Furthermore, it distributes better the load; the top 10% carry out 26% of the forwards and 28% of the handovers without any loss in performance."
2134894205;Security and Privacy Challenges in Cloud Computing Environments;2010.0;[];Cloud computing is an evolving paradigm with tremendous momentum, but its unique aspects exacerbate security and privacy challenges. This article explores the roadblocks and solutions to providing a trustworthy cloud computing environment.
2134963432;An obstacle detection method by fusion of radar and motion stereo;2002.0;[];In order to avoid collision with an object that blocks the course of a vehicle, measuring the distance to it and detecting positions of its side boundaries, are necessary. In the paper, an object detection method achieved by the fusion of millimeter-wave radar and a single video camera is proposed. We consider the method as the least expensive solution because at least one camera is necessary for lane marking detection. In the method, the distance is measured by the radar, and the boundaries are found from an image sequence, based on a motion stereo technique with the help of the distance measured by the radar. Since the method does not depend on the appearance of objects, it is capable of detecting not only an automobile but also other objects. Object detection by the method was confirmed through an experiment. In the experiment, both a stationary and a moving object were detected and a pedestrian as well as a vehicle was detected.
2135383305;CIXL2: a crossover operator for evolutionary algorithms based on population features;2005.0;[];"In this paper we propose a crossover operator for evolutionary algorithms with real values that is based on the statistical theory of population distributions. The operator is based on the theoretical distribution of the values of the genes of the best individuals in the population. The proposed operator takes into account the localization and dispersion features of the best individuals of the population with the objective that these features would be inherited by the offspring. Our aim is the optimization of the balance between exploration and exploitation in the search  :[90],""order to test the efficiency and robustness of this crossover, we have used a set of functions to be optimized with regard to different criteria, such as, multimodality, separability, regularity and epistasis. With this set of functions we can extract conclusions in function of the problem at hand. We analyze the results using ANOVA and multiple comparison statistical  :[149],""an example of how our crossover can be used to solve artificial intelligence problems, we have applied the proposed model to the problem of obtaining the weight of each network in a ensemble of neural networks. The results obtained are above the performance of standard methods."
2135692965;Podcasting in education: Are students as ready and eager as we think they are?;2010.0;[];Instructors in higher education are disseminating instructional content via podcasting, as many rally behind the technologyu0027s potential benefits. Others have expressed concern about the risks of deleterious effects that might accompany the adoption of podcasting, such as lower class attendance. Yet, relatively few studies have investigated studentsu0027 perceptions of podcasting for educational purposes, especially in relation to different podcasting forms: repetitive and supplemental. The present study explored studentsu0027 readiness and attitudes towards these two forms of podcasting to provide fundamental information for future researchers and educators. The results indicated that students may not be as ready or eager to use podcasting for repetitive or supplemental educational purposes as much as we think they are, but they could be persuaded.
2135725337;Analysis of smartphone user mobility traces for opportunistic data collection in wireless sensor networks;2013.0;[];The increasing ubiquity of smartphones coupled with the mobility of their users will allow the use of smartphones to enhance the operation of wireless sensor networks. In addition to accessing data from a wireless sensor network for personal use, and the generation of data through participatory sensing, we propose the use of smartphones to collect data from sensor nodes opportunistically. For this to be feasible, the mobility patterns of smartphone users must support opportunistic use. We analyze the dataset from the Mobile Data Challenge by Nokia, and we identify the significant patterns, including strong spatial and temporal localities. These patterns should be exploited when designing protocols and algorithms, and their existence supports the proposal for opportunistic data collection through smartphones.
2136189984;Learning deep structured semantic models for web search using clickthrough data;2013.0;[];Latent semantic models, such as LSA, intend to map a query to its relevant documents at the semantic level where keyword-based matching often fails. In this study we strive to develop a series of new latent semantic models with a deep structure that project queries and documents into a common low-dimensional space where the relevance of a document given a query is readily computed as the distance between them. The proposed deep structured semantic models are discriminatively trained by maximizing the conditional likelihood of the clicked documents given a query using the clickthrough data. To make our models applicable to large-scale Web search applications, we also use a technique called word hashing, which is shown to effectively scale up our semantic models to handle large vocabularies which are common in such tasks. The new models are evaluated on a Web document ranking task using a real-world data set. Results show that our best model significantly outperforms other latent semantic models, which were considered state-of-the-art in the performance prior to the work presented in this paper.
2136317921;Understanding mobility based on GPS data;2008.0;[];Both recognizing human behavior and understanding a useru0027s mobility from sensor data are critical issues in ubiquitous computing systems. As a kind of user behavior, the transportation modes, such as walking, driving, etc., that a user takes, can enrich the useru0027s mobility with informative knowledge and provide pervasive computing systems with more context information. In this paper, we propose an approach based on supervised learning to infer peopleu0027s motion modes from their GPS logs. The contribution of this work lies in the following two aspects. On one hand, we identify a set of sophisticated features, which are more robust to traffic condition than those other researchers ever used. On the other hand, we propose a graph-based post-processing algorithm to further improve the inference performance. This algorithm considers both the commonsense constraint of real world and typical user behavior based on location in a probabilistic manner. Using the GPS logs collected by 65 people over a period of 10 months, we evaluated our approach via a set of experiments. As a result, based on the change point-based segmentation method and Decision Tree-based inference model, the new features brought an eight percent improvement in inference accuracy over previous result, and the graph-based post-processing achieve a further four percent enhancement.
2136527772;State-of-the-Art Prescriptive Criteria Weight Elicitation;2012.0;[];Comparatively few of the vast amounts of decision analytical methods suggested have been widely spread in actual practice. Some approaches have nevertheless been more successful in this respect than others. Quantitative decision making has moved from the study of decision theory founded on a single criterion towards decision support for more realistic decision-making situations with multiple, often conflicting, criteria. Furthermore, the identified gap between normative and descriptive theories seems to suggest a shift to more prescriptive approaches. However, when decision analysis applications are used to aid prescriptive decision-making processes, additional demands are put on these applications to adapt to the users and the context. In particular, the issue of weight elicitation is crucial. There are several techniques for deriving criteria weights from preference statements. This is a cognitively demanding task, subject to different biases, and the elicited values can be heavily dependent on the method of assessment. There have been a number of methods suggested for assessing criteria weights, but these methods have properties which impact their applicability in practice. This paper provides a survey of state-of-the-art weight elicitation methods in a prescriptive setting.
2136714340;Serious games for movement therapy after stroke;2008.0;[];This paper details the development and testing of a serious-game based movement therapy aimed at encouraging stroke patients with upper limb motor disorders to practice physical exercises. The system contains a series of Virtual Reality (VR) games. A framework for VR movement therapy is described which consists of a number of serious games designed to encourage patientsu0027 physical activity in highly motivating, virtual environments where various factors such as size and gravity can be scaled to adapt to individual patientu0027s abilities and in-game performance. Another goal of this study is to determine whether the provision of serious games based interventions improves motor outcome after stroke. A pilot study with 8 participants who have a first hemispheric stroke shows improvements on impairment measurement and functional measurement shortly after completion of the intervention and 6 weeks after the intervention. Despite its limitations the findings of this study support the effectiveness of serious games in the treatment of participants with hemiplegia. The study also raises awareness of the benefits of using serious games in movement therapy after stroke.
2136794114;Implementation of an Embedded Web Server Application for Wireless Control of Brain Computer Interface Based Home Environments;2016.0;[];Brain Computer Interface (BCI) based environment control systems could facilitate life of people with neuromuscular diseases, reduces dependence on their caregivers, and improves their quality of life. As well as easy usage, low-cost, and robust system performance, mobility is an important functionality expected from a practical BCI system in real life. In this study, in order to enhance usersu0027 mobility, we propose internet based wireless communication between BCI system and home environment. We designed and implemented a prototype of an embedded low-cost, low power, easy to use web server which is employed in internet based wireless control of a BCI based home environment. The embedded web server provides remote access to the environmental control module through BCI and web interfaces. While the proposed system offers to BCI users enhanced mobility, it also provides remote control of the home environment by caregivers as well as the individuals in initial stages of neuromuscular disease. The input of BCI system is P300 potentials. We used Region Based Paradigm (RBP) as stimulus interface. Performance of the BCI system is evaluated on data recorded from 8 non-disabled subjects. The experimental results indicate that the proposed web server enables internet based wireless control of electrical home appliances successfully through BCIs.
2137648512;Integrative network alignment reveals large regions of global network similarity in yeast and human;2011.0;[];"Motivation: High-throughput methods for detecting molecular interactions have produced large sets of biological network data with much more yet to come. Analogous to sequence alignment, efficient and reliable network alignment methods are expected to improve our understanding of biological systems. Unlike sequence alignment, network alignment is computationally intractable. Hence, devising efficient network alignment heuristics is currently a foremost challenge in computational  :[61],""We introduce a novel network alignment algorithm, called Matching-based Integrative GRAph ALigner (MI-GRAAL), which can integrate any number and type of similarity measures between network nodes (e.g. proteins), including, but not limited to, any topological network similarity measure, sequence similarity, functional similarity and structural similarity. Hence, we resolve the ties in similarity measures and find a combination of similarity measures yielding the largest contiguous (i.e. connected) and biologically sound alignments. MI-GRAAL exposes the largest functional, connected regions of protein–protein interaction (PPI) network similarity to date: surprisingly, it reveals that 77.7% of proteins in the bakeru0027s yeast high-confidence PPI network participate in such a subnetwork that is fully contained in the human high-confidence PPI network. This is the first demonstration that species as diverse as yeast and human contain so large, continuous regions of global network similarity. :[61],""We apply MI-GRAALu0027s alignments to predict functions of un-annotated proteins in yeast, human and bacteria validating our predictions in the literature. Furthermore, using network alignment scores for PPI networks of different herpes viruses, we reconstruct their phylogenetic relationship. This is the first time that phylogeny is exactly reconstructed from purely topological alignments of PPI  :[252],""Supplementary files and MI-GRAAL executables:   :[259],""information:Supplementary data are available at Bioinformatics online."
2137847051;Understanding Gesture Expressivity through Muscle Sensing;2015.0;[];Expressivity is a visceral capacity of the human body. To understand what makes a gesture expressive, we need to consider not only its spatial placement and orientation but also its dynamics and the mechanisms enacting them. We start by defining gesture and gesture expressivity, and then we present fundamental aspects of muscle activity and ways to capture information through electromyography and mechanomyography. We present pilot studies that inspect the ability of users to control spatial and temporal variations of 2D shapes and that use muscle sensing to assess expressive information in gesture execution beyond space and time. This leads us to the design of a study that explores the notion of gesture power in terms of control and sensing. Results give insights to interaction designers to go beyond simplistic gestural interaction, towards the design of interactions that draw on nuances of expressive gesture.
2137982913;Attacker profiling in quantitative security assessment based on attack trees;2014.0;[];We present the results of research of limiting adversarial budget in attack games, and, in particular, in the failure-free attack tree models presented by Buldas-Stepanenko in 2012 and improved in 2013 by Buldas and Lenin. In the previously presented models attacker’s budget was assumed to be unlimited. It is natural to assume that the adversarial budget is limited and such an assumption would allow us to model the adversarial decision making more close to the one that might happen in real life. We analyze three atomic cases – the single atomic case, the atomic AND, and the atomic OR. Even these elementary cases become quite complex, at the same time, limiting adversarial budget does not seem to provide any better or more precise results compared to the failure-free models. For the limited model analysis results to be reliable, it is required that the adversarial reward is estimated with high precision, probably not achievable by providing expert estimations for the quantitative annotations on the attack steps, such as the cost or the success probability. It is doubtful that it is reasonable to face this com- plexity, as the failure-free model provides reliable upper bounds, being at the same time computationally less complex.
2138000876;Scheduling Power Consumption With Price Uncertainty;2011.0;[];The problem of causally scheduling power consumption to minimize the expected cost at the consumer side is considered. The price of electricity is assumed to be time-varying. The scheduler has access to past and current prices, but only statistical knowledge about future prices, which it uses to make an optimal decision in each time period. The scheduling problem is naturally cast as a Markov decision process. Algorithms to find decision thresholds for both noninterruptible and interruptible loads under a deadline constraint are then developed. Numerical results suggest that incorporating the statistical knowledge into the scheduling policies can result in significant savings, especially for short tasks. It is demonstrated with real price data from Commonwealth Edison that scheduling with mismatched modeling and online parameter estimation can still provide significant economic advantages to consumers.
2138677604;SLAW: A New Mobility Model for Human Walks;2009.0;[];Simulating human mobility is important in mobile networks because many mobile devices are either attached to or controlled by humans and it is very hard to deploy real mobile networks whose size is controllably scalable for performance evaluation. Lately various measurement studies of human walk traces have discovered several significant statistical patterns of human mobility. Namely these include truncated power-law distributions of flights, pause-times and inter-contact times, fractal way-points, and heterogeneously defined areas of individual mobility. Unfortunately, none of existing mobility models effectively captures all of these features. This paper presents a new mobility model called SLAW (self-similar least action walk) that can produce synthetic walk traces containing all these features. This is by far the first such model. Our performance study using using SLAW generated traces indicates that SLAW is effective in representing social contexts present among people sharing common interests or those in a single community such as university campus, companies and theme parks. The social contexts are typically common gathering places where most people visit during their daily lives such as student unions, dormitory, street malls and restaurants. SLAW expresses the mobility patterns involving these contexts by fractal way points and heavy-tail flights on top of the way points. We verify through simulation that SLAW brings out the unique performance features of various mobile network routing protocols.
2138784882;An evolutionary algorithm that constructs recurrent neural networks;1994.0;[];Standard methods for simultaneously inducing the structure and weights of recurrent neural networks limit every task to an assumed class of architectures. Such a simplification is necessary since the interactions between network structure and function are not well understood. Evolutionary computations, which include genetic algorithms and evolutionary programming, are population-based search methods that have shown promise in many similarly complex tasks. This paper argues that genetic algorithms are inappropriate for network acquisition and describes an evolutionary program, called GNARL, that simultaneously acquires both the structure and weights for recurrent networks. GNARLu0027s empirical acquisition method allows for the emergence of complex behaviors and topologies that are potentially excluded by the artificial architectural constraints imposed in standard network induction methods. u003e
2139076288;A probabilistic kernel method for human mobility prediction with smartphones;2015.0;[];Human mobility prediction is an important problem that has a large number of applications, especially in context-aware services. This paper presents a study on location prediction using smartphone data, in which we address modeling and application aspects. Building personalized location prediction models from smartphone data remains a technical challenge due to data sparsity, which comes from the complexity of human behavior and the typically limited amount of data available for individual users. To address this problem, we propose an approach based on kernel density estimation, a popular smoothing technique for sparse data. Our approach contributes to existing work in two ways. First, our proposed model can estimate the probability that a user will be at a given location at a specific time in the future, by using both spatial and temporal information via multiple kernel functions. Second, we also show how our probabilistic framework extends to a more practical task of location prediction for a time window in the future. Our approach is validated on an everyday life location dataset consisting of 133 smartphone users. Our method reaches an accuracy of 84% for the next hour, and an accuracy of 77% for the next three hours.
2139501017;Sequence to Sequence -- Video to Text;2015.0;[];Real-world videos often have complex dynamics, methods for generating open-domain video descriptions should be senstive to temporal structure and allow both input (sequence of frames) and output (sequence of words) of variable length. To approach this problem we propose a novel end-to-end sequence-to-sequence model to generate captions for videos. For this we exploit recurrent neural networks, specifically LSTMs, which have demonstrated state-of-the-art performance in image caption generation. Our LSTM model is trained on video-sentence pairs and learns to associate a sequence of video frames to a sequence of words in order to generate a description of the event in the video clip. Our model naturally is able to learn the temporal structure of the sequence of frames as well as the sequence model of the generated sentences, i.e. a language model. We evaluate several variants of our model that exploit different visual features on a standard set of YouTube videos and two movie description datasets (M-VAD and MPII-MD).
2140066605;Multiple Objective Optimization with Vector Evaluated Genetic Algorithms;1985.0;[];
2140251882;Mining interesting locations and travel sequences from GPS trajectories;2009.0;[];The increasing availability of GPS-enabled devices is changing the way people interact with the Web, and brings us a large amount of GPS trajectories representing peopleu0027s location histories. In this paper, based on multiple usersu0027 GPS trajectories, we aim to mine interesting locations and classical travel sequences in a given geospatial region. Here, interesting locations mean the culturally important places, such as Tiananmen Square in Beijing, and frequented public areas, like shopping malls and restaurants, etc. Such information can help users understand surrounding locations, and would enable travel recommendation. In this work, we first model multiple individualsu0027 location histories with a tree-based hierarchical graph (TBHG). Second, based on the TBHG, we propose a HITS (Hypertext Induced Topic Search)-based inference model, which regards an individualu0027s access on a location as a directed link from the user to that location. This model infers the interest of a location by taking into account the following three factors. 1) The interest of a location depends on not only the number of users visiting this location but also these usersu0027 travel experiences. 2) Usersu0027 travel experiences and location interests have a mutual reinforcement relationship. 3) The interest of a location and the travel experience of a user are relative values and are region-related. Third, we mine the classical travel sequences among locations considering the interests of these locations and usersu0027 travel experiences. We evaluated our system using a large GPS dataset collected by 107 users over a period of one year in the real world. As a result, our HITS-based inference model outperformed baseline approaches like rank-by-count and rank-by-frequency. Meanwhile, when considering the usersu0027 travel experiences and location interests, we achieved a better performance beyond baselines, such as rank-by-count and rank-by-interest, etc.
2141336889;Whole-home gesture recognition using wireless signals;2013.0;[];This paper presents WiSee, a novel gesture recognition system that leverages wireless signals (e.g., Wi-Fi) to enable whole-home sensing and recognition of human gestures. Since wireless signals do not require line-of-sight and can traverse through walls, WiSee can enable whole-home gesture recognition using few wireless sources. Further, it achieves this goal without requiring instrumentation of the human body with sensing devices. We implement a proof-of-concept prototype of WiSee using USRP-N210s and evaluate it in both an office environment and a two- bedroom apartment. Our results show that WiSee can identify and classify a set of nine gestures with an average accuracy of 94%.
2141599568;Linguistic Regularities in Continuous Space Word Representations;2013.0;[];Continuous space language models have recently demonstrated outstanding results across a variety of tasks. In this paper, we examine the vector-space word representations that are implicitly learned by the input-layer weights. We find that these representations are surprisingly good at capturing syntactic and semantic regularities in language, and that each relationship is characterized by a relation-specific vector offset. This allows vector-oriented reasoning based on the offsets between words. For example, the male/female relationship is automatically learned, and with the induced vector representations, “King Man + Woman” results in a vector very close to “Queen.” We demonstrate that the word vectors capture syntactic regularities by means of syntactic analogy questions (provided with this paper), and are able to correctly answer almost 40% of the questions. We demonstrate that the word vectors capture semantic regularities by using the vector offset method to answer SemEval-2012 Task 2 questions. Remarkably, this method outperforms the best previous systems.
2142486130;Software Engineering for Self-Adaptive Systems: A Second Research Roadmap;2013.0;[];The goal of this roadmap paper is to summarize the state-of-the-art and identify research challenges when developing, deploying and managing self-adaptive software systems. Instead of dealing with a wide range of topics associated with the field, we focus on four essential topics of self-adaptation: design space for self-adaptive solutions, software engineering processes for self-adaptive systems, from centralized to decentralized control, and practical run-time verification u0026 validation for self-adaptive systems. For each topic, we present an overview, suggest future directions, and focus on selected challenges. This paper complements and extends a previous roadmap on software engineering for self-adaptive systems published in 2009 covering a different set of topics, and reflecting in part on the previous paper. This roadmap is one of the many results of the Dagstuhl Seminar 10431 on Software Engineering for Self-Adaptive Systems, which took place in October 2010.
2142527556;Attack–defense trees;2014.0;[];"The advent of the information age has notably amplified the importance of security. Unfortunately security considerations still widely occur as an afterthought. For many companies, security is not a requirement to conduct business and is therefore readily neglected. However the lack of security may obstruct, impede and even ruin an otherwise flourishing enterprise. Only when internal computer networks shut down, web portals are inaccessible, mail servers are attacked, or similar incidents affect the day to day business of an enterprise, security enters into the field of vision of companies. As such, security by design is only slowly becoming accepted  security :[99],""researchers, there is no dispute that a reasonable approach to- wards uninterrupted business activities includes security measures and controls from the beginning. To support these efforts, many security models have been developed. Graphical security models are a type of security model that help illus- trate and guide the consideration of security throughout the lifecycle of a product, system or company. Their visual properties are especially well-suited to elucidate security requirements and corresponding security  the :[174],""last four years, we have developed a new graphical security model called attack–defense trees. The new framework, presented in this thesis, generalizes the well-known attack trees model. Attack–defense trees formally extend attack trees and enhance them with  :[213],""be able to deploy attack–defense trees as a security support tool, we have  them with :[227],""three different syntaxes: A visually appealing, graph-based syntax that is dedicated to representing security problems, an algebraic, term-based syntax that simplifies correct, formal and quantitative analysis of security scenarios and a textual syntax that is a compromise between succinct, visual representation and easy, computerized  have :[274],""also equipped attack–defense trees with a variety of semantics. This became necessary, since different applications require different interpretations of attack–defense trees. Besides the very specific and problem oriented propositional, De Morgan and multiset semantics, we have introduced equational semantics. The latter semantics is, in fact, an alternative, unified presentation of semantics based on equational theory. We have expressed the propositional and the multiset seman- tics in terms of the equational semantics. This facilitates algorithmic treatment since the two different semantics have a unified formal  :[213],""be able to :[360],""perform quantitative security analysis, we have introduced the notion of an attribute for attack–defense trees. To guarantee that the evaluation of an attribute on two or more semantically equal attack–defense trees results in the same value, we have introduced the notion of a compatibility condition between semantics and attributes. We have :[274],""also provided usability guidelines for attributes. These guidelines help a user to specify security-relevant questions that can unambiguously :[213],""be answered using  have :[436],""performed several case studies that allowed us to test and improve the attack–defense tree methodology. We have provided detailed explanations for our design choices during the case studies as well as extensive applicability guidelines that serve a prospective user of the attack–defense tree methodology as a user  have :[485],""demonstrated the usefulness of the formal foundations of attack–defense trees by relating attack–defense terms to other scientific research disciplines. Con- cretely, we have shown that attack–defense trees in the propositional semantics are computationally as complex as propositional attack trees. Moreover, we have described how to merge Bayesian networks with attack–defense trees and have il- lustrated that attack–defense trees in the propositional semantics are equivalent to a specific class of games frequently occurring in game  the thesis, we have :[561],""related the attack–defense tree methodology to other graphical security models in an extensive literature overview over similar methodologies."
2142742813;Learning from Multiple Partially Observed Views - an Application to Multilingual Text Categorization;2009.0;[];We address the problem of learning classifiers when observations have multiple views, some of which may not be observed for all examples. We assume the existence of view generating functions which may complete the missing views in an approximate way. This situation corresponds for example to learning text classifiers from multilingual collections where documents are not available in all languages. In that case, Machine Translation (MT) systems may be used to translate each document in the missing languages. We derive a generalization error bound for classifiers learned on examples with multiple artificially created views. Our result uncovers a trade-off between the size of the training set, the number of views, and the quality of the view generating functions. As a consequence, we identify situations where it is more interesting to use multiple views for learning instead of classical single view learning. An extension of this framework is a natural way to leverage unlabeled multi-view data in semi-supervised learning. Experimental results on a subset of the Reuters RCV1/RCV2 collections support our findings by showing that additional views obtained from MT may significantly improve the classification performance in the cases identified by our trade-off.
2143548801;Working day movement model;2008.0;[]; movement models, such as Random Waypoint, do not capture reliably the properties of movement in the real life scenarios. We present and analyse a movement model for delay-tolerant network simulations that is able to produce inter-contact time and contact time distributions that follow closely the ones found in the traces from the real-world measurement experiments. We validate the movement model using the ONE simulator.
2144479651;A Dispatching Model for Server-to-Customer Systems That Balances Efficiency and Equity;2013.0;[];The decision about which servers to dispatch to which customers is an important aspect of service systems. This decision is complicated when servers must be equitably---as well as efficiently---dispatched to customers. In this paper, we formulate a model for determining how to optimally dispatch distinguishable servers to prioritized customers given a set of equity constraints. These issues are examined through the lens of emergency medical service EMS dispatch, for which a Markov decision process model is developed that captures how to dispatch ambulances servers to prioritized patients customers. It is assumed that customers arrive sequentially, with the priority and location of each customer becoming known upon arrival. Four types of equity constraints are considered---two of which reflect customer equity and two of which reflect server equity---all of which draw upon the decision analytic and social science literature to compare the effects of different notions of equity on the resulting dispatching policies. The Markov decision processes are formulated as equity-constrained linear programming models. A computational example is applied to an EMS system to compare the different equity models.
2144577430;Video summarization and scene detection by graph modeling;2005.0;[];We propose a unified approach for video summarization based on the analysis of video structures and video highlights. Two major components in our approach are scene modeling and highlight detection. Scene modeling is achieved by normalized cut algorithm and temporal graph analysis, while highlight detection is accomplished by motion attention modeling. In our proposed approach, a video is represented as a complete undirected graph and the normalized cut algorithm is carried out to globally and optimally partition the graph into video clusters. The resulting clusters form a directed temporal graph and a shortest path algorithm is proposed to efficiently detect video scenes. The attention values are then computed and attached to the scenes, clusters, shots, and subshots in a temporal graph. As a result, the temporal graph can inherently describe the evolution and perceptual importance of a video. In our application, video summaries that emphasize both content balance and perceptual quality can be generated directly from a temporal graph that embeds both the structure and attention information.
2144661574;Applications of cumulants to array processing .I. Aperture extension and array calibration;1995.0;[];An interpretation for the use of cumulants in narrowband array processing problems is proposed. It is shown how fourth-order cumulants of multichannel observations increase the directional information compared with second-order statistics. Based on the interpretation, it is shown how cumulants can be used to increase the effective aperture of an arbitrary antenna array. The amount of partial information necessary to jointly calibrate an arbitrary array and estimate the directions of far-field sources is also investigated. It is proven that the presence of a doublet and use of fourth-order cumulants is sufficient to accomplish this task. The proposed approach is computationally efficient and more general than covariance-based algorithms that have addressed the calibration problem under constraints. A class of beamforming techniques is proposed to recover the source waveforms. Proposed estimation procedures are based on cumulants, which bring insensitivity to the spatial correlation structure of additive Gaussian measurement noise. Simulations are provided to illustrate the use of the proposed algorithms. u003e
2144709978;Judging Borrowers by the Company They Keep: Friendship Networks and Information Asymmetry in Online Peer-to-Peer Lending;2013.0;[];"We study the online market for peer-to-peer P2P lending, in which individuals bid on unsecured microloans sought by other individual borrowers. Using a large sample of consummated and failed listings from the largest online P2P lending marketplace, Prosper.com, we find that the online friendships of borrowers act as signals of credit quality. Friendships increase the probability of successful funding, lower interest rates on funded loans, and are associated with lower ex post default rates. The economic effects of friendships show a striking gradation based on the roles and identities of the friends. We discuss the implications of our findings for the disintermediation of financial markets and the design of decentralized electronic  :[111],""paper was accepted by Sandra Slaughter, information systems."
2144781570;Lower Extremity Exoskeletons and Active Orthoses: Challenges and State-of-the-Art;2008.0;[];In the nearly six decades since researchers began to explore methods of creating them, exoskeletons have progressed from the stuff of science fiction to nearly commercialized products. While there are still many challenges associated with exoskeleton development that have yet to be perfected, the advances in the field have been enormous. In this paper, we review the history and discuss the state-of-the-art of lower limb exoskeletons and active orthoses. We provide a design overview of hardware, actuation, sensory, and control systems for most of the devices that have been described in the literature, and end with a discussion of the major advances that have been made and hurdles yet to be overcome.
2146023544;Real-Time Computer Vision/DGPS-Aided Inertial Navigation System for Lane-Level Vehicle Navigation;2012.0;[];Many intelligent transportation system (ITS) applications will increasingly rely on lane-level vehicle positioning that requires high accuracy, bandwidth, availability, and integrity. Lane-level positioning methods must reliably work in real time in a wide range of environments, spanning rural to urban areas. Traditional positioning sensors such as the Global Navigation Satellite Systems may have poor performance in dense urban areas, where obstacles block satellite signals. This paper presents a sensor fusion technique that uses computer vision and differential pseudorange Global Positioning System (DGPS) measurements to aid an inertial navigation system (INS) in challenging environments where GPS signals are limited and/or unreliable. To supplement limited DGPS measurements, this method uses mapped landmarks that were measured through a priori observations (e.g., traffic light location data), taking advantage of existing infrastructure that is abundant within suburban/urban environments. For example, traffic lights are easily detected by color vision sensors in both day and night conditions. A tightly coupled estimation process is employed to use observables from satellite signals and known feature observables from a camera to correct an INS that is formulated as an extended Kalman filter. A traffic light detection method is also outlined, where the projected feature uncertainty ellipse is utilized to perform data association between a predicted feature and a set of detected features. Real-time experimental results from real-world settings are presented to validate the proposed localization method.
2146024157;Foundations of attack trees;2005.0;[];Attack trees have found their way to practice because they have proved to be an intuitive aid in threat analysis. Despite, or perhaps thanks to, their apparent simplicity, they have not yet been provided with an unambiguous semantics. We argue that such a formal interpretation is indispensable to precisely understand how attack trees can be manipulated during construction and analysis. We provide a denotational semantics, based on a mapping to attack suites, which abstracts from the internal structure of an attack tree, we study transformations between attack trees, and we study the attribution and projection of an attack tree.
2146645312;A linguistic approach for determining the topics of Spanish Twitter messages;2015.0;[];The vast number of opinions and reviews provided in Twitter is helpful in order to make interesting findings about a given industry, but given the huge number of messages published every day, it is important to detect the relevant ones. In this respect, the Twitter search functionality is not a practical tool when we want to poll messages dealing with a given set of general topics. This article presents an approach to classify Twitter messages into various topics. We tackle the problem from a linguistic angle, taking into account part-of-speech, syntactic and semantic information, showing how language processing techniques should be adapted to deal with the informal language present in Twitter messages. The TASS 2013 General corpus, a collection of tweets that has been specifically annotated to perform text analytics tasks, is used as the dataset in our evaluation framework. We carry out a wide range of experiments to determine which kinds of linguistic information have the greatest impact on this task and how they should be combined in order to obtain the best-performing system. The results lead us to conclude that relating features by means of contextual information adds complementary knowledge over pure lexical models, making it possible to outperform them on standard metrics for multilabel classification tasks.
2146774271;Virtual Progress: The Effect of Path Characteristics on Perceptions of Progress and Choice;2003.0;[];In goal-oriented services, consumers want to get transported from one well-defined state (start) to another (destination) state without much concern for intermediate states. A cost-based evaluation of such services should depend on the total cost associated with the service--i.e., the price and the amount of time taken for completion. In this paper, we demonstrate that the characteristics of the path to the final destination also influence evaluation and choice. Specifically, we show that segments of idle time and travel away from the final destination are seen as obstacles in the progress towards the destination, and hence lower the choice likelihood of the path. Further, we show that the earlier such obstacles occur during the service, the lower is the choice likelihood. We present an analytical model of consumer choice and test its predictions in a series of experiments. Our results show that in choosing between two services that cover the same displacement in the same time (i.e., identical average progress), consumer choice is driven by the perception of progress towards the goal (i.e., byvirtual progress). In a final experiment, we show that the effects of virtual progress may outweigh the effects of actual average progress.
2147017814;One Permutation Hashing;2012.0;[];"Minwise hashing is a standard procedure in the context of search, for efficiently estimating set similarities in massive binary data such as text. Recently, 6-bit minwise hashing has been applied to large-scale learning and sublinear time near-neighbor search. The major drawback of minwise hashing is the expensive preprocessing, as the method requires applying (e.g.,) k = 200 to 500 permutations on the data. This paper presents a simple solution called one permutation hashing. Conceptually, given a binary data matrix, we permute the columns once and divide the permuted columns evenly into k bins; and we store, for each data vector, the smallest nonzero location in each bin. The probability analysis illustrates that this one permutation scheme should perform similarly to the original (k-permutation) minwise hashing. Our experiments with training SVM and logistic regression confirm that one permutation hashing can achieve similar (or even better) accuracies compared to the k-permutation scheme. See more details in arXiv:1208.1259."
2147307034;An Analysis of the Technology Acceptance Model in Understanding University Students' Behavioral Intention to Use e-Learning;2009.0;[];Many universities implement e-learning for various reasons. It is obvious that the number of e-learning opportunities provided by higher educational institutes continues to grow in Korea. Yet little research has been done to verify the process of how university students adopt and use e-learning. A sample of 628 university students took part in the research. The structural equation modeling (SEM) technique was employed with the LISREL program to explain the adoption process. The general structural model, which included e-learning selfefficacy, subjective norm, system accessibility, perceived usefulness, perceived ease of use, attitude, and behavioral intention to use e-learning, was developed based on the technology acceptance model (TAM). The result proved TAM to be a good theoretical tool to understand users’ acceptance of e-learning. E-learning selfefficacy was the most important construct, followed by subjective norm in explicating the causal process in the model.
2147316766;Multi-dimensional students' evaluation of e-learning systems in the higher education context: An empirical investigation;2009.0;[];There has been little research on assessment of learning management systems (LMS) within educational organizations as both a web-based learning system for e-learning and as a supportive tool for blended learning environments. This study proposes a conceptual e-learning assessment model, hexagonal e-learning assessment model (HELAM) suggesting a multi-dimensional approach for LMS evaluation via six dimensions: (1) system quality, (2) service quality, (3) content quality, (4) learner perspective, (5) instructor attitudes, and (6) supportive issues. A survey instrument based on HELAM has been developed and applied to 84 learners. This sample consists of students at both undergraduate and graduate levels who are users of a web-based learning management system, U-Link, at Brunel University, UK. The survey instrument has been tested for content validity, reliability, and criterion-based predictive validity. The analytical results strongly support the appropriateness of the proposed model in evaluating LMSs through learnersu0027 satisfaction. The explanatory factor analysis showed that each of the six dimensions of the proposed model had a significant effect on the learnersu0027 perceived satisfaction. Findings of this research will be valuable for both academics and practitioners of e-learning systems.
2147595926;Induced ordered weighted averaging operators;1999.0;[];We briefly describe the Ordered Weighted Averaging (OWA) operator and discuss a methodology for learning the associated weighting vector from observational data. We then introduce a more general type of OWA operator called the Induced Ordered Weighted Averaging (IOWA) Operator. These operators take as their argument pairs, called OWA pairs, in which one component is used to induce an ordering over the second components which are then aggregated. A number of different aggregation situations have been shown to be representable in this framework. We then show how this tool can be used to represent different types of aggregation models.
2147709600;Optimizing web search using social annotations;2007.0;[];"This paper explores the use of social annotations to improve websearch. Nowadays, many services, e.g. del.icio.us, have been developed for web users to organize and share their favorite webpages on line by using social annotations. We observe that the social annotations can benefit web search in two aspects: 1) the annotations are usually good summaries of corresponding webpages; 2) the count of annotations indicates the popularity of webpages. Two novel algorithms are proposed to incorporate the above information into page ranking: 1) SocialSimRank (SSR)calculates the similarity between social annotations and webqueries; 2) SocialPageRank (SPR) captures the popularity of webpages. Preliminary experimental results show that SSR can find the latent semantic association between queries and annotations, while SPR successfully measures the quality (popularity) of a webpage from the web usersu0027 perspective. We further evaluate the proposed methods empirically with 50 manually constructed queries and 3000 auto-generated queries on a dataset crawledfrom delicious. Experiments show that both SSR and SPRbenefit web search significantly."
2147717514;Approximate nearest neighbors: towards removing the curse of dimensionality;1998.0;[];We present two algorithms for the approximate nearest neighbor problem in high-dimensional spaces. For data sets of size n living in R d , the algorithms require space that is only polynomial in n and d, while achieving query times that are sub-linear in n and polynomial in d. We also show applications to other high-dimensional geometric problems, such as the approximate minimum spanning tree. The article is based on the material from the authorsu0027 STOCu002798 and FOCSu002701 papers. It unifies, generalizes and simplifies the results from those papers.
2147854123;The changing usage of a mature campus-wide wireless network;2008.0;[];Wireless Local Area Networks (WLANs) are now commonplace on many academic and corporate campuses. As u0027u0027Wi-Fiu0027u0027 technology becomes ubiquitous, it is increasingly important to understand trends in the usage of these networks. This paper analyzes an extensive network trace from a mature 802.11 WLAN, including more than 550 access points and 7000 users over seventeen weeks. We employ several measurement techniques, including syslog messages, telephone records, SNMP polling and tcpdump packet captures. This is the largest WLAN study to date, and the first to look at a mature WLAN. We compare this trace to a trace taken after the networku0027s initial deployment two years prior. We found that the applications used on the WLAN changed dramatically, with significant increases in peer-to-peer and streaming multimedia traffic. Despite the introduction of a Voice over IP (VoIP) system that includes wireless handsets, our study indicates that VoIP has been used little on the wireless network thus far, and most VoIP calls are made on the wired network. We saw greater heterogeneity in the types of clients used, with more embedded wireless devices such as PDAs and mobile VoIP clients. We define a new metric for mobility, the u0027u0027session diameteru0027u0027. We use this metric to show that embedded devices have different mobility characteristics than laptops, and travel further and roam to more access points. Overall, users were surprisingly non-mobile, with half remaining close to home about 98% of the time.
2148021103;Navigation Technologies for Autonomous Underwater Vehicles;2008.0;[];With recent advances in battery capacity and the development of hydrogen fuel cells, autonomous underwater vehicles (AUVs) are being used to undertake longer missions that were previously performed by manned or tethered vehicles. As a result, more advanced navigation systems are needed to maintain an accurate position over a larger operational area. The accuracy of the navigation system is critical to the quality of the data collected during survey missions and the recovery of the AUV. Many different methods for navigation in different underwater environments have been proposed in the literature. In this correspondence paper, the state of the art in navigation technologies for AUVs is investigated for theoretical and operational systems. Their suitability for use in different environments is compared and current limitations of these methods are identified. In addition, new approaches to address these current problems and areas for future research are suggested. Finally, it is concluded that only geophysically referenced methods will enable AUVs to navigate accurately over large areas and that advances in underwater feature recognition are required before these methods can be implemented in operational AUVs.
2148372454;On the virtual array concept for the fourth-order direction finding problem;1999.0;[];For more than a decade, fourth-order (FO) direction finding (DF) methods have been developed for non-Gaussian signals. Recently, it has been shown, through the introduction of the virtual cross-correlation (VCC) concept, that the use of FO cumulants for the DF problem increases the effective aperture of an arbitrary antenna array, which eventually introduces the virtual array concept. The purpose of this correspondence is first to present this virtual array (VA) concept through an alternative way that is easier and more direct to handle than the VCC tool and, second, to present further results associated with this concept, not only for arrays with space diversity but also for arrays with angular and/or polarization diversity.
2148606018;Recent Advances in Optimal Reliability Allocation;2007.0;[];"Reliability has become a greater concern in recent years, because high-tech industrial processes with ever increasing levels of sophistication comprise most engineering systems today. To keep pace with this rapidly developing field, this paper provides a broad overview of recent research on reliability optimization problems and their solution methodologies. In particular, we address issues related to: 1) universal generating-function-based optimal multistate system design; 2) percentile life employed as a system performance measure; 3) multiobjective optimization of reliability systems, especially with uncertain component-reliability estimations; and 4) innovation and improvement in traditional reliability optimization problems, such as fault-tolerance mechanism and cold-standby redundancy-involved system design. New developments in optimization techniques are also emphasized in this paper, especially the methods of ant colony optimization and hybrid optimization. We believe that the interesting problems that are reviewed here are deserving of more attention in the literature. To that end, this paper concludes with a discussion of future challenges related to reliability optimization"
2149137922;Security metrics and security investment models;2010.0;[];Planning information security investment is somewhere between art and science. This paper reviews and compares existing scientific approaches and discusses the relation between security investment models and security metrics. To structure the exposition, the high-level security production function is decomposed into two steps: cost of security is mapped to a security level, which is then mapped to benefits. This allows to structure data sources and metrics, to rethink the notion of security productivity, and to distinguish sources of indeterminacy as measurement error and attacker behavior. It is further argued that recently proposed investment models, which try to capture more features specific to information security, should be used for all strategic security investment decisions beneath defining the overall security budget.
2149139265;Goal Programming Model for Fire and Emergency Service Facilities Site Selection;2010.0;[]; The aim of this paper is to apply Goal Programming to select a site for a Fire and Emergency Service station in Sfax region in Tunisia. The decision making process involves incommensurable and conflicting objectives including proximity to industrial firms and response time. The concept of satisfaction function is utilized to explicitly introduce the Decision-Makeru0027s preferences in the decision-making process.
2149167588;New Avenues in Opinion Mining and Sentiment Analysis;2013.0;[];The Web holds valuable, vast, and unstructured information about public opinion. Here, the history, current use, and future of opinion mining and sentiment analysis are discussed, along with relevant techniques and tools.
2149295419;Maximum Power Point Tracking Scheme for PV Systems Operating Under Partially Shaded Conditions;2008.0;[];Current-voltage and power-voltage characteristics of large photovoltaic (PV) arrays under partially shaded conditions are characterized by multiple steps and peaks. This makes the tracking of the actual maximum power point (MPP) [global peak (GP)] a difficult task. In addition, most of the existing schemes are unable to extract maximum power from the PV array under these conditions. This paper proposes a novel algorithm to track the global power peak under partially shaded conditions. The formulation of the algorithm is based on several critical observations made out of an extensive study of the PV characteristics and the behavior of the global and local peaks under partially shaded conditions. The proposed algorithm works in conjunction with a DC-DC converter to track the GP. In order to accelerate the tracking speed, a feedforward control scheme for operating the DC-DC converter is also proposed, which uses the reference voltage information from the tracking algorithm to shift the operation toward the MPP. The tracking time with this controller is about one-tenth as compared to a conventional controller. All the observations and conclusions, including simulation and experimental results, are presented.
2149298154;An overview of statistical learning theory;1999.0;[];Statistical learning theory was introduced in the late 1960u0027s. Until the 1990u0027s it was a purely theoretical analysis of the problem of function estimation from a given collection of data. In the middle of the 1990u0027s new types of learning algorithms (called support vector machines) based on the developed theory were proposed. This made statistical learning theory not only a tool for the theoretical analysis but also a tool for creating practical algorithms for estimating multidimensional functions. This article presents a very general overview of statistical learning theory including both theoretical and algorithmic aspects of the theory. The goal of this overview is to demonstrate how the  learning theory established conditions for generalization which are more general than those discussed in classical statistical paradigms and how the understanding of these conditions inspired new algorithmic approaches to function estimation problems.
2149427297;Learning to Rank for Information Retrieval;2009.0;[];Learning to rank for Information Retrieval (IR) is a task to automatically construct a ranking model using training data, such that the model can sort new objects according to their degrees of relevance, preference, or importance. Many IR problems are by nature ranking problems, and many IR technologies can be potentially enhanced by using learning-to-rank techniques. The objective of this tutorial is to give an introduction to this research direction. Specifically, the existing learning-to-rank algorithms are reviewed and categorized into three approaches: the pointwise, pairwise, and listwise approaches. The advantages and disadvantages with each approach are analyzed, and the relationships between the loss functions used in these approaches and IR evaluation measures are discussed. Then the empirical evaluations on typical learning-to-rank methods are shown, with the LETOR collection as a benchmark dataset, which seems to suggest that the listwise approach be the most effective one among all the approaches. After that, a statistical ranking theory is introduced, which can describe different learning-to-rank algorithms, and be used to analyze their query-level generalization abilities. At the end of the tutorial, we provide a summary and discuss potential future work on learning to rank.
2149616758;Regular Partitions of Hypergraphs: Regularity Lemmas;2007.0;[];Szemerediu0027s regularity lemma for graphs has proved to be a powerful tool with many subsequent applications. The objective of this paper is to extend the techniques developed by Nagle, Skokan, and the authors and obtain a stronger and more ‘user-friendly’ regularity lemma for hypergraphs.
2150834164;A Taxonomy of Hybrid Metaheuristics;2002.0;[];"Hybrid metaheuristics have received considerable interest these recent years in the field of combinatorial optimization. A wide variety of hybrid approaches have been proposed in the literature. In this paper, a taxonomy of hybrid metaheuristics is presented in an attempt to provide a common terminology and classification mechanisms. The taxonomy, while presented in terms of metaheuristics, is also applicable to most types of heuristics and exact optimization  an :[67],""illustration of the usefulness of the taxonomy an annoted bibliography is given which classifies a large number of hybrid approaches according to the taxonomy."
2151105244;Direction-of-arrival estimation in partly calibrated subarray-based sensor arrays;2004.0;[];The problem of direction-of-arrival (DOA) estimation in partly calibrated arrays is addressed. We assume that an array is composed of multiple well-calibrated subarrays of arbitrary known geometry, but there are imperfections between subarrays. We address the cases of unknown (or known with a certain error) intersubarray displacements, imperfect synchronization of subarrays in time, unknown propagation channel mismatches between subarrays, as well as combinations of these effects. A new subspace-based approach to DOA estimation is proposed, which is applicable to this general class of partly calibrated arrays. DOA identifiability issues for such arrays are discussed, and a relevant Cramer-Rao bound (CRB) is derived. Numerical examples illustrate the performance of the proposed estimators.
2151554678;No free lunch theorems for optimization;1997.0;[];"A framework is developed to explore the connection between effective optimization algorithms and the problems they are solving. A number of ""no free lunch"" (NFL) theorems are presented which establish that for any algorithm, any elevated performance over one class of problems is offset by performance over another class. These theorems result in a geometric interpretation of what it means for an algorithm to be well suited to an optimization problem. Applications of the NFL theorems to information-theoretic aspects of optimization and benchmark measures of performance are also presented. Other issues addressed include time-varying optimization problems and a priori ""head-to-head"" minimax distinctions between optimization algorithms, distinctions that result despite the NFL theoremsu0027 enforcing of a type of uniformity over all algorithms."
2151892596;Just-In-Time Data Virtualization: Lightweight Data Management with ViDa;2015.0;[];"As the size of data and its heterogeneity increase, traditional database system architecture becomes an obstacle to data analysis. Integrating and ingesting (loading) data into databases is quickly becoming a bottleneck in face of massive data as well as increasingly heterogeneous data formats. Still, state-of-the-art approaches typically rely on copying and transforming data into one (or few) repositories. Queries, on the other hand, are often ad-hoc and supported by pre-cooked operators which are not adaptive enough to optimize access to data. As data formats and queries increasingly vary, there is a need to depart from the current status quo of static query processing primitives and build dynamic, fully adaptive architectures. We build ViDa, a system which reads data in its raw format and processes queries using adaptive, just-in-time operators. Our key insight is use of virtualization, i.e., abstracting data and manipulating it regardless of its original format, and dynamic generation of operators. ViDa’s query engine is generated just-in-time; its caches and its query operators adapt to the current query and the workload, while also treating raw datasets as its native storage structures. Finally, ViDa features a language expressive enough to support heterogeneous data models, and to which existing languages can be translated. Users therefore have the power to choose the language best suited for an analysis."
2152210408;Towards flexible evolution of dynamically adaptive systems;2012.0;[];Modern software systems need to be continuously available under varying conditions. Their ability to dynamically adapt to their execution context is thus increasingly seen as a key to their success. Recently, many approaches were proposed to design and support the execution of Dynamically Adaptive Systems (DAS). However, the ability of a DAS to evolve is limited to the addition, update or removal of adaptation rules or reconfiguration scripts. These artifacts are very specific to the control loop managing such a DAS and runtime evolution of the DAS requirements may affect other parts of the DAS. In this paper, we argue to evolve all parts of the loop. We suggest leveraging recent advances in model-driven techniques to offer an approach that supports the evolution of both systems and their adaptation capabilities. The basic idea is to consider the control loop itself as an adaptive system.
2153134505;Exploring folksonomy for personalized search;2008.0;[];"As a social service in Web 2.0, folksonomy provides the users the ability to save and organize their bookmarks online with ""social annotations"" or ""tags"". Social annotations are high quality descriptors of the web pagesu0027 topics as well as good indicators of web usersu0027 interests. We propose a personalized search framework to utilize folksonomy for personalized search. Specifically, three properties of folksonomy, namely the categorization, keyword, and structure property, are explored. In the framework, the rank of a web page is decided not only by the term matching between the query and the web pageu0027s content but also by the topic matching between the useru0027s interests and the web pageu0027s topics. In the evaluation, we propose an automatic evaluation framework based on folksonomy data, which is able to help lighten the common high cost in personalized search evaluations. A series of experiments are conducted using two heterogeneous data sets, one crawled from Del.icio.us and the other from Dogear. Extensive experimental results show that our personalized search approach can significantly improve the search quality."
2153199811;Intercloud Security Considerations;2010.0;[];Cloud computing is a new design pattern for large, distributed data centers. Service providers offering applications including search, email, and social networks have pioneered this specific to their application. Recently they have expanded offerings to include compute-related capabilities such as virtual machines, storage, and complete operating system services. The cloud computing design yields breakthroughs in geographical distribution, resource utilization efficiency, and infrastructure automation. These “public clouds” have been replicated by IT vendors for corporations to build “private clouds” of their own. Public and private clouds offer their end consumers a “pay as you go” model - a powerful shift for computing, towards a utility model like the electricity system, the telephone system, or more recently the Internet. However, unlike those utilities, clouds cannot yet federate and interoperate. Such federation is called the “Intercloud”. Building the Intercloud is more than technical protocols. Ablueprint for an Intercloud economy must bearchitected with a technically sound foundation and topology. As part of the overall Intercloud Topology, this paper builds on the technology foundation emerging for the Intercloud and specifically delves into details of Intercloud security considerations such as Trust Model, Identity and Access Management, governance considerations and so on.
2153204928;On the evolution of user interaction in Facebook;2009.0;[];"Online social networks have become extremely popular; numerous sites allow users to interact and share content using social links. Users of these networks often establish hundreds to even thousands of social links with other users. Recently, researchers have suggested examining the activity network - a network that is based on the actual interaction between users, rather than mere friendship - to distinguish between strong and weak links. While initial studies have led to insights on how an activity network is structurally different from the social network itself, a natural and important aspect of the activity network has been disregarded: the fact that over time social links can grow stronger or weaker. In this paper, we study the evolution of activity between users in the Facebook social network to capture this notion. We find that links in the activity network tend to come and go rapidly over time, and the strength of ties exhibits a general decreasing trend of activity as the social network link ages. For example, only 30% of Facebook user pairs interact consistently from one month to the next. Interestingly, we also find that even though the links of the activity network change rapidly over time, many graph-theoretic properties of the activity network remain unchanged."
2153207204;Discovering regions of different functions in a city using human mobility and POIs;2012.0;[];The development of a city gradually fosters different functional regions, such as educational areas and business districts. In this paper, we propose a framework (titled DRoF) that Discovers Regions of different Functions in a city using both human mobility among regions and points of interests (POIs) located in a region. Specifically, we segment a city into disjointed regions according to major roads, such as highways and urban express ways. We infer the functions of each region using a topic-based inference model, which regards a region as a document, a function as a topic, categories of POIs (e.g., restaurants and shopping malls) as metadata (like authors, affiliations, and key words), and human mobility patterns (when people reach/leave a region and where people come from and leave for) as words. As a result, a region is represented by a distribution of functions, and a function is featured by a distribution of mobility patterns. We further identify the intensity of each function in different locations. The results generated by our framework can benefit a variety of applications, including urban planning, location choosing for a business, and social recommendations. We evaluated our method using large-scale and real-world datasets, consisting of two POI datasets of Beijing (in 2010 and 2011) and two 3-month GPS trajectory datasets (representing human mobility) generated by over 12,000 taxicabs in Beijing in 2010 and 2011 respectively. The results justify the advantages of our approach over baseline methods solely using POIs or human mobility.
2153225416;DBpedia - A crystallization point for the Web of Data;2009.0;[];The DBpedia project is a community effort to extract structured information from Wikipedia and to make this information accessible on the Web. The resulting DBpedia knowledge base currently describes over 2.6 million entities. For each of these entities, DBpedia defines a globally unique identifier that can be dereferenced over the Web into a rich RDF description of the entity, including human-readable definitions in 30 languages, relationships to other resources, classifications in four concept hierarchies, various facts as well as data-level links to other Web data sources describing the entity. Over the last year, an increasing number of data publishers have begun to set data-level links to DBpedia resources, making DBpedia a central interlinking hub for the emerging Web of Data. Currently, the Web of interlinked data sources around DBpedia provides approximately 4.7 billion pieces of information and covers domains such as geographic information, people, companies, films, music, genes, drugs, books, and scientific publications. This article describes the extraction of the DBpedia knowledge base, the current status of interlinking DBpedia with other data sources on the Web, and gives an overview of applications that facilitate the Web of Data around DBpedia.
2153579005;Distributed Representations of Words and Phrases and their Compositionality;2013.0;[];"The recently introduced continuous Skip-gram model is an efficient method for learning high-quality distributed vector representations that capture a large number of precise syntactic and semantic word relationships. In this paper we present several extensions that improve both the quality of the vectors and the training speed. By subsampling of the frequent words we obtain significant speedup and also learn more regular word representations. We also describe a simple alternative to the hierarchical softmax called negative  :[76],""inherent limitation of word representations is their indifference to word order and their inability to represent idiomatic phrases. For example, the meanings of ""Canada"" and ""Air"" cannot be easily combined to obtain ""Air Canada"". Motivated by this example, we present a simple method for finding phrases in text, and show that learning good vector representations for millions of phrases is possible."
2153602280;Fast nearest neighbor retrieval for bregman divergences;2008.0;[];We present a data structure enabling efficient nearest neighbor (NN) retrieval for bregman divergences. The family of bregman divergences includes many popular dissimilarity measures including KL-divergence (relative entropy), Mahalanobis distance, and Itakura-Saito divergence. These divergences present a challenge for efficient NN retrieval because they are not, in general, metrics, for which most NN data structures are designed. The data structure introduced in this work shares the same basic structure as the popular metric ball tree, but employs convexity properties of bregman divergences in place of the triangle inequality. Experiments demonstrate speedups over brute-force search of up to several orders of magnitude.
2153710291;Quasirandomness, Counting and Regularity for 3-Uniform Hypergraphs;2006.0;[];The main results of this paper are regularity and counting lemmas for 3-uniform hypergraphs. A combination of these two results gives a new proof of a theorem of Frankl and Rodl, of which Szemerediu0027s theorem for arithmetic progressions of length 4 is a notable consequence. Frankl and Rodl also prove regularity and counting lemmas, but the proofs here, and even the statements, are significantly different. Also included in this paper is a proof of Szemerediu0027s regularity lemma, some basic facts about quasirandomness for graphs and hypergraphs, and detailed explanations of the motivation for the definitions used.
2154098113;A method for speed optimized partial product reduction and generation of fast parallel multipliers using an algorithmic approach;1996.0;[];This paper presents a method and an algorithm for generation of a parallel multiplier, which is optimized for speed. This method is applicable to any multiplier size and adaptable to any technology for which speed parameters are known. Most importantly, it is easy to incorporate this method in silicon compilation or logic synthesis tools. The parallel multiplier produced by the proposed method outperforms other schemes used for comparison in our experiment. It uses the minimal number of cells in the partial product reduction tree. These findings are tested on design examples simulated in 1 /spl mu/ CMOS ASIC technology.
2154262285;Context-aware reconfiguration of autonomic managers in real-time control applications;2010.0;[];We consider autonomic applications to systems for which continuous perfect monitoring of state is not possible. We use Exact-State Observers (ESO) to provide enhanced information about the system state. To achieve optimal configuration of the autonomic controller itself, over a wide range of environmental operating conditions, and across a wide range of unique application domains, we implement a new architecture for dynamic supervision and control systems in which a policy-based autonomic engine automatically selects both its monitoring and actuator components to suit ambient operating conditions.   By using a suite of ESOs tuned for different tradeoffs between real-time responsiveness and extent of system disturbance tolerated, and a policy mechanism to contextually select the most appropriate observer at any given time, we achieve self-configuring and self-optimising behaviours whilst keeping the complexity, resource-requirements and adaptation latency low.
2154798037;Multimodal freight transportation planning: A literature review;2014.0;[];Multimodal transportation offers an advanced platform for more efficient, reliable, flexible, and sustainable freight transportation. Planning such a complicated system provides interesting areas in Operations Research. This paper presents a structured overview of the multimodal transportation literature from 2005 onward. We focus on the traditional strategic, tactical, and operational levels of planning, where we present the relevant models and their developed solution techniques. We conclude our review paper with an outlook to future research directions.
2154851992;DeepWalk: online learning of social representations;2014.0;[];We present DeepWalk, a novel approach for learning latent representations of vertices in a network. These latent representations encode social relations in a continuous vector space, which is easily exploited by statistical models. DeepWalk generalizes recent advancements in language modeling and unsupervised feature learning (or deep learning) from sequences of words to graphs.   DeepWalk uses local information obtained from truncated random walks to learn latent representations by treating walks as the equivalent of sentences. We demonstrate DeepWalku0027s latent representations on several multi-label network classification tasks for social networks such as BlogCatalog, Flickr, and YouTube. Our results show that DeepWalk outperforms challenging baselines which are allowed a global view of the network, especially in the presence of missing information. DeepWalku0027s representations can provide F1 scores up to 10% higher than competing methods when labeled data is sparse. In some experiments, DeepWalku0027s representations are able to outperform all baseline methods while using 60% less training data.   DeepWalk is also scalable. It is an online learning algorithm which builds useful incremental results, and is trivially parallelizable. These qualities make it suitable for a broad class of real world applications such as network classification, and anomaly detection.
2155328222;Thumbs Up or Thumbs Down? Semantic Orientation Applied to Unsupervised Classification of Reviews;2002.0;[];"This paper presents a simple unsupervised learning algorithm for classifying reviews as recommended (thumbs up) or not recommended (thumbs down). The classification of a review is predicted by the average semantic orientation of the phrases in the review that contain adjectives or adverbs. A phrase has a positive semantic orientation when it has good associations (e.g., ""subtle nuances"") and a negative semantic orientation when it has bad associations (e.g., ""very cavalier""). In this paper, the semantic orientation of a phrase is calculated as the mutual information between the given phrase and the word ""excellent"" minus the mutual information between the given phrase and the word ""poor"". A review is classified as recommended if the average semantic orientation of its phrases is positive. The algorithm achieves an average accuracy of 74% when evaluated on 410 reviews from Epinions, sampled from four different domains (reviews of automobiles, banks, movies, and travel destinations). The accuracy ranges from 84% for automobile reviews to 66% for movie reviews."
2155893237;Caffe: Convolutional Architecture for Fast Feature Embedding;2014.0;[];Caffe provides multimedia scientists and practitioners with a clean and modifiable framework for state-of-the-art deep learning algorithms and a collection of reference models. The framework is a BSD-licensed C++ library with Python and MATLAB bindings for training and deploying general-purpose convolutional neural networks and other deep models efficiently on commodity architectures. Caffe fits industry and internet-scale media needs by CUDA GPU computation, processing over 40 million images a day on a single K40 or Titan GPU (approx 2 ms per image). By separating model representation from actual implementation, Caffe allows experimentation and seamless switching among platforms for ease of development and deployment from prototyping machines to cloud environments.   Caffe is maintained and developed by the Berkeley Vision and Learning Center (BVLC) with the help of an active community of contributors on GitHub. It powers ongoing research projects, large-scale industrial applications, and startup prototypes in vision, speech, and multimedia.
2156607688;DAG-based attack and defense modeling: don’t miss the forest for the attack trees;2014.0;[];"This paper presents the current state of the art on attack and defense modeling approaches that are based on directed acyclic graphs (DAGs). DAGs allow for a hierarchical decomposition of complex scenarios into simple, easily understandable and quantifiable actions. Methods based on threat trees and Bayesian networks are two well-known approaches to security modeling. However there exist more than 30 DAG-based methodologies, each having different features and  :[67],""objective of this survey is to summarize the existing methodologies, compare their features, and propose a taxonomy of the described formalisms. This article also supports the selection of an adequate modeling technique depending on user requirements."
2156773695;A study on the use of non-parametric tests for analyzing the evolutionary algorithms' behaviour: a case study on the CEC'2005 Special Session on Real Parameter Optimization;2009.0;[];"In recent years, there has been a growing interest for the experimental analysis in the field of evolutionary algorithms. It is noticeable due to the existence of numerous papers which analyze and propose different types of problems, such as the basis for experimental comparisons of algorithms, proposals of different methodologies in comparison or proposals of use of different statistical techniques in algorithmsu0027  :[62],""this paper, we focus our study on the use of statistical techniques in the analysis of evolutionary algorithmsu0027 behaviour over optimization problems. A study about the required conditions for statistical analysis of the results is presented by using some models of evolutionary algorithms for real-coding optimization. This study is conducted in two ways: single-problem analysis and multiple-problem analysis. The results obtained state that a parametric statistical analysis could not be appropriate specially when we deal with multiple-problem results. In multiple-problem analysis, we propose the use of non-parametric statistical tests given that they are less restrictive than parametric ones and they can be used over small size samples of results. As a case study, we analyze the published results for the algorithms presented in the CECu00272005 Special Session on Real Parameter Optimization by using non-parametric test procedures."
2156935079;Learning multi-label scene classification;2004.0;[];"In classic pattern recognition problems, classes are mutually exclusive by definition. Classification errors occur when the classes overlap in the feature space. We examine a different situation, occurring when the classes are, by definition, not mutually exclusive. Such problems arise in semantic scene and document classification and in medical diagnosis. We present a framework to handle such problems and apply it to the problem of semantic scene classification, where a natural scene may contain multiple objects such that the scene can be described by multiple class labels (e.g., a field scene with a mountain in the background). Such a problem poses challenges to the classic pattern recognition paradigm and demands a different treatment. We discuss approaches for training and testing in this scenario and introduce new metrics for evaluating individual examples, class recall and precision, and overall accuracy. Experiments show that our methods are suitable for scene classification; furthermore, our work appears to generalize to other classification problems of the same nature."
2156985756;On the virtual array concept for higher order array processing;2005.0;[];For about two decades, many fourth order (FO) array processing methods have been developed for both direction finding and blind identification of non-Gaussian signals. One of the main interests in using FO cumulants only instead of second-order (SO) ones in array processing applications relies on the increase of both the effective aperture and the number of sensors of the considered array, which eventually introduces the FO Virtual Array concept presented elsewhere and allows, in particular, a better resolution and the processing of more sources than sensors. To still increase the resolution and the number of sources to be processed from a given array of sensors, new families of blind identification, source separation, and direction finding methods, at an order m=2q (q/spl ges/2) only, have been developed recently. In this context, the purpose of this paper is to provide some important insights into the mechanisms and, more particularly, to both the resolution and the maximal processing capacity, of numerous 2qth order array processing methods, whose previous methods are part of, by extending the Virtual Array concept to an arbitrary even order for several arrangements of the data statistics and for arrays with space, angular and/or polarization diversity.
2157025439;What do people ask their social networks, and why?: a survey study of status message q&a behavior;2010.0;[];People often turn to their friends, families, and colleagues when they have questions. The recent, rapid rise of online social networking tools has made doing this on a large scale easy and efficient. In this paper we explore the phenomenon of using social network status messages to ask questions. We conducted a survey of 624 people, asking them to share the questions they have asked and answered of their online social networks. We present detailed data on the frequency of this type of question asking, the types of questions asked, and respondentsu0027 motivations for asking their social networks rather than using more traditional search tools like Web search engines. We report on the perceived speed and quality of the answers received, as well as what motivates people to respond to questions seen in their friendsu0027 status messages. We then discuss the implications of our findings for the design of next-generation search tools.
2157327941;Can we ever catch up with the Web;2010.0;[];The Semantic Web is about to grow up. By efforts such as the Linking Open Data initiative, we finally find ourselves at the edge of a Web of Data becoming reality. Standards such as OWL 2, RIF and SPARQL 1.1 shall allow us to reason with and ask complex structured queries on this data, but still they do not play together smoothly and robustly enough to cope with huge amounts of noisy Web data. In this paper, we discuss open challenges relating to querying and reasoning with Web data and raise the question: can the burgeoning Web of Data ever catch up with the now ubiquitous HTML Web?
2157909376;Research Commentary---Information in Digital, Economic, and Social Networks;2013.0;[];Digital technologies have made networks ubiquitous. A growing body of research is examining these networks to gain a better understanding of how firms interact with their consumers, how people interact with each other, and how current and future digital artifacts will continue to alter business and society. The increasing availability of massive networked data has led to several streams of inquiry across fields as diverse as computer science, economics, information systems, marketing, physics, and sociology. Each of these research streams asks questions that at their core involve “information in networks”---its distribution, its diffusion, its inferential value, and its influence on social and economic outcomes. We suggest a broad direction for research into social and economic networks. Our analysis describes four kinds of investigation that seem most promising. The first studies how information technologies create and reveal networks whose connections represent social and economic relationships. The second examines the content that flows through networks and its economic, social, and organizational implications. A third develops theories and methods to understand and utilize the rich predictive information contained in networked data. A final area of inquiry focuses on network dynamics and how information technology affects network evolution. We conclude by discussing several important cross-cutting issues with implications for all four research streams, which must be addressed if the ensuing research is to be both rigorous and relevant. We also describe how these directions of inquiry are interconnected: results and ideas will pollinate across them, leading to a new cumulative research tradition.
2157954477;Toward Scalable Systems for Big Data Analytics: A Technology Tutorial;2014.0;[];Recent technological advancements have led to a deluge of data from distinctive domains (e.g., health care and scientific sensors, user-generated data, Internet and financial companies, and supply chain systems) over the past two decades. The term big data was coined to capture the meaning of this emerging trend. In addition to its sheer volume, big data also exhibits other unique characteristics as compared with traditional data. For instance, big data is commonly unstructured and require more real-time analysis. This development calls for new system architectures for data acquisition, transmission, storage, and large-scale data processing mechanisms. In this paper, we present a literature survey and system tutorial for big data analytics platforms, aiming to provide an overall picture for nonexpert readers and instill a do-it-yourself spirit for advanced audiences to customize their own big-data solutions. First, we present the definition of big data and discuss big data challenges. Next, we present a systematic framework to decompose big data systems into four sequential modules, namely data generation, data acquisition, data storage, and data analytics. These four modules form a big data value chain. Following that, we present a detailed survey of numerous approaches and mechanisms from research and industry communities. In addition, we present the prevalent Hadoop framework for addressing big data challenges. Finally, we outline several evaluation benchmarks and potential research directions for big data systems.
2159498975;Visual pattern recognition by moment invariants;1962.0;[];In this paper a theory of two-dimensional moment invariants for planar geometric figures is presented. A fundamental theorem is established to relate such moment invariants to the well-known algebraic invariants. Complete systems of moment invariants under translation, similitude and orthogonal transformations are derived. Some moment invariants under general two-dimensional linear transformations are also included. Both theoretical formulation and practical models of visual pattern recognition based upon these moment invariants are discussed. A simple simulation program together with its performance are also presented. It is shown that recognition of geometrical patterns and alphabetical characters independently of position, size and orientation can be accomplished. It is also indicated that generalization is possible to include invariance with parallel projection.
2160331356;An FPGA Logic Cell and Carry Chain Configurable as a 6:2 or 7:2 Compressor;2009.0;[];"To improve FPGA performance for arithmetic circuits that are dominated by multi-input addition operations, an FPGA logic block is proposed that can be configured as a 6:2 or 7:2 compressor. Compressors have been used successfully in the past to realize parallel multipliers in VLSI technology; however, the peculiar structure of FPGA logic blocks, coupled with the high cost of the routing network relative to ASIC technology, renders compressors ineffective when mapped onto the general logic of an FPGA. On the other hand, current FPGA logic cells have already been enhanced with carry chains to improve arithmetic functionality, for example, to realize fast ternary carry-propagate addition. The contribution of this article is a new FPGA logic cell that is specialized to help realize efficient compressor trees on FPGAs. The new FPGA logic cell has two variants that can respectively be configured as a 6:2 or a 7:2 compressor using additional carry chains that, coupled with lookup tables, provide the necessary functionality. Experiments show that the use of these modified logic cells significantly reduces the delay of compressor trees synthesized on FPGAs compared to state-of-the-art synthesis techniques, with a moderate increase in area and power consumption."
2160688157;Empirical examination of the adoption of WebCT using TAM;2007.0;[];   Web Course Tools (WebCT) have enhanced the ability and motivation of institutes of higher education to support e-learning. In this study, we extended the Technology Acceptance Model to include technical support as a precursor and then investigated the role of the extended model in user acceptance of WebCT. Responses from 836 university students were used to test the proposed structural model. The data showed that technical support has a significant direct effect on perceived ease of use and usefulness, while perceived ease of use and usefulness are the dominant factors affecting the attitude of students using WebCT. The results indicate the importance of perceived ease of use and perceived usefulness in mediating the relationship of technical support with attitude and WebCT usage.
2161693416;Sending mixed signals: multilevel reputation effects in peer-to-peer lending markets;2010.0;[];Online peer-to-peer (P2P) lending organizations enable an individual to obtain an unsecured loan from a collection of individuals without the participation of a bank. Previous research has addressed the use of reputation systems to reduce information asymmetry based on individual history within online markets. Within the last few years one of the market leaders in P2P lending, Prosper.com, has sought to replace the information vetting and monitoring typically done by the bank with a community of users free to select its community members based on any criteria it chooses. By embedding individual reputations within a community reputation, incentives become aligned for peers to select highly qualified borrowers and produce more costly information signals to reduce the adverse selection and moral hazard risk typical of any principle-agent relationship. This study draws on theory from the Principle-Agent perspective to empirically examine the signals that enhance community reputation.
2162076967;Routing in a delay tolerant network;2004.0;[];We formulate the delay-tolerant networking routing problem, where messages are to be moved end-to-end across a connectivity graph that is time-varying but whose dynamics may be known in advance. The problem has the added constraints of finite buffers at each node and the general property that no contemporaneous end-to-end path may ever exist. This situation limits the applicability of traditional routing approaches that tend to treat outages as failures and seek to find an existing end-to-end path. We propose a framework for evaluating routing algorithms in such environments. We then develop several algorithms and use simulations to compare their performance with respect to the amount of knowledge they require about network topology. We find that, as expected, the algorithms using the least knowledge tend to perform poorly. We also find that with limited additional knowledge, far less than complete global knowledge, efficient algorithms can be constructed for routing in such environments. To the best of our knowledge this is the first such investigation of routing issues in DTNs.
2162429446;On Patterns for Decentralized Control in Self-Adaptive Systems;2013.0;[];Self-adaptation is typically realized using a control loop. One prominent approach for organizing a control loop in self-adaptive systems is by means of four components that are responsible for the primary functions of self-adaptation: Monitor, Analyze, Plan, and Execute, together forming a MAPE loop. When systems are large, complex, and heterogeneous, a single MAPE loop may not be sufficient for managing all adaptation in a system, so multiple MAPE loops may be introduced. In self-adaptive systems with multiple MAPE loops, decisions about how to decentralize each of the MAPE functions must be made. These decisions involve how and whether the corresponding functions from multiple loops are to be coordinated (e.g., planning components coordinating to prepare a plan for an adaptation). To foster comprehension of self-adaptive systems with multiple MAPE loops and support reuse of known solutions, it is crucial that we document common design approaches for engineers. As such systematic knowledge is currently lacking, it is timely to reflect on these systems to: (a) consolidate the knowledge in this area, and (b) to develop a systematic approach for describing different types of control in self-adaptive systems. We contribute with a simple notation for describing interacting MAPE loops, which we believe helps in achieving (b), and we use this notation to describe a number of existing patterns of interacting MAPE loops, to begin to fulfill (a). From our study, we outline numerous remaining research challenges in this area.
2162870238;Why hasn't technology disrupted academics' teaching practices? Understanding resistance to change through the lens of activity theory;2008.0;[];The advent of the Internet heralded predictions that e-learning would transform and disrupt teaching practices in higher education. E-learning also promised to expand opportunities for lifelong and flexible learning, and offered a panacea for practical issues such as decreased funding and increasing student numbers. The anticipated disruption to teaching and learning has not come to fruition however. Although technology is now common place in most higher education institutions - most institutions have invested in a virtual learning environment (VLE) and employ staff dedicated to supporting e-learning - there is little evidence of significant impact on teaching practices and current implementations are accused of being focused on improving administration and replicating behaviourist, content-driven models. This paper discusses a preliminary analysis, rooted in Activity Theory, of the transformation of teaching practices, which did or did not take place in our university following the institution-wide deployment of a VLE. In particular, factors limiting a full uptake of the VLE more advanced functionalities by the wider university community are explored.
2162954765;Time message system for delay tolerant networks;2012.0;[];Communication in mobile ad hoc networks and delay tolerant networks seeks to address the technical routing issues of heterogeneous networks that may lack continuous network connectivity. This work proposes the Time Message System (TMS), a delay tolerant routing solution for wireless networks. The protocol predicts the distance in function of time between nodes according to the time of last meetings. TMS was designed for high node density IEEE 802.11 networks. Simulation results in such networks show that TMS can deliver the same number of messages earlier than a traditional delay tolerant routing solution well-known by the research community.
2163352848;Multiresolution gray-scale and rotation invariant texture classification with local binary patterns;2002.0;[];"Presents a theoretically very simple, yet efficient, multiresolution approach to gray-scale and rotation invariant texture classification based on local binary patterns and nonparametric discrimination of sample and prototype distributions. The method is based on recognizing that certain local binary patterns, termed ""uniform,"" are fundamental properties of local image texture and their occurrence histogram is proven to be a very powerful texture feature. We derive a generalized gray-scale and rotation invariant operator presentation that allows for detecting the ""uniform"" patterns for any quantization of the angular space and for any spatial resolution and presents a method for combining multiple operators for multiresolution analysis. The proposed approach is very robust in terms of gray-scale variations since the operator is, by definition, invariant against any monotonic transformation of the gray scale. Another advantage is computational simplicity as the operator can be realized with a few operations in a small neighborhood and a lookup table. Experimental results demonstrate that good discrimination can be achieved with the occurrence statistics of simple rotation invariant local binary patterns."
2163490621;Mobile communication for human needs: A comparison of smartphone use between the US and Korea;2014.0;[];This study deals with two studies that develop and compare a measure and model of hierarchical needs of smartphone use from US and Korean users. The first study examines the dimensionality of measure by conducting an exploratory factor analysis on 398 US and 331 Korean college students. Results identified five constructs of the smartphone basic needs (SBN) scale from the two samples: physiological, safety, belongingness, self-esteem, and self-actualization. The second study examines the relationships between the SBN and use behavior, which leads to life satisfaction. The relationship of the constructs was theoretically synthesized and tested. Results indicate that both samples believe that the smartphone fulfills the needs of safety and self-actualization that predict smartphone use and life satisfaction. Theoretical and cross-cultural implications are discussed.
2163605009;ImageNet Classification with Deep Convolutional Neural Networks;2012.0;[];"We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 37.5% and 17.0% which is considerably better than the previous state-of-the-art. The neural network, which has 60 million parameters and 650,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and three fully-connected layers with a final 1000-way softmax. To make training faster, we used non-saturating neurons and a very efficient GPU implementation of the convolution operation. To reduce overriding in the fully-connected layers we employed a recently-developed regularization method called ""dropout"" that proved to be very effective. We also entered a variant of this model in the ILSVRC-2012 competition and achieved a winning top-5 test error rate of 15.3%, compared to 26.2% achieved by the second-best entry."
2163784380;Self-Managed Systems: an Architectural Challenge;2007.0;[];Self-management is put forward as one of the means by which we could provide systems that are scalable, support dynamic composition and rigorous analysis, and are flexible and robust in the presence of change. In this paper, we focus on architectural approaches to self-management, not because the language-level or network-level approaches are uninteresting or less promising, but because we believe that the architectural level seems to provide the required level of abstraction and generality to deal with the challenges posed. A self-managed software architecture is one in which components automatically configure their interaction in a way that is compatible with an overall architectural specification and achieves the goals of the system. The objective is to minimise the degree of explicit management necessary for construction and subsequent evolution whilst preserving the architectural properties implied by its specification. This paper discusses some of the current promising work and presents an outline three-layer reference model as a context in which to articulate some of the main outstanding research challenges.
2163937424;Multiple Target Tracking Based on Undirected Hierarchical Relation Hypergraph;2014.0;[];Multi-target tracking is an interesting but challenging task in computer vision field. Most previous data association based methods merely consider the relationships (e.g. appearance and motion pattern similarities) between detections in local limited temporal domain, leading to their difficulties in handling long-term occlusion and distinguishing the spatially close targets with similar appearance in crowded scenes. In this paper, a novel data association approach based on undirected hierarchical relation hypergraph is proposed, which formulates the tracking task as a hierarchical dense neighborhoods searching problem on the dynamically constructed undirected affinity graph. The relationships between different detections across the spatiotemporal domain are considered in a high-order way, which makes the tracker robust to the spatially close targets with similar appearance. Meanwhile, the hierarchical design of the optimization process fuels our tracker to long-term occlusion with more robustness. Extensive experiments on various challenging datasets (i.e. PETS2009 dataset, ParkingLot), including both low and high density sequences, demonstrate that the proposed method performs favorably against the state-of-the-art methods.
2164059558;ADTool: security analysis with attack---defense trees;2013.0;[];ADTool is free, open source software assisting graphical modeling and quantitative analysis of security, using attack---defense trees. The main features of ADTool are easy creation, efficient editing, and automated bottom-up evaluation of security-relevant measures. The tool also supports the usage of attack trees, protection trees and defense trees, which are all particular instances of attack---defense trees.
2164500538;The estimation of the gradient of a density function, with applications in pattern recognition;1975.0;[];Nonparametric density gradient estimation using a generalized kernel approach is investigated. Conditions on the kernel functions are derived to guarantee asymptotic unbiasedness, consistency, and uniform consistency of the estimates. The results are generalized to obtain a simple mcan-shift estimate that can be extended in a k -nearest-neighbor approach. Applications of gradient estimation to pattern recognition are presented using clustering and intrinsic dimensionality problems, with the ultimate goal of providing further understanding of these problems in terms of density gradients.
2164551073;Performing thrill: designing telemetry systems and spectator interfaces for amusement rides;2008.0;[];Fairground: Thrill Laboratory was a series of live events that augmented the experience of amusement rides. A wearable telemetry system captured video, audio, heart-rate and acceleration data, streaming them live to spectator interfaces and a watching audience. In this paper, we present a study of this event, which draws on video recordings and post-event interviews, and which highlights the experiences of riders, spectators and ride operators. Our study shows how the telemetry system transformed riders into performers, spectators into an audience, and how the role of ride operator began to include aspects of orchestration, with the relationship between all three roles also transformed. Critically, the introduction of a telemetry system seems to have had the potential to re-connect riders/performers back to operators/orchestrators and spectators/audience, re-introducing a closer relationship that used to be available with smaller rides. Introducing telemetry to a real-world situation also creates significant complexity, which we illustrate by focussing on a moment of perceived crisis.
2165046421;An ant colony optimization algorithm for the redundancy allocation problem (RAP);2004.0;[];This paper uses an ant colony meta-heuristic optimization method to solve the redundancy allocation problem (RAP). The RAP is a well known NP-hard problem which has been the subject of much prior work, generally in a restricted form where each subsystem must consist of identical components in parallel to make computations tractable. Meta-heuristic methods overcome this limitation, and offer a practical way to solve large instances of the relaxed RAP where different components can be placed in parallel. The ant colony method has not yet been used in reliability design, yet it is a method that is expressly designed for combinatorial problems with a neighborhood structure, as in the case of the RAP. An ant colony optimization algorithm for the RAP is devised u0026 tested on a well-known suite of problems from the literature. It is shown that the ant colony method performs with little variability over problem instance or random number seed. It is competitive with the best-known heuristics for redundancy allocation.
2165136898;A systematic literature review of empirical evidence on computer games and serious games;2012.0;[];This paper examines the literature on computer games and serious games in regard to the potential positive impacts of gaming on users aged 14 years or above, especially with respect to learning, skill enhancement and engagement. Search terms identified 129 papers reporting empirical evidence about the impacts and outcomes of computer games and serious games with respect to learning and engagement and a multidimensional approach to categorizing games was developed. The findings revealed that playing computer games is linked to a range of perceptual, cognitive, behavioural, affective and motivational impacts and outcomes. The most frequently occurring outcomes and impacts were knowledge acquisition/content understanding and affective and motivational outcomes. The range of indicators and measures used in the included papers are discussed, together with methodological limitations and recommendations for further work in this area.
2165254608;Quasi-Monte Carlo methods with applications in finance;2009.0;[];We review the basic principles of quasi-Monte Carlo (QMC) methods, the randomizations that turn them into variance-reduction techniques, the integration error and variance bounds obtained in terms of QMC point set discrepancy and variation of the integrand, and the main classes of point set constructions: lattice rules, digital nets, and permutations in different bases. QMC methods are designed to estimate s-dimensional integrals, for moderate or large (perhaps infinite) values of s. In principle, any stochastic simulation whose purpose is to estimate an integral fits this framework, but the methods work better for certain types of integrals than others (e.g., if the integrand can be well approximated by a sum of low-dimensional smooth functions). Such QMC-friendly integrals are encountered frequently in computational finance and risk analysis. We summarize the theory, give examples, and provide computational results that illustrate the efficiency improvement achieved. This article is targeted mainly for those who already know Monte Carlo methods and their application in finance, and want an update of the state of the art on quasi-Monte Carlo methods.
2165806612;Mining long-term search history to improve search accuracy;2006.0;[];Long-term search history contains rich information about a useru0027s search preferences, which can be used as search context to improve retrieval performance. In this paper, we study statistical language modeling based methods to mine contextual information from long-term search history and exploit it for a more accurate estimate of the query language model. Experiments on real web search data show that the algorithms are effective in improving search accuracy for both fresh and recurring queries. The best performance is achieved when using clickthrough data of past searches that are related to the current query.
2165836036;Scalable optimal countermeasure selection using implicit enumeration on attack countermeasure trees;2012.0;[];Constraints such as limited security investment cost precludes a security decision maker from implementing all possible countermeasures in a system. Existing analytical model-based security optimization strategies do not prevail for the following reasons: (i) none of these model-based methods offer a way to find optimal security solution in the absence of probability assignments to the model, (ii) methods scale badly as size of the system to model increases and (iii) some methods suffer as they use attack trees (AT) whose structure does not allow for the inclusion of countermeasures while others translate the non-state-space model (e.g., attack response tree) into a state-space model hence causing state-space explosion. In this paper, we use a novel AT paradigm called attack countermeasure tree (ACT) whose structure takes into account attacks as well as countermeasures (in the form of detection and mitigation events). We use greedy and branch and bound techniques to study several objective functions with goals such as minimizing the number of countermeasures, security investment cost in the ACT and maximizing the benefit from implementing a certain countermeasure set in the ACT under different constraints. We cast each optimization problem into an integer programming problem which also allows us to find optimal solution even in the absence of probability assignments to the model. Our method scales well for large ACTs and we compare its efficiency with other approaches.
2166210850;Robust airplane detection in satellite images;2011.0;[];Automatic target detection in satellite images remains a challenging problem. The main difficulties lie in the cooccurrence of variations of target type, pose, and size in huge satellite image. In this paper, we propose a new airplane detection approach based on visual saliency computation and symmetry detection. The advantages are twofold. First, saliency and symmetry detection perform stably in obtaining target location and orientation information. Second, independent of target type, pose and size, saliency map and symmetry detection are computed only once. This saves a large amount of computational time but does not miss any targets. Experiments show that our method provides a promising way to detect airplanes in complex airport scenes.
2166406195;Single-copy routing in intermittently connected mobile networks;2004.0;[];Intermittently connected mobile networks are wireless networks where most of the time there does not exist a complete path from source to destination, or such a path is highly unstable and may break soon after it has been discovered. In this context, conventional routing schemes would fail. To deal with such networks we propose the use of an opportunistic hop-by-hop routing model. According to the model, a series of independent, local forwarding decisions are made, based on current connectivity and predictions of future connectivity information diffused through nodesu0027 mobility. The important issue here is how to choose an appropriate next hop. To this end, we propose and analyze via theory and simulations a number of routing algorithms. The champion algorithm turns out to be one that combines the simplicity of a simple random policy, which is efficient in finding good leads towards the destination, with the sophistication of utility-based policies that efficiently follow good leads. We also state and analyze the performance of an oracle-based optimal algorithm, and compare it to the online approaches. The metrics used in the comparison are the average message delivery delay and the number of transmissions per message delivered.
2166646151;TagSense: a smartphone-based approach to automatic image tagging;2011.0;[];Mobile phones are becoming the convergent platform for personal sensing, computing, and communication. This paper attempts to exploit this convergence towards the problem of automatic image tagging. We envision TagSense, a mobile phone based collaborative system that senses the people, activity, and context in a picture, and merges them carefully to create tags on-the-fly. The main challenge pertains to discriminating phone users that are in the picture from those that are not. We deploy a prototype of TagSense on 8 Android phones, and demonstrate its effectiveness through 200 pictures, taken in various social settings. While research in face recognition continues to improve image tagging, TagSense is an attempt to embrace additional dimensions of sensing towards this end goal. Performance comparison with Apple iPhoto and Google Picasa shows that such an out-of-band approach is valuable, especially with increasing device density and greater sophistication in sensing/learning algorithms.
2166761701;Mobile learning: A framework and evaluation;2007.0;[];Wireless data communications in form of Short Message Service (SMS) and Wireless Access Protocols (WAP) browsers have gained global popularity, yet, not much has been done to extend the usage of these devices in electronic learning (e-learning). This project explores the extension of e-learning into wireless/handheld (W/H) computing devices with the help of a mobile learning (m-learning) framework. This framework provides the requirements to develop m-learning applications that can be used to complement classroom or distance learning. A prototype application was developed to link W/H devices to three course websites. The m-learning applications were pilot-tested for two semesters with a total of 63 students from undergraduate and graduate courses at our university. The students used the m-learning environment with a variety of W/H devices and reported their experiences through a survey and interviews at the end of the semester. The results from this exploratory study provide a better understanding on the role of mobile technology in higher education.
2167221577;Continuous Locomotion-Mode Identification for Prosthetic Legs Based on Neuromuscular–Mechanical Fusion;2011.0;[];In this study, we developed an algorithm based on neuromuscular-mechanical fusion to continuously recognize a variety of locomotion modes performed by patients with transfemoral (TF) amputations. Electromyographic (EMG) signals recorded from gluteal and residual thigh muscles and ground reaction forces/moments measured from the prosthetic pylon were used as inputs to a phase-dependent pattern classifier for continuous locomotion-mode identification. The algorithm was evaluated using data collected from five patients with TF amputations. The results showed that neuromuscular-mechanical fusion outperformed methods that used only EMG signals or mechanical information. For continuous performance of one walking mode (i.e., static state), the interface based on neuromuscular-mechanical fusion and a support vector machine (SVM) algorithm produced 99% or higher accuracy in the stance phase and 95% accuracy in the swing phase for locomotion-mode recognition. During mode transitions, the fusion-based SVM method correctly recognized all transitions with a sufficient predication time. These promising results demonstrate the potential of the continuous locomotion-mode classifier based on neuromuscular-mechanical fusion for neural control of prosthetic legs.
2167389327;Supervised fuzzy clustering for the identification of fuzzy classifiers;2003.0;[];The classical fuzzy classifier consists of rules each one describing one of the classes. In this paper a new fuzzy model structure is proposed where each rule can represent more than one classes with different probabilities. The obtained classifier can be considered as an extension of the quadratic Bayes classifier that utilizes mixture of models for estimating the class conditional densities. A supervised clustering algorithm has been worked out for the identification of this fuzzy model. The relevant input variables of the fuzzy classifier have been selected based on the analysis of the clusters by Fisheru0027s interclass separability criteria. This new approach is applied to the well-known wine and Wisconsin breast cancer classification problems.
2167467982;Link Prediction and Recommendation across Heterogeneous Social Networks;2012.0;[];Link prediction and recommendation is a fundamental problem in social network analysis. The key challenge of link prediction comes from the sparsity of networks due to the strong disproportion of links that they have potential to form to links that do form. Most previous work tries to solve the problem in single network, few research focus on capturing the general principles of link formation across heterogeneous networks. In this work, we give a formal definition of link recommendation across heterogeneous networks. Then we propose a ranking factor graph model (RFG) for predicting links in social networks, which effectively improves the predictive performance. Motivated by the intuition that people make friends in different networks with similar principles, we find several social patterns that are general across heterogeneous networks. With the general social patterns, we develop a transfer-based RFG model that combines them with network structure information. This model provides us insight into fundamental principles that drive the link formation and network evolution. Finally, we verify the predictive performance of the presented transfer model on 12 pairs of transfer cases. Our experimental results demonstrate that the transfer of general social patterns indeed help the prediction of links.
2167521347;An SLA-based Broker for Cloud Infrastructures;2013.0;[];"The breakthrough of Cloud comes from its service oriented perspective where everything, including the infrastructure, is provided ""as a service"". This model is really attractive and convenient for both providers and consumers, as a consequence the Cloud paradigm is quickly growing and widely spreading, also in non commercial contexts. In such a scenario, we propose to incorporate some elements of volunteer computing into the Cloud paradigm through the Cloud@Home solution, involving into the mix nodes and devices provided by potentially any owners or administrators, disclosing high computational resources to contributors and also allowing to maximize their utilization. This paper presents and discusses the first step towards Cloud@Home: providing quality of service and service level agreement facilities on top of unreliable, intermittent Cloud providers. Some of the main issues and challenges of Cloud@Home, such as the monitoring, management and brokering of resources according to service level requirements are addressed through the design of a framework core architecture. All the tasks committed to the architectureu0027s modules and components, as well as the most relevant component interactions, are identified and discussed from both the structural and the behavioural viewpoints. Some encouraging experiments on an early implementation prototype deployed in a real testing environment are also documented in the paper."
2167748275;Learning revised models for planning in adaptive systems;2013.0;[];Environment domain models are a key part of the information used by adaptive systems to determine their behaviour. These models can be incomplete or inaccurate. In addition, since adaptive systems generally operate in environments which are subject to change, these models are often also out of date. To update and correct these models, the system should observe how the environment responds to its actions, and compare these responses to those predicted by the model. In this paper, we use a probabilistic rule learning approach, NoMPRoL, to update models using feedback from the running system in the form of execution traces. NoMPRoL is a technique for non-monotonic probabilistic rule learning based on a transformation of an inductive logic programming task into an equivalent abductive one. In essence, it exploits consistent observations by finding general rules which explain observations in terms of the conditions under which they occur. The updated models are then used to generate new behaviour with a greater chance of success in the actual environment encountered.
2167861526;General transitivity conditions for fuzzy reciprocal preference matrices;2003.0;[];"A reciprocal fuzzy matrix (relation) is a non-negative matrix Q = {qij} such that qij + qji = 1 for all i,j ∈ {1,2,..., n}. We define general transitivity conditions (named FG-transitivities) for fuzzy reciprocal preference relations and show that they generalize some well-known transitivities. We also study relationships of these conditions with two models of rational preferences (the so-called ""utility"" model and the ""multidimensional"" model)."
2168023489;Decentralised Progressive Signal Systems for Organic Traffic Control;2008.0;[];An increased mobility and the resulting rising traffic demands lead to serious congestion problems in many cities. Although there is not a single solution that will solve traffic congestion and the related environmental and economical problems, traffic light coordination is an important factor in achieving efficient networks. This paper presents a new distributed approach for dynamic traffic light coordination that relies on locally available traffic data and communication among neighboring intersections. The coordination mechanism is combined with an organic traffic control approach to form an adaptive, distributed control system with learning capabilities. The efficiency of the resulting organic system is demonstrated in a simulation-based evaluation.
2168689650;Recent developments in graph matching;2000.0;[];Graphs are a powerful and versatile tool useful in various subfields of science and engineering. In many applications, for example, in pattern recognition and computer vision, it is required to measure the similarity of objects. When graphs are used for the representation of structured objects, then the problem of measuring object similarity turns into the problem of computing the similarity of graphs, which is also known as graph matching. In this paper, similarity measures on graphs and related algorithms are reviewed. Also theoretical work showing various relations between different similarity measures is discussed. Other topics to be addressed include graph clustering and efficient indexing of large databases of graphs.
2168822268;Optimizing Optimum-Path Forest Classification for Huge Datasets;2010.0;[];Traditional pattern recognition techniques can not handle the classification of large datasets with both efficiency and effectiveness. In this context, the Optimum-Path Forest (OPF) classifier was recently introduced, trying to achieve high recognition rates and low computational cost. Although OPF was much faster than Support Vector Machines for training, it was slightly slower for classification. In this paper, we present the Efficient OPF (EOPF), which is an enhanced and faster version of the traditional OPF, and validate it for the automatic recognition of white matter and gray matter in magnetic resonance images of the human brain.
2169046918;Attribute-Based Access Control with Efficient Revocation in Data Outsourcing Systems;2011.0;[];Some of the most challenging issues in data outsourcing scenario are the enforcement of authorization policies and the support of policy updates. Ciphertext-policy attribute-based encryption is a promising cryptographic solution to these issues for enforcing access control policies defined by a data owner on outsourced data. However, the problem of applying the attribute-based encryption in an outsourced architecture introduces several challenges with regard to the attribute and user revocation. In this paper, we propose an access control mechanism using ciphertext-policy attribute-based encryption to enforce access control policies with efficient attribute and user revocation capability. The fine-grained access control can be achieved by dual encryption mechanism which takes advantage of the attribute-based encryption and selective group key distribution in each attribute group. We demonstrate how to apply the proposed mechanism to securely manage the outsourced data. The analysis results indicate that the proposed scheme is efficient and secure in the data outsourcing systems.
2169064301;A comparative study of Artificial Bee Colony algorithm;2009.0;[];Artificial Bee Colony (ABC) algorithm is one of the most recently introduced swarm-based algorithms. ABC simulates the intelligent foraging behaviour of a honeybee swarm. In this work, ABC is used for optimizing a large set of numerical test functions and the results produced by ABC algorithm are compared with the results obtained by genetic algorithm, particle swarm optimization algorithm, differential evolution algorithm and evolution strategies. Results show that the performance of the ABC is better than or similar to those of other population-based algorithms with the advantage of employing fewer control parameters.
2169223373;Personalized Web search for improving retrieval effectiveness;2004.0;[];Current Web search engines are built to serve all users, independent of the special needs of any individual user. Personalization of Web search is to carry out retrieval for each user incorporating his/her interests. We propose a novel technique to learn user profiles from usersu0027 search histories. The user profiles are then used to improve retrieval effectiveness in Web search. A user profile and a general profile are learned from the useru0027s search history and a category hierarchy, respectively. These two profiles are combined to map a user query into a set of categories which represent the useru0027s search intention and serve as a context to disambiguate the words in the useru0027s query. Web search is conducted based on both the user query and the set of categories. Several profile learning and category mapping algorithms and a fusion algorithm are provided and evaluated. Experimental results indicate that our technique to personalize Web search is both effective and efficient.
2169239645;Cyber Physical Systems: Design Challenges;2008.0;[];Cyber-Physical Systems (CPS) are integrations of computation and physical processes. Embedded computers and networks monitor and control the physical processes, usually with feedback loops where physical processes affect computations and vice versa. The economic and societal potential of such systems is vastly greater than what has been realized, and major investments are being made worldwide to develop the technology. There are considerable challenges, particularly because the physical components of such systems introduce safety and reliability requirements qualitatively different from those in general- purpose computing. Moreover, physical components are qualitatively different from object-oriented software components. Standard abstractions based on method calls and threads do not work. This paper examines the challenges in designing such systems, and in particular raises the question of whether todayu0027s computing and networking technologies provide an adequate foundation for CPS. It concludes that it will not be sufficient to improve design processes, raise the level of abstraction, or verify (formally or otherwise) designs that are built on todayu0027s abstractions. To realize the full potential of CPS, we will have to rebuild computing and networking abstractions. These abstractions will have to embrace physical dynamics and computation in a unified way.
2169491762;Supervised pattern classification based on optimum-path forest;2009.0;[];"We present a supervised classification method which represents each class by one or more optimum-path trees rooted at some key samples, called prototypes. The training samples are nodes of a complete graph, whose arcs are weighted by the distances between the feature vectors of their nodes. Prototypes are identified in all classes and the minimization of a connectivity function by dynamic programming assigns to each training sample a minimum-cost path from its most strongly connected prototype. This competition among prototypes partitions the graph into an optimum-path forest rooted at them. The class of the samples in an optimum-path tree is assumed to be the same of its root. A test sample is classified similarly, by identifying which tree would contain it, if the sample were part of the training set. By choice of the graph model and connectivity function, one can devise other optimum-path forest classifiers. We present one of them, which is fast, simple, multiclass, parameter independent, does not make any assumption about the shapes of the classes, and can handle some degree of overlapping between classes. We also propose a general algorithm to learn from errors on an evaluation set without increasing the training set, and show the advantages of our method with respect to SVM, ANN-MLP, and k-NN classifiers in several experiments with datasets of various types. © 2009 Wiley Periodicals, Inc. Int J Imaging Syst Technol, 19, 120–131,  :[232],""preliminary version of the paper was presented at the 12th International Workshop on Combinatorial Image Analysis (Papa et al.,2008a)."
2169529055;Kernel-Based Weighted Multi-view Clustering;2012.0;[];Exploiting multiple representations, or views, for the same set of instances within a clustering framework is a popular practice for boosting clustering accuracy. However, some of the available sources may be misleading (due to noise, errors in measurement etc.) in revealing the true structure of the data, thus, their inclusion in the clustering process may have negative influence. This aspect seems to be overlooked in the multi-view literature where all representations are equally considered. In this work, views are expressed in terms of given kernel matrices and a weighted combination of the kernels is learned in parallel to the partitioning. Weights assigned to kernels are indicative of the quality of the corresponding viewsu0027 information. Additionally, the combination scheme incorporates a parameter that controls the admissible sparsity of the weights to avoid extremes and tailor them to the data. Two efficient iterative algorithms are proposed that alternate between updating the view weights and recomputing the clusters to optimize the intra-cluster variance from different perspectives. The conducted experiments reveal the effectiveness of our methodology compared to other multi-view methods.
2169822912;PBTrust: A Priority-Based Trust Model for Service Selection in General Service-Oriented Environments;2010.0;[];How to choose the best service provider (agent), which a service consumer can trust in terms of the quality and success rate of the service in an open and dynamic environment, is a challenging problem in many service-oriented applications such as Internet-based grid systems, e-trading systems, as well as service-oriented computing systems. This paper presents a Priority-Based Trust (PBTrust) model for service selection in general service-oriented environments. The PBTrust is robust and novel from several perspectives. (1) The reputation of a service provider is derived from referees who are third parties and had interactions with the provider in a rich context format, including attributes of the service, the priority distribution on attributes and a rating value for each attribute from a third party, (2) The concept of `Similarityu0027 is introduced to measure the difference in terms of distributions of priorities on attributes between requested service and a refereed service in order to precisely predict the performance of a potential provider on the requested service, (3) The concept of general performance of a service provider on a service in history is also introduced to improve the success rate on the requested service. The experimental results can prove that PBtrust has a better performance than that of the CR model in a service-oriented environment.
2170224461;Network Analysis of an Emergent Massively Collaborative Creation Community - How Can People Create Videos Collaboratively Without Collaboration? -;2009.0;[];The Web technology enables numerous people to collaborate in creation. We designate it as massively collaborative creation via the Web. It is becoming an important activity such as Wikipedia and Yahoo! QA. As an example of massively collaborative creation, we particularly examine video development on Nico Nico Douga, which is a video sharing website that is popular in Japan. We specifically examine videos on Hatsune Miku, a version of a singing synthesizer application software that has inspired not only song creation but also songwriting, illustration, and video editing. As described herein, creators of interact to create new contents though their social network. We analyzed the process of developing thousands of videos based on creators’ social networks. The social network reveals interesting features. Different categories of creators serve different roles in evolving the network. We also extracted communities from the network and observed different community structures and investigated the evolving nature of the network using motif analysis.
2170240176;Character-level Convolutional Networks for Text Classification;2015.0;[];This article offers an empirical exploration on the use of character-level convolutional networks (ConvNets) for text classification. We constructed several large-scale datasets to show that character-level convolutional networks could achieve state-of-the-art or competitive results. Comparisons are offered against traditional models such as bag of words, n-grams and their TFIDF variants, and deep learning models such as word-based ConvNets and recurrent neural networks.
2170253147;Regular Partitions of Hypergraphs: Counting Lemmas;2007.0;[];We continue the study of regular partitions of hypergraphs. In particular, we obtain corresponding counting lemmas for the regularity lemmas for hypergraphs from our paper ‘Regular Partitions of Hypergraphs: Regularity Lemmas’ (in this issue).
2171104217;Trust as a facilitator in cloud computing: a survey;2012.0;[];" :[0],""computing offers massively scalable, elastic resources (e.g., data, :[0],""computing power, and services) over the internet from remote data centres to the consumers. The growing market penetration, with an evermore diverse provider and service landscape, turns Cloud :[0],""computing marketplaces a highly competitive one. In this highly competitive and distributed service environment, the assurances are insufficient for the consumers to identify the dependable and trustworthy Cloud  :[65],""paper provides a landscape and discusses incentives and hindrances to adopt Cloud :[0],""computing from Cloud consumers’ perspective. Due to these hindrances, potential consumers are not sure whether they can trust the Cloud providers in offering dependable services. Trust-aided unified evaluation framework by leveraging trust and reputation systems can be used to assess trustworthiness (or dependability) of Cloud providers. Hence, cloud-related specific parameters (QoS + ) are required for the trust and reputation systems in Cloud environments. We identify the essential properties and corresponding research challenges to integrate the QoS + parameters into trust and reputation systems. Finally, we survey and analyse the existing trust and reputation systems in various application domains, characterizing their individual strengths and weaknesses. Our work contributes to understanding 1) why trust establishment is important in the Cloud :[0],""computing landscape, 2) how trust can act as a facilitator in this context and 3) what are the exact requirements for trust and reputation models (or systems) to support the consumers in establishing trust on Cloud providers."
2171637483;Constructing adapted lattice rules using problem-dependent criteria;2012.0;[];We describe a new software tool named Lattice Builder, designed to construct lattice point sets for quasi-Monte Carlo integration via randomly-shifted lattice rules. This tool permits one to search for good lattice parameters in terms of various uniformity criteria, for an arbitrary number of points and arbitrary dimension. It also constructs lattices that are extensible in the number of points and in the dimension. A numerical illustration is given.
2173213060;MapReduce: simplified data processing on large clusters;2008.0;[];MapReduce is a programming model and an associated implementation for processing and generating large datasets that is amenable to a broad variety of real-world tasks. Users specify the computation in terms of a map and a reduce function, and the underlying runtime system automatically parallelizes the computation across large-scale clusters of machines, handles machine failures, and schedules inter-machine communication to make efficient use of the network and disks. Programmers find the system easy to use: more than ten thousand distinct MapReduce programs have been implemented internally at Google over the past four years, and an average of one hundred thousand MapReduce jobs are executed on Googleu0027s clusters every day, processing a total of more than twenty petabytes of data per day.
2173226459;A novel genetic programming approach for epileptic seizure detection;2016.0;[];HighlightsA Constructive Genetic Programming (CGP) is proposed for epileptic seizure detection.Five different experiments were performed to show the performance of CGP on EEG signal.CGP produces the highest classification accuracy among other classifiers. The human brain is a delicate mix of neurons (brain cells), electrical impulses and chemicals, known as neurotransmitters. Any damage has the potential to disrupt the workings of the brain and cause seizures. These epileptic seizures are the manifestations of epilepsy. The electroencephalograph (EEG) signals register average neuronal activity from the cerebral cortex and label changes in activity over large areas. A detailed analysis of these electroencephalograph (EEG) signals provides valuable insights into the mechanisms instigating epileptic disorders. Moreover, the detection of interictal spikes and epileptic seizures in an EEG signal plays an important role in the diagnosis of epilepsy. Automatic seizure detection methods are required, as these epileptic seizures are volatile and unpredictable. This paper deals with an automated detection of epileptic seizures in EEG signals using empirical mode decomposition (EMD) for feature extraction and proposes a novel genetic programming (GP) approach for classifying the EEG signals. Improvements in the standard GP approach are made using a Constructive Genetic Programming (CGP) in which constructive crossover and constructive subtree mutation operators are introduced. A hill climbing search is integrated in crossover and mutation operators to remove the destructive nature of these operators. A new concept of selecting the Globally Prime offspring is also presented to select the best fitness offspring generated during crossover. To decrease the time complexity of GP, a new dynamic fitness value computation (DFVC) is employed to increase the computational speed. We conducted five different sets of experiments to evaluate the performance of the proposed model in the classification of different mixtures of normal, interictal and ictal signals, and the accuracies achieved are outstandingly high. The experimental results are compared with the existing methods on same datasets, and these results affirm the potential use of our method for accurately detecting epileptic seizures in an EEG signal.
2179530124;A 1.1 V 2y-nm 4.35 Gb/s/pin 8 Gb LPDDR4 Mobile Device With Bandwidth Improvement Techniques;2015.0;[];The demands on higher bandwidth with reduced power consumption in mobile market are driving mobile DRAM with advanced design techniques. Proposed LPDDR4 in this paper achieves over 39% improvement in power efficiency and over 4.3 Gbps data rate with 1.1 V supply voltage. These are challenging targets compared with those of LPDDR3. This work describes design schemes employed in LPDDR4 to satisfy these requirements, such as multi-channel-per-die architecture, multiple training modes, low-swing interface, DQS and clock frequency dividing, and internal reference for data and command-address signals. This chip was fabricated in a 3-metal 2y-nm DRAM CMOS process.
2180046157;Statistical mechanics approximation of biogeography-based optimization;2016.0;[];Biogeography-based optimization BBO is an evolutionary algorithm inspired by biogeography, which is the study of the migration of species between habitats. This paper derives a mathematical description of the dynamics of BBO based on ideas from statistical mechanics. Rather than trying to exactly predict the evolution of the population, statistical mechanics methods describe the evolution of statistical properties of the population fitness. This paper uses the one-max problem, which has only one optimum and whose fitness function is the number of 1s in a binary string, to derive equations that predict the statistical properties of BBO each generation in terms of those of the previous generation. These equations reveal the effect of migration and mutation on the population fitness dynamics of BBO. The results obtained in this paper are similar to those for the simple genetic algorithm with selection and mutation. The paper also derives equations for the population fitness dynamics of general separable functions, and we find that the results obtained for separable functions are the same as those for the one-max problem. The statistical mechanics theory of BBO is shown to be in good agreement with simulation.
2180507614;GPS Error Correction With Pseudorange Evaluation Using Three-Dimensional Maps;2015.0;[];The accuracy of the positions of a pedestrian is very important and useful information for the statistics, advertisement, and safety of different applications. Although the GPS chip in a smartphone is currently the most convenient device to obtain the positions, it still suffers from the effect of multipath and nonline-of-sight propagation in urban canyons. These reflections could greatly degrade the performance of a GPS receiver. This paper describes an approach to estimate a pedestrian position by the aid of a 3-D map and a ray-tracing method. The proposed approach first distributes the numbers of position candidates around a reference position. The weighting of the position candidates is evaluated based on the similarity between the simulated pseudorange and the observed pseudorange. Simulated pseudoranges are calculated using a ray-tracing simulation and a 3-D map. Finally, the proposed method was verified through field experiments in an urban canyon in Tokyo. According to the results, the proposed approach successfully estimates the reflection and direct paths so that the estimate appears very close to the ground truth, whereas the result of a commercial GPS receiver is far from the ground truth. The results show that the proposed method has a smaller error distance than the conventional method.
2181306409;HERO – A Home Based Routing in Pocket Switched Networks;2012.0;[];"Pocket switched networks (PSNs) take advantage of human mobility to distribute data. Investigations on real-world trace data indicate that human mobility follows a simple reproducible pattern: a human being usually visits a few places at high frequencies. These most frequently visited places form the home of a node, which is exploited in this paper to design two HomE based ROuting (HERO) algorithms. In the basic HERO, the first encountered relay whose home contains the place where the destination resides is selected to deliver the data. The enhanced HERO, on the other hand, continuously selects a better relay that visits the destination place at a higher frequency. In both algorithms, each node only needs to maintain and exchange its relatively stable home information and/or the corresponding visiting frequencies; therefore no global networking information and no frequent information update are needed, resulting in a low burden on the network due to its low communication and storage overheads. Moreover, HERO involves only simple arithmetic operations, thus causing little computation overhead at the mobile nodes. The simulation results indicate that both HERO algorithms outperform the state-of-the art."
2186845332;Learning semantic representations using convolutional neural networks for web search;2014.0;[];This paper presents a series of new latent semantic models based on a convolutional neural network (CNN) to learn low-dimensional semantic vectors for search queries and Web documents. By using the convolution-max pooling operation, local contextual information at the word n-gram level is modeled first. Then, salient local fea-tures in a word sequence are combined to form a global feature vector. Finally, the high-level semantic information of the word sequence is extracted to form a global vector representation. The proposed models are trained on clickthrough data by maximizing the conditional likelihood of clicked documents given a query, us-ing stochastic gradient ascent. The new models are evaluated on a Web document ranking task using a large-scale, real-world data set. Results show that our model significantly outperforms other se-mantic models, which were state-of-the-art in retrieval performance prior to this work.
2187206419;Intracranial hemorrhage detection by 3D voxel segmentation on brain CT images;2015.0;[];Intracranial hemorrhage (ICH) is a vital disease which occurs due to leakage or rupture of blood vessels within the brain tissue. In this paper, we propose a novel three-dimensional (3D) method for segmenting hemorrhage regions from a series of brain computed tomography (CT) images. This method combines a supervoxel approach for rough segmentation and three-dimensional graph cuts for refined segmentation. The main novelty of this method is to generalize traditional 2D segmentation of intracranial hemorrhage to a 3D approach, so that the intra-frame information of CT images is utilized to obtain better segmentation results. To evaluate the method, a brain CT database is built, which consists of 20 patients with intracranial hemorrhage at different sizes and locations. Experimental results demonstrate that the proposed approach provides segmentation which is similar to the manually labeled ground truth and outperforms existing 3D methods in accuracy and time-complexity.
2187483593;Linear dimensionality reduction: survey, insights, and generalizations;2015.0;[];Linear dimensionality reduction methods are a cornerstone of analyzing high dimensional data, due to their simple geometric interpretations and typically attractive computational properties. These methods capture many data features of interest, such as covariance, dynamical structure, correlation between data sets, input-output relationships, and margin between data classes. Methods have been developed with a variety of names and motivations in many fields, and perhaps as a result the connections between all these methods have not been highlighted. Here we survey methods from this disparate literature as optimization programs over matrix manifolds. We discuss principal component analysis, factor analysis, linear multidimensional scaling, Fisheru0027s linear discriminant analysis, canonical correlations analysis, maximum autocorrelation factors, slow feature analysis, sufficient dimensionality reduction, undercomplete independent component analysis, linear regression, distance metric learning, and more. This optimization framework gives insight to some rarely discussed shortcomings of well-known methods, such as the suboptimality of certain eigenvector solutions. Modern techniques for optimization over matrix manifolds enable a generic linear dimensionality reduction solver, which accepts as input data and an objective to be optimized, and returns, as output, an optimal low-dimensional projection of the data. This simple optimization framework further allows straightforward generalizations and novel variants of classical methods, which we demonstrate here by creating an orthogonal-projection canonical correlations analysis. More broadly, this survey and generic solver suggest that linear dimensionality reduction can move toward becoming a blackbox, objective-agnostic numerical technology.
2193214538;Quantitative Risk Analysis in Information Security Management: A Modern Fairy Tale;2015.0;[];According to conventional wisdom, information security management must start with a quantitative risk analysis. Such an analysis works fine in theory, but it hardly works in practice. Baseline requirements, vulnerability management, and qualitative risk analysis can combine to provide a viable alternative.
2194213766;An update to the systematic literature review of empirical evidence of the impacts and outcomes of computer games and serious games;2016.0;[];Continuing interest in digital games indicated that it would be useful to update Connolly et al.u0027s (2012) systematic literature review of empirical evidence about the positive impacts and outcomes of games. Since a large number of papers was identified in the period from 2009 to 2014, the current review focused on 143 papers that provided higher quality evidence about the positive outcomes of games. Connolly et al.u0027s multidimensional analysis of games and their outcomes provided a useful framework for organising the varied research in this area. The most frequently occurring outcome reported for games for learning was knowledge acquisition, while entertainment games addressed a broader range of affective, behaviour change, perceptual and cognitive and physiological outcomes. Games for learning were found across varied topics with STEM subjects and health the most popular. Future research on digital games would benefit from a systematic programme of experimental work, examining in detail which game features are most effective in promoting engagement and supporting learning. The current systematic literature review updates Author (date).The review looks at impacts and outcomes of playing digital games from 2009 to 2014.Multi-component coding of papers, games and learning outcomes was used.Many papers were found with 143 papers providing high quality evidence.Games for entertainment and learning addressed different outcomes.
2194775991;Deep Residual Learning for Image Recognition;2016.0;[];Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers—8× deeper than VGG nets [40] but still having lower complexity. An ensemble of these residual nets achieves 3.57% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC u0026 COCO 2015 competitions1, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.
2120010140;Vehicle and Guard Rail Detection Using Radar and Vision Data Fusion;2007.0;[];This paper describes a vehicle detection system fusing radar and vision data. Radar data are used to locate areas of interest on images. Vehicle search in these areas is mainly based on vertical symmetry. All the vehicles found in different image areas are mixed together, and a series of filters is applied in order to delete false detections. In order to speed up and improve system performance, guard rail detection and a method to manage overlapping areas are also included. Both methods are explained and justified in this paper. The current algorithm analyzes images on a frame-by-frame basis without any temporal correlation. Two different statistics, namely: 1) frame based and 2) event based, are computed to evaluate vehicle detection efficiency, while guard rail detection efficiency is computed in terms of time savings and correct detection rates. Results and problems are discussed, and directions for future enhancements are provided
2117539524;ImageNet Large Scale Visual Recognition Challenge;2015.0;[];The ImageNet Large Scale Visual Recognition Challenge is a benchmark in object category classification and detection on hundreds of object categories and millions of images. The challenge has been run annually from 2010 to present, attracting participation from more than fifty institutions. This paper describes the creation of this benchmark dataset and the advances in object recognition that have been possible as a result. We discuss the challenges of collecting large-scale ground truth annotation, highlight key breakthroughs in categorical object recognition, provide a detailed analysis of the current state of the field of large-scale image classification and object detection, and compare the state-of-the-art computer vision accuracy with human accuracy. We conclude with lessons learned in the 5 years of the challenge, and propose future directions and improvements.
2168081761;Biogeography-Based Optimization;2008.0;[];Biogeography is the study of the geographical distribution of biological organisms. Mathematical equations that govern the distribution of organisms were first discovered and developed during the 1960s. The mindset of the engineer is that we can learn from nature. This motivates the application of biogeography to optimization problems. Just as the mathematics of biological genetics inspired the development of genetic algorithms (GAs), and the mathematics of biological neurons inspired the development of artificial neural networks, this paper considers the mathematics of biogeography as the basis for the development of a new field: biogeography-based optimization (BBO). We discuss natural biogeography and its mathematics, and then discuss how it can be used to solve optimization problems. We see that BBO has features in common with other biology-based optimization methods, such as GAs and particle swarm optimization (PSO). This makes BBO applicable to many of the same types of problems that GAs and PSO are used for, namely, high-dimension problems with multiple local optima. However, BBO also has some features that are unique among biology-based optimization methods. We demonstrate the performance of BBO on a set of 14 standard benchmarks and compare it with seven other biology-based optimization algorithms. We also demonstrate BBO on a real-world sensor selection problem for aircraft engine health estimation.
2101812529;A New Metaheuristic Bat-Inspired Algorithm;2010.0;[];Metaheuristic algorithms such as particle swarm optimization, firefly algorithm and harmony search are now becoming powerful methods for solving many tough optimization problems. In this paper, we propose a new metaheuristic method, the Bat Algorithm, based on the echolocation behaviour of bats. We also intend to combine the advantages of existing algorithms into the new bat algorithm. After a detailed formulation and explanation of its implementation, we will then compare the proposed algorithm with other existing algorithms, including genetic algorithms and particle swarm optimization. Simulations show that the proposed algorithm seems much superior to other algorithms, and further studies are also discussed.
2172153090;Privacy in content-oriented networking: threats and countermeasures;2013.0;[];As the Internet struggles to cope with scalability, mobility, and security issues, new network architectures are being proposed to better accommodate the needs of modern systems and applications. In particular, Content-Oriented Networking (CON) has emerged as a promising next-generation Internet architecture: it sets to decouple content from hosts, at the network layer, by naming data rather than hosts. CON comes with a potential for a wide range of benefits, including reduced congestion and improved delivery speed by means of content caching, simpler configuration of network devices, and security at the data level. However, it remains an interesting open question whether or not, and to what extent, this emerging networking paradigm bears new privacy challenges. In this paper, we provide a systematic privacy analysis of CON and the common building blocks among its various architectural instances in order to highlight emerging privacy threats, and analyze a few potential countermeasures. Finally, we present a comparison between CON and todayu0027s Internet in the context of a few privacy concepts, such as, anonymity, censoring, traceability, and confidentiality.
2210977594;Weighted Multi-view Clustering with Feature Selection;2016.0;[];In recent years, combining multiple sources or views of datasets for data clustering has been a popular practice for improving clustering accuracy. As different views are different representations of the same set of instances, we can simultaneously use information from multiple views to improve the clustering results generated by the limited information from a single view. Previous studies mainly focus on the relationships between distinct data views, which would get some improvement over the single-view clustering. However, in the case of high-dimensional data, where each view of data is of high dimensionality, feature selection is also a necessity for further improving the clustering results. To overcome this problem, this paper proposes a novel algorithm termed Weighted Multi-view Clustering with Feature Selection (WMCFS) that can simultaneously perform multi-view data clustering and feature selection. Two weighting schemes are designed that respectively weight the views of data points and feature representations in each view, such that the best view and the most representative feature space in each view can be selected for clustering. Experimental results conducted on real-world datasets have validated the effectiveness of the proposed method. HighlightsThis paper proposes a new multi-view data clustering algorithm.The new method considers both view weighting and feature weighting.An EM-like method is designed to get the local optimum solution.Extensive experiments have been conducted to show the effectiveness.
2214098284;Developing a General Extended Technology Acceptance Model for E-Learning (GETAMEL) by analysing commonly used external factors;2016.0;[];To identify the most commonly used external factors of Technology Acceptance Model (TAM) in the context of e-learning adoption, a quantitative meta-analysis of 107 papers covering the last ten years was performed. The results show that Self-Efficacy, Subjective Norm, Enjoyment, Computer Anxiety and Experience are the most commonly used external factors of TAM. The effects of these commonly used external factors on TAMu0027s two main constructs, Perceived Ease of Use (PEOU) and Perceived Usefulness (PU), have been studied across a range of e-learning technology types and e-learning user types. The results show that the best predictor of studentu0027s PEOU of e-learning systems is Self-Efficacy (β?=?0.352), followed by Enjoyment (β?=?0.341), Experience (β?=?0.221), Computer Anxiety (β?=?-0.199) and Subjective Norm (β?=?0.195). The best predictor of studentu0027s PU of e-learning systems is Enjoyment (β?=?0.452), followed by Subjective Norm (β?=?0.301), Self-Efficacy (β?=?0.174) and Experience (β?=?0.169). Using these external factors and their effect sizes on PEOU and PU, this study proposes a General Extended Technology Acceptance Model for E-Learning (GETAMEL). We performed a meta-analysis of 107 recent e-learning acceptance studies.We identified the most commonly used external factors of Technology Acceptance Model.They are Self-Efficacy, Subjective Norm, Enjoyment, Computer Anxiety and Experience.We identified the effects of these factors on studentsu0027 perceptions of e-learning.As a result the study proposes an extended technology acceptance model for e-learning.
2224486821;Genetic Approximations for the Failure-Free Security Games;2015.0;[];This paper deals with computational aspects of attack trees, more precisely, evaluating the expected adversarial utility in the failure-free game, where the adversary is allowed to re-run failed atomic attacks an unlimited number of times. It has been shown by Buldas and Lenin that exact evaluation of this utility is an NP-complete problem, so a computationally feasible approximation is needed. In this paper we consider a genetic approach for this challenge. Since genetic algorithms depend on a number of non-trivial parameters, we face a multi-objective optimization problem and we consider several heuristic criteria to solve it.
2231771044;Serving DBpedia with DOLCE --- More than Just Adding a Cherry on Top;2015.0;[];Large knowledge bases, such as DBpedia, are most often created heuristically due to scalability issues. In the building process, both random as well as systematic errors may occur. In this paper, we focus on finding systematic errors, or anti-patterns, in DBpedia. We show that by aligning the DBpedia ontology to the foundational ontology DOLCE-Zero, and by combining reasoning and clustering of the reasoning results, errors affecting millions of statements can be identified at a minimal workload for the knowledge base designer.
2232317135;SCA: A Sine Cosine Algorithm for solving optimization problems;2016.0;[];   This paper proposes a novel population-based optimization algorithm called Sine Cosine Algorithm (SCA) for solving optimization problems. The SCA creates multiple initial random candidate solutions and requires them to fluctuate outwards or towards the best solution using a mathematical model based on sine and cosine functions. Several random and adaptive variables also are integrated to this algorithm to emphasize exploration and exploitation of the search space in different milestones of optimization. The performance of SCA is benchmarked in three test phases. Firstly, a set of well-known test cases including unimodal, multi-modal, and composite functions are employed to test exploration, exploitation, local optima avoidance, and convergence of SCA. Secondly, several performance metrics (search history, trajectory, average fitness of solutions, and the best solution during optimization) are used to qualitatively observe and confirm the performance of SCA on shifted two-dimensional test functions. Finally, the cross-section of an aircraftu0027s wing is optimized by SCA as a real challenging case study to verify and demonstrate the performance of this algorithm in practice. The results of test functions and performance metrics prove that the algorithm proposed is able to explore different regions of a search space, avoid local optima, converge towards the global optimum, and exploit promising regions of a search space during optimization effectively. The SCA algorithm obtains a smooth shape for the airfoil with a very low drag, which demonstrates that this algorithm can highly be effective in solving real problems with constrained and unknown search spaces. Note that the source codes of the SCA algorithm are publicly available at  http://www.alimirjalili.com/SCA.html .
2233116163;Quantized Convolutional Neural Networks for Mobile Devices;2016.0;[];Recently, convolutional neural networks (CNN) have demonstrated impressive performance in various computer vision tasks. However, high performance hardware is typically indispensable for the application of CNN models due to the high computation complexity, which prohibits their further extensions. In this paper, we propose an efficient framework, namely Quantized CNN, to simultaneously speed-up the computation and reduce the storage and memory overhead of CNN models. Both filter kernels in convolutional layers and weighting matrices in fully-connected layers are quantized, aiming at minimizing the estimation error of each layeru0027s response. Extensive experiments on the ILSVRC-12 benchmark demonstrate 4 ~ 6× speed-up and 15 ~ 20× compression with merely one percentage loss of classification accuracy. With our quantized CNN model, even mobile devices can accurately classify images within one second.
2238935679;A survey of biogeography-based optimization;2017.0;[];Optimization is a classical issue and in many areas that are bound up with people’s daily life. In current decades, with the development of human civilization and industry society, many complicated optimization problems are raised. In the meantime, corresponding novel approaches are constantly proposed for solving these problems. One of them is meta-heuristics, which is inspired from natural phenomena and contains many kinds of algorithms. The classical meta-heuristic algorithms have exhibited their superiority in dealing optimization problems, especially for specific problems such as combinatorial optimization. As a novel meta-heuristic algorithm, biogeography-based optimization (BBO), inspired from the science of biogeography, has its own characteristics and exhibits a huge potential in computation and optimization. According to current investigations and analysis on this algorithm, it has not only achieved a great success in numerical optimization problems, but also been implemented in various kinds of applications, and drawn worldwide attentions. In this paper, we present a survey for this algorithm. First, we introduce the basic operators of BBO, including migration and mutation. For migration operator, it mimics species migration among islands, which provides a recombination way for candidate solutions to interact with each other so that the whole population can be improved. Besides linear migration model, several other popular migration models are also introduced and the corresponding performances are analyzed. For mutation operator, the design of BBO is different from other meta-heuristics. In standard BBO, different candidate solutions have different migration rates and the rate assignment is influential to BBO’s performance. Second, we summarized some popular variants of BBO and related hybrid algorithms that significantly enhance BBO’s performance. This part introduces the development of this algorithm and helps readers understand the way to choose a suitable version of BBO for a given problem. The way to improve algorithms’ performances helps readers design new variants of BBO for specific problems. Third, we present the evaluation of BBO’s performance for both numerical and practical problems. The results demonstrate BBO is competent to solve optimization problems. Despite so many achievements of BBO, some open issues that should be considered and solved in future work in order to make this algorithm more competitive in meta-heuristics.
2241295367;Typical Target Detection in Satellite Images Based on Convolutional Neural Networks;2015.0;[];With the rapid technological development of various different satellite sensors, a huge volume of high-resolution image data sets can now be obtained and widely used in military and civilian fields. Detecting typical targets in satellite images is a challenging task due to the complicated background. Traditional manually engineered features (i.e. HOG, Gabor feature and Hough transform, etc.) do not work well for massive high-resolution remote sensing image data. Thus, we are expected to find an efficient way to automatically learn the presentations from the massive image data and increase the computational efficiency of target detection. Comparing to the general objects in nature images, the edge information of targets in satellite images shows more distinctive and concise characteristics. This paper proposes a new target detection framework based on Edge Boxes and Convolutional Neural Networks (CNN). CNN can learn rich features automatically and is invariant to small rotation and shifts, has achieved state of-the-art performance in many image classification databases. Edge Boxes can generate a smaller set of object proposals based on the edges of objects. The proposed method can reduce the computational time of the detector. Extensive experiments demonstrate that the proposed framework is effective in typical target detection systems.
2247173740;ComProFITS: A web-based platform for human resources competence assessment;2015.0;[];An efficient assessment of human resource competences, followed by the goal oriented analysis of the results, support the identification of the competence gaps in organizations and the allocation of resources towards identified gaps. This paper presents the basic idea as well as the scientific and implementation results of the ComProFITS project as an innovative web-based platform for the evaluation of existing employees and the recruitment of new employees in organizations. The platform integrates research on the statistical assessment of competences, an innovative competence pyramid and many alternative methods of employee evaluation and recruitment. The initial version of the platform is being applied and used in the IT sector in a large organization in Spain with the goal of extending it to the Spanish and European wide enterprises in different sectors.
2247394048;Multiple Anonymized Social Networks Alignment;2015.0;[];"Users nowadays are normally involved in multiple (usually more than two) online social networks simultaneously to enjoy more social network services. Some of the networks that users are involved in can share common structures either due to the analogous network construction purposes or because of the similar social network features. However, the social network datasets available in research are usually pre-anonymized and accounts of the shared users in different networks are mostly isolated without any known connections. In this paper, we want to identify such connections between the shared usersu0027 accounts in multiple social networks (i.e., the anchor links), which is formally defined as the M-NASA (Multiple Anonymized Social Networks Alignment) problem. M-NASA is very challenging to address due to (1) the lack of known anchor links to build models, (2) the studied networks are anonymized, where no usersu0027 personal profile or attribute information is available, and (3) the ""transitivity law"" and the ""one-to-one property"" based constraints on anchor links. To resolve these challenges, a novel two-phase network alignment framework UMA (Unsupervised Multi-network Alignment) is proposed in this paper. Extensive experiments conducted on multiple real-world partially aligned social networks demonstrate that UMA can perform very well in solving the M-NASA problem."
2247776437;OPEM: A Static-Dynamic Approach for Machine-Learning-Based Malware Detection;2013.0;[];Malware is any computer software potentially harmful to both computers and networks. The amount of malware is growing every year and poses a serious global security threat. Signature-based detection is the most extended method in commercial antivirus software, however, it consistently fails to detect new malware. Supervised machine learning has been adopted to solve this issue. There are two types of features that supervised malware detectors use: (i) static features and (ii) dynamic features. Static features are extracted without executing the sample whereas dynamic ones requires an execution. Both approaches have their advantages and disadvantages. In this paper, we propose for the first time, OPEM, an hybrid unknown malware detector which combines the frequency of occurrence of operational codes (statically obtained) with the information of the execution trace of an executable (dynamically obtained). We show that this hybrid approach enhances the performance of both approaches when run separately.
2248369874;Which Type of Citation Analysis Generates the Most Accurate Taxonomy of Scientific and Technical Knowledge;2017.0;[];In 1965, Price foresaw the day when a citation-based taxonomy of science and technology would be delineated and correspondingly used for science policy. A taxonomy needs to be comprehensive and accurate if it is to be useful for policy making, especially now that policy makers are utilizing citation-based indicators to evaluate people, institutions and laboratories. Determining the accuracy of a taxonomy, however, remains a challenge. Previous work on the accuracy of partition solutions is sparse, and the results of those studies, although useful, have not been definitive. In this study we compare the accuracies of topic-level taxonomies based on the clustering of documents using direct citation, bibliographic coupling, and co-citation. Using a set of new gold standards-articles with at least 100 references-we find that direct citation is better at concentrating references than either bibliographic coupling or co-citation. Using the assumption that higher concentrations of references denote more accurate clusters, direct citation thus provides a more accurate representation of the taxonomy of scientific and technical knowledge than either bibliographic coupling or co-citation. We also find that discipline-level taxonomies based on journal schema are highly inaccurate compared to topic-level taxonomies, and recommend against their use.
2249408652;Beat the Mean Bandit;2011.0;[];The Dueling Bandits Problem is an online learning framework in which actions are restricted to noisy comparisons between pairs of strategies (also called bandits). It models settings where absolute rewards are difficult to elicit but pairwise preferences are readily available. In this paper, we extend the Dueling Bandits Problem to a relaxed setting where preference magnitudes can violate transitivity. We present the first algorithm for this more general Dueling Bandits Problem and provide theoretical guarantees in both the online and the PAC settings. We also show that the new algorithm has stronger guarantees than existing results even in the original Dueling Bandits Problem, which we validate empirically.
2250539671;Glove: Global Vectors for Word Representation;2014.0;[];Recent methods for learning vector space representations of words have succeeded in capturing fine-grained semantic and syntactic regularities using vector arithmetic, but the origin of these regularities has remained opaque. We analyze and make explicit the model properties needed for such regularities to emerge in word vectors. The result is a new global logbilinear regression model that combines the advantages of the two major model families in the literature: global matrix factorization and local context window methods. Our model efficiently leverages statistical information by training only on the nonzero elements in a word-word cooccurrence matrix, rather than on the entire sparse matrix or on individual context windows in a large corpus. The model produces a vector space with meaningful substructure, as evidenced by its performance of 75% on a recent word analogy task. It also outperforms related models on similarity tasks and named entity recognition.
2260424533;Using a Portable Device for Online Single-Trial MRCP Detection and Classification;2015.0;[];In the past decade, the use of movement-related cortical potentials (MRCPs) for brain computer interface-based rehabilitation protocols has increased manifolds. Such systems suffer severely from high frequency colored noise making it extremely difficult to recognize these signals with high accuracy on a single-trial basis. All previous work in this domain has mainly focused on offline systems using computing power of lab computers in which the detection of the MRCPs is done independent to the classification of the type of movement. The main focus of this work is to test the detection of the presence of the MRCP signal as well as its classification into different types of movements in a single online system (portable Raspberry Pi II) where the classification system takes over only after the presence of MRCP signal has been detected. To achieve this, the MRCP signal was first spatially (Laplacian) and later band pass filtered to improve the signal to noise ratio, then a matched filter was applied to detect the signal. This was obtained with a detection latency of −458 ± 97 ms before the movement execution. Then six temporal features were extracted from 400 ms data after the point of detection to be classified by a standard linear support vector machine. The overall accuracy of 73 % was achieved for the online detection and classification for four different types of movements which is very close to the base line accuracy of 74 % using the offline system. The whole system was tested on Matlab and verified on a Raspberry Pi II as a portable device. The results show that the online implementation of such a system is feasible and can be adapted for stroke patient rehabilitation.
2260771218;Feature Selection: A Data Perspective;2017.0;[];Feature selection, as a data preprocessing strategy, has been proven to be effective and efficient in preparing data (especially high-dimensional data) for various data-mining and machine-learning problems. The objectives of feature selection include building simpler and more comprehensible models, improving data-mining performance, and preparing clean, understandable data. The recent proliferation of big data has presented some substantial challenges and opportunities to feature selection. In this survey, we provide a comprehensive and structured overview of recent advances in feature selection research. Motivated by current challenges and opportunities in the era of big data, we revisit feature selection research from a data perspective and review representative feature selection algorithms for conventional data, structured data, heterogeneous data and streaming data. Methodologically, to emphasize the differences and similarities of most existing feature selection algorithms for conventional data, we categorize them into four main groups: similarity-based, information-theoretical-based, sparse-learning-based, and statistical-based methods. To facilitate and promote the research in this community, we also present an open source feature selection repository that consists of most of the popular feature selection algorithms (http://featureselection.asu.edu/). Also, we use it as an example to show how to evaluate feature selection algorithms. At the end of the survey, we present a discussion about some open problems and challenges that require more attention in future research.
2267075006;Fuzzy Entropic Ordered Weighted Averaging Operator and Its Application in Group Decision Making;2016.0;[];In the paper, we develop a new method for multiple attribute group decision making for fuzzy numbers. The fuzzy entropic weighted averaging FEOWA operator is an extension of the entropic ordered weighted averaging operator, which unifies the fuzzy entropy and the ordered weighted averaging operator in the same formulation. Then, some of its main properties by utilizing some operational laws of fuzzy numbers are studied. We also present the generalized entropic ordered weighted averaging operator and the fuzzy generalized entropic ordered weighted averaging operator. Moreover, a method based on the FEOWA operator for decision making is presented. Finally, a numerical example illustrates the applicability and effectiveness of the proposed method.
2272899302;Social Data Visualization System for Understanding Diffusion Patterns on Twitter: A Case Study on Korean Enterprises;2015.0;[];"Online social media have been playing an important role of creating and diffusing information to many users. It means the users can get cognitive influence to the other users. Thus, it is important to understand how the information can be diffused by interactions among users through online social media. In this paper, we design a social media monitoring system (called ""TweetPulseu0027u0027) which can analyze and show meaningful diffusion patterns (DP) among the users. Particularly, TweetPulse focuses on visualizing information diffusion in Twitter, given a certain time duration. Also, this work has investigated the relationships 1) between DP and event detecting, 2) between DP and emotional words, and 3) between DP and the number of followers of the users. Thereby, to understand the continuous patterns of the information diffusion, we propose two different types of analytic methods, which are 1) macroscopic approach and 2) microscopic approach. For evaluating the proposed method, we have collected and preprocessed the dataset during about 4 months (14 March 2012 to 12 July 2012). As a conclusion, TweetPulse has helped users to easily understand DP from a large scale dataset streaming through Twitter."
2273847690;Empath: Understanding Topic Signals in Large-Scale Text;2016.0;[];"Human language is colored by a broad range of topics, but existing text analysis tools only focus on a small number of them. We present Empath, a tool that can generate and validate new lexical categories on demand from a small set of seed terms (like ""bleed"" and ""punch"" to generate the category violence). Empath draws connotations between words and phrases by deep learning a neural embedding across more than 1.8 billion words of modern fiction. Given a small set of seed words that characterize a category, Empath uses its neural embedding to discover new related terms, then validates the category with a crowd-powered filter. Empath also analyzes text across 200 built-in, pre-validated categories we have generated from common topics in our web dataset, like neglect, government, and social media. We show that Empathu0027s data-driven, human validated categories are highly correlated (r=0.906) with similar categories in LIWC."
2275719586;An adaptive-to-model test for partially parametric single-index models;2017.0;[];Residual marked empirical process-based tests are commonly used in regression models. However, they suffer from data sparseness in high-dimensional space when there are many covariates. This paper has three purposes. First, we suggest a partial dimension reduction adaptive-to-model testing procedure that can be omnibus against general global alternative models although it fully use the dimension reduction structure under the null hypothesis. This feature is because that the procedure can automatically adapt to the null and alternative models, and thus greatly overcomes the dimensionality problem. Second, to achieve the above goal, we propose a ridge-type eigenvalue ratio estimate to automatically determine the number of linear combinations of the covariates under the null and alternative hypotheses. Third, a Monte-Carlo approximation to the sampling null distribution is suggested. Unlike existing bootstrap approximation methods, this gives an approximation as close to the sampling null distribution as possible by fully utilising the dimension reduction model structure under the null model. Simulation studies and real data analysis are then conducted to illustrate the performance of the new test and compare it with existing tests.
2276957120;A new measure of divergence with its application to multi-criteria decision making under fuzzy environment;2017.0;[];Divergence measure is an important tool for determining the amount of discrimination between two probability distributions. Since the introduction of fuzzy sets, divergence measures between two fuzzy sets have gained attention for their applications in various fields. Exponential entropy measure has some advantages over Shannonu0027s entropy. In this paper, we used the idea of Jensen---Shannon divergence to define a new divergence measure called `fuzzy Jensen-exponential divergence (FJSD)u0027 for measuring the discrimination/difference between two fuzzy sets. The measure is demonstrated to satisfy some very elegant properties, which shows its strength for applications in multi-criteria decision-making problems. Further, we develop a method to solve multi-criteria decision-making problems under fuzzy phenomenon by utilizing the proposed measure and demonstrate by a numerical example.
2279359805;Hierarchical Geographical Tags Based Routing Scheme in Delay/Disruption Tolerant Mobile Ad Hoc Networks;2014.0;[];Delay/disruption tolerant mobile ad hoc networks and opportunistic networks make it possible to support the communications among mobile hosts in intermittent connected scenarios. Routing in such environments is difficult because of little information about the topology of the partitioned network and transient transmit opportunities between hosts. In this paper, we present a hierarchical geographical tags based routing scheme (HGTR) in such situation, which combines geographic information assistant forwarding with encounter-based forwarding by exploiting the knowledge about the behavior of the nodes. In our HGTR, network territory is partitioned level-by-level and attached with hierarchical tags to perform this geographic-based forwarding. Messages are greedily transmitted to the home location of the destination node where the particular node has a high probability to appear, which remarkably increasing the efficiency. During the transmission, metrics of tag similarity and visiting probability are utilized to make forward decision. We illustrate the routing process and evaluate the performance of our proposed solution in comparison with various routing solutions such as Direct Delivery, Epidemic, Prophet and MaxProp. Simulation results show that our proposed solution provides higher delivery ratio compare to a number of well-known protocols, and incurs acceptable extra overhead.
2281806033;Linked Open Vocabularies (LOV): A gateway to reusable semantic vocabularies on the Web;2016.0;[];One of the major barriers to the deployment of Linked Data is the difficulty that data publishers have in determining which vocabularies to use to describe the semantics of data. This system report describes Linked Open Vocabularies (LOV), a high quality catalogue of reusable vocabularies for the description of data on the Web. The LOV initiative gathers and makes visible indicators that have not been previously harvested such as the interconnections between vocabularies, version history along with past and current referent (individual or organization). LOV goes beyond existing Semantic Web vocabulary search engines and takes into consideration the valueu0027s property type, matched with a query, to improve vocabulary terms scoring. By providing an extensive range of data access methods (SPARQL endpoint, API, data dump or UI), we try to facilitate the reuse of well-documented vocabularies in the Linked Data ecosystem. We conclude that the adoption in many applications and methods of LOV shows the benefits of such a set of vocabularies and related features to aid the design and publication of data on the Web.
2281954672;Multiple Sensor Fusion and Classification for Moving Object Detection and Tracking;2016.0;[];The accurate detection and classification of moving objects is a critical aspect of advanced driver assistance systems. We believe that by including the object classification from multiple sensor detections as a key component of the objectu0027s representation and the perception process, we can improve the perceived model of the environment. First, we define a composite object representation to include class information in the core objectu0027s description. Second, we propose a complete perception fusion architecture based on the evidential framework to solve the detection and tracking of moving objects problem by integrating the composite representation and uncertainty management. Finally, we integrate our fusion approach in a real-time application inside a vehicle demonstrator from the   interactIVe   IP European project, which includes three main sensors: radar, lidar, and camera. We test our fusion approach using real data from different driving scenarios and focusing on four objects of interest: pedestrian, bike, car, and truck.
2286878790;Force Sensing Resistor and Evaluation of Technology for Wearable Body Pressure Sensing;2016.0;[];"Wearable technologies are gaining momentum and widespread diffusion. Thanks to devices such as activity trackers, in form of bracelets, watches, or anklets, the end-users are becoming more and more aware of their daily activity routine, posture, and training and can modify their motor-behavior. Activity trackers are prevalently based on inertial sensors such as accelerometers and gyroscopes. Loads we bear with us and the interface pressure they put on our body also affect posture. A contact interface pressure sensing wearable would be beneficial to complement inertial activity trackers. What is precluding force sensing resistors (FSR) to be the next best seller wearable? In this paper, we provide elements to answer this question. We build an FSR based on resistive material (Velostat) and printed conductive ink electrodes on polyethylene terephthalate (PET) substrate; we test its response to pressure in the range 0–2.7 kPa. We present a state-of-the-art review, filtered by the need to identify technologies adequate for wearables. We conclude that the repeatability is the major issue yet unsolved."
2289047752;Pythagorean Fuzzy Choquet Integral Based MABAC Method for Multiple Attribute Group Decision Making;2016.0;[];In this paper, we define the Choquet integral operator for Pythagorean fuzzy aggregation operators, such as Pythagorean fuzzy Choquet integral average PFCIA operator and Pythagorean fuzzy Choquet integral geometric PFCIG operator. The operators not only consider the importance of the elements or their ordered positions but also can reflect the correlations among the elements or their ordered positions. It is worth pointing out that most of the existing Pythagorean fuzzy aggregation operators are special cases of our operators. Meanwhile, some basic properties are discussed in detail. Later, we propose two approaches to multiple attribute group decision making with attributes involving dependent and independent by the PFCIA operator and multi-attributive border approximation area comparison MABAC in Pythagorean fuzzy environment. Finally, two illustrative examples have also been taken in the present study to verify the developed approaches and to demonstrate their practicality and effectiveness.
2289831356;Fifty years of graph matching, network alignment and network comparison;2016.0;[];In this paper we survey methods for performing a comparative graph analysis and explain the history, foundations and differences of such techniques of the last 50 years. While surveying these methods, we introduce a novel classification scheme by distinguishing between methods for deterministic and random graphs. We believe that this scheme is useful for a better understanding of the methods, their challenges and, finally, for applying the methods efficiently in an interdisciplinary setting of data science to solve a particular problem involving comparative network analysis.
2290883490;The Whale Optimization Algorithm;2016.0;[];The Whale Optimization Algorithm inspired by humpback whales is proposed.The WOA algorithm is benchmarked on 29 well-known test functions.The results on the unimodal functions show the superior exploitation of WOA.The exploration ability of WOA is confirmed by the results on multimodal functions.The results on structural design problems confirm the performance of WOA in practice. This paper proposes a novel nature-inspired meta-heuristic optimization algorithm, called Whale Optimization Algorithm (WOA), which mimics the social behavior of humpback whales. The algorithm is inspired by the bubble-net hunting strategy. WOA is tested with 29 mathematical optimization problems and 6 structural design problems. Optimization results prove that the WOA algorithm is very competitive compared to the state-of-art meta-heuristic algorithms as well as conventional methods. The source codes of the WOA algorithm are publicly available at http://www.alimirjalili.com/WOA.html
2291303685;18.1 A 20nm 9Gb/s/pin 8Gb GDDR5 DRAM with an NBTI monitor, jitter reduction techniques and improved power distribution;2016.0;[];A 9Gb/s/pin 8Gb GDDR5 DRAM is implemented using a 20nm CMOS process. To cover operation up to 9Gb/s, which is the highest data-rate among implemented GDDR5 DRAMs [1], this work includes an NBTI monitor, a WCK clock receiver with equalizing and duty-cycle correction modes, CML-to-CMOS converters with wide range operation, active resonant loads at the end of WCK lane, and an on-chip de-emphasis circuit at a 4-to-1 multiplexer output as shown in Fig. 18.1.1. In addition, extra power pads improve the power distribution and release the frequency limitation at the memory core.
2293286118;User profiling approaches for demographic recommender systems;2016.0;[];Many DRSs are available in our daily life and many online services will be more personalized if demographic data is taken into account.Unipolar or bipolar similarity measures can be used for categorical attributes profile.Treating age as a fuzzy variable improves the system performance and reflects the real life case.Results of the unified profiling approaches are almost similar with minor differences.Single-attribute profiling approach brings to light the advantage of each attribute of the profile. Many of our daily life decisions rely on demographic data, which is a good indicator for closeness of people. However, the lack of these data for many online systems let them search for explicit or implicit alternatives. Among many, collaborative filtering is the alternative solutions especially for e-commerce applications where many users are reluctant to disclose their demographic data. This paper explores, discusses and examines many user-profiling approaches for demographic recommender systems (DRSs). These approaches span many alternatives for profiling users in terms of the attribute types, attribute representations, and the profiling way. We present layout, description, and appropriate similarity computation methods for each one of them. A detailed comparison between these different approaches is given using many experiments conducted on a real dataset. The pros and cons of each approach are illustrated for more advantage that may open a window for future work.
2293347572;Parallel Counters;1973.0;[];Multiple-input circuits that count the number of their inputs that are in a given state (normally logic ONE) are called parallel counters. In this paper three separate types of counters are described, analyzed, and compared. The first counter consists of a network of full adders. The second counter uses a combination of full adders and fast adders (that may be realized with READ-ONLY memories), while the third type of counter uses quasi-digital (i.e., analog current summing) techniques to generate an analog signal proportional to the count which is then digitized.
2293633461;Sentiment Analysis: Adjectives and Adverbs are better than Adjectives Alone;2007.0;[];To date, there is almost no work on the use of adverbs in sentiment analysis, nor has there been any work on the use of adverb-adjective combinations (AACs). We propose an AAC-based sentiment analysis technique that uses a linguistic analysis of adverbs of degree. We define a set of general axioms (based on a classification of adverbs of degree into five categories) that all adverb scoring techniques must satisfy. Instead of aggregating scores of both adverbs and adjectives using simple scoring functions, we propose an axiomatic treatment of AACs based on the linguistic classification of adverbs. Three specific AAC scoring methods that satisfy the axioms are presented. We describe the results of experiments on an annotated set of 200 news articles (annotated by 10 students) and compare our algorithms with some existing sentiment analysis algorithms. We show that our results lead to higher accuracy based on Pearson correlation with human subjects.
2294347342;The network data repository with interactive graph analytics and visualization;2015.0;[];(NR) is the first interactive data repository with a web-based platform for visual interactive analytics. Unlike other data repositories (e.g., UCI ML Data Repository, and SNAP), the network data repository (networkrepository.com) allows users to not only download, but to interactively analyze and visualize such data using our web-based interactive graph analytics platform. Users can in real-time analyze, visualize, compare, and explore data along many different dimensions. The aim of NR is to make it easy to discover key insights into the data extremely fast with little effort while also providing a medium for users to share data, visualizations, and insights. Other key factors that differentiate NR from the current data repositories is the number of graph datasets, their size, and variety. While other data repositories are static, they also lack a means for users to collaboratively discuss a particular dataset, corrections, or challenges with using the data for certain applications. In contrast, NR incorporates many social and collaborative aspects that facilitate scientific research, e.g., users can discuss each graph, post observations, and visualizations.
2294865516;Word embedding revisited: a new representation learning and explicit matrix factorization perspective;2015.0;[];Recently significant advances have been witnessed in the area of distributed word representations based on neural networks, which are also known as word embeddings. Among the new word embedding models, skip-gram negative sampling (SGNS) in the word2vec toolbox has attracted much attention due to its simplicity and effectiveness. However, the principles of SGNS remain not well understood, except for a recent work that explains SGNS as an implicit matrix factorization of the pointwise mutual information (PMI) matrix. In this paper, we provide a new perspective for further understanding SGNS. We point out that SGNS is essentially a representation learning method, which learns to represent the co-occurrence vector for a word. Based on the representation learning view, SGNS is in fact an explicit matrix factorization (EMF) of the wordsu0027 co-occurrence matrix. Furthermore, extended supervised word embedding can be established based on our proposed representation learning view.
2295985801;Genetic programming for feature construction and selection in classification on high-dimensional data;2016.0;[];Classification on high-dimensional data with thousands to tens of thousands of dimensions is a challenging task due to the high dimensionality and the quality of the feature set. The problem can be addressed by using feature selection to choose only informative features or feature construction to create new high-level features. Genetic programming (GP) using a tree-based representation can be used for both feature construction and implicit feature selection. This work presents a comprehensive study to investigate the use of GP for feature construction and selection on high-dimensional classification problems. Different combinations of the constructed and/or selected features are tested and compared on seven high-dimensional gene expression problems, and different classification algorithms are used to evaluate their performance. The results show that the constructed and/or selected feature sets can significantly reduce the dimensionality and maintain or even increase the classification accuracy in most cases. The cases with overfitting occurred are analysed via the distribution of features. Further analysis is also performed to show why the constructed feature can achieve promising classification performance.
2295996570;Uncomfortable interactions;2012.0;[];We argue for deliberately and systematically creating uncomfortable interactions as part of powerful cultural experiences. We identify the potential benefits of uncomfortable interactions under the general headings of entertainment, enlightenment and sociality. We then review artworks and performances that have employed discomfort, including two complementary examples from the worlds of entertainment and performance. From this, we articulate a suite of tactics for designing four primary forms of discomfort referred to as visceral, cultural, control and intimate. We discuss how moments of discomfort need to be embedded into an overall experience which requires a further consideration of the dramatic acts of exposition, rising action, climax, falling action, and denouement. Finally, we discuss an ethical framework for uncomfortable interactions which leads us to revisit key issues of consent, withdrawal, privacy and risk.
2296242593;Automatic design of an effective image filter based on an evolutionary algorithm for venous analysis;2016.0;[];"Medical doctors and clinical technologists operate specific, complicated diagnostic systems to assess venous diseases. Instead of using such expensive equipment, low-cost infrared cameras can capture vein images noninvasively and simply. However, the recorded image has a possibility to result in low contrast and a low signal-to-noise (S/N) ratio. An effective image filtering method to estimate venous changes will solve this problem and enable the early detection of disease. For this study, a novel filtering method based on the genetic algorithm (GA) with the expectation–maximization algorithm was proposed for the visualization of vein shapes; its effectiveness was evaluated by images acquired from a near-infrared (780 nm) camera. The novel filter was able to be automatically designed by the GA to improve the worse S/N ratio of vein images, with an unknown correct answer image. If the proposed filtering method is incorporated into e-healthcare applications, it could be widely distributed through smartphones or tablets and facilitate finding abnormal veins at an early stage."
2298294084;Action recognition from only somatosensory information using spectral learning in a hidden Markov model;2016.0;[];Human action classification is fundamental technology for robots that have to interpret a humanu0027s intended actions and make appropriate responses, as they will have to do if they are to be integrated into our daily lives. Improved measurement of human motion, using an optical motion capture system or a depth sensor, allows robots to recognize human actions from superficial motion data, such as camera images containing human actions or positions of human bodies. But existing technology for motion recognition does not handle the contact force that always exists between the human and the environment that the human is acting upon. More specifically, humans perform feasible actions by controlling not only their posture but also the contact forces. Furthermore these contact forces require appropriate muscle tensions in the full body. These muscle tensions or activities are expected to be useful for robots observing human actions to estimate the humanu0027s somatosensory states and consequently understand the intended action. This paper proposes a novel approach to classifying human actions using only the activities of all the muscles in the human body. Continuous spatio-temporal data of the activity of an individual muscle is encoded into a discrete hidden Markov model (HMM), and the set of HMMs for all the muscles forms a classifier for the specific action. Our classifiers were tested on muscle activities estimated from captured human motions, electromyography data, and reaction forces. The results demonstrate their superiority over commonly used HMM-based classifiers. This paper proposes an approach to classifying somatosensory information in the human full body into the action categories.The muscle activities in the human full body are estimated from captured motions, ground reaction forces, and EMG data.The discrete hidden Markov models to be optimized by spectral learning are adopted for the action classifiers.
2240086230;Research Priorities for Robust and Beneficial Artificial Intelligence;2015.0;[];Success in the quest for artificial intelligence has the potential to bring unprecedented benefits to humanity, and it is therefore worthwhile to investigate how to maximize these benefits while avoiding potential pitfalls. This article gives numerous examples (which should by no means be construed as an exhaustive list) of such worthwhile research aimed at ensuring that AI remains robust and beneficial.
2300436939;Fuzzy Linguistic Induced Generalized OWA Operator and Its Application in Fuzzy Linguistic Decision Making;2016.0;[];With respect to multiple attribute group decision making problems, in which the attribute values take the form of fuzzy linguistic scale variables, some decision analysis approaches are proposed. In this paper, we introduce the fuzzy linguistic induce generalized ordered weighted averaging operator and study some of its main properties by utilizing some operational laws of fuzzy linguistic scale variables. We end the paper with a numerical example of the new approach in a fuzzy linguistic decision making.
2302255633;Identity Mappings in Deep Residual Networks;2016.0;[];Deep residual networks have emerged as a family of extremely deep architectures showing compelling accuracy and nice convergence behaviors. In this paper, we analyze the propagation formulations behind the residual building blocks, which suggest that the forward and backward signals can be directly propagated from one block to any other block, when using identity mappings as the skip connections and after-addition activation. A series of ablation experiments support the importance of these identity mappings. This motivates us to propose a new residual unit, which makes training easier and improves generalization. We report improved results using a 1001-layer ResNet on CIFAR-10 (4.62 % error) and CIFAR-100, and a 200-layer ResNet on ImageNet. Code is available at: https://github.com/KaimingHe/resnet-1k-layers.
2308426365;A multi objective optimization approach for flexible job shop scheduling problem under random machine breakdown by evolutionary algorithms;2016.0;[];"This paper addresses the stable scheduling of multi-objective problem in flexible job shop scheduling with random machine breakdown. Recently, numerous studies are conducted about robust scheduling; however, implementing a scheme which prevents a tremendous change between scheduling and after machine breakdown (preschedule and realized schedule, respectively) can be critical for utilizing available resources. The stability of the schedule can be detected by a slight deviation of start and completion time of each job between preschedule and realized schedule under the uncertain conditions. In this paper, two evolutionary algorithms, NSGA-II and NRGA, are applied to combine the improvement of makespan and stability simultaneously. A simulation approach is used to evaluate the state and condition of the machine breakdowns. After the introduction of the evaluation criteria, the proposed algorithms are tested on a variety of benchmark problems. Finally, through performing statistical tests, the algorithm with higher performance in each criterion is identified. A methodology for addressing multi objective flexible job shop scheduling problem is proposed.Stability and makespan considered to optimize simultaneously in presence of machine breakdown.NRGA, NSGAII, and simulation used to tackle the problem.In three criteria NSGAII is leading algorithm and for two criteria NRGA is the leading one."
2308672085;Delay/Disruption Tolerant Network-Based Message Forwarding for a River Pollution Monitoring Wireless Sensor Network Application.;2016.0;[];Communications from remote areas that may be of interest is still a problem. Many innovative projects applied to remote sites face communications difficulties. The GOLDFISH project was an EU-funded project for river pollution monitoring in developing countries. It had several sensor clusters, with floating WiFi antennas, deployed along a downstream river’s course. Sensor clusters sent messages to a Gateway installed on the riverbank. This gateway sent the messages, through a backhaul technology, to an Internet server where data was aggregated over a map. The communication challenge in this scenario was produced by the antennas’ movement and network backhaul availability. Since the antennas were floating on the river, communications could be disrupted at any time. Also, 2G/3G availability near the river was not constant. For non-real-time applications, we propose a Delay/Disruption Tolerant Network (DTN)-based solution where all nodes have persistent storage capabilities and DTN protocols to be able to wait minutes or hours to transmit. A mechanical backhaul will periodically visit the river bank where the gateway is installed and it will automatically collect sensor data to be carried to an Internet-covered spot. The proposed forwarding protocol delivers around 98% of the messages for this scenario, performing better than other well-known DTN routing protocols.
2311142332;A Novel Approach Based on Similarity Measure for Pythagorean Fuzzy Multiple Criteria Group Decision Making;2016.0;[];"The Pythagorean fuzzy set, as a new extension of intuitionistic fuzzy set, has recently been developed to manage the complex uncertainty in practical group decision problems. The purpose of this article is to develop a new decision method based on similarity measure to address multiple criteria group decision making problems within Pythagorean fuzzy environment based on Pythagorean fuzzy numbers PFNs. The contribution of this article is fivefold: 1 An accuracy function of PFNs is defined and a new ranking method for PFNs is proposed; 2 new Pythagorean fuzzy aggregating operators are developed; 3 a novel similarity measure for PFNs is presented, and some desirable properties are discussed; 4 a simple and effective Pythagorean fuzzy group decision method is introduced; and 5 The proposed method is applied to address the selection problem of photovoltaic cells."
2313339486;Fostering pre-service teachers' self-regulated learning through self- and peer assessment of wiki projects;2016.0;[];This study investigates whether self-regulated learning of pre-service early-childhood teachers is a viable pedagogy to improve the quality of their wiki-based projects. A total of 76 early-childhood pre-service teachers in their first year of study at a teacher training institute in Hong Kong participated in this study. The tasks involved were based on seven principles for supporting and developing self-regulated learning. The pre-service teachers formed groups to apply the skills and knowledge that were taught by the author by creating educational wiki sites for young children to learn about a topic of their choice. This was then followed by preliminary-self-assessment, class presentations of the wiki projects, peer assessment, revision of the projects and final-self-assessment. Indeed, the pre-service teachersu0027 involvement in assessing wiki projects perfectly aligns with the collaborative spirit of authoring wiki projects.Quantitative data were collected using a questionnaire and the self- and peer assessment reports, whilst qualitative data were collected from a focus group held with six of the students. It is noteworthy that the students tended to be more demanding of themselves than of their classmates, as shown by a significant difference between preliminary-self-assessment and peer assessment in the scores for three out of four criteria (p value?=?0.011). The mean for preliminary-self-made video was rated the lowest by preliminary-self-assessment but it was rated the highest by peers. Since the p-value is 0.024, which is less than 0.05, this difference is significant. The focus-group participants thought their peers and the teacher provided helpful feedback, but did not find the self-assessment particularly useful. Although the data from the questionnaires and focus groups show that the pre-service teachers considered formative assessment approaches helpful, and all the mean final-self-assessment scores improved, the differences between preliminary-self-assessment and final-self-assessment were not significant. The research findings show that the students were active wiki creators and assessors and yet they did not have enough confidence in themselves. This study investigates whether self-regulated learning can improve the quality of their wiki-based projects.Finding suggest that students tended to be more demanding of themselves than of their classmates.The differences between preliminary- and final-self-assessment were not significant.The research findings show that the students were active wiki creators and assessors.
2314700336;Folksonomy-based personalized search by hybrid user profiles in multiple levels;2016.0;[];Recently, some systems have allowed users to rate and annotate resources, e.g., MovieLens, and we consider that it provides a way to identify favorite and non-favorite tags of a user by integrating his or her rating and tags. In this paper, we review and elaborate on the limitations of the current research on user profiling for personalized search in collaborative tagging systems. We then propose a new multi-level user profiling model by integrating tags and ratings to achieve personalized search, which can reflect not only a useru0027s likes but also a his or her dislikes. To the best of our knowledge, this is the first effort to integrate ratings and tags to model multi-level user profiles for personalized search.
2317337556;Dynamic rate adaptation for improved throughput and delay in wireless network coded broadcast;2014.0;[];In this paper, we provide theoretical and simulation-based study of the delivery delay performance of a number of existing throughput-optimal coding schemes and use the results to design a new dynamic rate adaptation scheme that achieves improved overall throughput-delay performance. Under a baseline rate control scheme, the receiversu0027 delay performance is examined. Based on their Markov states, the knowledge difference between the sender and receiver, three distinct methods for packet delivery are identified: zero state, leader state, and coefficient-based delivery. We provide analyses of each of these and show that, in many cases, zero state delivery alone presents a tractable approximation of the expected packet delivery behavior. Interestingly, while coefficient-based delivery has so far been treated as a secondary effect in the literature, we find that the choice of coefficients is extremely important in determining the delay, and a well-chosen encoding scheme can, in fact, contribute a significant improvement to the delivery delay. Based on our delivery delay model, we develop a dynamic rate adaptation scheme that uses performance prediction models to determine the sender transmission rate. Surprisingly, taking this approach leads us to the simple conclusion that the sender should regulate its addition rate based on the total number of undelivered packets stored at the receivers. We show that despite its simplicity, our proposed dynamic rate adaptation scheme results in noticeably improved throughput-delay performance over existing schemes in the literature.
2320065976;WDM/OCDM Energy-Efficient Networks Based on Heuristic Ant Colony Optimization;2016.0;[];The ant colony optimization for continuous domains    $(\text{ACO}_{\mathbb{R}})$   approach is deployed in order to solve two resource allocation (RA) optimization problems associated to the signal-to-noise plus interference ratio metric with quality-of-service constraints in the context of hybrid wavelength-division multiplexing/optical code-division multiplexing networks. The    $\text{ACO}_{\mathbb{R}}$  - based RA optimization strategy allows optimally regulating the transmitted optical powers, as well as maximizing the overall energy efficiency (sum EE) of the optical network. In this context, a suitable model for heuristic optimization approach is developed, with emphasis on the network performance under optimized    $\text{ACO}_{\mathbb{R}}$   input parameters. Extensive simulation results for both power allocation and EE optimization problems are discussed taking into account realistic networks operation scenarios. Computational complexity analysis is performed in order to obtain a suitable, yet sturdy, algorithm regarding the robustness   versus   complexity tradeoff. The performance and complexity of the proposed heuristic approach are compared with a disciplined convex optimization approach based on CvX tools.
2323909273;Visualizing High-Dimensional Data: Advances in the Past Decade;2017.0;[];Massive simulations and arrays of sensing devices, in combination with increasing computing resources, have generated large, complex, high-dimensional datasets used to study phenomena across numerous fields of study. Visualization plays an important role in exploring such datasets. We provide a comprehensive survey of advances in high-dimensional data visualization that focuses on the past decade. We aim at providing guidance for data practitioners to navigate through a modular view of the recent advances, inspiring the creation of new visualizations along the enriched visualization pipeline, and identifying future opportunities for visualization research.
2326678594;Correlation analysis between user’s emotional comments and popularity measures;2014.0;[];
2328136677;Attribute-based Access Control for ICN Naming Scheme;2018.0;[];"Information Centric Networking (ICN) is a new network architecture that aims to overcome the weakness of existing IP-based networking architecture. Instead of establishing a connection between the communicating hosts, ICN focuses on the content, i.e., data, transmitted in network. Content copies in ICN can be cached at different locations. The content is out of its owneru0027s control once it is published. Thus, enforcing access control policies on distributed content copies is crucial in ICN. Attribute-Based Encryption (ABE) is a feasible approach to enforce such control mechanisms in this environment. However, applying ABE in ICN faces two challenges: from management perspective, it is complicated to manage attributes in distributed manners; from privacy protection perspective, unlike in traditional networks, the enforced content access policies are public to all the ICN users. Thus, it is desirable that unauthorized content viewers are not able to retrieve the access policy. To this end, a privacy-preserving access control scheme for ICN and its corresponding attribute management solution are presented in this paper. The proposed approach is compatible with existing flat name based ICN architectures."
2329959957;Integrated Management of Energy Resources in Residential Buildings—A Markovian Approach;2018.0;[];The residential sector is gaining a more active role in the management of energy resources with the deployment of microgeneration based on renewables, the development of bidirectional communication infrastructures, and the new technologies of the power grid systems including smart metering. The consumer becomes an active agent and should be able to make decisions about the usage of his loads, while keeping the quality of the energy services provided. We propose an integrated energy management approach using Markovian processes that, based on weather conditions, local storage of renewable energy, time based tariffs, and grid/technical constraints, optimally allocates the available resources to fulfill the energy requirements. The goal is to reduce the energy bill, while maintaining the consumer’s satisfaction. The results show that a significant bill reduction can be obtained depending on the tariff structure and technology used. A comparison with a genetic algorithm used for the same purpose is also provided.
2332912335;Simple additive weighting-A metamodel for multiple criteria decision analysis methods;2016.0;[];"We interpret rankings provided by any MCDA method in terms of the Simple Additive Weighting.Weights of SAW which preserve rankings produced by a given MCDA method are identified.Ranges of weights resulting with the same rankings as by a given MCDA method are identified as well.An alternative explanatory framework for MCDA methods is offered. Multiple Criteria Decision Analysis (MCDA) methods, such as ELECTRE, PROMETEE, AHP, TOPSIS, VIKOR, have been applied to solving numerous real-life decision making problems in business and management. However, the mechanics of those methods is not easily understandable and it is often seen by users without much formal training as a kind of ""scientific witchcraft"".In order to make those popular MCDA methods more transparent, we provide a simple framework for interpretations of rankings they produce. The framework builds on the classical results of MCDA, in particular on the preference capture mechanism proposed by Zionts and Wallenius in seventies of the last century, based on Simple Additive Weighting.The essence and the potential impact of our contribution is that given a ranking produced by an MCDA method, we show how to derive weights for the Simple Additive Weighting which yield the same ranking as the given method. In that way we establish a common framework for almost no-cost posterior analysis, interpretation and comparison of rankings produced by MCDA methods in the expert systems environment. We show the working of the concept taking the TOPSIS method in focus, but it applies in the same way to any other MCDM method.We illustrate our reasoning with numerical examples taken from literature."
2334208235;Optimization of Full versus Incremental Periodic Backup Policy;2016.0;[];This paper models repairable computing systems performing a mission that is successful if the system can accomplish a specified amount of work within the allowed mission time or deadline. During the mission the system is subject to a sequence of full and incremental data backup procedures to facilitate an effective system recovery and avoid repeating the entire mission work from the very beginning when a system failure happens. The repair time is fixed while the system time-to-failure can follow any arbitrary type of distributions. This paper makes novel contributions by first developing a new numerical algorithm to evaluate mission success probability and expected completion time of the considered repairable real-time computing systems subject to mixed full and incremental backups. Correctness of the proposed evaluation algorithm is verified using Monte Carlo simulations. We make another new contribution by formulating and solving the backup schedule optimization problem that finds the full and incremental backup frequencies maximizing the mission success probability. Through illustrative examples, effects of different parameters (including the system time-to-failure distribution parameter, maximum allowed mission time, data backup and retrieval times, storage availability, repair time and efficiency) on the mission success probability and expected completion time as well as on the optimal backup schedule solution are investigated.
2339277944;Software and attack centric integrated threat modeling for quantitative risk assessment;2016.0;[];One step involved in the security engineering process is threat modeling. Threat modeling involves understanding the complexity of the system and identifying all of the possible threats, regardless of whether or not they can be exploited. Proper identification of threats and appropriate selection of countermeasures reduces the ability of attackers to misuse the system. This paper presents a quantitative, integrated threat modeling approach that merges software and attack centric threat modeling techniques. The threat model is composed of a system model representing the physical and network infrastructure layout, as well as a component model illustrating component specific threats. Component attack trees allow for modeling specific component contained attack vectors, while system attack graphs illustrate multi-component, multi-step attack vectors across the system. The Common Vulnerability Scoring System (CVSS) is leveraged to provide a standardized method of quantifying the low level vulnerabilities in the attack trees. As a case study, a railway communication network is used, and the respective results using a threat modeling software tool are presented.
2341607273;An Analysis of WhatsApp Usage for Communication Between Consulting and Emergency Physicians;2016.0;[];The aim of this study was to evaluate WhatsApp messenger usage for communication between consulting and emergency physicians. A retrospective, observational study was conducted in the emergency department (ED) of a tertiary care university hospital between January 2014 and June 2014. A total of 614 consultations requested by using the WhatsApp application were evaluated, and 519 eligible consultations were included in the study. The WhatsApp messages that were transferred to consultant physicians consisted of 510 (98.3 %) photographic images, 517 (99.6 %) text messages, 59 (11.3 %) videos, and 10 (1.9 %) voice messages. Consultation was most frequently requested from the orthopedics clinic (n?=?160, 30.8 %). The majority of requested consultations were terminated only by evaluation via WhatsApp messages. (n?=?311, 59.9 %). Most of the consulting physicians were outside of the hospital or were mobile at the time of the consultation (n?=?292, 56.3 %). The outside consultation request rate was significantly higher for night shifts than for day shifts (p?=?.004), and the majority of outside consultation request were concluded by only WhatsApp application (p?u003c?.001). WhatsApp is useful a communication tool between physicians, especially for ED consultants who are outside the hospital, because of the ability to transfer large amounts of clinical and radiological data during a short period of time.
2341964010;LODVader: An Interface to LOD Visualization, Analyticsand DiscovERy in Real-time;2016.0;[];"The Linked Open Data (LOD) cloud is in danger of becoming a black box. Simple questions such as ""What kind of datasets are in the LOD cloud?"", ""In what way(s) are these datasets connected?"" -- albeit frequently asked -- are at the moment still difficult to answer due to the lack of proper tooling support. The infrequent update of the static LOD cloud diagram adds to the current dilemma, since there is neither reliable nor timely-updated information to perform an interactive search, analysis or in particular visualization in order to gain insight into the current state of Linked Open Data. In this paper, we propose a new hybrid system which combines LOD Visualisation, Analytics and DiscovERy (LODVader) to aid in answering the above questions. LODVader is equipped with (1) a multi-layer LOD cloud visualization component comprising datasets, subsets and vocabularies, (2) dataset analysis components that extend the state of the art with new similarity measures and efficient link extracting techniques and (3) a fast search index that is an entry point for dataset discovery. At its core, LODVader employs a timely-updated index using a complex cluster of Bloom filters as a fast search index with low memory footprint. This BF cluster is able to efficiently perform analysis on link and dataset similarities based on stored predicate and object information, which -- once inverted -- can be employed to discover invalid links by displaying the Dark LOD Cloud. By combining all these features, we allow for an up-to-date, multi-dimensional LOD cloud analysis, which -- to the best of our knowledge -- was not possible before."
2342405812;Interaction in Motion: Designing Truly Mobile Interaction;2016.0;[];"The use of technology while being mobile now takes place in many areas of peopleu0027s lives in a wide range of scenarios, for example users cycle, climb, run and even swim while interacting with devices. Conflict between locomotion and system use can reduce interaction performance and also the ability to safely move. We discuss the risks of such ""interaction in motion"", which we argue make it desirable to design with locomotion in mind. To aid such design we present a taxonomy and framework based on two key dimensions: relation of interaction task to locomotion task, and the amount that a locomotion activity inhibits use of input and output interfaces. We accompany this with four strategies for interaction in motion. With this work, we ultimately aim to enhance our understanding of what being ""mobile"" actually means for interaction, and help practitioners design truly mobile interactions."
2343246984;Human gait recognition based on deterministic learning through multiple views fusion;2016.0;[];Robust gait recognition via deterministic learning and multiple views fusion.Multiple views fusion strategy is employed to form synthesized silhouette.Human gait can be characterized with four time-varying gait features.Gait variability are effectively modeled via deterministic learning.A rapid recognition scheme is chosen for encouraging recognition accuracy. Gait characteristics extracted from one single camera are limited and not comprehensive enough to develop a robust recognition system. This paper proposes a robust gait recognition method using multiple views fusion and deterministic learning. First, a multiple-views fusion strategy is introduced, in which gaits collected under different views are synthesized as a kind of synthesized silhouette images. Second, the synthesized silhouettes are characterized with four kinds of time-varying gait features, including three width features of the silhouette and one silhouette area feature. Third, gait variability underlying different individualsu0027 time-varying gait features is effectively modeled by using deterministic learning algorithm. This kind of variability reflects the change of synthesized silhouettes while preserving temporal dynamics information of human walking. Gait patterns are represented as the gait variability underlying time-varying gait features and a rapid recognition scheme is presented in published gait databases. Experimental results show that encouraging recognition accuracy can be achieved.
2344305373;Metropolis biogeography-based optimization;2016.0;[];Biogeography-based optimization (BBO) is a new population-based evolutionary algorithm (EA). Although the exploitation level of BBO is good researchers found some weaknesses in its exploration. This study proposes a new hybridization between BBO and simulated annealing (SA) to enhance its performance. In this proposed algorithm, the inferior migrated islands will not be selected unless they pass the Metropolis criterion of SA and so the new algorithm is called MpBBO. The performance of MpBBO is evaluated using 36 benchmark functions with five different cooling strategies of SA and then compared with the original and essentially modified BBO models. The results show the exponential, inverse, and inverse linear cooling strategies provide best solutions, but they are the slowest. Among these three strategies, the exponential cooling rate can compromise between the solution quality and CPU time compared with the others. Also, the inverse cooling rate is competitive and wins when the mutation stage is completely disabled. The F-test and T-test show that MpBBO has significant differences. Further, it has been observed, through sensitivity analysis, that MpBBO behaves like BBO and SA. The parameters of SA and BBO can affect the performance of MpBBO, which has more immunity against trapping into local optimums. Moreover, the superiority of MpBBO appears when it is compared with non-simplified migration-based BBO models as well as other hybrid/non-hybrid EAs.
2344681960;A Wireless BCI and BMI System for Wearable Robots;2016.0;[];To increase the performance of a brain–computer interface and brain–machine interface system, we propose some methods and algorithms for electroencephalograph (EEG) signal analysis. The recorded EEG signal is transmitted to the computer and the upper limb robotic arm interface via a bluetooth. To obtain effective commands from brain, the recorded EEG signal is processed by a front filter, denoise filter, feature extraction, and classification, while the personal computer software and upper limb arm are driven by EEG-based commands. Through the encoders and gyroscopes on the upper limb arm, we can acquire some feedback signals in real time, such as joint angle, arm accelerated speed, and angular speed. The theory of wavelet denoising method, common spatial pattern algorithm and linear discriminant analysis algorithm are investigated in this paper. The simulations and experiments demonstrate the effectiveness and accuracy of these algorithms on EEG signal denoising, feature extraction, and classification.
2344934580;On-Road Vehicle Detection and Tracking Using MMW Radar and Monovision Fusion;2016.0;[];With the potential to increase road safety and provide economic benefits, intelligent vehicles have elicited a significant amount of interest from both academics and industry. A robust and reliable vehicle detection and tracking system is one of the key modules for intelligent vehicles to perceive the surrounding environment. The millimeter-wave radar and the monocular camera are two vehicular sensors commonly used for vehicle detection and tracking. Despite their advantages, the drawbacks of these two sensors make them insufficient when used separately. Thus, the fusion of these two sensors is considered as an efficient way to address the challenge. This paper presents a collaborative fusion approach to achieve the optimal balance between vehicle detection accuracy and computational efficiency. The proposed vehicle detection and tracking design is extensively evaluated with a real-world data set collected by the developed intelligent vehicle. Experimental results show that the proposed system can detect on-road vehicles with 92.36% detection rate and 0% false alarm rate, and it only takes ten frames (0.16 s) for the detection and tracking of each vehicle. This system is installed on Kuafu-II intelligent vehicle for the fourth and fifth autonomous vehicle competitions, which is called “Intelligent Vehicle Future Challenge” in China.
2345115205;Biogeography-based optimization with covariance matrix based migration;2016.0;[];Display Omitted Covariance matrix-based migration (CMM) is proposed.CMM significantly enhances the rotational invariance of BBO.A novel CMM-BBO approach is developed.Numeric simulations show CMM-BBO effectively improves the performance of BBO. Biogeography-based optimization (BBO) is a new evolutionary algorithm. The major problem of basic BBO is that its migration operator is rotationally variant, which leaves BBO performing poorly in non-separable problems. To overcome this drawback of BBO, in this paper, we propose the covariance matrix based migration (CMM) to relieve BBOu0027s dependence upon the coordinate system so that BBOu0027s rotational invariance is enhanced. By embedding the CMM into BBO, we put forward a new BBO approach, namely biogeography-based optimization with covariance matrix based migration, called CMM-BBO. Specifically, CMM-BBO algorithms are developed by the CMM operator being randomly combined with the original migration in various existing BBO variants. Numeric simulations on 37 benchmark functions show that our CMM-BBO approach effectively improves the performance of the existing BBO algorithms.
2345559987;A novel approach of cluster based optimal ranking of clicked URLs using genetic algorithm for effective personalized web search;2016.0;[];Genetic Algorithm (GA) is applied for generating the cluster based optimal ranked clicked URLs for effective Personalized Web Search(PWS).The computational overhead associated with the use of GA has no impact on the online performance of the PWS using clustered query sessions.The experimental results were compared on the basis of percent improvement in average precision of PWS both with and without optimal ranked clicked URLs over Classic IR in the domain Academics, Entertainment and Sports.The statistically verified results show more percent improvement in average precision of PWS with optimal ranked clicked URLs than PWS without optimal ranked clicked URLs over Classic IR. In this paper a novel approach is proposed for generating the optimal ranked clicked URLs using genetic algorithm (GA) based on clustered web query sessions for effective personalized web search. Experimental study was conducted on the data set of web query sessions captured in the domains academics, entertainment and sports to test the effectiveness of clusterwise optimal ranked clicked URLs for personalized web search (PWS). The results, which are verified statistically shows an improvement in the average precision of the personalized web search based on optimal ranked clicked URLs over both Classic IR and personalized web search without optimal ranked clicked URLs. Thus the effectiveness of personalized web search using optimal ranked clicked URLs is confirmed for better customizing the web search according to the information need of the user.
2345848912;A New Generalized Pythagorean Fuzzy Information Aggregation Using Einstein Operations and Its Application to Decision Making;2016.0;[];The objective of this article is to extend and present an idea related to weighted aggregated operators from fuzzy to Pythagorean fuzzy sets PFSs. The main feature of the PFS is to relax the condition that the sum of the degree of membership functions is less than one with the square sum of the degree of membership functions is less than one. Under these environments, aggregator operators, namely, Pythagorean fuzzy Einstein weighted averaging PFEWA, Pythagorean fuzzy Einstein ordered weighted averaging PFEOWA, generalized Pythagorean fuzzy Einstein weighted averaging GPFEWA, and generalized Pythagorean fuzzy Einstein ordered weighted averaging GPFEOWA, are proposed in this article. Some desirable properties corresponding to it have also been investigated. Furthermore, these operators are applied to decision-making problems in which experts provide their preferences in the Pythagorean fuzzy environment to show the validity, practicality, and effectiveness of the new approach. Finally, a systematic comparison between the existing work and the proposed work has been given.
2356613439;Investigating the influence of the most commonly used external variables of TAM on students' Perceived Ease of Use (PEOU) and Perceived Usefulness (PU) of e-portfolios;2016.0;[];Engagement with e-portfolios has been shown to improve studentsu0027 learning. However, what influences students to accept e-portfolios is a question that needs careful study. The purpose of this study is to investigate the influence of Self-Efficacy, Subjective Norm, Enjoyment, Computer Anxiety and Experience on studentsu0027 Perceived Ease of Use (PEOU) and Perceived Usefulness (PU) of an e-portfolio system and their Behavioural Intention (BI) to use the system for learning. To do this, the study tested and used the General Extended Technology Acceptance Model for E-Learning (GETAMEL) in the context of e-portfolios. Valid data were collected from 242 UK undergraduate students who had been introduced to e-portfolios. The data set was analysed using SPSS software. Results showed that the best predictor of studentu0027s Perceived Ease of Use of the e-portfolio is Experience, followed by Enjoyment, Self-Efficacy and Subjective Norm. The best predictor of studentu0027s Perceived Usefulness of the e-portfolio is Perceived Ease of Use followed by Enjoyment. Both Perceived Ease of Use and Perceived Usefulness predict studentu0027s Behavioural Intention to Use the e-portfolio. The findings improve understanding regarding acceptance of e-portfolio systems and this work is therefore of particular interest to researchers, developers and practitioners of e-portfolios. We validated and used the GETAMEL to examine studentsu0027 e-portfolio adoption.Four variables affected Perceived Ease of Use (PEOU) of the e-portfolio.The four variables are Experience, Enjoyment, Self-Efficacy and Subjective Norm.PEOU and Enjoyment affected Perceived Usefulness (PU) of the e-portfolio.Both PEOU and PU affected studentsu0027 intention to use the e-portfolio.
2366141641;node2vec: Scalable Feature Learning for Networks;2016.0;[];Prediction tasks over nodes and edges in networks require careful effort in engineering features used by learning algorithms. Recent research in the broader field of representation learning has led to significant progress in automating prediction by learning the features themselves. However, present feature learning approaches are not expressive enough to capture the diversity of connectivity patterns observed in networks. Here we propose node2vec, an algorithmic framework for learning continuous feature representations for nodes in networks. In node2vec, we learn a mapping of nodes to a low-dimensional space of features that maximizes the likelihood of preserving network neighborhoods of nodes. We define a flexible notion of a nodeu0027s network neighborhood and design a biased random walk procedure, which efficiently explores diverse neighborhoods. Our algorithm generalizes prior work which is based on rigid notions of network neighborhoods, and we argue that the added flexibility in exploring neighborhoods is the key to learning richer representations. We demonstrate the efficacy of node2vec over existing state-of-the-art techniques on multi-label classification and link prediction in several real-world networks from diverse domains. Taken together, our work represents a new way for efficiently learning state-of-the-art task-independent representations in complex networks.
2377836894;Radar and stereo vision fusion for multitarget tracking on the special Euclidean group;2016.0;[];Reliable scene analysis, under varying conditions, is an essential task in nearly any assistance or autonomous system application, and advanced driver assistance systems (ADAS) are no exception. ADAS commonly involve adaptive cruise control, collision avoidance, lane change assistance, traffic sign recognition, and parking assistance-with the ultimate goal of producing a fully autonomous vehicle. The present paper addresses detection and tracking of moving objects within the context of ADAS. We use a multisensor setup consisting of a radar and a stereo camera mounted on top of a vehicle. We propose to model the sensors uncertainty in polar coordinates on Lie Groups and perform the objects state filtering on Lie groups, specifically, on the product of two special Euclidean groups, i.e., SE ( 2 ) 2 . To this end, we derive the designed filter within the framework of the extended Kalman filter on Lie groups. We assert that the proposed approach results with more accurate uncertainty modeling, since used sensors exhibit contrasting measurement uncertainty characteristics and the predicted target motions result with banana-shaped uncertainty contours. We believe that accurate uncertainty modeling is an important ADAS topic, especially when safety applications are concerned. To solve the multitarget tracking problem, we use the joint integrated probabilistic data association filter and present necessary modifications in order to use it on Lie groups. The proposed approach is tested on a real-world dataset collected with the described multisensor setup in urban traffic scenarios. Radar and stereo camera integration for tracking in ADAS.Detection and tracking of moving objects by filtering on matrix Lie groups.State space formed as a product of two special Euclidean groups.Employed banana-shaped uncertainties typical for range-bearing sensors and vehicles in motion.JIPDA filter for multitarget tracking on matrix Lie groups.
2383696055;EMPress: Practical Hand Gesture Classification with Wrist-Mounted EMG and Pressure Sensing;2016.0;[];Practical wearable gesture tracking requires that sensors align with existing ergonomic device forms. We show that combining EMG and pressure data sensed only at the wrist can support accurate classification of hand gestures. A pilot study with unintended EMG electrode pressure variability led to exploration of the approach in greater depth. The EMPress technique senses both finger movements and rotations around the wrist and forearm, covering a wide range of gestures, with an overall 10-fold cross validation classification accuracy of 96%. We show that EMG is especially suited to sensing finger movements, that pressure is suited to sensing wrist and forearm rotations, and their combination is significantly more accurate for a range of gestures than either technique alone. The technique is well suited to existing wearable device forms such as smart watches that are already mounted on the wrist.
2387462954;Asymmetric Transitivity Preserving Graph Embedding;2016.0;[];Graph embedding algorithms embed a graph into a vector space where the structure and the inherent properties of the graph are preserved. The existing graph embedding methods cannot preserve the asymmetric transitivity well, which is a critical property of directed graphs. Asymmetric transitivity depicts the correlation among directed edges, that is, if there is a directed path from u to v, then there is likely a directed edge from u to v. Asymmetric transitivity can help in capturing structures of graphs and recovering from partially observed graphs. To tackle this challenge, we propose the idea of preserving asymmetric transitivity by approximating high-order proximity which are based on asymmetric transitivity. In particular, we develop a novel graph embedding algorithm, High-Order Proximity preserved Embedding (HOPE for short), which is scalable to preserve high-order proximities of large scale graphs and capable of capturing the asymmetric transitivity. More specifically, we first derive a general formulation that cover multiple popular high-order proximity measurements, then propose a scalable embedding algorithm to approximate the high-order proximity measurements based on their general formulation. Moreover, we provide a theoretical upper bound on the RMSE (Root Mean Squared Error) of the approximation. Our empirical experiments on a synthetic dataset and three real-world datasets demonstrate that HOPE can approximate the high-order proximities significantly better than the state-of-art algorithms and outperform the state-of-art algorithms in tasks of reconstruction, link prediction and vertex recommendation.
2391555403;FINAL: Fast Attributed Network Alignment;2016.0;[];"Multiple networks naturally appear in numerous high-impact applications. Network alignment (i.e., finding the node correspondence across different networks) is often the very first step for many data mining tasks. Most, if not all, of the existing alignment methods are solely based on the topology of the underlying networks. Nonetheless, many real networks often have rich attribute information on nodes and/or edges. In this paper, we propose a family of algorithms FINAL to align attributed networks. The key idea is to leverage the node/edge attribute information to guide (topology-based) alignment process. We formulate this problem from an optimization perspective based on the alignment consistency principle, and develop effective and scalable algorithms to solve it. Our experiments on real networks show that (1) by leveraging the attribute information, our algorithms can significantly improve the alignment accuracy (i.e., up to a 30% improvement over the existing methods); (2) compared with the exact solution, our proposed fast alignment algorithm leads to a more than 10 times speed-up, while preserving a 95% accuracy; and (3) our on-query alignment method scales linearly, with an around 90% ranking accuracy compared with our exact full alignment method and a near real-time response time."
2393256679;Bispectral Analysis of EEG for Emotion Recognition;2016.0;[];"   Emotion recognition from electroencephalogram (EEG) signals is one of the most challenging tasks. Bispectral analysis offers a way of gaining phase information by detecting phase relationships between frequency components and characterizing the non- Gaussian information contained in the EEG signals. In this paper, we explore derived features of bispectrum for quantification of emotions using a Valence-Arousal emotion model; and arrive at a feature vector through backward sequential search. Cross- validated accuracies of 64.84% for Low/High Arousal classification and 61.17% for Low/High Valence were obtained on the DEAP data set based on the proposed features; comparable to classification accuracies reported in the literature."
2393319904;Structural Deep Network Embedding;2016.0;[];Network embedding is an important method to learn low-dimensional representations of vertexes in networks, aiming to capture and preserve the network structure. Almost all the existing network embedding methods adopt shallow models. However, since the underlying network structure is complex, shallow models cannot capture the highly non-linear network structure, resulting in sub-optimal network representations. Therefore, how to find a method that is able to effectively capture the highly non-linear network structure and preserve the global and local structure is an open yet important problem. To solve this problem, in this paper we propose a Structural Deep Network Embedding method, namely SDNE. More specifically, we first propose a semi-supervised deep model, which has multiple layers of non-linear functions, thereby being able to capture the highly non-linear network structure. Then we propose to exploit the first-order and second-order proximity jointly to preserve the network structure. The second-order proximity is used by the unsupervised component to capture the global network structure. While the first-order proximity is used as the supervised information in the supervised component to preserve the local network structure. By jointly optimizing them in the semi-supervised deep model, our method can preserve both the local and global network structure and is robust to sparse networks. Empirically, we conduct the experiments on five real-world networks, including a language network, a citation network and three social networks. The results show that compared to the baselines, our method can reconstruct the original network significantly better and achieves substantial gains in three applications, i.e. multi-label classification, link prediction and visualization.
2395579298;A survey of transfer learning;2016.0;[];Machine learning and data mining techniques have been used in numerous real-world applications. An assumption of traditional machine learning methodologies is the training data and testing data are taken from the same domain, such that the input feature space and data distribution characteristics are the same. However, in some real-world machine learning scenarios, this assumption does not hold. There are cases where training data is expensive or difficult to collect. Therefore, there is a need to create high-performance learners trained with more easily obtained data from different domains. This methodology is referred to as transfer learning. This survey paper formally defines transfer learning, presents information on current solutions, and reviews applications applied to transfer learning. Lastly, there is information listed on software downloads for various transfer learning solutions and a discussion of possible future research work. The transfer learning solutions surveyed are independent of data size and can be applied to big data environments.
2395948124;Ramsey numbers of a fixed odd-cycle and generalized books and fans;2016.0;[];For graphs G and H , the Ramsey number R ( G , H ) is the smallest positive integer N such that any red/blue edge coloring of K N contains either a red G or a blue H . Let G + H be the graph obtained from disjoint G and H by adding edges connecting G and H completely. It is shown that R ( C 2 m + 1 , K p + n K 1 ) = 2 ( n + p - 1 ) + 1 and R ( C 2 m + 1 , K 1 + n H ) = 2 h n + 1 , where m , p ź 1 and H of order h are fixed and n is large. Our tools for proofs are Regularity Lemma and Stability Lemma.
2396040005;Symmetric Pythagorean Fuzzy Weighted Geometric/Averaging Operators and Their Application in Multicriteria Decision-Making Problems;2016.0;[];Pythagorean fuzzy sets PFSs, originally proposed by Yager, are a new tool to deal with vagueness with the square sum of the membership degree and the nonmembership degree equal to or less than 1, which have much stronger ability than Atanassovu0027s intuitionistic fuzzy sets to model such uncertainty. In this paper, we modify the existing score function and accuracy function for Pythagorean fuzzy number to make it conform to PFSs. Associated with the given operational laws, we define some novel Pythagorean fuzzy weighted geometric/averaging operators for Pythagorean fuzzy information, which can neutrally treat the membership degree and the nonmembership degree, and investigate the relationships among these operators and those existing ones. At length, a practical example is provided to illustrate the developed operators and to make a comparative analysis.
2397273960;Local and global existence of mild solutions for a class of nonlinear fractional reaction-diffusion equations with delay;2016.0;[];   In this paper, we investigate the local and global existence of mild solutions to a class of nonlinear fractional reaction–diffusion equations with delay via the theory of    β   -resolvent family and fixed point theorems.
2398934262;Algorithm 958: Lattice Builder: A General Software Tool for Constructing Rank-1 Lattice Rules;2016.0;[];We introduce a new software tool and library named Lattice Builder, written in C++, that implements a variety of construction algorithms for good rank-1 lattice rules. It supports exhaustive and random searches, as well as component-by-component (CBC) and random CBC constructions, for any number of points, and for various measures of (non)uniformity of the points. The measures currently implemented are all shift-invariant and represent the worst-case integration error for certain classes of integrands. They include, for example, the weighted Pα square discrepancy, the Rα criterion, and figures of merit based on the spectral test, with projection-dependent weights. Each of these measures can be computed as a finite sum. For the Pα and Rα criteria, efficient specializations of the CBC algorithm are provided for projection-dependent, order-dependent, and product weights. For numbers of points that are integer powers of a prime base, the construction of embedded rank-1 lattice rules is supported through any of these algorithms, and through a fast CBC algorithm, with a variety of possibilities for the normalization of the merit values of individual embedded levels and for their combination into a single merit value. The library is extensible, thanks to the decomposition of the algorithms into decoupled components, which makes it easy to implement new types of weights, new search domains, new figures of merit, and so on.
2399992960;Computational Complexity, NP Completeness and Optimization Duality: A Survey.;2012.0;[];We survey research that studies the connection between the computational complexity of optimization problems on the one hand, and the duality gap between the primal and dual optimization problems on the other. To our knowledge, this is the first survey that connects the two very important areas. We further look at a similar phenomenon in finite model theory relating to complexity and optimization.
2300242332;XNOR-Net: ImageNet Classification Using Binary Convolutional Neural Networks;2016.0;[];We propose two efficient approximations to standard convolutional neural networks: Binary-Weight-Networks and XNOR-Networks. In Binary-Weight-Networks, the filters are approximated with binary values resulting in 32\(\times \) memory saving. In XNOR-Networks, both the filters and the input to convolutional layers are binary. XNOR-Networks approximate convolutions using primarily binary operations. This results in 58\(\times \) faster convolutional operations (in terms of number of the high precision operations) and 32\(\times \) memory savings. XNOR-Nets offer the possibility of running state-of-the-art networks on CPUs (rather than GPUs) in real-time. Our binary networks are simple, accurate, efficient, and work on challenging visual tasks. We evaluate our approach on the ImageNet classification task. The classification accuracy with a Binary-Weight-Network version of AlexNet is the same as the full-precision AlexNet. We compare our method with recent network binarization methods, BinaryConnect and BinaryNets, and outperform these methods by large margins on ImageNet, more than \(16\,\%\) in top-1 accuracy. Our code is available at: http://allenai.org/plato/xnornet.
2400676556;On the Powers of 2.;2014.0;[];In 2013 the function field sieve algorithm for computing discrete logarithms in finite fields of small characteristic underwent a series of dramatic improvements, culminating in the first heuristic quasi-polynomial time algorithm, due to Barbulescu, Gaudry, Joux and Thome. In this article we present an alternative descent method which is built entirely from the on-the-fly degree two elimination method of Gologlu, Granger, McGuire and Zumbragel. This also results in a heuristic quasi-polynomial time algorithm, for which the descent does not require any relation gathering or linear algebra eliminations and interestingly, does not require any smoothness assumptions about non-uniformly distributed polynomials. These properties make the new descent method readily applicable at currently viable bitlengths and better suited to theoretical analysis.
2401752081;Understanding the influence of social information sources on e-government adoption.;2012.0;[];
2405981446;Learning and evolution in dynamic software product lines;2016.0;[];A Dynamic Software Product Line (DSPL) aims at managing run-time adaptations of a software system. It is built on the assumption that context changes that require these adaptations at run-time can be anticipated at design-time. Therefore, the set of adaptation rules and the space of configurations in a DSPL are predefined and fixed at design-time. Yet, for large-scale and highly distributed systems, anticipating all relevant context changes during design-time is often not possible due to the uncertainty of how the context may change. Such design-time uncertainty therefore may mean that a DSPL lacks adaptation rules or configurations to properly reconfigure itself at run-time. We propose an adaptive system model to cope with design-time uncertainty in DSPLs. This model combines learning of adaptation rules with evolution of the DSPL configuration space. It takes particular account of the mutual dependencies between evolution and learning, such as using feedback from unsuccessful learning to trigger evolution. We describe concrete steps for learning and evolution to show how such feedback can be exploited. We illustrate the use of such a model with a running example from the cloud computing domain.
2406010710;Towards next-generation routing protocols for pocket switched networks;2016.0;[];A pocket switched network (PSN) is dynamically formed by people who carry portable handheld devices. Interest in PSNs is driven by the increasing number of handheld devices, the several wireless interfaces they possess, as well as their ability to store, carry and forward data. The lack of fixed network topology distinguishes PSNs from traditional networks, and unlike other types of mobile networks, nodes in PSNs closely follow human movement patterns. As a result, PSNs are faced with new challenges especially in the aspect of routing. Although various routing protocols have been proposed, most of them focus on optimizing the performance of networking primitives for traditional networks such as unicast, broadcast and multicast. However, these primitives themselves appear to be insufficient due to new application opportunities presented by PSNs. This paper adopts a user scenario based approach to determine the current state of PSN routing protocols. Specifically, four modes of data transfer are established from six generalized PSN user scenarios. Due to the wide range of existing routing proposals, a new taxonomy is proposed to facilitate analysis of their compatibility with the established modes of data transfer. The analysis provides new insights into application based routing approaches for realizing next-generation PSN routing protocols. A detailed characterization of PSN user scenarios and respective data transfer modes.An exhaustive analysis of existing PSN routing solutions.A novel classification for PSN routing complementing gaps of existing taxonomies.A scenario-driven set of design guidelines for next-generation PSN routing solutions.An instructive list of future directions towards real-world PSN implementations.
2407212869;A review of automatic selection methods for machine learning algorithms and hyper-parameter values;2016.0;[];Machine learning studies automatic algorithms that improve themselves through experience. It is widely used for analyzing and extracting value from large biomedical data sets, or “big biomedical data,” advancing biomedical research, and improving healthcare. Before a machine learning model is trained, the user of a machine learning software tool typically must manually select a machine learning algorithm and set one or more model parameters termed hyper-parameters. The algorithm and hyper-parameter values used can greatly impact the resulting model’s performance, but their selection requires special expertise as well as many labor-intensive manual iterations. To make machine learning accessible to layman users with limited computing expertise, computer science researchers have proposed various automatic selection methods for algorithms and/or hyper-parameter values for a given supervised machine learning problem. This paper reviews these methods, identifies several of their limitations in the big biomedical data environment, and provides preliminary thoughts on how to address these limitations. These findings establish a foundation for future research on automatically selecting algorithms and hyper-parameter values for analyzing big biomedical data.
2414387118;A Novel Correlation Coefficients between Pythagorean Fuzzy Sets and Its Applications to Decision-Making Processes;2016.0;[];Pythagorean fuzzy set PFS is one of the most successful in terms of representing comprehensively uncertain and vague information. Considering that the correlation coefficient plays an important role in statistics and engineering sciences, in this paper, after pointing out the weakness of the existing correlation coefficients between intuitionistic fuzzy sets IFSs, we propose a novel correlation coefficient and weighted correlation coefficient formulation to measure the relationship between two PFSs. Pairs of membership, nonmembership, and hesitation degree as a vector representation with the two elements have been considered during formulation. Numerical examples of pattern recognition and medical diagnosis have been taken to demonstrate the efficiency of the proposed approach. Results computed by the proposed approach are compared with the existing indices.
2414996405;Multi-view semi-supervised learning for image classification;2016.0;[];With the massive growth of digital image data uploaded to the Internet, classifying each image into appropriate semantic category with respect to its image content for image index and image retrieval has become an increasingly difficult and laborious task. To deal with this issue, we propose a novel multi-view semi-supervised learning framework which leverages the information contained in pseudo-labeled images to improve the prediction performance of image classification using multiple views of an image. In the training process, labeled images are first adopted to train view-specific classifiers independently using uncorrelated and sufficient views, and each view-specific classifier is then iteratively re-trained with respect to a measure of confidence using initial labeled samples and additional pseudo-labeled samples. In the classification process, the maximum entropy principle is utilized to assign appropriate category labels to unlabeled images via optimally trained view-specific classifiers. Experimental results on a general-purpose image database demonstrate the effectiveness and efficiency of the proposed multi-view semi-supervised image classification scheme.
2416573145;Novel migration operators of biogeography-based optimization and Markov analysis;2017.0;[];Biogeography-based optimization (BBO) is a nature-inspired optimization algorithm and has been developed in both theory and practice. In canonical BBO, migration operator is crucial to affect algorithm’s performance. In migration operator, a good solution has a large probability to be selected as an immigrant, while a poor solution has a large probability to be selected as an emigrant. The features in an emigrant will be completely replaced by the features in the corresponding immigrant. Hence, the migration operator in canonical BBO causes a significant deterioration of population diversity, and therefore, the algorithm’s performance worsens. In this paper, we propose three novel migration operators to enhance the exploration ability of BBO. To present a mathematical proof, Markov analysis is conducted to confirm the advantages of the proposed migration operators over existing ones. In addition, a number of benchmark tests are carried out to empirically assess the performance of the proposed migration operators, on both low-dimensional and high-dimensional numerical optimization problems. The comparison results demonstrate that the proposed migration operators are feasible and effective to enhance BBO’s performance.
2416799949;Edge Computing: Vision and Challenges;2016.0;[];The proliferation of Internet of Things (IoT) and the success of rich cloud services have pushed the horizon of a new computing paradigm, edge computing, which calls for processing the data at the edge of the network. Edge computing has the potential to address the concerns of response time requirement, battery life constraint, bandwidth cost saving, as well as data safety and privacy. In this paper, we introduce the definition of edge computing, followed by several case studies, ranging from cloud offloading to smart home and city, as well as collaborative edge to materialize the concept of edge computing. Finally, we present several challenges and opportunities in the field of edge computing, and hope this paper will gain attention from the community and inspire more research in this direction.
2417863416;SymPy: symbolic computing in Python;2017.0;[];SymPy is an open source computer algebra system written in pure Python. It is built with a focus on extensibility and ease of use, through both interactive and programmatic applications. These characteristics have led SymPy to become the standard symbolic library for the scientific Python ecosystem. This paper presents the architecture of SymPy, a description of its features, and a discussion of select domain specific submodules. The supplementary materials provide additional examples and further outline details of the architecture and features of SymPy.
2422225805;Position Accuracy Improvement by Implementing the DGNSS-CP Algorithm in Smartphones.;2016.0;[];The position accuracy of Global Navigation Satellite System (GNSS) modules is one of the most significant factors in determining the feasibility of new location-based services for smartphones. Considering the structure of current smartphones, it is impossible to apply the ordinary range-domain Differential GNSS (DGNSS) method. Therefore, this paper describes and applies a DGNSS-correction projection method to a commercial smartphone. First, the local line-of-sight unit vector is calculated using the elevation and azimuth angle provided in the position-related output of Android’s LocationManager, and this is transformed to Earth-centered, Earth-fixed coordinates for use. To achieve position-domain correction for satellite systems other than GPS, such as GLONASS and BeiDou, the relevant line-of-sight unit vectors are used to construct an observation matrix suitable for multiple constellations. The results of static and dynamic tests show that the standalone GNSS accuracy is improved by about 30%–60%, thereby reducing the existing error of 3–4 m to just 1 m. The proposed algorithm enables the position error to be directly corrected via software, without the need to alter the hardware and infrastructure of the smartphone. This method of implementation and the subsequent improvement in performance are expected to be highly effective to portability and cost saving.
2466248227;Optimizing Single-Trial EEG Classification by Stationary Matrix Logistic Regression in Brain–Computer Interface;2016.0;[];In addition to the noisy and limited spatial resolution characteristics of the electroencephalography (EEG) signal, the intrinsic nonstationarity in the EEG data makes the single-trial EEG classification an even more challenging problem in brain–computer interface (BCI). Variations of the signal properties within a session often result in deteriorated classification performance. This is mainly attributed to the reason that the routine feature extraction or classification method does not take the changes in the signal into account. Although several extensions to the standard feature extraction method have been proposed to reduce the sensitivity to nonstationarity in data, they optimize different objective functions from that of the subsequent classification model, and thereby, the extracted features may not be optimized for the classification. In this paper, we propose an approach that directly optimizes the classifier’s discriminativity and robustness against the within-session nonstationarity of the EEG data through a single optimization paradigm, and show that it can greatly improve the performance, in particular for the subjects who have difficulty in controlling a BCI. Moreover, the experimental results on two benchmark data sets demonstrate that our approach significantly outperforms the compared approaches in reducing classification error rates.
2466535637;Stroke Tissue Pattern Recognition Based on CT Texture Analysis;2016.0;[];The main objective of this paper is a texture-based solution to the problem of acute stroke tissue recognition on computed tomography images. Our proposed method of early stroke indication was based on two fundamental steps: (i) segmentation of potential areas with distorted brain tissue (selection of regions of interest), and (ii) acute stroke tissue recognition by extracting and then classifying a set of well-differentiating features. The proposed solution used various numerical image descriptors determined in several image transformation domains: 2D Fourier domain, polar 2D Fourier domain, and multiscale domains (i.e., wavelet, complex wavelet, and contourlet domain). The obtained results indicate the possibility of relatively effective detection of early stroke symptoms in CT images. Selected normal or pathological blocks were classified by LogitBoost with the accuracy close to 75 % with the use of our adjusted cross-validation procedure.
2467327288;A mixed-strategy based gravitational search algorithm for parameter identification of hydraulic turbine governing system;2016.0;[];A Mixed-Strategy based Gravitational Search Algorithm (MS-GSA) is proposed in this paper, in which three improvement strategies are mixed and integrated in the standard GSA to enhance the optimization ability. The first improvement strategy is introducing elite agentu0027s guidance into movement function to accelerate convergence speed. The second one is designing an adaptive gravitational constant function to keep a balance between the exploration and exploitation in the searching process. And the third improvement strategy is the mutation strategy based on the Cauchy and Gaussian mutations for overcoming the shortages of premature. The MS-GSA has been verified by comparing with 7 popular meta-heuristics algorithms on 23 typical basic benchmark functions and 7 CEC2005 composite benchmark functions. The results on these benchmark functions show that the MS-GSA has achieved significantly better performance than other algorithms. The effectiveness and significance of the results have been verified by Wilcoxonu0027s test. Finally, the MS-GSA is employed to solve the parameter identification problem of Hydraulic turbine governing system (HTGS). It is shown that the MS-GSA is able to identify the parameters of HTGS effectively with higher accuracy compared with existing methods.
2468154960;Intuitionistic fuzzy induced ordered entropic weighted averaging operator for group decision making;2016.0;[];
2470139095;A Benchmark Dataset and Evaluation Methodology for Video Object Segmentation;2016.0;[];Over the years, datasets and benchmarks have proven their fundamental importance in computer vision research, enabling targeted progress and objective comparisons in many fields. At the same time, legacy datasets may impend the evolution of a field due to saturated algorithm performance and the lack of contemporary, high quality data. In this work we present a new benchmark dataset and evaluation methodology for the area of video object segmentation. The dataset, named DAVIS (Densely Annotated VIdeo Segmentation), consists of fifty high quality, Full HD video sequences, spanning multiple occurrences of common video object segmentation challenges such as occlusions, motionblur and appearance changes. Each video is accompanied by densely annotated, pixel-accurate and per-frame ground truth segmentation. In addition, we provide a comprehensive analysis of several state-of-the-art segmentation approaches using three complementary metrics that measure the spatial extent of the segmentation, the accuracy of the silhouette contours and the temporal coherence. The results uncover strengths and weaknesses of current approaches, opening up promising directions for future works.
2470142083;An Uncertain Future: Forecasting from Static Images Using Variational Autoencoders;2016.0;[];In a given scene, humans can easily predict a set of immediate future events that might happen. However, pixel-level anticipation in computer vision is difficult because machine learning struggles with the ambiguity in predicting the future. In this paper, we focus on predicting the dense trajectory of pixels in a scene—what will move in the scene, where it will travel, and how it will deform over the course of one second. We propose a conditional variational autoencoder as a solution to this problem. In this framework, direct inference from the image shapes the distribution of possible trajectories while latent variables encode information that is not available in the image. We show that our method predicts events in a variety of scenes and can produce multiple different predictions for an ambiguous future. We also find that our method learns a representation that is applicable to semantic vision tasks.
2470259419;Incorporating outlier detection and replacement into a non-parametric framework for movement and distortion correction of diffusion MR images;2016.0;[];"Despite its great potential in studying brain anatomy and structure, diffusion magnetic resonance imaging (dMRI) is marred by artefacts more than any other commonly used MRI technique. In this paper we present a non-parametric framework for detecting and correcting dMRI outliers (signal loss) caused by subject  :[46],""loss (dropout) affecting a whole slice, or a large connected region of a slice, is frequently observed in diffusion weighted images, leading to a set of unusable measurements. This is caused by bulk (subject or physiological) motion during the diffusion encoding part of the imaging sequence. We suggest a method to detect slices affected by signal :[46],""loss and replace them by a non-parametric prediction, in order to minimise their impact on subsequent analysis. The outlier detection and replacement, as well as correction of other dMRI distortions (susceptibility-induced distortions, eddy currents (EC) and subject motion) are performed within a single framework, allowing the use of an integrated approach for distortion correction. Highly realistic simulations have been used to evaluate the method with respect to its ability to detect outliers (types 1 and 2 errors), the impact of outliers on retrospective correction of movement and distortion and the impact on estimation of commonly used diffusion tensor metrics, such as fractional anisotropy (FA) and mean diffusivity (MD). Data from a large imaging project studying older adults (the Whitehall Imaging sub-study) was used to demonstrate the utility of the method when applied to datasets with severe subject  :[240],""results indicate high sensitivity and specificity for detecting outliers and that their deleterious effects on FA and MD can be almost completely corrected."
2470673105;Hierarchical Attention Networks for Document Classification;2016.0;[];"We propose a hierarchical attention network for document classification. Our model has two distinctive characteristics: (i) it has a hierarchical structure that mirrors the hierarchical structure of documents; (ii) it has two levels of attention mechanisms applied at the wordand sentence-level, enabling it to attend differentially to more and less important content when constructing the document representation. Experiments conducted on six large scale text classification tasks demonstrate that the proposed architecture outperform previous methods by a substantial margin. Visualization of the attention layers illustrates that the model selects qualitatively informative words and sentences."
2472763926;A Partially Observable Markov Decision Process Approach to Residential Home Energy Management;2018.0;[];Real-time pricing (RTP) is a utility-offered dynamic pricing program to incentivize customers to make changes in their energy usage. A home energy management system (HEMS) automates the energy usage in a smart home in response to utility pricing signals. We present three new HEMS techniques—one myopic approach and two non-myopic partially observable Markov decision process (POMDP) approaches—for minimizing the household electricity bill in such a RTP market. In a simulation study, we compare the performance of the new HEMS methods with a mathematical lower bound and the status quo. We show that the non-myopic POMDP approach can provide a 10%–30% saving over the status quo.
2474884296;Probabilistic linguistic term sets in multi-attribute group decision making;2016.0;[];When expressing preferences in qualitative setting, several possible linguistic terms with different weights (represented by probabilities) may be considered at the same time. The probabilistic distribution is usually hard to be provided completely and ignorance may exist. In this paper, we first propose a novel concept called probabilistic linguistic term set (PLTS) to serve as an extension of the existing tools. Then we put forward some basic operational laws and aggregation operators for PLTSs. After that, we develop an extended TOPSIS method and an aggregation-based method respectively for multi-attribute group decision making (MAGDM) with probabilistic linguistic information, and apply them to a practical case concerning strategy initiatives. Finally, the strengths and weaknesses of our methods are clarified by comparing them with some similar techniques.
2482503770;Time series analysis of carrier phase differences for dual-frequency GPS high-accuracy positioning;2016.0;[];The GPS position based on carrier phase measurement is produced and developed for the exaltation of positioning accuracy. However, in practice, carrier phase observations often include various errors besides essential parameters. Sometimes even satellite signals may be temporarily interrupted and lead to cycle slips. Most of errors can be eliminated with differencing of carrier phase observation. But phase center offsets (PCOs) are a key source of errors in GPS precise measurements and hardly eliminated or weakened by difference methods. Meanwhile as a gross error, cycle slips decreases badly positioning accuracy but canu0027t remove by differencing approach. For all this, based on the feature of cycle slips and time series analysis theory, a quickly and convenient method of eliminating various errors, detecting and correcting cycle slips and PCOs is presented in this paper for high accuracy positioning solution. The key of this method is to build difference series between carrier phase observations of satellites, stations and epochs. These difference series can be used to eliminate various errors implied in observation data, and highlight the features of cycle slips. Based on the time-series analysis theory, the model of difference series of carrier phase observation data is built, cycle slips can be quickly detected by the stability of this time series. At last, the experiment results show that the new method can not only be suitable both for static and dynamic measurements of dual-frequency GPS receivers, but also can eliminate most measurement errors of dual-frequency GPS receivers, improve the efficiency of detection and correction of PCOs and cycle slips less than 1 cycle.
2483716586;Energy efficient adaptive optical CDMA random access protocol based on particle swarm optimization;2017.0;[];In this paper, an energy efficient adaptive optical code division multiple access (OCDMA) random access protocol based on particle swarm optimization (PSO) is described. This protocol is based on the S-ALOHA with power and rate allocation based on PSO. This scheme evaluates jointly optimal power and rate allocation PSO based under the random access protocol as a new and simplified scheme for high performance, high energy efficiency suitable for OCDMA systems. The aim is to maximize the aggregate throughput, subject to predetermined quality of service restrictions and energy efficiency constraint in terms of the signal-to-noise-plus interference ratio of each user class. Numerical results are discussed taking into account realistic network operation scenario.
2489519702;Attack Trees for Practical Security Assessment: Ranking of Attack Scenarios with ADTool 2.0;2016.0;[];"In this tool demonstration paper we present the ADTool2.0: an open-source software tool for design, manipulation and analysis of attack trees. The tool supports ranking of attack scenarios based on quantitative attributes entered by the user; it is scriptable; and it incorporates attack trees with sequential conjunctive refinement."
2492461111;On the risk prediction and analysis of soft information in finance reports;2017.0;[];"We attempt in this paper to utilize soft information in financial reports to analyze financial risk among companies. Specifically, on the basis of the text information in financial reports, which is the so-called soft information, we apply analytical techniques to study relations between texts and financial risk. Furthermore, we conduct a study on financial sentiment analysis by using a finance-specific sentiment lexicon to examine the relations between financial sentiment words and financial risk. A large collection of financial reports published annually by publicly-traded companies is employed to conduct our experiments; moreover, two analytical techniques – regression and ranking methods – are applied to conduct these analyses. The experimental results show that, based on a bag-of-words model, using only financial sentiment words results in performance comparable to using the whole texts; this confirms the importance of financial sentiment words with respect to risk prediction. In addition to this performance comparison, via the learned models, we draw attention to some strong and interesting correlations between texts and financial risk. These valuable findings yield greater insight and understanding into the usefulness of soft information in financial reports and can be applied to a broad range of financial and accounting applications."
2493343568;European Union Regulations on Algorithmic Decision-Making and a “Right to Explanation”;2017.0;[];We summarize the potential impact that the European Union’s new General Data Protection Regulation will have on the routine use of machine learning algorithms. Slated to take effect as law across the EU in 2018, it will restrict automated individual decision-making (that is, algorithms that make decisions based on user-level predictors) which “significantly affect” users. The law will also effectively create a “right to explanation,” whereby a user can ask for an explanation of an algorithmic decision that was made about them. We argue that while this law will pose large challenges for industry, it highlights opportunities for computer scientists to take the lead in designing algorithms and evaluation frameworks which avoid discrimination and enable explanation.
2497672654;PerSaDoR: Personalized social document representation for improving web search;2016.0;[];"   In this paper, we discuss a contribution towards the integration of social information in the index structure of an IR system. Since each user has his/her own understanding and point of view of a given document, we propose an approach in which the index model provides a Personalized Social Document Representation (PerSaDoR) of each document per user based on his/her activities in a social tagging system. The proposed approach relies on matrix factorization to compute the PerSaDoR of documents that match a query, at query time. The complexity analysis shows that our approach scales linearly with the number of documents that match the query, and thus, it can scale to very large datasets. PerSaDoR has been also intensively evaluated by an offline study and by a user survey operated on a large public dataset from  :[138],""significant benefits for personalized search compared to state of the art methods."
2507278300;Analysing the influence of the fitness function on genetically programmed bots for a real-time strategy game;2017.0;[];   Finding the global best strategy for an autonomous agent (bot) in a RTS game is a hard problem, mainly because the techniques applied to do this must deal with uncertainty and real-time planning in order to control the game agents. This work describes an approach applying a Genetic Programming (GP) algorithm to create the behavioural engine of bots able to play a simple RTS. Normally it is impossible to know in advance what kind of strategies will be the best in the most general case of this problem. So GP, which searches the general decision tree space, has been introduced and used successfully. However, it is not straightforward what fitness function would be the most convenient to guide the evolutionary process in order to reach the best solutions and also being less sensitive to the uncertainty present in the context of games. Thus, in this paper three different evaluation functions have been proposed, and a detailed analysis of their performance has been conducted. The paper also analyses several aspects of the obtained bots, in addition to their final performance on battles, such as the evolution of the decision trees (behavioural models) themselves, or the influence on the results of noise or uncertainty. The results show that a victory-based fitness, which prioritises the number of victories, contributes to generate better bots, on average, than other functions based on other numerical aspects of the battles, such as the number of resources gathered, or the number of units generated.
2507595475;Quantitative Verification and Synthesis of Attack-Defence Scenarios;2016.0;[];Attack-defence trees are a powerful technique for formally evaluating attack-defence scenarios. They represent in an intuitive, graphical way the interaction between an attacker and a defender who compete in order to achieve conflicting objectives. We propose a novel framework for the formal analysis of quantitative properties of complex attack-defence scenarios, using an extension of attack-defence trees which models temporal ordering of actions and allows explicit dependencies in the strategies adopted by attackers and defenders. We adopt a game-theoretic approach, translating attack-defence trees to two-player stochastic games, and then employ probabilistic model checking techniques to formally analyse these models. This provides a means to both verify formally specified security properties of the attack-defence scenarios and, dually, to synthesise strategies for attackers or defenders which guarantee or optimise some quantitative property, such as the probability of a successful attack, the expected cost incurred, or some multi-objective trade-off between the two. We implement our approach, building upon the PRISM-games model checker, and apply it to a case study of an RFID goods management system.
2508329181;Uses and Gratifications factors for social media use in teaching: Instructors’ perspectives:;2018.0;[];This research was motivated by an interest in understanding how social media are applied in teaching in higher education. Data were collected using an online questionnaire, completed by 333 instructors in higher education, that asked about general social media use and specific use in teaching. Education and learning theories suggest three potential reasons for instructors to use social media in their teaching: (1) exposing students to practices, (2) extending the range of the learning environment, and (3) promoting learning through social interaction and collaboration. Answers to open-ended questions about how social media were used in teaching, and results of a factor analysis of coded results, revealed six distinct factors that align with these reasons for use: (1) facilitating student engagement, (2) instructor’s organization for teaching, (3) engagement with outside resources, (4) enhancing student attention to content, (5) building communities of practice, and (6) resource discovery. These factors ac...
2508982726;Denoising of diffusion MRI using random matrix theory;2016.0;[];   We introduce and evaluate a post-processing technique for fast denoising of diffusion-weighted MR images. By exploiting the intrinsic redundancy in diffusion MRI using universal properties of the eigenspectrum of random covariance matrices, we remove noise-only principal components, thereby enabling signal-to-noise ratio enhancements. This yields parameter maps of improved quality for visual, quantitative, and statistical interpretation. By studying statistics of residuals, we demonstrate that the technique suppresses local signal fluctuations that solely originate from thermal noise rather than from other sources such as anatomical detail. Furthermore, we achieve improved precision in the estimation of diffusion parameters and fiber orientations in the human brain without compromising the accuracy and spatial resolution.
2512971201;Deep Neural Networks for YouTube Recommendations;2016.0;[];YouTube represents one of the largest scale and most sophisticated industrial recommendation systems in existence. In this paper, we describe the system at a high level and focus on the dramatic performance improvements brought by deep learning. The paper is split according to the classic two-stage information retrieval dichotomy: first, we detail a deep candidate generation model and then describe a separate deep ranking model. We also provide practical lessons and insights derived from designing, iterating and maintaining a massive recommendation system with enormous user-facing impact.
2513539187;A Unified Fisher’s Ratio Learning Method for Spatial Filter Optimization;2017.0;[];To detect the mental task of interest, spatial filtering has been widely used to enhance the spatial resolution of electroencephalography (EEG). However, the effectiveness of spatial filtering is undermined due to the significant nonstationarity of EEG. Based on regularization, most of the conventional stationary spatial filter design methods address the nonstationarity at the cost of the interclass discrimination. Moreover, spatial filter optimization is inconsistent with feature extraction when EEG covariance matrices could not be jointly diagonalized due to the regularization. In this paper, we propose a novel framework for a spatial filter design. With Fisher’s ratio in feature space directly used as the objective function, the spatial filter optimization is unified with feature extraction. Given its ratio form, the selection of the regularization parameter could be avoided. We evaluate the proposed method on a binary motor imagery data set of 16 subjects, who performed the calibration and test sessions on different days. The experimental results show that the proposed method yields improvement in classification performance for both single broadband and filter bank settings compared with conventional nonunified methods. We also provide a systematic attempt to compare different objective functions in modeling data nonstationarity with simulation studies.
2513787030;SAR Target Recognition via Supervised Discriminative Dictionary Learning and Sparse Representation of the SAR-HOG Feature;2016.0;[];Automatic target recognition (ATR) in synthetic aperture radar (SAR) images plays an important role in both national defense and civil applications. Although many methods have been proposed, SAR ATR is still very challenging due to the complex application environment. Feature extraction and classification are key points in SAR ATR. In this paper, we first design a novel feature, which is a histogram of oriented gradients (HOG)-like feature for SAR ATR (called SAR-HOG). Then, we propose a supervised discriminative dictionary learning (SDDL) method to learn a discriminative dictionary for SAR ATR and propose a strategy to simplify the optimization problem. Finally, we propose a SAR ATR classifier based on SDDL and sparse representation (called SDDLSR), in which both the reconstruction error and the classification error are considered. Extensive experiments are performed on the MSTAR database under standard operating conditions and extended operating conditions. The experimental results show that SAR-HOG can reliably capture the structures of targets in SAR images, and SDDL can further capture subtle differences among the different classes. By virtue of the SAR-HOG feature and SDDLSR, the proposed method achieves the state-of-the-art performance on MSTAR database. Especially for the extended operating conditions (EOC) scenario “Training     17 ∘    —Testing     45 ∘    ”, the proposed method improves remarkably with respect to the previous works.
2514438535;A systematic analysis of term reuse and term overlap across biomedical ontologies;2017.0;[];Reusing ontologies and their terms is a principle and best practice that most ontology development methodologies strongly encourage. Reuse comes with the promise to support the semantic interoperability and to reduce engineering costs. In this paper, we present a descriptive study of the current extent of term reuse and overlap among biomedical ontologies. We use the corpus of biomedical ontologies stored in the BioPortal repository, and analyze different types of reuse and overlap constructs. While we find an approximate term overlap between 25–31%, the term reuse is only  90% semantic similarity, hinting that ontology developers tend to reuse terms that are sibling or parent–child nodes. We validate this finding by analyzing the logs generated from a Protege plugin that enables developers to reuse terms from BioPortal. We find most reuse constructs were 2-level subtrees on the higher levels of the class hierarchy. We developed a Web application that visualizes reuse dependencies and overlap among ontologies, and that proposes similar terms from BioPortal for a term of interest. We also identified a set of error patterns that indicate that ontology developers did intend to reuse terms from other ontologies, but that they were using different and sometimes incorrect representations. Our results stipulate the need for semi-automated tools that augment term reuse in the ontology engineering process through personalized recommendations.
2514350728;Bridging Two Worlds: Reconciling Practical Risk Assessment Methodologies with Theory of Attack Trees;2016.0;[];Security risk treatment often requires a complex cost-benefit analysis to be carried out in order to select countermeasures that optimally reduce risks while having minimal costs. According to ISO/IEC 27001, risk treatment relies on catalogues of countermeasures, and the analysts are expected to estimate the residual risks. At the same time, recent advancements in attack tree theory provide elegant solutions to this optimization problem. In this paper we propose to bridge the gap between these two worlds by introducing optimal countermeasure selection problem on attack-defense trees into the TRICK security risk assessment methodology.
2514754342;Generalized Orthopair Fuzzy Sets;2017.0;[];We note that orthopair fuzzy subsets are such that that their membership grades are pairs of values, from the unit interval, one indicating the degree of support for membership in the fuzzy set and the other support against membership. We discuss two examples, Atanassovu0027s classic intuitionistic sets and a second kind of intuitionistic set called Pythagorean. We note that for classic intuitionistic sets the sum of the support for and against is bounded by one, while for the second kind, Pythagorean, the sum of the squares of the support for and against is bounded by one. Here we introduce a general class of these sets called q-rung orthopair fuzzy sets in which the sum of the   ${\rm{q}}$  th power of the support for and the   ${\rm{q}}$  th power of the support against is bonded by one. We note that as q increases the space of acceptable orthopairs increases and thus gives the user more freedom in expressing their belief about membership grade. We investigate various set operations as well as aggregation operations involving these types of sets.
2515420228;Modelling Attack-defense Trees Using Timed Automata;2016.0;[];"Performing a thorough security risk assessment of an organisation has always been challenging, but with the increased reliance on outsourced and off-site third-party services, i.e., “cloud services”, combined with internal (legacy) IT-infrastructure and -services, it has become a very difficult and time-consuming task. One of the traditional tools available to ease the burden of performing a security risk assessment and structure security analyses in general is attack trees, a tree-based formalism inspired by fault trees, a well-known formalism used in safety  :[81],""this paper we study an extension of traditional attack trees, called attack-defense trees, in which not only the attacker’s actions are modelled, but also the defensive actions taken by the attacked party. In :[81],""this work we use the attack-defense tree as a goal an attacker wants to achieve, and separate the behaviour of the attacker and defender from the attack-defense-tree. We give a fully stochastic timed semantics for the behaviour of the attacker by introducing attacker profiles that choose actions probabilistically and execute these according to a probability density. Lastly, the stochastic semantics provides success probabilitites for individual actions. Furthermore, we show how to introduce costs of attacker actions. Finally, we show how to automatically encode it all with a network of timed automata, an encoding that enables us to apply state-of-the-art model checking tools and techniques to perform fully automated quantitative and qualitative analyses of the modelled system."
2516230096;Dynamic convexification within nested Benders decomposition using Lagrangian relaxation: An application to the strategic bidding problem;2017.0;[];Many decomposition algorithms like Benders decomposition and stochastic dual dynamic programming are limited to convex optimization problems. In this paper, we utilize a dynamic convexification method that makes use of Lagrangian relaxation to overcome this limitation and enables the modeling of non-convex multi-stage problems using decomposition algorithms. Though the algorithm is confined by the duality gap of the problem being studied, the computed upper bounds (for maximization problems) are at least as good as those found via a linear programming relaxation approach. We apply the method to the strategic bidding problem for a hydroelectric producer, in which we ask: What is the revenue-maximizing production schedule for a single price-maker hydroelectric producer in a deregulated, bid-based market? Because the price-maker’s future revenue function has a sawtooth shape, we model it using mixed-integer linear programming. To remedy the non-concavity issues associated with modeling the future revenue function as a mixed-integer linear program, we model the price-maker’s bidding decision utilizing both Benders decomposition and Lagrangian relaxation. We demonstrate the utility of our algorithm through an illustrative example and through three case studies in which we model electricity markets in El Salvador, Honduras, and Nicaragua.
2518226408;Compliance-based Multi-dimensional Trust Evaluation System for determining trustworthiness of Cloud Service Providers;2017.0;[];   This paper addresses the problem of determining trustworthiness of Cloud Service Providers (CSPs) in a cloud environment. For the current work, trustworthiness is defined as the degree of compliance of a CSP to the promised quantitative QoS parameters as defined in the Service Level Agreement (SLA). Due to large number of CSPs offering similar kinds of services in the cloud environment, it has become a challenging task for Cloud Clients (CCs) to identify and differentiate between the trustworthy and untrustworthy CSPs. At present, there is no trust evaluation system that allows CCs to evaluate the trustworthiness of CSPs on the basis of their adherence to the SLA i.e. the compliance provided by the CSPs to CCs as per the SLAs. This paper proposes a Compliance-based Multi-dimensional Trust Evaluation System (CMTES) that enables CCs to determine the trustworthiness of a CSP from different perspectives, as trust is a subjective concept. Such a system is of great help to CCs who want to choose a CSP from a pool of CSPs, satisfying their desired QoS requirements. The framework enables us to evaluate the trustworthiness of a CSP from the CC’s perspective, Cloud Auditor’s perspective, Cloud Broker’s perspective and Peers’ perspective. Experimental results show that the CMTES is effective and stable in differentiating trustworthy and untrustworthy CSPs. The validation of the CMTES has been done with the help of synthetic data due to lack of standardized dataset and its applicability has been demonstrated with the help of a case study involving the use of real cloud data.
2518812438;Biogeography-based learning particle swarm optimization;2017.0;[];This paper explores biogeography-based learning particle swarm optimization (BLPSO). Specifically, based on migration of biogeography-based optimization (BBO), a new biogeography-based learning strategy is proposed for particle swarm optimization (PSO), whereby each particle updates itself by using the combination of its own personal best position and personal best positions of all other particles through the BBO migration. The proposed BLPSO is thoroughly evaluated on 30 benchmark functions from CEC 2014. The results are very promising, as BLPSO outperforms five well-established PSO variants and several other representative evolutionary algorithms.
2520060407;A Stochastic Framework for Quantitative Analysis of Attack-Defense Trees;2016.0;[];Cyber attacks are becoming increasingly complex, practically sophisticated and organized. Losses due to such attacks are important, varying from the loss of money to business reputation spoilage. Therefore, there is a great need for potential victims of cyber attacks to deploy security solutions that allow the identification and/or prediction of potential cyber attacks, and deploy defenses to face them. In this paper, we propose a framework that incorporates Attack-Defense trees (ADTrees) and Continuous Time Markov Chains (CTMCs) to systematically represent attacks, defenses, and their interaction. This solution allows to perform quantitative security assessment, with an aim to predict and/or identify attacks and find the best and appropriate defenses to reduce the impact of attacks.
2520744072;Investigating white matter fibre density and morphology using fixel-based analysis;2017.0;[];Voxel-based analysis of diffusion MRI data is increasingly popular. However, most white matter voxels contain contributions from multiple fibre populations (often referred to as crossing fibres), and therefore voxel-averaged quantitative measures (e.g. fractional anisotropy) are not fibre-specific and have poor interpretability. Using higher-order diffusion models, parameters related to fibre density can be extracted for individual fibre populations within each voxel (‘fixels’), and recent advances in statistics enable the multi-subject analysis of such data. However, investigating within-voxel microscopic fibre density alone does not account for macroscopic differences in the white matter morphology (e.g. the calibre of a fibre bundle). In this work, we introduce a novel method to investigate the latter, which we call fixel-based morphometry (FBM). To obtain a more complete measure related to the total number of white matter axons, information from both within-voxel microscopic fibre density and macroscopic morphology must be combined. We therefore present the FBM method as an integral piece within a comprehensive fixel-based analysis framework to investigate measures of fibre density, fibre-bundle morphology (cross-section), and a combined measure of fibre density and cross-section. We performed simulations to demonstrate the proposed measures using various transformations of a numerical fibre bundle phantom. Finally, we provide an example of such an analysis by comparing a clinical patient group to a healthy control group, which demonstrates that all three measures provide distinct and complementary information. By capturing information from both sources, the combined fibre density and cross-section measure is likely to be more sensitive to certain pathologies and more directly interpretable.
2520811994;A risk assessment model for selecting cloud service providers;2016.0;[];The Cloud Adoption Risk Assessment Model is designed to help cloud customers in assessing the risks that they face by selecting a specific cloud service provider. It evaluates background information obtained from cloud customers and cloud service providers to analyze various risk scenarios. This facilitates decision making an selecting the cloud service provider with the most preferable risk profile based on aggregated risks to security, privacy, and service delivery. Based on this model we developed a prototype using machine learning to automatically analyze the risks of representative cloud service providers from the Cloud Security Alliance Security, Trust u0026 Assurance Registry.
2520843716;Multiple solutions for impulsive problems with non-autonomous perturbations☆;2017.0;[];   In this article, we study the existence of multiple solutions for nonlinear impulsive problems with small non-autonomous perturbations. We show the existence of at least three distinct classical solutions by using variational methods and a three critical points theorem.
2522016266;Individual differences in categorical perception of speech: Cue weighting and executive function;2016.0;[];"   This study examined individual differences in categorical perception and the use of multiple acoustic cues in the perception of the stop voicing contrast. Goals were to investigate whether gradiency of speech perception was related to listeners’ differential sensitivity to acoustic cues and to individual differences in executive function. The experiment included two speech perception tasks (visual analogue scaling [VAS] and anticipatory eye movement [AEM]) administered to 30 English-speaking adults in two separate experimental sessions. Stimuli were a /ta/ to /da/ continuum that systematically varied VOT and  f0 . Findings were that some listeners had a more gradient pattern of responses on the VAS task; the listeners who had a gradient response pattern on the VAS task also showed more sensitivity to  f0  on the AEM task. The patterns were consistent across individuals tested on two separate occasions. These results suggest that variability in how categorically listeners perceive speech sounds is consistent and systematic within individuals."
2525851376;LEO: scheduling sensor inference algorithms across heterogeneous mobile processors and network resources;2016.0;[];Mobile apps that use sensors to monitor user behavior often employ resource heavy inference algorithms that make computational offloading a common practice. However, existing schedulers/offloaders typically emphasize one primary offloading aspect without fully exploring complementary goals (e.g., heterogeneous resource management with only partial visibility into underlying algorithms, or concurrent sensor app execution on a single resource) and as a result, may overlook performance benefits pertinent to sensor processing.   We bring together key ideas scattered in existing offloading solutions to build LEO -- a scheduler designed to maximize the performance for the unique workload of continuous and intermittent mobile sensor apps without changing their inference accuracy. LEO makes use of domain specific signal processing knowledge to smartly distribute the sensor processing tasks across the broader range of heterogeneous computational resources of high-end phones (CPU, co-processor, GPU and the cloud). To exploit short-lived, but substantial optimization opportunities, and remain responsive to the needs of near real-time apps such as voice-based natural user interfaces, LEO runs as a service on a low-power co-processor unit (LPU) to perform both frequent and joint schedule optimization for concurrent pipelines. Depending on the workload and network conditions, LEO is between 1.6 and 3 times more energy efficient than conventional cloud offloading with CPU-bound sensor sampling. In addition, even if a general-purpose scheduler is optimized directly to leverage an LPU, we find LEO still uses only a fraction (
2530395818;Equality of opportunity in supervised learning;2016.0;[];We propose a criterion for discrimination against a specified sensitive attribute in supervised learning, where the goal is to predict some target based on available features. Assuming data about the predictor, target, and membership in the protected group are available, we show how to optimally adjust any learned predictor so as to remove discrimination according to our definition. Our framework also improves incentives by shifting the cost of poor classification from disadvantaged groups to the decision maker, who can respond by improving the classification accuracy. We enourage readers to consult the more complete manuscript on the arXiv.
2531428766;Balance Ninja: Towards the Design of Digital Vertigo Games via Galvanic Vestibular Stimulation;2016.0;[];Vertigo -- the momentary disruption of the stability of perception -- is an intriguing game element that underlies many unique play experiences, such as spinning in circles as children to rock climbing as adults, yet vertigo is relatively unexplored when it comes to digital play. In this paper we explore the potential of Galvanic Vestibular Stimulation (GVS) as a game design tool for digital vertigo games. We detail the design and evaluation of a novel two player GVS game, Balance Ninja. From study observations and analysis of Balance Ninja (N=20), we present three design themes and six design strategies that can be used to aid game designers of future digital vertigo games. With this work we aim to highlight that vertigo can be a valuable digital game element that helps to expand the range of games we play.
2532629220;Detecting outlier pairs in complex network based on link structure and semantic relationship;2017.0;[];   In this paper, we propose an outlier pair detection method, called LSOutPair, which discovers the vast differences between link structure and semantic relationship. LSOutPair addresses three important challenges: (1) how can we measure the target objectu0027s link similarity among multi-typed objects and multi-typed relations? (2) how can we measure the semantic similarity using the short texts? (3) how can we find the objects’ maximum differences between link structure and semantic relationship? To tackle these challenges, LSOutPair applies three main techniques: (1) two matrices are used to store link similarity and semantic similarity, (2) a  k -step index algorithm, which calculates the term weighting for each object, (3) applying the linear transformation of Frobenius norm to matrices can obtain the top-K outlier pairs. LSOutPair considers link and semantics in complex network simultaneously, which is a new attempt in data mining. Substantial experiments show that LSOutPair is very effective for outlier pair detection.
2533940363;Using Attack-Defense Trees to Analyze Threats and Countermeasures in an ATM: A Case Study;2016.0;[];Securing automated teller machines (ATMs), as critical and complex infrastructure, requires a precise understanding of the associated threats. This paper reports on the application of attack-defense trees to model and analyze the security of ATMs. We capture the most dangerous multi-stage attack scenarios applicable to ATM structures, and establish a practical experience report, where we reflect on the process of modeling ATM threats via attack-defense trees. In particular, we share our insights into the benefits and drawbacks of attack-defense tree modeling, as well as best practices and lessons learned.
2536015822;A Deep Relevance Matching Model for Ad-hoc Retrieval;2016.0;[];In recent years, deep neural networks have led to exciting breakthroughs in speech recognition, computer vision, and natural language processing (NLP) tasks. However, there have been few positive results of deep models on ad-hoc retrieval tasks. This is partially due to the fact that many important characteristics of the ad-hoc retrieval task have not been well addressed in deep models yet. Typically, the ad-hoc retrieval task is formalized as a matching problem between two pieces of text in existing work using deep models, and treated equivalent to many NLP tasks such as paraphrase identification, question answering and automatic conversation. However, we argue that the ad-hoc retrieval task is mainly about relevance matching while most NLP matching tasks concern semantic matching, and there are some fundamental differences between these two matching tasks. Successful relevance matching requires proper handling of the exact matching signals, query term importance, and diverse matching requirements. In this paper, we propose a novel deep relevance matching model (DRMM) for ad-hoc retrieval. Specifically, our model employs a joint deep architecture at the query term level for relevance matching. By using matching histogram mapping, a feed forward matching network, and a term gating network, we can effectively deal with the three relevance matching factors mentioned above. Experimental results on two representative benchmark collections show that our model can significantly outperform some well-known retrieval models as well as state-of-the-art deep matching models.
2538584992;Estimators for global information in mobile opportunistic network;2013.0;[];Lack of global knowledge in a delay tolerant network proves detrimental, for a replication based routing strategy, in terms of resource utilization. It has often been noticed that nodes keep on replicating a message although it has already been delivered to the destination since such information remains unknown to most of the nodes in the network. Moreover recovery mechanisms (like VACCINE for epidemic routing) take time to propagate and incur additional transmission overhead. Global information such as number of nodes in the network, node meeting schedule, and whether a message has already been delivered to the destination is vital for optimal network performance. In this paper we propose model for estimating some of these vital information and show that replication based routing algorithm like epidemic routing, can perform better with minimum resource utilization. First, we use a uniformly minimum variance unbiased estimator (UMVUE) to estimate the number of nodes in the network. Second, we estimate the number of replicas of any given message in the network, at any given instant (t). Third, we show that epidemic routing with a node population of N can perform best with only N/2 message replicas for any given message and that further replication only leads additional overhead without any performance improvement. We show the simulated results using random waypoint mobility model and Shanghai taxi trace.
2540507509;Brain Computer Interface : control Signals Review;2017.0;[];   Brain Computer Interface (BCI) is defined as a combination of hardware and software that allows brain activities to control external devices or even computers. The research in this field has attracted academia and industry alike. The objective is to help severely disabled people to live their life as regular persons as much as possible. Some of these disabilities are categorized as neurological neuromuscular disorders. A BCI system goes through many phases including preprocessing, feature extraction, signal classifications, and finally control. Large body of research are found at each phase and this might confuse researchers and BCI developers. This article is a review to the state-of-the-art work in the field of BCI. The main focus of this review is on the Brain control signals, their types and classifications. In addition, this survey reviews the current BCI technology in terms of hardware and software where the most used BCI devices are described as well as the most utilized software platforms are explained. Finally, BCI challenges and future directions are stated. Due to the limited space and large body of literature in the field of BCI, another two review articles are planned. One of these articles reviews the up-to-date BCI algorithms and techniques for signal processing, feature extraction, signals classification, and control. Another article will be dedicated to BCI systems and applications. The three articles are written as base and guidelines for researchers and developers pursue the work in the field of BCI.
2546663644;Nontrivial solutions for a fractional advection dispersion equation in anomalous diffusion;2017.0;[];   In this paper, we consider the existence of nontrivial solutions for a class of fractional advection–dispersion equations. A new existence result is established by introducing a suitable fractional derivative Sobolev space and using the critical point theorem.
2549162964;A combination of transductive and inductive learning for handling non-stationarities in motor imagery classification;2016.0;[];
2549437329;Instance categorization by support vector machines to adjust weights in AdaBoost for imbalanced data classification;2017.0;[];We proposed an improved weighted SVM for imbalanced classification.Different factor scores are computed by categorizing instances based on the SVM margin.Numerical experiments showed the proposed method outperforming the existing approaches in terms of F-measure AUC. To address class imbalance in data, we propose a new weight adjustment factor that is applied to a weighted support vector machine (SVM) as a weak learner of the AdaBoost algorithm. Different factor scores are computed by categorizing instances based on the SVM margin and are assigned to related instances. The SVM margin is used to define borderline and noisy instances, and the factor scores are assigned to only borderline instances and positive noise. The adjustment factor is then employed as a multiplier to the instance weight in the AdaBoost algorithm when learning a weighted SVM. Using 10 real class-imbalanced datasets, we compare the proposed method to a standard SVM and other SVMs combined with various sampling and boosting methods. Numerical experiments show that the proposed method outperforms existing approaches in terms of F-measure and area under the receiver operating characteristic curve, which means that the proposed method is useful for relaxing the class-imbalance problem by addressing well-known degradation issues such as overlap, small disjunct, and data shift problems.
2551054676;Deep convolutional neural networks for classification of mild cognitive impaired and Alzheimer's disease patients from scalp EEG recordings;2016.0;[];In spite of the worldwide financial and research efforts made, the pathophysiological mechanism at the basis of Alzheimeru0027s disease (AD) is still poorly understood. Previous studies using electroencephalography (EEG) have focused on the slowing of oscillatory brain rhythms, coupled with complexity reduction of the corresponding time-series and their enhanced compressibility. These analyses have been typically carried out on single channels. However, limited investigations have focused on the possibility yielded by computational intelligence methodologies and novel machine learning approaches applied to multichannel schemes. The study at screening level on EEG recordings of subjects at risk could be useful to highlight the emergence of underlying AD progression (or at least support any further clinical investigation). In this work, the representational power of Deep Learning on Convolutional Neural Networks (CNN) is exploited to generate suitable sets of features that are then able to classify EEG patterns of AD from a prodromal version of dementia (Mild Cognitive Impairment, MCI) and from age-matched Healthy Controls (HC). The processing system here used enforces a series of convolutional-subsampling layers in order to derive a multivariate assembly of latent, novel patterns, finally used to categorize sets of EEG from different classes of subjects. The final processor here proposed is able to reach an averaged 80% of correct classification with good performance on both sensitivity and specificity by using a Multilayered Feedforward Perceptron (MLP) of the standard type as a final block of the procedure.
2553939816;PROFILING THE LEARNING STYLES OF STUDENTS IN CYBER UNIVERSITY;2004.0;[];An empirical application of the DeLone and McLean model in private sector organizations of Kuwait is presented. Seven organizations representing the seven sectors in the Kuwaiti Stock market participated. Certain direct associations between the variables in the original DeLone and McLean model were supported from initial correlation analysis. Subsequent regression analyses confirmed these associations. Information quality and system quality impact user satisfaction significantly. System usage has a significant influence on individual impact.
2557894284;Performance comparison of NSGA-II and NSGA-III on various many-objective test problems;2016.0;[];Recently NSGA-III has been frequently used for performance comparison of newly proposed evolutionary many-objective optimization algorithms. That is, NSGA-III has been used as a benchmark algorithm for evolutionary many-objective optimization. However, unfortunately, its source code is not available from the authors of the NSGA-III paper. This leads to an undesirable situation where a different implementation is used in a different study. Moreover, comparison is usually performed on DTLZ and WFG test problems. As a result, the performance of NSGA-III on a wide variety of many-objective test problems is still unclear whereas it has been frequently used for performance comparison in the literature. In this paper, we evaluate the performance of NSGA-III in comparison with NSGA-II on four totally different types of many-objective test problems with 3–10 objectives: DTLZ1-4 problems, their maximization variants, distance minimization problems, and knapsack problems. We use two different implementations of NSGA-II and NSGA-III. We show through computational experiments that NSGA-III does not always outperform NSGA-II even for ten-objective problems. That is, their comparison results depend not only on the number of objectives but also on the type of test problems. The choice of test problems has a larger effect on their comparison results than the number of objectives in our computational experiments. We also demonstrate that totally different results are obtained from different implementations of NSGA-III for some test problems.
2559900391;Genetic programming for evolving figure-ground segmentors from multiple features;2017.0;[];   Figure-ground segmentation is a crucial preprocessing step for many image processing and computer vision tasks. Since different object classes need specific segmentation rules, the top-down approach, which learns from the object information, is more suitable to solve segmentation problems than the bottom-up approach. A problem faced by most existing top-down methods is that they require much human work/intervention, meanwhile introducing human bias. As genetic programming (GP) does not require users to specify the structure of solutions, we apply it to evolve segmentors that can conduct the figure-ground segmentation automatically and accurately. This paper aims to determine what kind of image information is necessary for GP to evolve capable segmentors (especially for images with high variations, e.g. varied object shapes or cluttered backgrounds). Therefore, seven different terminal sets are exploited to evolve segmentors, and images from four datasets (bitmap, Brodatz texture, Weizmann and Pascal databases), which are increasingly difficult for segmentation tasks, are selected for testing. Results show that the proposed GP based method can be successfully applied to diverse types of images. In addition, intensity based features are not sufficient for complex images, whereas features containing spectral and statistical information are necessary. Compared with four widely-used segmentation techniques, our method obtains consistently better segmentation performance.
2560164496;A Generic Framework for Modeling MAC Protocols in Wireless Sensor Networks;2017.0;[];Wireless sensor networks are employed in many applications, such as health care, environmental sensing, and industrial monitoring. An important research issue is the design of efficient medium access control (MAC) protocols, which have an essential role for the reliability, latency, throughput, and energy efficiency of communication, especially as communication is typically one of the most energy consuming tasks. Therefore, analytical models providing a clear understanding of the fundamental limitations of the different MAC schemes, as well as convenient way to investigate their performance and optimize their parameters, are required. In this paper, we propose a generic framework for modeling MAC protocols, which focuses on energy consumption, latency, and reliability. The framework is based on absorbing Markov chains, and can be used to compare different schemes and evaluate new approaches. The different steps required to model a specific MAC using the proposed framework are illustrated through a study case. Moreover, to exemplify how the proposed framework can be used to evaluate new MAC paradigms, evaluation of the novel  pure-asynchronous  approach, enabled by emerging ultra-low-power wake-up receivers, is done using the proposed framework. Experimental measurements on real hardware were performed to set framework parameters with accurate energy consumption and latency values, to validate the framework, and to support our results.
2560205181;Data augmentation for unbalanced face recognition training sets;2017.0;[];Face recognition remains a challenging problem. While one-to-one face verification has been largely tackled, verification-based classification problem still demands effort. To further enhance the verification models, one solution is to fully utilize the unbalanced training sets, where, while abundant samples are provided for some subjects, there are often so few samples available for the rest. These subjects with too few samples can contribute little to the model learning. Therefore, before training a model, algorithms usually perform data augmentation on the whole dataset, especially on subjects with insufficient samples. In this paper, a new augmentation method is proposed, targeting on data augmentation for face classification algorithms. Instead of directly manipulating the input image, we perform virtual sample generating on feature level. The distribution of feature maps is first estimated, then random noise consistent to the distribution is applied to the feature vectors of training samples. Our method is based on Joint Bayesian Face Analysis, and we also develop an algorithm to boost the whole procedure. We conduct experiments based on high dimensional LBP features and features extracted by a shallow Convolutional Neural Network, and succeed to verify the effectiveness of this method, using image data from benchmark dataset LFW.
2560474170;FlowNet 2.0: Evolution of Optical Flow Estimation with Deep Networks;2017.0;[];The FlowNet demonstrated that optical flow estimation can be cast as a learning problem. However, the state of the art with regard to the quality of the flow has still been defined by traditional methods. Particularly on small displacements and real-world data, FlowNet cannot compete with variational methods. In this paper, we advance the concept of end-to-end learning of optical flow and make it work really well. The large improvements in quality and speed are caused by three major contributions: first, we focus on the training data and show that the schedule of presenting data during training is very important. Second, we develop a stacked architecture that includes warping of the second image with intermediate optical flow. Third, we elaborate on small displacements by introducing a subnetwork specializing on small motions. FlowNet 2.0 is only marginally slower than the original FlowNet but decreases the estimation error by more than 50%. It performs on par with state-of-the-art methods, while running at interactive frame rates. Moreover, we present faster variants that allow optical flow computation at up to 140fps with accuracy matching the original FlowNet.
2560480567;An energy-efficient opportunistic routing protocol in delay tolerant networks;2016.0;[];In delay tolerant networks (DTN), connectivity between nodes is not guaranteed and a message is delivered to a destination node from a source node via opportunistic contacts between intermediate nodes. In DTN, nodes are operated using battery and thus, an energy-efficient operation is crucial, especially for disaster environment. In this paper, we consider PRoPHET protocol as a base protocol since it is the standardized DTN protocol and thus, is regarded as one of the representative DTN routing protocol. Then, we propose an energy-efficient opportunistic routing protocol in DTN, where messages are forwarded based on nodeu0027s remaining battery and delivery predictability in PRoPHET protocol, analyze the performance of the proposed scheme, and compare the performance with PRoPHET and PRoPHET with periodic sleep in detail, from the aspect of delivery ratio, overhead ratio, delivery latency, and ratio of alive nodes. Simulation results show that the proposed protocol has better delivery probability and overhead ratio, in spite of small increase of delivery latency.
2561467560;On the use of feature selection to improve the detection of sea oil spills in SAR images;2017.0;[];Fast and effective oil spill detection systems are crucial to ensure a proper response to environmental emergencies caused by hydrocarbon pollution on the oceanu0027s surface. Typically, these systems uncover not only oil spills, but also a high number of look-alikes. The feature extraction is a critical and computationally intensive phase where each detected dark spot is independently examined. Traditionally, detection systems use an arbitrary set of features to discriminate between oil spills and look-alikes phenomena. However, Feature Selection (FS) methods based on Machine Learning (ML) have proved to be very useful in real domains for enhancing the generalization capabilities of the classifiers, while discarding the existing irrelevant features. In this work, we present a generic and systematic approach, based on FS methods, for choosing a concise and relevant set of features to improve the oil spill detection systems. We have compared five FS methods: Correlation-based feature selection (CFS), Consistency-based filter, Information Gain, ReliefF and Recursive Feature Elimination for Support Vector Machine (SVM-RFE). They were applied on a 141-input vector composed of features from a collection of outstanding studies. Selected features were validated via a Support Vector Machine (SVM) classifier and the results were compared with previous works. Test experiments revealed that the classifier trained with the 6-input feature vector proposed by SVM-RFE achieved the best accuracy and Cohenu0027s kappa coefficient (87.1% and 74.06% respectively). This is a smaller feature combination with similar or even better classification accuracy than previous works. The presented finding allows to speed up the feature extraction phase without reducing the classifier accuracy. Experiments also confirmed the significance of the geometrical features since 75.0% of the different features selected by the applied FS methods as well as 66.67% of the proposed 6-input feature vector belong to this category. HighlightsFive feature selection methods were applied to improve oil spill detection systems.Feature selection discarded irrelevant features and improved the classifier accuracy.The SVM-RFE feature selection method obtained the best accuracy results.A 6-input SVM classifier showed an accuracy of 87.1% and a Kappa statistic of 74.06%.75.0% of the unique selected features belong to the geometrical category.
2562476021;Reading the videos: temporal labeling for crowdsourced time-sync videos based on semantic embedding;2016.0;[];"Recent years have witnessed the boom of online sharing media contents, which raise significant challenges in effective management and retrieval. Though a large amount of efforts have been made, precise retrieval on video shots with certain topics has been largely ignored. At the same time, due to the popularity of novel time-sync comments, or so-called ""bullet-screen comments"", video semantics could be now combined with timestamps to support further research on temporal video labeling. In this paper, we propose a novel video understanding framework to assign temporal labels on highlighted video shots. To be specific, due to the informal expression of bullet-screen comments, we first propose a temporal deep structured semantic model (T-DSSM) to represent comments into semantic vectors by taking advantage of their temporal correlation. Then, video highlights are recognized and labeled via semantic vectors in a supervised way. Extensive experiments on a real-world dataset prove that our framework could effectively label video highlights with a significant margin compared with baselines, which clearly validates the potential of our framework on video understanding, as well as bullet-screen comments interpretation."
2562526353;On-demand video processing in wireless networks;2016.0;[];"The vast adoption of mobile devices with cameras has greatly assisted in the proliferation of the creation and distribution of videos. For a variety of purposes, valuable information may be extracted from these videos. While the computational capability of mobile devices has greatly improved recently, video processing is still a demanding task for mobile devices. Given a network consisting of mobile devices and video-clouds, mobile devices may be able to upload videos to video-clouds, which are more computationally capable for these processing tasks. However, due to networking constraints, when a video processing task is initiated through a query, most videos will not likely have been uploaded to the video-clouds, especially when the query is about a recent event. We investigate the problem of minimal query response time for processing videos stored across a network; however, this problem is a strongly NP-hard problem. To deal with this, we first propose a greedy algorithm with bounded performance. To further deal with the dynamics of the transmission rate between mobile devices and video-clouds, we propose an adaptive algorithm. To evaluate these algorithms, we built an on-demand video processing system. Based on the measurements of the system, we perform simulations to extensively evaluate the proposed algorithms. We also perform experiments on a small testbed to examine the realized system performance. Results show the performance of the greedy algorithm is close to the optimal and much better than other approaches, and the adaptive algorithm performs better with more dynamic transmission rates."
2565516711;A survey of deep neural network architectures and their applications;2017.0;[];This work was supported in part the Royal Society of the UK, the National Natural Science Foundation of China under Grants 61329301, 61374010, and 61403319, and the Alexander von Humboldt Foundation of Germany.
2565646189;Order picking with multiple pickers and due dates – Simultaneous solution of Order Batching, Batch Assignment and Sequencing, and Picker Routing Problems;2017.0;[];In manual picker-to-parts order picking systems of the kind considered in this article, human operators (order pickers) walk or ride through the warehouse, retrieving items from their storage location in order to satisfy a given demand specified by customer orders. Each customer order is characterized by a certain due date until which all requested items included in the order are to be retrieved and brought to the depot. For the actual picking process, customer orders may be grouped (batched) into more substantial picking orders (batches). The items of a picking order are then collected on a picker tour through the warehouse. Thus, the picking process of each customer order in the batch is only completed when the picker returns to the depot after the last item of the batch has been picked. Whether and to which extend due dates are violated (tardiness) depends on how the customer orders are batched, how the batches are assigned to order pickers, how the assigned batches are sequenced and how the pickers are routed through the warehouse. Existing literature has only treated special aspects of this problem (i.e. the batching problem or the routing problem) so far. In this paper, for the first time, an approach is proposed which considers all aspects simultaneously. A mathematical model of the problem is introduced that allows for solving small problem instances in reasonable computing times. For larger instances, a variable neighborhood descent (VND) algorithm is presented which includes various neighborhood structures regarding the batching and sequencing problem. Furthermore, two sophisticated routing algorithms are integrated into the VND algorithm. By means of numerical experiments, it is shown that this algorithm provides solutions of excellent quality.
2568007340;A multi-objective decision model for investment in energy savings and emission reductions in coal mining;2017.0;[];"Coal-mining companies should clearly be investing in energy savings and treatment technologies aimed at reducing both their energy consumption and their pollution levels in order to meet the requirements of government regulations relating to environmental protection; however, when considering the types of technical equipment to use and the timing of their investment, these companies also need to consider their profits and the costs of such investment. We therefore propose a ""multi-objective mixed integer non-linear programming"" (MMINLP) model of investment in energy savings and emission reductions designed to handle this type of decision-making problem. Given three objectives (maximum profits, minimal energy consumption and minimal pollution), we develop a hybrid mixed-coding ``particle swarm optimization and multi-objective non-dominated sorting genetic algorithm-II"" (PSO–NSGA-II) to optimize the continuous and discrete decision variables as a means of helping companies to reach the optimum decision. We also integrate the subtractive clustering-multi-criteria tournament decision-gain analysis method (SC–MTD–GAM) to select the best trade-off solutions on the optimal Pareto fronts. Finally, we carry out a case study of investment decisions on energy savings and emission reductions in the Zhenzhou Coal Industry (Group) Co., Ltd., China, with the results revealing that the proposed model can support decision making for energy savings and emission reductions in coal mining areas. As compared with the NSGA-II and ``non-dominated sorting particle swarm optimization"" (NSPSO) algorithms, the proposed PSO–NSGA-II is found to have better convergence, coverage and uniformity."
2569168369;A Hybrid Feature Selection With Ensemble Classification for Imbalanced Healthcare Data: A Case Study for Brain Tumor Diagnosis;2016.0;[];Electronic health records (EHRs) are providing increased access to healthcare data that can be made available for advanced data analysis. This can be used by the healthcare professionals to make a more informed decision providing improved quality of care. However, due to the inherent heterogeneous and imbalanced characteristics of medical data from EHRs, data analysis task faces a big challenge. In this paper, we address the challenges of imbalanced medical data about a brain tumor diagnosis problem. Morphometric analysis of histopathological images is rapidly emerging as a valuable diagnostic tool for neuropathology. Oligodendroglioma is one type of brain tumor that has a good response to treatment provided the tumor subtype is recognized accurately. The genetic variant, 1p-/19q-, has recently been found to have high chemosensitivity, and has morphological attributes that may lend it to automated image analysis and histological processing and diagnosis. This paper aims to achieve a fast, affordable, and objective diagnosis of this genetic variant of oligodendroglioma with a novel data mining approach combining a feature selection and ensemble-based classification. In this paper, 63 instances of brain tumor with oligodendroglioma are obtained due to prevalence and incidence of the tumor variant. In order to minimize the effect of an imbalanced healthcare data set, a global optimization-based hybrid wrapper-filter feature selection with ensemble classification is applied. The experiment results show that the proposed approach outperforms the standard techniques used in brain tumor classification problem to overcome the imbalanced characteristics of medical data.
2569711038;Prevalence of Internet Addiction and associated risk factors in Jordanian school students;2017.0;[];Internet Addiction has become a public health issue that cannot be neglected. In Jordan, there is a need to investigate this issue among school students. This descriptive correlational study aimed to assess the prevalence of Internet Addition and associated risk factors in Jordanian school students. The participants (N=716) aged 1218 years were selected randomly from ten public schools in Amman Governorate in Jordan. Socio-demographical data, patterns of Internet usage, the Youngu0027s Internet Addiction Tool (YIAT), the Symptom Checklist-anxiety scale (SCL-anxiety), and the Center for Epidemiological Studies Depression Scale for Children (CES-DC) were used. The findings showed that the prevalence of severe Internet Addiction was 6.3%. The highest prevalence of Internet Addiction was among students with family monthly income u003e1400$/, their fathers completed elementary education and mothers completed university and higher education, and their academic performance was poor. The friendu0027s home was the favorite place among Internet addicted students. Chatting was the highest reason for Internet Addiction. The students were experiencing anxiety and depression symptoms had the highest prevalence of Internet Addiction (10.3%, 8.2%, respectively). There was a statistically significant relationship between the age, school grade, family income, academic performance, average hours of Internet daily usage during school days and holidays, anxiety, depression, and Internet Addiction. These findings emphasize the importance of developing and implementing interventions such as preventive measures and early diagnosis of Internet Addiction among school students. Furthermore, counseling programs are recommended to increase the awareness of families regarding Internet Addiction and their responsibilities in providing guidance and support for their children. Internet Addiction (IA) has become a public health issue that cannot be neglected.The prevalence of Internet Addiction was 6.3%.Age, school grade, family income, and academic performance were associated risk factors of IA.Duration of Internet daily usage, anxiety, and depression were also risk factors of IA.Developing and implementing interventions regarding IA among school students are essential.
2570343428;YOLO9000: Better, Faster, Stronger;2017.0;[];We introduce YOLO9000, a state-of-the-art, real-time object detection system that can detect over 9000 object categories. First we propose various improvements to the YOLO detection method, both novel and drawn from prior work. The improved model, YOLOv2, is state-of-the-art on standard detection tasks like PASCAL VOC and COCO. Using a novel, multi-scale training method the same YOLOv2 model can run at varying sizes, offering an easy tradeoff between speed and accuracy. At 67 FPS, YOLOv2 gets 76.8 mAP on VOC 2007. At 40 FPS, YOLOv2 gets 78.6 mAP, outperforming state-of-the-art methods like Faster RCNN with ResNet and SSD while still running significantly faster. Finally we propose a method to jointly train on object detection and classification. Using this method we train YOLO9000 simultaneously on the COCO detection dataset and the ImageNet classification dataset. Our joint training allows YOLO9000 to predict detections for object classes that dont have labelled detection data. We validate our approach on the ImageNet detection task. YOLO9000 gets 19.7 mAP on the ImageNet detection validation set despite only having detection data for 44 of the 200 classes. On the 156 classes not in COCO, YOLO9000 gets 16.0 mAP. YOLO9000 predicts detections for more than 9000 different object categories, all in real-time.
2571625070;The impact of peer review on creative self-efficacy and learning performance in Web 2.0 learning activities;2016.0;[];"Introduction Creativity is considered as one of the core competences in contemporary education (U.S. Department of Education, 2010). Learning to be creative is thus infused into formal educational contexts to foster the creativity of students to address this requirement. It is considered important to develop effective practices and pedagogies for encouraging and enhancing studentsu0027 creativity in schools (Lassig, 2009). Recently, more and more Web 2.0 technologies, referred to as Internet-based applications, have been developed to empower users to interact and collaborate with each other as creators of user-generated content in an online community (Ou0027Reilly, 2005). Web 2.0 technologies, such as blogs, wikis and social networking platforms, have been increasingly applied to promote open and creative learning experiences in various educational settings. For instance, the studies by Aragon, Poon, Monroy-Hernandez and Aragon (2009) and Liu, Liu, Chen, Lin and Chen (2011b) have demonstrated the effects of such Web 2.0 platforms on augmenting creative activities, as they can afford a platform for students to create and share their creative works. Therefore, it is believed that the Web 2.0 technologies can enhance learner participation and creativity in educational settings (Greenhow, Robelia. u0026 Hughes, 2009; McLoughlin u0026 Lee, 2010; Ravenscroft, 2009). However, many studies have pointed out the significant contrast between the creative nature of Web 2.0 learning activities and the structured learning that takes place in schools (Bennett, Bishop, Dalgarno, Waycott, u0026 Kennedy, 2012; Mao, 2014). Formal education is restricted by the pre-defined curriculum in which students have to attain certain knowledge (Jimoyiannis, Tsiotakis, Roussinos, u0026 Siorenta, 2013). However, the acquisition of the knowledge and assessment required in formal education may restrict the creative process. On the one hand, although the open and creative features of Web 2.0 learning activities may support active learning, previous studies have found that studentsu0027 work in such activities may be perfunctory and may lack critical construction of knowledge (Tess, 2013). On the other hand, the assessment of studentsu0027 performance, which is often conducted within a certain framework, may also be inconsistent with and interfere with the open and creative features of Web 2.0 learning activities (Hemmi, Bayne, u0026 Land, 2009). Such assessment may also impact significantly studentsu0027 motivation to participate in Web 2.0 learning activities. It is widely believed that a critical pathway for developing studentsu0027 creativity is to model creative practices for students (Sternberg u0026 Williams, 1996). This study thus attempted to propose an approach to leveraging Web 2.0 learning activities and classroom teaching to help students develop both specific knowledge and creativity. This approach is mainly based on Csikzentmihalyiu0027s system model of creativity (1999) which asserts that ""creativity is a process that can be observed only at the intersection where individuals, domains, and field interact"" (Csikzentmihalyi, 1999, p. 314). From Csikzentmihalyiu0027s perspective, individuals create new elements while knowing and operating old elements in a domain. However, not all elements are accepted as new elements; rather, new elements are ""sanctioned by some group entitled to make decisions as to what should or should not be included in the domain"" (Csikzentmihalyi, 1999, p. 315). On the one hand, these groups, i.e. the field, determine the value and originality of the new elements. On the other hand, individuals receive critical feedback to improve the elements they have created. From the perspective of Csikzentmihalyiu0027s system model, the review process is a central component of the creative practice that students need to experience in order to understand the creative process. This study thus proposes integrating peer review activities with Web 2. …"
2571692900;Aligning users across social networks using network embedding;2016.0;[];"In this paper, we adopt the representation learning approach to align users across multiple social networks where the social structures of the users are exploited. In particular, we propose to learn a network embedding with the followership/ followee-ship of each user explicitly modeled as input/output context vector representations so as to preserve the proximity of users with ""similar"" followers/followees in the embedded space. For the alignment, we add both known and potential anchor users across the networks to facilitate the transfer of context information across networks. We solve both the network embedding problem and the user alignment problem simultaneously under a unified optimization framework. The stochastic gradient descent and negative sampling algorithms are used to address scalability issues. Extensive experiments on real social network datasets demonstrate the effectiveness and efficiency of the proposed approach compared with several state-of-the-art methods."
2572725329;Threshold Dynamics in Stochastic SIRS Epidemic Models with Nonlinear Incidence and Vaccination;2017.0;[];In this paper, the dynamical behaviors for a stochastic SIRS epidemic model with nonlinear incidence and vaccination are investigated. In the models, the disease transmission coefficient and the removal rates are all affected by noise. Some new basic properties of the models are found. Applying these properties, we establish a series of new threshold conditions on the stochastically exponential extinction, stochastic persistence, and permanence in the mean of the disease with probability one for the models. Furthermore, we obtain a sufficient condition on the existence of unique stationary distribution for the model. Finally, a series of numerical examples are introduced to illustrate our main theoretical results and some conjectures are further proposed.
2572926828;Predict anchor links across social networks via an embedding approach;2016.0;[];Predicting anchor links across social networks has important implications to an array of applications, including cross-network information diffusion and cross-domain recommendation. One challenging problem is: whether and to what extent we can address the anchor link prediction problem, if only structural information of networks is available. Most existing methods, unsupervised or supervised, directly work on networks themselves rather than on their intrinsic structural regularities, and thus their effectiveness is sensitive to the high dimension and sparsity of networks. To offer a robust method, we propose a novel supervised model, called PALE, which employs network embedding with awareness of observed anchor links as supervised information to capture the major and specific structural regularities and further learns a stable cross-network mapping for predicting anchor links. Through extensive experiments on two realistic datasets, we demonstrate that PALE significantly outperforms the state-of-the-art methods.
2573428132;A Green Trust Management Scheme to Mitigate Trust-Distortion Attacks on MANETs;2016.0;[];Due to the lack of a centralized management, mobile ad hoc networks (MANETs) are prone to various types of attacks. Trust management systems has been proposed to allow such networks to identify misbehaving nodes, enforce their collaboration in network functions. However, the trust management can be challenged by unique characteristics of MANETs in terms of absence of centralized management, severe resource constraints, important network dynamics. Mainly, trust-distortion attacks may attempt to deceive nodes u0027estimation on other nodesu0027 trustworthiness by generating dishonest recommendations or fulfilling double-face conducts. Besides, the energy devoted to establish the trust system may be important, reducing the network lifetime, producing environmental pollution. In this paper, we propose a green trust management scheme, called GTMS, capable to handle different trust-distortion attacks in a multi-attack environment, with minimum energy consumption. To achieve this, GTMS reinforces the knowledge of nodes on their neighborhood to inhibit trust-distortion attacks. Moreover, it self-adapts the frequency of its local, remote knowledge monitoring according to the network context to minimize energy consumption. GTMS is characterized by its energy efficiency, high quality of knowledge, resistance to trust-distortion attacks, protocol independence, self-adaptation, low computational intensiveness. Simulation results prove that GTMS significantly outperforms the existing alternatives in the literature in presence of simultaneous, contradictory different trust-distortion attacks.
2576787043;A knowledge-based system for breast cancer classification using fuzzy logic method;2017.0;[];A knowledge-based system is proposed for breast cancer disease classification.The knowledge-based system uses EM, PCA, CART and fuzzy rule-based methods.WDBC and Mammographic mass datasets are used for the method evaluation.The accuracies obtained by the method are respectively 0.932 and 0.941 for WDBC and Mammographic mass datasets. Breast cancer has become a common disease around the world. Expert systems are valuable tools that have been successful for the disease diagnosis. In this research, we accordingly develop a new knowledge-based system for classification of breast cancer disease using clustering, noise removal, and classification techniques. Expectation Maximization (EM) is used as a clustering method to cluster the data in similar groups. We then use Classification and Regression Trees (CART) to generate the fuzzy rules to be used for the classification of breast cancer disease in the knowledge-based system of fuzzy rule-based reasoning method. To overcome the multi-collinearity issue, we incorporate Principal Component Analysis (PCA) in the proposed knowledge-based system. Experimental results on Wisconsin Diagnostic Breast Cancer and Mammographic mass datasets show that proposed methods remarkably improves the prediction accuracy of breast cancer. The proposed knowledge-based system can be used as a clinical decision support system to assist medical practitioners in the healthcare practice.
2577537809;Accurate Object Localization in Remote Sensing Images Based on Convolutional Neural Networks;2017.0;[];In this paper, we focus on tackling the problem of automatic accurate localization of detected objects in high-resolution remote sensing images. The two major problems for object localization in remote sensing images caused by the complex context information such images contain are achieving generalizability of the features used to describe objects and achieving accurate object locations. To address these challenges, we propose a new object localization framework, which can be divided into three processes: region proposal, classification, and accurate object localization process. First, a region proposal method is used to generate candidate regions with the aim of detecting all objects of interest within these images. Then, generic image features from a local image corresponding to each region proposal are extracted by a combination model of 2-D reduction convolutional neural networks (CNNs). Finally, to improve the location accuracy, we propose an unsupervised score-based bounding box regression (USB-BBR) algorithm, combined with a nonmaximum suppression algorithm to optimize the bounding boxes of regions that detected as objects. Experiments show that the dimension-reduction model performs better than the retrained and fine-tuned models and the detection precision of the combined CNN model is much higher than that of any single model. Also our proposed USB-BBR algorithm can more accurately locate objects within an image. Compared with traditional features extraction methods, such as elliptic Fourier transform-based histogram of oriented gradients and local binary pattern histogram Fourier, our proposed localization framework shows robustness when dealing with different complex backgrounds.
2579007238;SPARQLES: Monitoring public SPARQL endpoints;2017.0;[];"Fujitsu Laboratories Limited   :[4],""Project   :[4],""Project   :[4],""Project   :[13],""Nucleus Center for Semantic Web Research "
2579937741;Opportunistic Sensor Data Collection with Bluetooth Low Energy;2017.0;[];Bluetooth Low Energy (BLE) has gained very high momentum, as witnessed by its widespread presence in smartphones, wearables and other consumer electronics devices. This fact can be leveraged to carry out opportunistic sensor data collection (OSDC) in scenarios where a sensor node cannot communicate with infrastructure nodes. In such cases, a mobile entity (e.g., a pedestrian or a vehicle) equipped with a BLE-enabled device can collect the data obtained by the sensor node when both are within direct communication range. In this paper, we characterize, both analytically and experimentally, the performance and trade-offs of BLE as a technology for OSDC, for the two main identified approaches, and considering the impact of its most crucial configuration parameters. Results show that a BLE sensor node running on a coin cell battery can achieve a lifetime beyond one year while transferring around 10 Mbit/day, in realistic OSDC scenarios.
2581656659;Mining Coastal Land Use Sequential Pattern and Its Land Use Associations Based on Association Rule Mining;2017.0;[];Abstract: Research on the land use of the coastal zone in the sea–land direction will not only reveal its land use distribution, but may also indicate the interactions between inland land use and the ocean through associations between inland land use and seaward land use indirectly. However, in the existing research, few have paid attention to the land use in sea–land direction, let alone the sequential relationship between land-use types. The sequential relationship would be useful in land use planning and rehabilitation of the landscape in the sea–land direction, and the association between land-use types, particularly the inland land use and seaward land use, is not discussed. Therefore, This study presents a model named ARCLUSSM (Association Rules-based Coastal Land use Spatial Sequence Model) to mine the sequential pattern of land use with interesting associations in the sea–land direction of the coastal zone. As a case study, the typical coastal zone of Bohai Bay and the Yellow River delta in China was used. The results are as follows: firstly, 27 interesting association patterns of land use in the sea–land direction of the coastal zone were mined easily. Both sequential relationship and distance between land-use types for 27 patterns among six land-use types were mined definitely, and the sequence of the six land-use types tended to be tidal flat u003e shrimp pond u003e reservoir/artificial pond u003e settlement u003e river u003e dry land in sea–land direction. These patterns would offer specific support for land-use planning and rehabilitation of the coastal zone. There were 19 association patterns between seaward and landward land-use types. These patterns showed strong associations between seaward and landward land-use types. It indicated that the landward land use might have some impacts on the seaward land use, or in the other direction, which may help to reveal the interactions between inland land use and the ocean. Thus, the ARCLUSSM was an efficient tool to mine the sequential relationship and distance between land-use types with interesting association rules in the sea–land direction, which would offer practicable advice to appropriate coastal zone management and planning, and might reveal the interactions between inland land use and the ocean.
2583803680;Label Informed Attributed Network Embedding;2017.0;[];Attributed network embedding aims to seek low-dimensional vector representations for nodes in a network, such that original network topological structure and node attribute proximity can be preserved in the vectors. These learned representations have been demonstrated to be helpful in many learning tasks such as network clustering and link prediction. While existing algorithms follow an unsupervised manner, nodes in many real-world attributed networks are often associated with abundant label information, which is potentially valuable in seeking more effective joint vector representations. In this paper, we investigate how labels can be modeled and incorporated to improve attributed network embedding. This is a challenging task since label information could be noisy and incomplete. In addition, labels are completely distinct with the geometrical structure and node attributes. The bewildering combination of heterogeneous information makes the joint vector representation learning more difficult. To address these issues, we propose a novel Label informed Attributed Network Embedding (LANE) framework. It can smoothly incorporate label information into the attributed network embedding while preserving their correlations. Experiments on real-world datasets demonstrate that the proposed framework achieves significantly better performance compared with the state-of-the-art embedding algorithms.
2583980768;Prioritizing Security Spending: A Quantitative Analysis of Risk Distributions for Different Business Profiles.;2015.0;[];This paper aims to understand if, and to what extent, business details about an organization can help provide guidelines for better resource allocation across different preventive measures, in order to effectively protect, detect, and recover from, different forms of security incidents. Existing work on analyzing the distribution of risk across different incident categories, most notably Verizon’s latest Data Breach Investigations Report, provide recommendations based solely on business sector information. In this paper, we leverage a broader set of publicly available business details to provide a more fine-grained analysis. Specifically, we use incident reports collected in the VERIS Community Database (VCDB), as well as data from Alexa Web Information Service (AWIS), to train and test a sequence of classifiers/predictors. We show that compared to using business sector information alone, our method can achieve the same accuracy by allowing organizations to focus on a sparser set of incident types, thus achieving the same level of protection by spending less resources on security through more judicious prioritization.
2585560244;Accelerating Binarized Convolutional Neural Networks with Software-Programmable FPGAs;2017.0;[];Convolutional neural networks (CNN) are the current stateof-the-art for many computer vision tasks. CNNs outperform older methods in accuracy, but require vast amounts of computation and memory. As a result, existing CNN applications are typically run on clusters of CPUs or GPUs. Studies into the FPGA acceleration of CNN workloads has achieved reductions in power and energy consumption. However, large GPUs outperform modern FPGAs in throughput, and the existence of compatible deep learning frameworks give GPUs a significant advantage in programmability. Recent research in machine learning demonstrates the potential of very low precision CNNs -- i.e., CNNs with binarized weights and activations. Such binarized neural networks (BNNs) appear well suited for FPGA implementation, as their dominant computations are bitwise logic operations and their memory requirements are reduced. A combination of low-precision networks and high-level design methodology may help address the performance and productivity gap between FPGAs and GPUs. In this paper, we present the design of a BNN accelerator that is synthesized from C++ to FPGA-targeted Verilog. The accelerator outperforms existing FPGA-based CNN accelerators in GOPS as well as energy and resource efficiency.
2585770658;RHSBoost: Improving classification performance in imbalance data;2017.0;[];Imbalance data are defined as a dataset whose proportion of classes is severely skewed. Classification performance of existing models tends to deteriorate due to class distribution imbalance. In addition, over-representation by majority classes prevents a classifier from paying attention to minority classes, which are generally more interesting. An effective ensemble classification method called RHSBoost has been proposed to address the imbalance classification problem. This classification rule uses random undersampling and ROSE sampling under a boosting scheme. According to the experimental results, RHSBoost appears to be an attractive classification model for imbalance data.
2585831915;How can a sparse representation be made applicable for very low-dimensional data?;2017.0;[];"We analyze the problem of sparse representation on low-dimensional data.We extend applicable scope of sparse representations via a novel perspective.An effective method to double the dimensionality is proposed for classification. The sparse representation has achieved notable performance in the field of pattern classification, and has been adopted in many expert and intelligent applications such as access control and surveillance. However, sparse representation does not work as well for low-dimensional data as it does for high-dimensional data. For data of very low dimensionality, sparse representation methods usually have severe drawbacks; consequently, wider applications of sparse representations are seriously restricted. In this paper, we focus on this challenging problem and propose a very effective method for using sparse representations with low-dimensional data. Compared with the conventional sparse representation method, the proposed method achieves considerable improvement of classification accuracy by increasing the dimensionality of the data. Moreover, the proposed method is mathematically tractable and quite computationally efficient."
2586478530;OppNet: Enabling citizen-centric urban IoT data collection through opportunistic connectivity service;2016.0;[];Urban IoT data collection is challenging due to the limitations of the fixed sensing infrastructures. Instead of transmitting data directly through expensive cellular networks, citizen-centric data collection scheme through opportunistic network takes advantage of human mobility as well as cheap WiFi and D2D communication. In this paper, we present OppNet, which implements a context aware data forwarding algorithm and fills the gap between theoretical modelling of opportunistic networking and real deployment of citizen-centric data collection system. According to the results from a 3-day real-life experiment, OppNet shows consistent performance in terms of number of hops and time delay. Moreover, the underlying social structure can be clearly identified by analysing social contact data collected through OppNet.
2586586754;Why do faculty members use or not use social networking sites for education;2017.0;[];Social networking sites (SNS) are a popular Internet-based technology that enable users to communicate and interact with each other. SNS have attracted a great deal of scholarly attention and have been used as an educational tool in recent years. However, there has been little research examining motivations and obstacles to teaching with SNS in higher education. Therefore, the purpose of this study is to investigate why faculty members in higher education prefer to use or not to use SNS for educational purposes. To this end, faculty membersu0027 current SNS usage, their purposes for using SNS, and the opinions of faculty members about their uses of SNS for education were investigated. A mixed method approach was utilized to collect data from a total of 658 faculty members from eight different state universities in Turkey. The findings show that nearly half of the faculty members who possessed an SNS account reported that they use SNS for educational purposes. Their greatest motivating factor to use SNS is that SNS provide a means for fast and effective communication. The main factor which impeded SNS usage by faculty members is their concerns about privacy. Faculty membersu0027 motivations regarding SNS use in their courses investigated.Insights from faculty members on their hesitation to use SNSs explored.A survey design was used to collect data from a total of 658 faculty members.Full set of motivating and impeding factors are discussed and explained.
2586890322;Wearable brain imager/BMI technology for structural, vascular and functional extraction;2016.0;[];Portable, low power, low-cost, flexible wearable Brain Machine Interfaces (BMIs) for real time brain imaging, with far higher potential spatial resolution than EEG, are being designed and prototyped using new chip architectures. Preliminary depth-indexed readings of real-time vascular and probable neural activity in the brain have been demonstrated using arrays of ultra-high dynamic range (160dB DANL) 1cm2 Ultra Wide Band (UWB) pulser/antenna/sampler assemblies. These ICs generate 5–300ps pulses and detect the resulting reflections from brain tissue boundaries. They cancel out 1/ƒ noise, and are sensitive enough to detect energy reflected from deep tissues, as well as the signal transmitted through even the longest dimensions of the head, and display vascular pulsation and, for the first time, rudimentary indications of functional brain activity.
2587008894;Evolving boxes for fast vehicle detection;2017.0;[];"We perform fast vehicle detection from traffic surveillance cameras. A novel deep learning framework, namely Evolving Boxes, is developed that proposes and refines the object boxes under different feature representations. Specifically, our framework is embedded with a light-weight proposal network to generate initial anchor boxes as well as to early discard unlikely regions; a fine-turning network produces detailed features for these candidate boxes. We show intriguingly that by applying different feature fusion techniques, the initial boxes can be refined for both localization and recognition. We evaluate our network on the recent DETRAC benchmark and obtain a significant improvement over the state-of-the-art Faster RCNN by 9.5% mAP. Further, our network achieves 9–13 FPS detection speed on a moderate commercial GPU."
2589233777;Pythagorean Fuzzy Information Measures and Their Applications;2017.0;[];
2589747988;Surface EMG-Based Inter-Session Gesture Recognition Enhanced by Deep Domain Adaptation;2017.0;[];High-density surface electromyography (HD-sEMG) is to record muscles’ electrical activity from a restricted area of the skin by using two dimensional arrays of closely spaced electrodes. This technique allows the analysis and modelling of sEMG signals in both the temporal and spatial domains, leading to new possibilities for studying next-generation muscle-computer interfaces (MCIs). sEMG-based gesture recognition has usually been investigated in an intra-session scenario, and the absence of a standard benchmark database limits the use of HD-sEMG in real-world MCI. To address these problems, we present a benchmark database of HD-sEMG recordings of hand gestures performed by 23 participants, based on an 8 × 16 electrode array, and propose a deep-learning-based domain adaptation framework to enhance sEMG-based inter-session gesture recognition. Experiments on NinaPro, CSL-HDEMG and our CapgMyo dataset validate that our approach outperforms state-of-the-arts methods on intra-session and effectively improved inter-session gesture recognition.
2591940486;Facial Expression Recognition Utilizing Local Direction-Based Robust Features and Deep Belief Network;2017.0;[];Emotional health plays very vital role to improve people’s quality of lives, especially for the elderly. Negative emotional states can lead to social or mental health problems. To cope with emotional health problems caused by negative emotions in daily life, we propose efficient facial expression recognition system to contribute in emotional healthcare system. Thus, facial expressions play a key role in our daily communications, and recent years have witnessed a great amount of research works for reliable facial expressions recognition (FER) systems. Therefore, facial expression evaluation or analysis from video information is very challenging and its accuracy depends on the extraction of robust features. In this paper, a unique feature extraction method is presented to extract distinguished features from the human face. For person independent expression recognition, depth video data is used as input to the system where in each frame, pixel intensities are distributed based on the distances to the camera. A novel robust feature extraction process is applied in this work which is named as local directional position pattern (LDPP). In LDPP, after extracting local directional strengths for each pixel such as applied in typical local directional pattern (LDP), top directional strength positions are considered in binary along with their strength sign bits. Considering top directional strength positions with strength signs in LDPP can differentiate edge pixels with bright as well as dark regions on their opposite sides by generating different patterns whereas typical LDP only considers directions representing the top strengths irrespective of their signs as well as position orders (i.e., directions with top strengths represent 1 and rest of them 0), which can generate the same patterns in this regard sometimes. Hence, LDP fails to distinguish edge pixels with opposite bright and dark regions in some cases which can be overcome by LDPP. Moreover, the LDPP capabilities are extended through principal component analysis (PCA) and generalized discriminant analysis (GDA) for better face characteristic illustration in expression. The proposed features are finally applied with deep belief network (DBN) for expression training and recognition.
2592843051;Hermes: Latency Optimal Task Assignment for Resource-constrained Mobile Computing;2017.0;[];With mobile devices increasingly able to connect to cloud servers from anywhere, resource-constrained devices can potentially perform offloading of computational tasks to either save local resource usage or improve performance. It is of interest to find optimal assignments of tasks to local and remote devices that can take into account the application-specific profile, availability of computational resources, and link connectivity, and find a balance between energy consumption costs of mobile devices and latency for delay-sensitive applications. We formulate an NP-hard problem to minimize the application latency while meeting prescribed resource utilization constraints. Different from most of existing works that either rely on the integer programming solver, or on heuristics that offer no theoretical performance guarantees, we propose Hermes, a novel fully polynomial time approximation scheme (FPTAS). We identify for a subset of problem instances, where the application task graphs can be described as serial trees, Hermes provides a solution with latency no more than   $(1+\epsilon)$       times of the minimum while incurring complexity that is polynomial in problem size and    $\frac{1}{\epsilon}$      . We further propose an online algorithm to learn the unknown dynamic environment and guarantee that the performance gap compared to the optimal strategy is bounded by a logarithmic function with time. Evaluation is done by using real data set collected from several benchmarks, and is shown that Hermes improves the latency by   $16$       percent compared to a previously published heuristic and increases CPU computing time by only    $0.4$       percent of overall latency.
2592965444;On the correspondence between reciprocal relations and strongly complete fuzzy relations;2017.0;[];   Fuzzy relations and reciprocal relations are two popular tools for representing degrees of preference. It is important to note that they carry a different semantics and cannot be equated directly. We propose a simple transformation based on implication operators that allows to establish a one-to-one correspondence between both formalisms. It sets the basis for a common framework in which properties such as transitivity can be studied and definitions belonging to different formalisms can be compared. As a byproduct, we propose a new family of upper bound functions for cycle-transitivity. Finally, we unveil some interesting equivalences between types of transitivity that were left uncompared till now.
2593530789;Approaches to Pythagorean Fuzzy Stochastic Multi-criteria Decision Making Based on Prospect Theory and Regret Theory with New Distance Measure and Score Function;2017.0;[];In this paper, we initiate a new axiomatic definition of Pythagorean fuzzy distance measure, which is expressed by Pythagorean fuzzy number that will reduce the information loss and remain more original information. Then, the objective weights of various criteria are determined via grey system theory. Combining objective weights with subjective weights, we present the combined weights, which can reflect both the subjective considerations of the decision maker and the objective information. Meanwhile, a novel score function is proposed. Later, we present two algorithms to solve stochastic multicriteria decision making problem, which takes prospect preference and regret aversion of decision makers into consideration in the decision process. Finally, the effectiveness and feasibility of approach is demonstrated by a numerical example.
2593688479;23.1 An 8Gb 12Gb/s/pin GDDR5X DRAM for cost-effective high-performance applications;2017.0;[];Over the last years, GDDR5 has emerged as the dominant standard for applications requiring high system bandwidth like graphic cards and game consoles. However, GDDR5 data rates are saturating due to limitations in the clock frequency and column-access cycle time (t CCD ). To reach the data rate of 9Gb/s/pin [1], a GDDR5 DRAM has to be clocked at 2.25GHz and operate at a t CCD  of 888ps. This combination makes the design of control logic, data path and memory core difficult in a typical DRAM process. Still, the industry is demanding higher system bandwidth to enable continuous improvements in the visual computing arena. For this purpose, an 8Gb GDDR5X DRAM has been developed reaching a data rate of 12Gb/s/pin, which surpasses the fastest published GDDR5 [1] by 33%. This paper introduces GDDR5X and discusses relevant circuit techniques in clock generation, receiver and transmitter design to enable the higher data rates on a conventional DRAM process.
2594144796;Umaka-Yummy Data: A Place to Facilitate Communication between Data Providers and Consumers.;2016.0;[];
2594575977;A New Decomposition-Based NSGA-II for Many-Objective Optimization;2018.0;[];Multiobjective evolutionary algorithms (MOEAs) have proven their effectiveness and efficiency in solving problems with two or three objectives. However, recent studies show that MOEAs face many difficulties when tackling problems involving a larger number of objectives as their behavior becomes similar to a random walk in the search space since most individuals are nondominated with respect to each other. Motivated by the interesting results of decomposition-based approaches and preference-based ones, we propose in this paper a new decomposition-based dominance relation to deal with many-objective optimization problems and a new diversity factor based on the penalty-based boundary intersection method. Our reference point-based dominance (RP-dominance), has the ability to create a strict partial order on the set of nondominated solutions using a set of well-distributed reference points. The RP-dominance is subsequently used to substitute the Pareto dominance in nondominated sorting genetic algorithm-II (NSGA-II). The augmented MOEA, labeled as RP-dominance-based NSGA-II, has been statistically demonstrated to provide competitive and oftentimes better results when compared against four recently proposed decomposition-based MOEAs on commonly-used benchmark problems involving up to 20 objectives. In addition, the efficacy of the algorithm on a realistic water management problem is showcased.
2598689838;User Identity Linkage across Online Social Networks: A Review;2017.0;[];The increasing popularity and diversity of social media sites has encouraged more and more people to participate on multiple online social networks to enjoy their services. Each user may create a user identity, which can includes profile, content, or network information, to represent his or her unique public figure in every social network. Thus, a fundamental question arises -- can we link user identities across online social networks? User identity linkage across online social networks is an emerging task in social media and has attracted increasing attention in recent years. Advancements in user identity linkage could potentially impact various domains such as recommendation and link prediction. Due to the unique characteristics of social network data, this problem faces tremendous challenges. To tackle these challenges, recent approaches generally consist of (1) extracting features and (2) constructing predictive models from a variety of perspectives. In this paper, we review key achievements of user identity linkage across online social networks including stateof- the-art algorithms, evaluation metrics, and representative datasets. We also discuss related research areas, open problems, and future research directions for user identity linkage across online social networks.
2598771954;The Concept of a Linguistic Variable and its Application to Approximate Reasoning-I;1974.0;[];One of the fundamental tenets of modern science is that a phenomenon cannot be claimed to be well understood until it can be characterized in quantitative terms.l Viewed in this perspective, much of what constitutes the core of scientific knowledge may be regarded as a reservoir of concepts and techniques which can be drawn upon to construct mathematical models of various types of systems and thereby yield quantitative information concerning their behavior.
2604659713;Strengthening Adaptation in Cyber-Physical Systems via Meta-Adaptation Strategies;2017.0;[];The dynamic nature of complex Cyber-Physical Systems puts extra requirements on their functionalities: they not only need to be dependable, but also able to adapt to changing situations in their environment. When developing such systems, however, it is often impossible to explicitly design for all potential situations up front and provide corresponding strategies. Situations that come out of this “envelope of adaptability” can lead to problems that end up by applying an emergency fail-safe strategy to avoid complete system failure. The existing approaches to self-adaptation cannot typically cope with such situations better—while they are adaptive (and can apply learning) in choosing a strategy, they still rely on a pre-defined set of strategies not flexible enough to deal with those situations adequately. To alleviate this problem, we propose the concept of meta-adaptation strategies, which extends the limits of adaptability of a system by constructing new strategies at runtime to reflect the changes in the environment. Though the approach is generally applicable to most approaches to self-adaptation, we demonstrate our approach on IRM-SA—a design method and associated runtime model for self-adaptive distributed systems based on component ensembles. We exemplify the meta-adaptation strategies concept by providing three concrete meta-adaptation strategies and show its feasibility on an emergency coordination case study.
2604677623;Using Feature Clustering for GP-Based Feature Construction on High-Dimensional Data;2017.0;[];"Feature construction is a pre-processing technique to create new features with better discriminating ability from the original features. Genetic programming (GP) has been shown to be a prominent technique for this task. However, applying GP to high-dimensional data is still challenging due to the large search space. Feature clustering groups similar features into clusters, which can be used for dimensionality reduction by choosing representative features from each cluster to form the feature subset. Feature clustering has been shown promising in feature selection; but has not been investigated in feature construction for classification. This paper presents the first work of utilising feature clustering in this area. We propose a cluster-based GP feature construction method called CGPFC which uses feature clustering to improve the performance of GP for feature construction on high-dimensional data. Results on eight high-dimensional datasets with varying difficulties show that the CGPFC constructed features perform better than the original full feature set and features constructed by the standard GP constructor based on the whole feature set."
2604795503;Matching Node Embeddings for Graph Similarity.;2017.0;[];
2604920622;Projection Model for Fusing the Information of Pythagorean Fuzzy Multicriteria Group Decision Making Based on Geometric Bonferroni Mean.;2017.0;[];
2605350416;Neural Collaborative Filtering;2017.0;[];"In recent years, deep neural networks have yielded immense success on speech recognition, computer vision and natural language processing. However, the exploration of deep neural networks on recommender systems has received relatively less scrutiny. In this work, we strive to develop techniques based on neural networks to tackle the key problem in recommendation --- collaborative filtering --- on the basis of implicit  :[62],""some recent work has employed deep learning for recommendation, they primarily used it to model auxiliary information, such as textual descriptions of items and acoustic features of musics. When it comes to model the key factor in collaborative filtering --- the interaction between user and item features, they still resorted to matrix factorization and applied an inner product on the latent features of users and  :[128],""replacing the inner product with a neural architecture that can learn an arbitrary function from data, we present a general framework named NCF, short for Neural network-based Collaborative Filtering. NCF is generic and can express and generalize matrix factorization under its framework. To supercharge NCF modelling with non-linearities, we propose to leverage a multi-layer perceptron to learn the user-item interaction function. Extensive experiments on two real-world datasets show significant improvements of our proposed NCF framework over the state-of-the-art methods. Empirical evidence shows that using deeper layers of neural networks offers better recommendation performance."
2605997098;Multi-Label Learning with Global and Local Label Correlation;2018.0;[];"It is well-known that exploiting label correlations is important to multi-label learning. Existing approaches either assume that the label correlations are global and shared by all instances; or that the label correlations are local and shared only by a data subset. In fact, in the real-world applications, both cases may occur that some label correlations are globally applicable and some are shared only in a local group of instances. Moreover, it is also a usual case that only partial labels are observed, which makes the exploitation of the label correlations much more difficult. That is, it is hard to estimate the label correlations when many labels are absent. In this paper, we propose a new multi-label approach  GLOCAL  dealing with both the full-label and the missing-label cases, exploiting global and local label correlations simultaneously, through learning a latent label representation and optimizing label manifolds. The extensive experimental studies validate the effectiveness of our approach on both full-label and missing-label data."
2606013575;Examining the students behavioral intention to use e-learning in Azerbaijan? The General Extended Technology Acceptance Model for E-learning approach;2017.0;[];Azerbaijan has successfully incorporated modern Information Communication Technologies (ICT) in the education system. The major goal is to raise the standard of education. The factors that affect university studentsu0027 behavioral intention (BI) to use e-learning for educational purposes in Azerbaijan are worthy of study. This is an empirical study of the use of the General Extended Technology Acceptance Model for E-learning (GETAMEL) developed by Abdullah and Ward (2016) in order to determine the factors that affect undergraduate studentsu0027 BI to use an e-learning system. The data was collected from 714 undergraduate and masters students using a convenient sampling technique and the responses were analyzed using Structural Equation Modeling (SEM). It is seen that the Subjective norm (SN), Experience (EXP) and Enjoyment (ENJOY) positively and significantly influence studentsu0027 perceived usefulness (PU) of e-learning, while Computer anxiety (CA) has a negatively effect. EXP, ENJOY and Self-efficacy (SE) positively and significantly affect their perceived ease of use (PEOU) of e-learning. It is also seen that SN has a positive and significant impact on BI to use e-learning, while Technological innovation (TI) significantly moderates the relationship between SN and PU, PU and BI to use e-learning. This study is the first to determine a negative and significant relationship between CA and PU, in the context of students e-learning. This study is also one of the very few that uses the GETAMEL model for e-learning settings. The results have significant practical implications for educational institutions and decision-makers, in terms of the design of the e-learning system in universities. Display Omitted The GETAMEL model successfully applies to e-learning acceptance domain.Computer anxiety negatively affects perceived usefulness and perceived ease of use.Subjective norm does not have an impact on perceived ease of use of e-learning system.Self-efficacy does not affect perceived usefulness of e-learning system.perceived ease of use does not affect perceived usefulness of e-learning system.
2606295484;A reliable IoT system for Personal Healthcare Devices;2018.0;[];Healthcare applications in IoT systems have been receiving increasing attention because they help facilitate remote monitoring of patients. In this paper, we propose a reliable oneM2M-based IoT system for Personal Healthcare Devices. In order to use a Personal Healthcare Device as an Application Dedicated Node in the proposed system, a protocol conversion between ISO/IEEE 11073 protocol messages and oneM2M protocol messages is performed in gateways located between Personal Healthcare Devices and the PHD management server. The proposed oneM2M-based IoT system for Personal Healthcare Device is constructed, and evaluated in various experiments. The experiments show that the protocol conversion performs effectively, and that the conversion process does not cause the system to suffer serious performance degradation, even when the number of Application Dedicated Node is quite large.Some Personal Healthcare Device data is too precious to lose due to system failures under u-healthcare environments. However, until now, few studies have focused on fault-tolerant health data services. Therefore, we also propose a fault-tolerant algorithm for the reliable IoT system in which gateways on the same layer in the system are linked to form a daisy chain for fault tolerance at the level, and a gateway stores the backup copy of the previous gateway positioned immediately ahead of the gateway in the daisy chain. The upper-layered gateway stores the parity data of the daisy chain as well. In this manner, as many as two gateway faults occurred at the same time can be recovered. For experiments, the resource trees of the oneM2M-based IoT system were expanded to store information on daisy chains, backup copies, and parity. Our experiments reveal that the proposed algorithm can recover from faults on gateways in the oneM2M-based IoT system.
2606808050;Segmentation of ischemic stroke area from CT brain images;2016.0;[];"Due to its wide availability, speed, and price, Computer Tomography (CT) without contrast injection has been the first imaging modality for the evaluation of acute ischemic stroke. However, the early signs of brain ischemia are subtle and not visible on CT scan. For this regard, we propose, in this paper, an approach for ischemic detection based on contrast enhancement of CT brain images. Our method has three stages: preprocessing, image enhancement and classification. First, skull bones and noise are removed; then a multi-scale contrast enhancement algorithm based on Laplacian Pyramid (LP) is developed to enhance the contrast of the brain CT images for identification of hypo-attenuation of acute stroke. Finally, “fuzzy c means” classification is applied to extract the ischemic area from normal tissues. The execution of our method gives impressive results."
2606906085;Big Data Analytics for System Stability Evaluation Strategy in the Energy Internet;2017.0;[];With the significant improvements in the Energy Internet, we have witnessed the explosion of multisource energy big data, whose characteristics of vast volume, fast velocity, and diverse variety not only formulate an essential infrastructure of the Energy Internet, but also bring threats to the systemu0027s stability. In this paper, we concern with the system-level stability issues in the Energy Internet and study how to maintain a stable and healthy energy network environment. To this end, we propose a system-level stability evaluation model in the Energy Internet based on a critical energy function to explore small disturbance stability region (SDSR), where SDSR can be acquired via estimating the operational data threshold of distributed generations. The threshold is estimated based on energy consumption rather than equilibrium nodes, which applies the energy function theory and reduces the computation complexity. Moreover, in our proposed model, we add the big data approximate analytics algorithm into hyperplane fitting to optimize and analyze the SDSR. Simulation results on SDSR in a single dominant oscillation mode and multiple dominant oscillation mode have demonstrated the advantages and superiority of our proposed method over the prior schemes.
2607002674;Cloud computing review: concepts, technology, challenges and security;2017.0;[];Cloud computing delivers IT-related capabilities as a service through internet to multiple customers and these services are charged based on consumption. Many cloud computing providers such as Google, Microsoft, Yahoo, IBM and Amazon are moving towards adoption of cloud technology leading to a considerable escalation in the usage of various cloud services. Amazon is the pioneer in this field because of its more number of architectural features compared to others. To meet the needs of cloud service providers and customers various open source tools and commercial tools are being developed. Though many more developments have been taken place in the cloud computing area, many challenges such as security, interoperability, resource scheduling, virtualisation etc. are yet to be fine tuned. This paper reviews cloud computing paradigm in terms of its historical evolution, concepts, technology, tools and various challenges. Systematic literature review (SLR) of 77 selected papers, published from 2000 to 2015 is done to properly understand the nuances of the cloud computing paradigm. Since security is the major challenge in cloud computing, it is discussed separately in detail. This review paper helps researchers who would like to begin their research career in cloud computing area.
2607172905;VideoMec: a metadata-enhanced crowdsourcing system for mobile videos;2017.0;[];The exponential growth of mobile videos has enabled a variety of video crowdsourcing applications. However, existing crowdsourcing approaches require all video files to be uploaded, wasting a large amount of bandwidth since not all crowdsourced videos are useful. Moreover, it is difficult for applications to find desired videos based on user-generated annotations, which can be inaccurate or miss important information. To address these issues, we present VideoMec, a video crowdsourcing system that automatically generates video descriptions based on various geographical and geometrical information, called metadata, from multiple embedded sensors in off-the-shelf mobile devices. With VideoMec, only a small amount of metadata needs to be uploaded to the server, hence reducing the bandwidth and energy consumption of mobile devices. Based on the uploaded metadata, VideoMec supports comprehensive queries for applications to find and fetch desired videos. For time-sensitive applications, it may not be possible to upload all desired videos in time due to limited wireless bandwidth and large video files. Thus, we formalize two optimization problems and propose efficient algorithms to select the most important videos to upload under bandwidth and time constraints. We have implemented a prototype of VideoMec, evaluated its performance, and demonstrated its effectiveness based on real experiments.
2608689243;Wireless Big Data Computing in Smart Grid;2017.0;[];The development of smart grid brings great improvement in the efficiency, reliability, and economics to power grid. However, at the same time, the volume and complexity of data in the grid explode. To address this challenge, big data technology is a strong candidate for the analysis and processing of smart grid data. In this article, we propose a big data computing architecture for smart grid analytics, which involves data resources, transmission, storage, and analysis. In order to enable big data computing in smart grid, a communication architecture is then described consisting of four main domains. Key technologies to enable big-data-aware wireless communication for smart grid are investigated. As a case study of the proposed architecture, we introduce a big-data- enabled storage planning scheme based on wireless big data computing. A hybrid approach is adopted for the optimization including GA for storage planning and a game theoretic inner optimization for daily energy scheduling. Simulation results indicate that the proposed storage planning scheme greatly reduce
2609552253;IoT Interoperability—On-Demand and Low Latency Transparent Multiprotocol Translator;2017.0;[];In the Industrial Internet of Things (IIoT) there is a clear need for a high level of interoperability between independently developed systems, often from different vendors. Traditional methods of interoperability including protocol gateways and adapters are often used at the network layer. Recent work on application interoperability has emphasized the use of middleware or protocol proxy/gateway. However, middleware tends to move the interoperability problem rather than solving it, and there are scalability issues with increasing the number of proxies, reconfiguration effort, and required bandwidth and processing overheads. This paper proposes a secure, on-demand, and transparent protocol translator for the IIoT. Targeting the challenge of interoperability between Internet protocol-based communication protocols, this paper analyzes current solutions and develops a set of requirements to be met by IoT protocol interoperability. The proposed protocol translator is not a middleware, it is a service-oriented architecture-based participant, it is used on-demand when needed, it does not introduce design time dependencies, it operates transparently, it supports low-latency, and it is secured through the use of Arrowhead authorization and authentication.
2610551401;A Green Communication Model for 5G Systems;2017.0;[];Small cell networks (SCNs) are envisaged as a key technology enabling the fifth-generation (5G) wireless communication system to address the challenge of rising mobile data demand. Green communications will be another major attribute of 5G systems, as power consumption from the information and communication technology sector is forecast to increase significantly by 2030. Accordingly, energy-efficient SCN design has attracted significant attention from researchers in recent years. In addition, to enable the ubiquitous deployment of dense small cells, service providers require energy-efficient backhauling solutions. In this paper, we present an energy-efficient communication model for 5G heterogeneous networks (HetNets). The proposed model considers both the access and backhaul network elements. We formulate and present an analytical model to calculate the optimum number of small cells that need to be kept active at various times of the day in order to minimize power consumption while meeting users’ quality of service demands. Based on our critical investigation of backhaul power consumption, we also isolate and present two energy-efficient backhauling solutions for 5G HetNets. Simulated results reveal that the proposed green communication model saves up to 48% more power than other existing models.
2610935556;Neural Ranking Models with Weak Supervision;2017.0;[];Despite the impressive improvements achieved by unsupervised deep neural networks in computer vision and NLP tasks, such improvements have not yet been observed in ranking for information retrieval. The reason may be the complexity of the ranking problem, as it is not obvious how to learn from queries and documents when no supervised signal is available. Hence, in this paper, we propose to train a neural ranking model using weak supervision, where labels are obtained automatically without human annotators or any external resources (e.g., click data). To this aim, we use the output of an unsupervised ranking model, such as BM25, as a weak supervision signal. We further train a set of simple yet effective ranking models based on feed-forward neural networks. We study their effectiveness under various learning scenarios (point-wise and pair-wise models) and using different input representations (i.e., from encoding query-document pairs into dense/sparse vectors to using word embedding representation). We train our networks using tens of millions of training instances and evaluate it on two standard collections: a homogeneous news collection (Robust) and a heterogeneous large-scale web collection (ClueWeb). Our experiments indicate that employing proper objective functions and letting the networks to learn the input representation based on weakly supervised data leads to impressive performance, with over 13% and 35% MAP improvements over the BM25 model on the Robust and the ClueWeb collections. Our findings also suggest that supervised neural ranking models can greatly benefit from pre-training on large amounts of weakly labeled data that can be easily obtained from unsupervised IR models.
2611050466;Secure, efficient and revocable data sharing scheme for vehicular fogs;2018.0;[];With the rapid development of vehicular networks, the problem of data sharing in vehicular networks has attached much attention. However, existing data access control schemes in cloud computing cannot be applied to the scenario of vehicular networks, because cloud computing paradigm cannot satisfy the rigorous requirement posed by latency-sensitive mobile application. Fog Computing is a paradigm that extends Cloud computing and services to the edge of the network. The vehicular fog is the ideal platform to achieve data sharing in vehicular networks. In this paper, we propose a revocable data sharing scheme for vehicular fogs. We construct a new multi-authority ciphertext policy attribute-based encryption (CP-ABE) scheme with efficient decryption to realize data access control in vehicular network system, and design an efficient user and attribute revocation method for it. The analysis and the simulation results show that our scheme is secure and highly efficient.
2612872092;Graph embedding techniques, applications, and performance: A survey;2018.0;[];   Graphs, such as social networks, word co-occurrence networks, and communication networks, occur naturally in various real-world applications. Analyzing them yields insight into the structure of society, language, and different patterns of communication. Many approaches have been proposed to perform the analysis. Recently, methods which use the representation of graph nodes in vector space have gained traction from the research community. In this survey, we provide a comprehensive and structured analysis of various graph embedding techniques proposed in the literature. We first introduce the embedding task and its challenges such as scalability, choice of dimensionality, and features to be preserved, and their possible solutions. We then present three categories of approaches based on factorization methods, random walks, and deep learning, with examples of representative algorithms in each category and analysis of their performance on various tasks. We evaluate these state-of-the-art methods on a few common datasets and compare their performance against one another. Our analysis concludes by suggesting some potential applications and future directions. We finally present the open-source Python library we developed, named GEM ( Graph Embedding Methods , available at  https://github.com/palash1992/GEM ), which provides all presented algorithms within a unified interface to foster and facilitate research on the topic.
2616444582;An intelligent support system for automatic detection of cerebral vascular accidents from brain CT images;2017.0;[];   Objective  This paper presents a Radial Basis Functions Neural Network (RBFNN) based detection system, for automatic identification of Cerebral Vascular Accidents (CVA) through analysis of Computed Tomographic (CT) images.    Methods  For the design of a neural network classifier, a Multi Objective Genetic Algorithm (MOGA) framework is used to determine the architecture of the classifier, its corresponding parameters and input features by maximizing the classification precision, while ensuring generalization.  This approach considers a large number of input features, comprising first and second order pixel intensity statistics, as well as symmetry/asymmetry information with respect to the ideal mid-sagittal line.    Results  Values of specificity of 98% and sensitivity of 98% were obtained, at pixel level, by an ensemble of non-dominated models generated by MOGA, in a set of 150 CT slices (1,867,602 pixels), marked by a NeuroRadiologist. This approach also compares favorably at a lesion level with three other published solutions, in terms of specificity (86% compared with 84%), degree of coincidence of marked lesions (89% compared with 77%) and classification accuracy rate (96% compared with 88%).
2619104685;A Survey on Energy Internet Communications for Sustainability;2017.0;[];Energy Internet (EI) is proposed as the evolution of smart grid, aiming to integrate various forms of energy into a highly flexible and efficient grid that provides energy packing and routing functions, similar to the Internet. As an essential part in EI system, a scalable and interoperable communication infrastructure is critical in system construction and operation. In this article, we survey the recent research efforts on EI communications. The motivation and key concepts of EI are first introduced, followed by the key technologies and standardizations enabling the EI communications as well as security issues. Open challenges in system complexity, efficiency, reliability are explored and recent achievements in these research topics are summarized as well.
2619568410;Towards fog-driven IoT eHealth: Promises and challenges of IoT in medicine and healthcare;2018.0;[];   Internet of Things (IoT) offers a seamless platform to connect people and objects to one another for enriching and making our lives easier. This vision carries us from compute-based centralized schemes to a more distributed environment offering a vast amount of applications such as smart wearables, smart home, smart mobility, and smart cities. In this paper we discuss applicability of IoT in healthcare and medicine by presenting a holistic architecture of IoT eHealth ecosystem. Healthcare is becoming increasingly difficult to manage due to insufficient and less effective healthcare services to meet the increasing demands of rising aging population with chronic diseases. We propose that this requires a transition from the clinic-centric treatment to patient-centric healthcare where each agent such as hospital, patient, and services are seamlessly connected to each other. This patient-centric IoT eHealth ecosystem needs a multi-layer architecture: (1) device, (2) fog computing and (3) cloud to empower handling of complex data in terms of its variety, speed, and latency. This fog-driven IoT architecture is followed by various case examples of services and applications that are implemented on those layers. Those examples range from mobile health, assisted living, e-medicine, implants, early warning systems, to population monitoring in smart cities. We then finally address the challenges of IoT eHealth such as data management, scalability, regulations, interoperability, device–network–human interfaces, security, and privacy.
2619597416;Gamification: a systematic review of design frameworks;2017.0;[];Learner’s motivation difficulties are recognized as a problem in diverse educational scenarios, reaching up to university degrees. Among other techniques that are often applied by instructors to counteract this issue, those related to the use of gaming elements seem to very promising. In this context, considering the use of game-like properties in learning scenarios, known as gamification, has received increasing interest by academia in recent years. However, its application in higher education can be challenging, due to some unwanted effects caused by the lack of proven design methodologies have been detected. Choosing the adequate formal process for gamification design has become an important success requirement. This work presents a systematic review of the gamification design frameworks discussed in the literature, providing a useful resource to educational practitioners as well as gamification designers and researchers. A total of 2314 unique works are initially recorded, based on queries in databases, libraries, journals and search engines. After applying a systematic filtering process, a definitive list of 40 works is more closely analysed. Next to review over relevant literature, an assessment of the main features found in the discussed approaches is given, while also categorizing them according to their main application field and its suitability in higher educational environments.
2620003726;δ-shock model based on Polya process and its optimal replacement policy;2017.0;[];   Shock models are of great interest in engineering reliability. Among the others, the  δ -shock model has been widely studied in the literature. In this model, the system breaks down due to the arrivals of two successive shocks which are too close to each other. That is, the system fails when the time between two consecutive shocks falls below a fixed threshold  δ . In the literature, the  δ -shock model has been mostly studied by assuming that shocks arrive according to a renewal process so that the interarrival times between shocks are independent and identically distributed. In the current paper, we consider the case when the shock arrival process is described by a Polya process which has dependent interarrival times. In particular, we obtain survival function and mean lifetime of the system and study the optimal replacement policy for the  δ -shock model based on Polya process.
2620419788;Multi-objective differential evolution with performance-metric-based self-adaptive mutation operator for chemical and biochemical dynamic optimization problems;2017.0;[];Display OmittedFramework of the proposed algorithm. The proposed algorithm can automatically select a suitable mutation operator to solve a particular type of problems.MODE-PMSMO is compared with seven MOEAs on ten MOPs.MODE-PMSMO is utilized to four actual multi-objective dynamic optimization problems. Each mutation operator of differential evolution (DE) algorithm is generally suitable for certain specific types of multi-objective optimization problems (MOPs) or particular stages of the evolution. To automatically select an appropriate mutation operator for solving MOPs in different phases of the evolution, a multi-objective differential evolution with performance-metric-based self-adaptive mutation operator (MODE-PMSMO) is proposed in this study. In MODE-PMSMO, a modified inverted generational distance (IGD) is utilized to evaluate the performance of each mutation operator and guide the evolution of mutation operators. The proposed MODE-PMSMO is then compared with seven multi-objective evolutionary algorithms (MOEAs) on five bi-objective and five tri-objective optimization problems. Generally, MODE-PMSMO exhibits the best average performance among all compared algorithms on ten MOPs. Additionally, MODE-PMSMO is employed to solve four typical multi-objective dynamic optimization problems in chemical and biochemical processes. Experimental results indicate that MODE-PMSMO is suitable for solving these actual problems and can provide a set of nondominated solutions for references of decision makers.
2623597181;Semantics for Specialising Attack Trees based on Linear Logic;2017.0;[];
2648699835;End-to-End Neural Ad-hoc Ranking with Kernel Pooling;2017.0;[];This paper proposes K-NRM, a kernel based neural model for document ranking. Given a query and a set of documents, K-NRM uses a translation matrix that models word-level similarities via word embeddings, a new kernel-pooling technique that uses kernels to extract multi-level soft match features, and a learning-to-rank layer that combines those features into the final ranking score. The whole model is trained end-to-end. The ranking layer learns desired feature patterns from the pairwise ranking loss. The kernels transfer the feature patterns into soft-match targets at each similarity level and enforce them on the translation matrix. The word embeddings are tuned accordingly so that they can produce the desired soft matches. Experiments on a commercial search engineu0027s query log demonstrate the improvements of K-NRM over prior feature-based and neural-based states-of-the-art, and explain the source of K-NRMu0027s advantage: Its kernel-guided embedding encodes a similarity metric tailored for matching query words to document words, and provides effective multi-level soft matches.
2657631929;Methods for Interpreting and Understanding Deep Neural Networks;2018.0;[];   This paper provides an entry point to the problem of interpreting a deep neural network model and explaining its predictions. It is based on a tutorial given at ICASSP 2017. As a tutorial paper, the set of methods covered here is not exhaustive, but sufficiently representative to discuss a number of questions in interpretability, technical challenges, and possible applications. The second part of the tutorial focuses on the recently proposed layer-wise relevance propagation (LRP) technique, for which we provide theory, recommendations, and tricks, to make most efficient use of it on real data.
2668739872;New approach to detect and classify stroke in skull CT images via analysis of brain tissue densities;2017.0;[];"   Background and Objective  Cerebral vascular accident (CVA), also known as stroke, is an important health problem worldwide and it affects 16 million people worldwide every year. About 30% of those that have a stroke die and 40% remain with serious physical limitations. However, recovery in the damaged region is possible if treatment is performed immediately. In the case of a stroke, Computed Tomography (CT) is the most appropriate technique to confirm the occurrence and to investigate its extent and severity. Stroke is an emergency problem for which early identification and measures are difficult; however, computer-aided diagnoses (CAD) can play an important role in obtaining information imperceptible to the human eye. Thus, this work proposes a new method for extracting features based on radiological density patterns of the brain, called Analysis of Brain Tissue Density (ABTD).    Methods  The proposed method is a specific approach applied to CT images to identify and classify the occurrence of stroke diseases. The evaluation of the results of the ABTD extractor proposed in this paper were compared with extractors already established in the literature, such as features from Gray-Level Co-Occurrence Matrix (GLCM), Local binary patterns (LBP), Central Moments (CM), Statistical Moments (SM), Hu’s Moment (HM) and Zernike’s Moments (ZM). Using a database of 420 CT images of the skull, each extractor was applied with the classifiers such as MLP, SVM, kNN, OPF and Bayesian to classify if a CT image represented a healthy brain or one with an ischemic or hemorrhagic stroke.    Results  ABTD had the shortest extraction time and the highest average accuracy (99.30%) when combined with OPF using the Euclidean distance. Also, the average accuracy values for all classifiers were higher than 95%.    Conclusions  The relevance of the results demonstrated that the ABTD method is a useful algorithm to extract features that can potentially be integrated with CAD systems to assist in stroke diagnosis."
2725049817;An Introduction to Neural Information Retrieval;2018.0;[];
2729664112;Energy management for stationary electric energy storage systems: A systematic literature review;2018.0;[];Electric Energy Storage Systems (EESS) have received an increased attention in recent years due to their important role in an active management of energy supply systems. Fueled by the increasing shares of intermittent Renewable Energy Sources (RES) in todayu0027s energy supply, balancing energy demand and energy supply over time becomes more and more challenging. EESS are recognized as a key technology to overcome this challenge by storing energy and converting it back when needed. Even though some EESS solutions are already available on the market, EESS suffer from technical limitations and entail high investment costs. Energy management is responsible for managing the operations of EESS and the interactions with the surrounding systems. An optimal energy management is an important precondition to ensure economic viability of EESS.
2730468065;Proxy-assisted access control scheme of cloud data for smart cities;2017.0;[];Security and privacy for smart cities have attached much attention. As cloud computing can provide secure and reliable data storage and data sharing, it has become an important infrastructure for smart cities. However, in the cloud storage system, the cloud server cannot be fully trusted. Therefore, new solutions of access control of cloud data need to be proposed. In this paper, we construct a proxy-assisted access control scheme. We use the method of ciphertext policy attribute-based encryption (CP-ABE) to realize access control of cloud data for smart cities. Since major computation of decryption is outsourced to the cloud server, our scheme can achieve efficient user decryption. We also present an efficient user and attribute revocation method for our scheme which achieves both forward security and backward security. Finally, we give secure and performance analysis of our scheme, which show that our scheme is secure and highly efficient.
2732140844;DeltaIoT: a self-adaptive internet of things exemplar;2017.0;[];Internet of Things (IoT) consists of networked tiny embedded computers (motes) that are capable of monitoring and controlling the physical world. Examples range from building security monitoring to smart factories. A central problem of IoT is minimising the energy consumption of the motes, while guaranteeing high packet delivery performance, regardless of uncertainties such as sudden changes in traffic load and communication interference. Traditionally, to deal with uncertainties the network settings are either hand-tuned or over-provisioned, resulting in continuous network maintenance or inefficiencies. Enhancing the IoT network with self-adaptation can automate these tasks. This paper presents DeltaIoT, an exemplar that enables researchers to evaluate and compare new methods, techniques and tools for self-adaptation in IoT. DeltaIoT is the first exemplar for research on self-adaptation that provides both a simulator for offline experimentation and a physical setup that can be accessed remotely for real-world experimentation.
2735483683;New Divergence and Entropy Measures for Intuitionistic Fuzzy Sets on Edge Detection;2018.0;[];Edges of the image play an important role in the field of digital image processing and computer vision. The edges reduce the amount of data, extract useful information from the image and preserve significant structural properties of an input image. Further, these edges can be used for object and facial expression detection. In this paper, we will propose new intuitionistic fuzzy divergence and entropy measures with its proof of validity for intuitionistic fuzzy sets. A new and significant technique has been developed for edge detection. To check the robustness of the proposed method, obtained results are compared with Canny, Sobel and Chaira methods. Finally, mean square error (MSE) and peak signal-to-noise ratio (PSNR) have been calculated and PSNR values of proposed method are always equal or greater than the PSNR values of existing methods. The detected edges of the various sample images are found to be true, smooth and sharpen.
2738231971;Hybrid invasive weed/biogeography-based optimization;2017.0;[];"   We propose a new variant of the ecologically-inspired optimization method known as invasive weed optimization (IWO). The proposed algorithm features three new components that are typically not present in IWO: (1) migration; (2) gradient descent; and (3) mutation. In standard IWO, each individual uses only its own features (that is, independent solution variables) to randomly distribute new seeds over the search space. In other words, there is no sharing of features among individuals. We propose the application of the migration operator from biogeography-based optimization (BBO) to include the feature-sharing capability in IWO. This modification improves the quality of the distributed seeds (that is, new candidate solutions) in the population. To further improve the local search ability of IWO, we propose the use of gradient descent. Mutation is activated under certain conditions to increase the diversity of the population, and escape local optima. We demonstrate the performance of this new hybrid IWO/BBO on a set of single-objective benchmarks, and on a real-world cyber–physical system problem to optimize a user intent recognition system for transfemoral amputees. Hybrid IWO/BBO is compared to standard IWO, BBO, and 10 other optimization algorithms. The Kruskal–Wallis and Wilcoxon signed-rank tests are used to statistically compare the algorithms. The results for hybrid IWO/BBO present promising improvements over standard IWO. For instance, out of 25 benchmarks, hybrid IWO/BBO performs better than IWO on 18 problems with dimension 30. Hybrid IWO/BBO shows competitive performance with comparison to the 10 other state-of-the-art optimization algorithms."
2740721704;Deep Pyramid Convolutional Neural Networks for Text Categorization;2017.0;[];
2740864224;Re-designed flipped learning model in an academic course: The role of co-creation and co-regulation;2017.0;[];   In traditional flipped classroom (FC), learning of new content mostly occurs through watching videos and transferring information from instructor to students utilizing technological tools. The present study devised and examined a novel extension of the FC model. This model adds components that acknowledge the roles of instructor, learners, peer assessment, and embedded evaluation. Moreover, it highlights the value of technology and digital tools in supporting and enhancing active individual and collaborative learning, and the development of self-regulated strategies in in-class and out-of-class settings. The model was investigated in a qualitative study, which was conducted in a blended academic course, including synchronous and asynchronous lessons. The participants were 36 graduate students who were studying towards a Master Degree in Education. The paper analyzed learning experiences and their interpretations by the students. In contrast to traditional FC model, the findings revealed active learning of students in both in- and out-of-class settings that took place before, during, and after the lesson. The instructor promoted extensive independent learning, learning regulation, continuous dialogue and collaborative interactions among peers. The re-designed model highlights co-creation of the course content and of digital learning outcomes by students, self-regulation and teamwork co-regulation, which are rare in higher education.
2740920897;Deep matrix factorization models for recommender systems;2017.0;[];Recommender systems usually make personalized recommendation with user-item interaction ratings, implicit feedback and auxiliary information. Matrix factorization is the basic idea to predict a personalized ranking over a set of items for an individual user with the similarities among users and items. In this paper, we propose a novel matrix factorization model with neural network architecture. Firstly, we construct a user-item matrix with explicit ratings and non-preference implicit feedback. With this matrix as the input, we present a deep structure learning architecture to learn a common low dimensional space for the representations of users and items. Secondly, we design a new loss function based on binary cross entropy, in which we consider both explicit ratings and implicit feedback for a better optimization. The experimental results show the effectiveness of both our proposed model and the loss function. On several benchmark datasets, our model outperformed other state-of-the-art methods. We also conduct extensive experiments to evaluate the performance within different experimental settings.
2741602058;Learning bilingual word embeddings with (almost) no bilingual data;2017.0;[];
2741789041;Explaining teaching uses of Wikipedia through faculty personal and contextual features;2017.0;[];" :[0],""purpose of this paper is to explore which personal and contextual factors affect the use of Wikipedia as a teaching resource in higher education  :[25],""research question is approached by investigating faculty perceptions and attitudes in two large Spanish universities. For this purpose, a comprehensive empirical study has been employed, based on an online survey to faculty members and the inclusion of a decision-making model in the  :[68],""provide evidence that a combination of cultural, social and subjective factors influences the decision to use Wikipedia. This decision is not only associated with lecturers’ individual characteristics, but mostly with surrounding influences. Teaching uses are more frequent when academics have close reference models and when they perceive that Wikipedia is being positively valued by their   :[125],""present study provides a creative framework to analyze the main determining factors of Wikipedia usage by faculty. The inclusion of both internal and external factors in the decision process has proved to be a valuable   study :[162],""detects the main factors affecting the negative or reluctant attitude toward Wikipedia and provides some recommendations to overcome these  study :[183],""widens the scope of previous investigations supplying a new :[25],""research framework and including, for the first time, a prominent online university in the analysis in order to discard the potential effects of digital and information illiteracy among students and faculty members."
2742118814;Optimal mission abort policy for systems in a random environment with variable shock rate;2018.0;[];   To enhance survivability of many real-world critical systems (e.g., aircrafts and human space flight systems), mission abort procedures are often utilized in practice. Specifically, the mission objectives of these systems can be aborted in cases where a certain malfunction condition is met or some obstacles/ hazards occur. Then a rescue or recovery procedure is initiated to enhance survivability. Traditional system reliability models typically cannot address the effects of mission aborts, and thus are not applicable to analyzing systems subject to mission abort requirements. In this paper, we first develop a methodology to model and evaluate mission success probability (MSP) and survivability of systems experiencing both internal failures and external shocks. We consider a policy when a mission is aborted and a rescue procedure is activated if the m-th shock occurs before time ξ since the start of a mission. We demonstrate the tradeoff between system survivability and MSP that should be balanced by the proper choice of the decision variables m and ξ. An illustrative example of a mission performed by an unmanned aerial vehicle is presented.
2742900359;Scenario reduction in stochastic programming;2003.0;[];Given a convex stochastic programming problem with a discrete initial probability distribution, the problem of optimal scenario reduction is stated as follows: Determine a scenario subset of prescribed cardinality and a probability measure based on this set that is the closest to the initial distribution in terms of a natural (or canonical) probability metric. Arguments from stability analysis indicate that Fortet-Mourier type probability metrics may serve as such canonical metrics. Efficient algorithms are developed that determine optimal reduced measures approximately. Numerical experience is reported for reductions of electrical load scenario trees for power management under uncertainty. For instance, it turns out that after 50% reduction of the scenario tree the optimal reduced tree still has about 90% relative accuracy.
2743692535;An assistance device to help people with trunk impairment maintain posture;2017.0;[];People with trunk impairment cannot lean forward because of the dysfunction of the trunk resulting from events such as cervical cord injury (CCI). It is therefore difficult for such people to work at a table because they may easily fall from their wheelchair, and it is also hard for them to return to their original position. This limits the activities of daily living (ADLs) of people with trunk impairment. These problems can be solved to some extent with equipment such as a wheelchair belt or a spinal orthosis that can help the person to maintain his or her posture. However, people cannot move freely with this equipment. Furthermore, if this equipment is used for a long time, there is a risk of physical pain and skin issues. In this study, we developed a device that assists the trunk of people with trunk impairment when they lean forward. This device supports people with trunk impairment so that they may take their meals at the table and prevents them from falling over their wheelchair without hindering their daily performance when they are sitting normally. The effectiveness of our proposed device was verified by experiments involving having a meal, operating a wheelchair, and colliding with a curb. Our device can help people with trunk impairment by improving their ADLs and quality of life (QOL).
2743897093;A Reinforcement Learning-Based Framework for the Generation and Evolution of Adaptation Rules;2017.0;[];"One of the challenges in self-adaptive systems concerns how to make adaptation to themselves at runtime in response to possible and even unexpected changes from the environment and/or user goals. A feasible solution to this challenge is rule-based adaptation, in which, adaptation decisions are made according to predefined rules that specify what particular actions should be performed to react to different changing events from the environment. Although it has the characteristic of highly- efficient decision making for adaptation, rule-based adaptation has two limitations: 1. no guarantee that those predefined rules will lead to optimal or nearly-optimal adaptation results; 2. weak support to evolve these rules to cope with non-stationary environment and changeable user goals at runtime. In this paper, we propose a reinforcement learning-based framework to the generation and evolution of software adaptation rules. This framework manifests two key capabilities for self-adaptation: 1. the capability of automatically learning adaptation rules from different goal settings at the offline phase; 2. the capability of automatically evolving adaptation rules from real-time information about the environment and user goals at the online phase. The two capabilities are built on the combination of reinforcement learning and case-based reasoning techniques. This framework improves the existing rule-based adaptation from two points: the flexibility of adaptation logic, and the quality of adaptation rules. We evaluate this framework through a case study of an E-commerce web application, which shows that this framework improves both the efficiency and effectiveness of self-adaptation."
2744822502;Adding Self-Improvement to an Autonomic Traffic Management System;2017.0;[];Autonomic Computing and self-adaptive systems are a response to the increasing complexity required to cope with changing environments and varying system resources. However, the complexity of the adaptation logic itself increases with the available information in particular for distributed systems. This leads to uncertainty at runtime resulting in incompleteness in the representation of adaptation goals, models, or rules. Self-improvement which changes the adaptation logic at runtime through meta-adaptation addresses the uncertainty issue.In this paper, we present and discuss a self-improvement case study for an autonomic traffic management system. We adapt parameters of the adaptation logic through rule learning as well as the structure of the adaptation logic, e.g., from central to decentralized control. We show that the resulting implementation enables continuous self-improvement of the system even in situations that have not been taken into account at design time.
2745022580;Preference modeling experiments with surrogate weighting procedures for the PROMETHEE method;2018.0;[];One of the main tasks in a multi-criteria decision-making process is to define weights for the evaluation criteria. However, in many situations, the decision-maker (DM) may not be confident about defining specific values for these weights and may prefer to use partial information to represent the values of such weights with surrogate weights. Although for the additive model, the use of surrogate weighting procedures has been already explored in the literature, there is a gap with regard to experimenting with such kind of preference modeling in outranking based methods, such as PROMETHEE, for which there already are applications with surrogate weights in the literature. Thus, this paper presents an experimental study on preference modeling based on simulation so as to increase understanding and acceptance of a recommendation obtained when using surrogate weights within the PROMETHEE method. The main approaches to surrogate weights in the literature (EW, RS, RR and ROC) have been evaluated for choice and ranking problematics throughout statistical procedures, including Kendallu0027s tau coefficient. The surrogate weighting procedure that most faithfully represents a DMu0027s value system according to this analysis is the ROC procedure.
2746747652;Some q‐Rung Orthopair Fuzzy Aggregation Operators and their Applications to Multiple‐Attribute Decision Making;2018.0;[];The q-rung orthopair fuzzy sets (q-ROFs) are an important way to express uncertain information, and they are superior to the intuitionistic fuzzy sets and the Pythagorean fuzzy sets. Their eminent characteristic is that the sum of the qth power of the membership degree and the qth power of the degrees of non-membership is equal to or less than 1, so the space of uncertain information they can describe is broader. Under these environments, we propose the q-rung orthopair fuzzy weighted averaging operator and the q-rung orthopair fuzzy weighted geometric operator to deal with the decision information, and their some properties are well proved. Further, based on these operators, we presented two new methods to deal with the multi-attribute decision making problems under the fuzzy environment. Finally, we used some practical examples to illustrate the validity and superiority of the proposed method by comparing with other existing methods.
2747053038;Multi-label learning based on label-specific features and local pairwise label correlation;2018.0;[];   Multi-label learning has drawn great attention in recent years. One of its tasks aims to build classification models for the problem where each instance associates with a set of labels. In order to exploit discriminative features for classification, some methods are proposed to construct label-specific features. However, these methods neglect the correlation among labels. In this paper, we propose a new method called LF-LPLC for multi-label learning, which integrates Label-specific features and local pairwise label correlation simultaneously. Firstly, we convert the original feature space to a low dimensional label-specific feature space, and therefore each label has a specific representation of its own. Then, we exploit the local correlation between each pair of labels by means of nearest neighbor techniques. According to the local correlation, the label-specific features of each label are expanded by uniting the related data from other label-specific features. With such a framework, it enriches the labels’ semantic information and solves the imbalanced class-distribution problem. Finally, for each label, based on its label-specific features we construct a binary classification algorithm to test unlabeled instances. Comprehensive experiments are conducted on a collection of benchmark data sets. Comparison results with the state-of-the-art approaches validate the competitive performance of our proposed method.
2747436277;Predicting and Deterring Default with Social Media Information in Peer-to-Peer Lending;2017.0;[];AbstractThis study examines the predictive power of self-disclosed social media information on borrowers’ default in peer-to-peer (P2P) lending and identifies social deterrence as a new underlying mechanism that explains the predictive power. Using a unique data set that combines loan data from a large P2P lending platform with social media presence data from a popular social media site, borrowers’ self-disclosure of their social media account and their social media activities are shown to predict borrowers’ default probability. Leveraging a social media marketing campaign that increases the credibility of the P2P platform and lenders disclosing loan default information on borrowers’ social media accounts as a natural experiment, a difference-in-differences analysis finds a significant decrease in loan default rate and increase in default repayment probability after the event, indicating that borrowers are deterred by potential social stigma. The results suggest that borrowers’ social information can be u...
2751013881;Learning attentional recurrent neural network for visual tracking;2017.0;[];We propose a novel online Attentional Recurrent Neural Network (ARNN) model for visual tracking, which exploits the feature maps of Convolutional Neural Network (CNN) inside a bounding box to identify whether this target is the one appeared in previous frames. Attention mechanism is adopted for both different parts of targets and different scales of object features. The former attention model is able to select important regions to better trace the target while the latter one learns to weight the multiple scale features for accurate object location. We jointly train the recurrent network with the region based and scale based attention mechanism. The outstanding performances in the experiments validate the effectiveness of our proposed ARNN and show that ARNN outperforms the state-of-the-art tracking methods.
2751159520;Deep learning based on Batch Normalization for P300 signal detection;2018.0;[];Detecting P300 signals from electroencephalography (EEG) is the key to establishing a P300 speller, which is a type of braincomputer interface (BCI) system based on the oddball paradigm that allows users to type messages simply by controlling eye-gazes. The convolutional neural network (CNN) is an approach that has achieved good P300 detection performances. However, the standard CNN may be prone to overfitting and the convergence may be slow. To address these issues, we develop a novel CNN, termed BN3, for detecting P300 signals, where Batch Normalization is introduced in the input and convolutional layers to alleviate over-fitting, and the rectified linear unit (ReLU) is employed in the convolutional layers to accelerate training. Since our model is fully data-driven, it is capable of automatically capturing the discriminative spatio-temporal features of the P300 signal. The results obtained on previous BCI competition P300 data sets show that BN3 both achieves the state-of-the-art character recognition performance and that it outperforms existing detection approaches with small flashing epoch numbers. BN3 can be used to improve the character recognition performance in P300 speller systems.
2751169498;Collecting sensed data with opportunistic networks: The case of contact information overhead;2017.0;[];The rising human population in urban environments drives the mission towards smart cities, which envisions a wide deployment of sensors in order to improve the quality of living. In this regard, opportunistic networks (OppNets) present an economical means of collecting delay tolerant data from sensors to their respective gateways for providing various Smart City services. Due to the distributed nature of the network, encounter-based routing protocols achieve acceptable throughput by requiring nodes to exchange and update contact information on an encounter basis. Unfortunately, sufficient insight into the associated overhead is lacking in the literature. Hence, we contribute by modelling contact information overhead and investigating its impact on OppNet routing, particularly in terms of data exchange success and energy consumption on portable handheld devices. Our findings reveal that the expected contact information overhead in Smart City scenarios significantly reduces data exchange success and increases energy consumption on portable handheld devices, thereby threatening the feasibility of the technology. We address this issue by proposing an algorithm that can be incorporated into encounter-based routing protocols to reduce contact information overhead without compromising throughput. Simulation results show that our proposed algorithm reduces the average contact information overhead, increases throughput and reduces average energy consumption.
2751698421;Mission Abort Policy in Heterogeneous Nonrepairable 1-Out-of-N Warm Standby Systems;2018.0;[];Many real-world critical systems, such as aircraft and human space flight systems, utilize mission aborts to enhance the survivability of the system. Specifically, the mission objectives of these systems can be aborted in cases where a certain malfunction condition is met, and a rescue or recovery procedure is then initiated for system survival. Traditional system reliability models typically cannot address the effects of mission aborts, and thus are not applicable to analyzing systems subject to mission abort requirements. In this paper, we first develop a numerical methodology to model and evaluate mission success probability and system survivability of 1-out-of- N  warm standby systems subject to constant or adaptive mission abort policies. The system components are heterogeneous, characterized by different performances and different types of time-to-failure distributions. Based on the proposed evaluation method, we make another new contribution by formulating and solving the optimal mission abort problem, as well as a combined optimization problem that identifies the mission abort policy and component activation sequence maximizing mission success probability while achieving the desired level of system survivability. Efficiencies of constant and adaptive mission abort policies are compared through examples. Examples also demonstrate the tradeoff between system survivability and mission success probability due to the utilization of a mission abort policy. Such a tradeoff analysis can help identify optimal decisions on system mission abort and standby policies, promoting safe and reliable operation of warm standby systems.
2752930373;Hashing with Angular Reconstructive Embeddings;2018.0;[];Large-scale search methods are increasingly critical for many content-based visual analysis applications, among which hashing-based approximate nearest neighbor search techniques have attracted broad interests due to their high efficiency in storage and retrieval. However, existing hashing works are commonly designed for measuring data similarity by the Euclidean distances. In this paper, we focus on the problem of learning compact binary codes using the cosine similarity. Specifically, we proposed novel angular reconstructive embeddings (ARE) method, which aims at learning binary codes by minimizing the reconstruction error between the cosine similarities computed by original features and the resulting binary embeddings. Furthermore, we devise two efficient algorithms for optimizing our ARE in continuous and discrete manners, respectively. We extensively evaluate the proposed ARE on several large-scale image benchmarks. The results demonstrate that ARE outperforms several state-of-the-art methods.
2753634799;Multiscale Quantization for Fast Similarity Search;2017.0;[];We propose a multiscale quantization approach for fast similarity search on large, high-dimensional datasets. The key insight of the approach is that quantization methods, in particular product quantization, perform poorly when there is large variance in the norms of the data points. This is a common scenario for real- world datasets, especially when doing product quantization of residuals obtained from coarse vector quantization. To address this issue, we propose a multiscale formulation where we learn a separate scalar quantizer of the residual norm scales. All parameters are learned jointly in a stochastic gradient descent framework to minimize the overall quantization error. We provide theoretical motivation for the proposed technique and conduct comprehensive experiments on two large-scale public datasets, demonstrating substantial improvements in recall over existing state-of-the-art methods.
2753944674;On the order of Accuracy of Finite Difference Operators on Diagonal Norm Based Summation-by-Parts Form;2018.0;[];In this paper we generalize results regarding the order of accuracy of finite difference operators on summation-by-parts (SBP) form, previously known to hold on uniform grids, to grids with arbitra ...
2754124646;RINGA: Design and verification of finite state machine for self-adaptive software at runtime;2018.0;[];   Context  In recent years, software environments such as the cloud and Internet of Things (IoT) have become increasingly sophisticated, and as a result, development of adaptable software has become very important. Self-adaptive software is appropriate for todayu0027s needs because it changes its behavior or structure in response to a changing environment at runtime. To adapt to changing environments, runtime verification is an important requirement, and research that integrates traditional verification with self-adaptive software is in high demand.    Objective  Model checking is an effective static verification method for software, but existing problems at runtime remain unresolved. In this paper, we propose a self-adaptive software framework that applies model checking to software to enable verification at runtime.    Method  The proposed framework consists of two parts: the design of self-adaptive software using a finite state machine and the adaptation of the software during runtime. For the first part, we propose two finite state machines for self-adaptive software called the self-adaptive finite state machine (SA-FSM) and abstracted finite state machine (A-FSM). For the runtime verification part, a self-adaptation process based on a MAPE (monitoring, analyzing, planning, and executing) loop is implemented.    Results  We performed an empirical evaluation with several model-checking tools (i.e., NuSMV and CadenceSMV), and the results show that the proposed method is more efficient at runtime. We also investigated a simple example application in six scenarios related to the IoT environment. We implemented Android and Arduino applications, and the results show the practical usability of the proposed self-adaptive framework at runtime.    Conclusions  We proposed a framework for integrating model checking with a self-adaptive software lifecycle. The results of our experiments showed that the proposed framework can achieve verify self-adaptation software at runtime.
2754941027;Hesitant Fuzzy 2-Dimension Linguistic Term Set and its Application to Multiple Attribute Group Decision Making;2018.0;[];Hesitant fuzzy linguistic term set (HFLTS) is a powerful tool for solving the situations in which a decision maker hesitates among several consecutive linguistic terms in providing his or her preference to an alternative. To reflect the different importance degrees or weights of all possible linguistic terms in a HFLTS, an extension of HFLTS called probabilistic linguistic term set (PLTS) is proposed through adding probabilities. Note that for a PLTS, the importance information of all possible linguistic terms is described by crisp numbers. However, in practical applications, especially under uncertain environment, it may be difficult for decision makers to provide the importance information by crisp numbers. To accurately preserve the complete evaluation information provided by decision makers, motivated by the idea of 2-dimension linguistic variables, this paper proposes the concept of hesitant fuzzy 2-dimension linguistic term set, which includes not only possible linguistic terms expressing the evaluation value to an object, but also the importance degree of each linguistic term denoted by a linguistic term. Firstly, the operations and comparison laws between hesitant fuzzy 2-dimension linguistic elements (HF2DLEs) are defined. Then, some generalized aggregation operators are proposed for aggregating HF2DLEs, such as generalized hesitant fuzzy 2-dimension linguistic weighted average (G-HF2DLWA) operator, generalized hesitant fuzzy 2-dimension linguistic ordered weighted average (G-HF2DLOWA) operator and generalized hesitant fuzzy 2-dimension linguistic hybrid weighted average (G-HF2DLHWA) operator. Furthermore, some desirable properties and special cases of these operators are discussed. Based on the G-HF2DLOWA and G-HF2DLWA operators, an approach to multiple attribute group decision making is developed under hesitant fuzzy 2-dimension linguistic environment. Finally, a numerical example is given to verify the practicality and effectiveness of the proposed method.
2755032532;Bispectral analysis of spontaneous EEG activity from patients with moderate dementia due to Alzheimer's disease;2017.0;[];"Dementia due to Alzheimeru0027s disease (AD) is a common disorder with a great impact on the patientsu0027 quality of life. The aim of this pilot study was to characterize spontaneous electroencephalography (EEG) activity in dementia due to AD using bispectral analysis. Five minutes of EEG activity were recorded from 17 patients with moderate dementia due to AD and 19 age-matched controls. Bispectrum results revealed that AD patients are characterized by an increase of phase coupling at low frequencies in comparison with controls. Additionally, some bispectral features calculated from the bispectrum showed significant differences between both groups (p u003c 0.05, Mann-Whitney U test with Bonferroniu0027s correction). Finally, a stepwise logistic regression analysis with a leave-one-out cross-validation procedure was used for classification purposes. An accuracy of 86.11% (sensitivity = 88.24%; specificity =84.21%) was achieved. This study suggests the usefulness of bispectral analysis to provide further insights into the underlying brain dynamics associated with AD."
2755940713;Computational intelligence approaches for classification of medical data: State-of-the-art, future challenges and research directions;2018.0;[];   The explosive growth of data in volume, velocity and diversity that are produced by medical applications has contributed to abundance of big data. Current solutions for efficient data storage and management cannot fulfill the needs of heterogeneous data. Therefore, by applying computational intelligence (CI) approaches in medical data helps get better management, faster performance and higher level of accuracy in detection. This paper aims to investigate the state-of-the-art of computational intelligence approaches in medical data and to categorize the existing CI techniques, used in medical fields, as single and hybrid. In addition, the techniques and methodologies, their limitations and performances are presented in this study. The limitations are addressed as challenges to obtain a set of requirements for Computational Intelligence Medical Data (CIMD) in establishing an efficient CIMD architectural design. The results show that on the one hand Support Vector Machine (SVM) and Artificial Immune Recognition System (AIRS) as a single based computational intelligence approach were the best methods in medical applications. On the other hand, the hybridization of SVM with other methods such as SVM-Genetic Algorithm (SVM-GA), SVM-Artificial Immune System (SVM-AIS), SVM-AIRS and fuzzy support vector machine (FSVM) had great performances achieving better results in terms of accuracy, sensitivity and specificity.
2756095124;Multi-label classification using hierarchical embedding;2018.0;[];Multi-label learning deals with the classification of data with multiple labels.Output space with many labels is tackle by modeling inter-label correlations.Use of parametrization and embedding have been the prime focus.A piecewise-linear embedding using maximum margin matrix factorization is proposed.Our experimental analysis manifests the superiority of our proposed method. Multi-label learning is concerned with the classification of data with multiple class labels. This is in contrast to the traditional classification problem where every data instance has a single label. Multi-label classification (MLC) is a major research area in the machine learning community and finds application in several domains such as computer vision, data mining and text classification. Due to the exponential size of the output space, exploiting intrinsic information in feature and label spaces has been the major thrust of research in recent years and use of parametrization and embedding have been the prime focus in MLC. Most of the existing methods learn a single linear parametrization using the entire training set and hence, fail to capture nonlinear intrinsic information in feature and label spaces. To overcome this, we propose a piecewise-linear embedding which uses maximum margin matrix factorization to model linear parametrization. We hypothesize that feature vectors which conform to similar embedding are similar in some sense. Combining the above concepts, we propose a novel hierarchical matrix factorization method for multi-label classification. Practical multi-label classification problems such as image annotation, text categorization and sentiment analysis can be directly solved by the proposed method. We compare our method with six well-known algorithms on twelve benchmark datasets. Our experimental analysis manifests the superiority of our proposed method over state-of-art algorithm for multi-label learning.
2757656223;Data Scientists in Software Teams: State of the Art and Challenges;2018.0;[];The demand for analyzing large scale telemetry, machine, and quality data is rapidly increasing in software industry. Data scientists are becoming popular within software teams, e.g., Facebook, LinkedIn and Microsoft are creating a new career path for data scientists. In this paper, we present a large-scale survey with 793 professional data scientists at Microsoft to understand their educational background, problem topics that they work on, tool usages, and activities. We cluster these data scientists based on the time spent for various activities and identify 9 distinct clusters of data scientists, and their corresponding characteristics. We also discuss the challenges that they face and the best practices they share with other data scientists. Our study finds several trends about data scientists in the software engineering context at Microsoft, and should inform managers on how to leverage data science capability effectively within their teams.
2761398658;Generic and Universal Parallel Matrix Summation with a Flexible Compression Goal for Xilinx FPGAs;2018.0;[];Bit matrix compression is a highly relevant operation in computer arithmetic. Essentially being a multi-operand addition, it is the key operation behind fast multiplication and many higher-level operations such as multiply-accumulate, the computation of the dot product or the implementation of FIR filters. Compressor implementations have been constantly evolving for greater efficiency both in general and in the context of concrete applications or specific implementation technologies. This paper is building on this history and describes a generic implementation of a bit matrix compressor for Xilinx FPGAs, which does not require a generator tool. It contributes FPGA-oriented metrics for the evaluation of elementary parallel bit counters, a systematic analysis and partial decomposition of previously proposed counters and a fully implemented construction heuristic with a flexible compression target matching the device capabilities. The generic implementation is agnostic of the aspect ratio of the input matrix and can be used for multiplication the same way as it can be for single-column population count operations.
2761896323;Network Embedding as Matrix Factorization: Unifying DeepWalk, LINE, PTE, and node2vec;2018.0;[];"Since the invention of word2vec, the skip-gram model has significantly advanced the research of network embedding, such as the recent emergence of the DeepWalk, LINE, PTE, and node2vec approaches. In this work, we show that all of the aforementioned models with negative sampling can be unified into the matrix factorization framework with closed forms. Our analysis and proofs reveal that: (1) DeepWalk empirically produces a low-rank transformation of a networku0027s normalized Laplacian matrix; (2) LINE, in theory, is a special case of DeepWalk when the size of verticesu0027 context is set to one; (3) As an extension of LINE, PTE can be viewed as the joint factorization of multiple networks» Laplacians; (4) node2vec is factorizing a matrix related to the stationary distribution and transition probability tensor of a 2nd-order random walk. We further provide the theoretical connections between skip-gram based network embedding algorithms and the theory of graph Laplacian. Finally, we present the NetMF method as well as its approximation algorithm for computing network embedding. Our method offers significant improvements over DeepWalk and LINE for conventional network mining tasks. This work lays the theoretical foundation for skip-gram based network embedding methods, leading to a better understanding of latent network representation learning."
2763708625;Angle estimation and mutual coupling self-calibration for ULA-based bistatic MIMO radar;2018.0;[];   In this paper, we propose an effective scheme for angle estimation and array mutual coupling (MC) self-calibration in uniform linear arrays (ULA)-based bistatic multiple-input multiple-output (MIMO) radar. By exploiting the multidimensional inherent structure, the array data is formulated into a trilinear model. The transmit and receive direction matrices are primarily estimated via trilinear decomposition, after which the least square (LS) method is applied to obtain a rough angle estimation. Refined angles are achieved via two one-dimensional local searches. The MC coefficients are obtained via LS by utilizing the estimated angle prior. The proposed scheme can achieve automatic pairing of the estimated angles without any virtual aperture loss, thus it has better angle and MC estimation performance than existing methods. Numerical simulations verify the improvement of our scheme.
2763929596;Global and intrinsic geometric structure embedding for unsupervised feature selection;2018.0;[];   Dimensionality reduction becomes a significant problem due to the proliferation of high dimensional data. Sparse preserving projection (SPP) obtains the intrinsic geometric structure of the data, which contains natural discriminating information, and avoids the selection of parameters as well. However, SPP neglects the global structures since it computes the sparse representation of each data individually. Low rank representation (LRR), another commonly used dimensionality reduction method, finds the lowest rank representation of all data jointly, and is capable of capturing the global structures of data. Therefore in this paper, we propose a method, global and intrinsic geometric structure embedding for unsupervised feature selection (GGEFS), by constructing a low-rank-sparse graph. Our GGEFS method contains the loss of information, the preservation of structural information and the sparse regularization of projection matrix, on which we impose  l  2,1/2 -matrix norm to select sparser and discriminative features. An effective iterative algorithm based on Lagrange Multiplier method is described to solve GGEFS. Extensive experimental results demonstrate that the proposed algorithm outperform several state-of-the-art unsupervised feature selection methods.
2765877758;UA-DETRAC 2017: Report of AVSS2017 & IWT4S Challenge on Advanced Traffic Monitoring;2017.0;[];The rapid advances of transportation infrastructure have led to a dramatic increase in the demand for smart systems capable of monitoring traffic and street safety. Fundamental to these applications are a community-based evaluation platform and benchmark for object detection and multi-object tracking. To this end, we organize the AVSS2017 Challenge on Advanced Traffic Monitoring, in conjunction with the International Workshop on Traffic and Street Surveillance for Safety and Security (IWT4S), to evaluate the state-of-the-art object detection and multi-object tracking algorithms in the relevance of traffic surveillance. Submitted algorithms are evaluated using the large-scale UA-DETRAC benchmark and evaluation protocol. The benchmark, the evaluation toolkit and the algorithm performance are publicly available from the website http://detrac-db.rit.albany.edu.
2766228646;A survey on the usability and practical applications of Graphical Security Models;2017.0;[];This paper presents and discusses the current state of Graphical Security Models (GrSM), in terms of four GrSM phases: (i) generation, (ii) representation, (iii) evaluation, and (iv) modification. Although many studies focused on improving the usability, efficiency, and functionality of GrSMs (e.g., by using various model types and evaluation techniques), the networked system is evolving with many hosts and frequently changing topologies (e.g., Cloud, SDN, IoT etc.). To investigate the usability of GrSMs, this survey summarizes the characteristics of past research studies in terms of their development and computational complexity analysis, and specify their applications in terms of security metrics, availability of tools and their applicable domains. We also discuss the practical issues of modeling security, differences of GrSMs and their usability for future networks that are large and dynamic.
2766262086;Selectivity and longevity of peripheral-nerve and machine interfaces: A review;2017.0;[];For those individuals with upper-extremity amputation, a daily normal living activity is no longer possible or it requires additional effort and time. With the aim of restoring their sensory and motor functions, theoretical and technological investigations have been carried out in the field of neuroprosthetic systems. For transmission of sensory feedback, several interfacing modalities including indirect (non-invasive), direct-to-peripheral-nerve (invasive), and cortical stimulation have been applied. Peripheral nerve interfaces demonstrate an edge over the cortical interfaces due to the sensitivity in attaining cortical brain signals. The peripheral nerve interfaces are highly dependent on interface designs and are required to be biocompatible with the nerves to achieve prolonged stability and longevity. Another criterion is the selection of nerves that allows minimal invasiveness and damages as well as high selectivity for a large number of nerve fascicles. In this paper, we review the nerve-machine interface modalities noted above with more focus on peripheral nerve interfaces, which are responsible for provision of sensory feedback. The invasive interfaces for recording and stimulation of electro-neurographic signals include intra-fascicular, regenerative-type interfaces that provide multiple contact channels to a group of axons inside the nerve and the extra-neural-cuff-type interfaces that enable interaction with many axons around the periphery of the nerve. Section 9 summarizes the advancements made to date in the field of neuroprosthetics toward the achievement of a bidirectional nerve-machine interface with more focus on sensory feedback. In Section 10, the authors propose a hybrid interface technique for achieving better selectivity and long-term stability using the available nerve interfacing techniques.
2766984662;High-Speed tracking-by-detection without using image information;2017.0;[];Tracking-by-detection is a common approach to multi-object tracking. With ever increasing performances of object detectors, the basis for a tracker becomes much more reliable. In combination with commonly higher frame rates, this poses a shift in the challenges for a successful tracker. That shift enables the deployment of much simpler tracking algorithms which can compete with more sophisticated approaches at a fraction of the computational cost. We present such an algorithm and show with thorough experiments its potential using a wide range of object detectors. The proposed method can easily run at 100K fps while outperforming the state-of-the-art on the DETRAC vehicle tracking dataset.
2767053645;Ultrasound-Based Sensing Models for Finger Motion Classification;2018.0;[];Motions of the fingers are complex since hand grasping and manipulation are conducted by spatial and temporal coordination of forearm muscles and tendons. The dominant methods based on surface electromyography (sEMG) could not offer satisfactory solutions for finger motion classification due to its inherent nature of measuring the electrical activity of motor units at the skinu0027s surface. In order to recognize morphological changes of forearm muscles for accurate hand motion prediction, ultrasound imaging is employed to investigate the feasibility of detecting mechanical deformation of deep muscle compartments in potential clinical applications. In this study, finger motion classification has been represented as subproblems: recognizing the discrete finger motions and predicting the continuous finger angles. Predefined 14 finger motions are presented in both sEMG signals and ultrasound images and captured simultaneously. Linear discriminant analysis classifier shows the ultrasound has better average accuracy (95.88%) than the sEMG (90.14%). On the other hand, the study of predicting the metacarpophalangeal (MCP) joint angle of each finger in nonperiod movements also confirms that classification method based on ultrasound achieves better results (average correlation 0.89   $\pm$    0.07 and NRMSE 0.15   $\pm$   0.05) than sEMG (0.81   $\pm$   0.09 and 0.19   $\pm$   0.05). The research outcomes evidently demonstrate that the ultrasound can be a feasible solution for muscle-driven machine interface, such as accurate finger motion control of prostheses and wearable robotic devices.
2767106145;A systematic study of the class imbalance problem in convolutional neural networks;2018.0;[];"   In this study, we systematically investigate the impact of class imbalance on classification performance of convolutional neural networks (CNNs) and compare frequently used methods to address the issue. Class imbalance is a common problem that has been comprehensively studied in classical machine learning, yet very limited systematic research is available in the context of deep learning. In our study, we use three benchmark datasets of increasing complexity, MNIST, CIFAR-10 and ImageNet, to investigate the effects of imbalance on classification and perform an extensive comparison of several methods to address the issue: oversampling, undersampling, two-phase training, and thresholding that compensates for prior class probabilities. Our main evaluation metric is area under the receiver operating characteristic curve (ROC AUC) adjusted to multi-class tasks since overall accuracy metric is associated with notable difficulties in the context of imbalanced data. Based on results from our experiments we conclude that (i) the effect of class imbalance on classification performance is detrimental; (ii) the method of addressing class imbalance that emerged as dominant in almost all analyzed scenarios was oversampling; (iii) oversampling should be applied to the level that completely eliminates the imbalance, whereas the optimal undersampling ratio depends on the extent of imbalance; (iv) as opposed to some classical machine learning models, oversampling does not cause overfitting of CNNs; (v) thresholding should be applied to compensate for prior class probabilities when overall number of properly classified cases is of interest."
2767309578;Krylov subspace methods for functions of fractional differential operators;2017.0;[];
2767544481;The Importance of Pen Motion Pattern Groups for Semi-Automatic Classification of Handwriting into Mental Workload Classes;2018.0;[];In this paper, we introduce the pen motion pattern groups (PMPGs) and their contribution to the classification of handwriting into cognitive mental workload classes. We demonstrate the importance of PMPGs by providing an efficient semi-automatic machine learning-based classification framework that distinguishes between handwritten texts written by the same person under different mental workloads. Our evaluation framework is non-language-dependent since we used stroke features, which are not language-specific, and it takes into account the variability in behavioral biometrics between different writers. The handwritten text data was collected using the Computerized Penmanship Evaluation Tool. This digitizer provided accurate temporal measures throughout the writing. As a first stage, the participants were asked to write a given text in the Hebrew language. Then, as a second stage, the participants’ cognitive workload was manipulated by asking them to hold a number in their memory during the entire writing task. In our experiments, we show that incorporating the PMPGs into the classification process yielded an average cognitive load discrimination accuracy of 92.16%, which decreased to 72.90% when the PMPGs were not considered. The separation of handwritten strokes into PMPGs allows us to account for the fact that the strokes are affected differently under different cognitive mental workloads. This novel distinction between PMPGs is important since the handwriting process in each PMPG is different from a perceptual motor and brain-hand control point of view. Moreover, most of the features that are influenced by cognitive workload are those that cannot be discerned by an expert when looking at a handwritten text on paper, such as azimuth, tilt, velocity, acceleration, and pressure.
2767865048;The Impact of Message Replication on the Performance of Opportunistic Networks for Sensed Data Collection;2017.0;[];Opportunistic networks (OppNets) provide a scalable solution for collecting delay‑tolerant data from sensors for their respective gateways. Portable handheld user devices contribute significantly to the scalability of OppNets since their number increases according to user population and they closely follow human movement patterns. Hence, OppNets for sensed data collection are characterised by high node population and degrees of spatial locality inherent to user movement. We study the impact of these characteristics on the performance of existing OppNet message replication techniques. Our findings reveal that the existing replication techniques are not specifically designed to cope with these characteristics. This raises concerns regarding excessive message transmission overhead and throughput degradations due to resource constraints and technological limitations associated with portable handheld user devices. Based on concepts derived from the study, we suggest design guidelines to augment existing message replication techniques. We also follow our design guidelines to propose a message replication technique, namely Locality Aware Replication (LARep). Simulation results show that LARep achieves better network performance under high node population and degrees of spatial locality as compared with existing techniques.
2768061901;Rainbow matchings in Dirac bipartite graphs;2017.0;[];We show the existence of rainbow perfect matchings in $\mu n$-bounded edge colourings of Dirac bipartite graphs, for a sufficiently small $\muu003e0$. As an application of our results, we obtain several results on the existence of rainbow $k$-factors in Dirac graphs and rainbow spanning subgraphs of bounded maximum degree on graphs with large minimum degree.
2768506985;Level Set Based on Brain Radiological Densities for Stroke Segmentation in CT Images;2017.0;[];Cardiovascular diseases (CVD) are the leading cause of death worldwide, and every year more people die of these diseases. Aiming to assist medical diagnoses through Computerized Tomography (CT) scans, this work proposes a new approach to segment CT images of the brain damaged by stroke. The proposed method takes into account two improvements of the level set method based on the likelihood of Normal distribution. The first improvement is to handle the grayscale image input according to a range analysis of the image intensity scale, adopting 80 HU for the window width and 40 HU for the center level. In addition, we propose an optimal level set initialization, where the zero level set is determined by analyzing the brain density. These improvements to the level set method generate efficient stroke segmentation in CT images of the brain. The results of the proposed method are compared against those of the level set algorithm based on the coherent propagation method, and also those from the Watershed and Region Growing algorithms using a ground truth built by a specialist. The experimental results show that the proposed method presents superior performance, and that it is a promising tool to assist medical diagnoses.
2768956845;Applications of Deep Learning and Reinforcement Learning to Biological Data;2018.0;[];Rapid advances in hardware-based technologies during the past decades have opened up new possibilities for life scientists to gather multimodal data in various application domains, such as  omics ,  bioimaging ,  medical imaging , and  (brain/body)–machine interfaces . These have generated novel opportunities for development of dedicated data-intensive machine learning techniques. In particular, recent research in deep learning (DL), reinforcement learning (RL), and their combination (deep RL) promise to revolutionize the future of artificial intelligence. The growth in computational power accompanied by faster and increased data storage, and declining computing costs have already allowed scientists in various fields to apply these techniques on data sets that were previously intractable owing to their size and complexity. This paper provides a comprehensive survey on the application of DL, RL, and deep RL techniques in mining biological data. In addition, we compare the performances of DL techniques when applied to different data sets across various application domains. Finally, we outline open issues in this challenging research area and discuss future development perspectives.
2769278513;Pythagorean fuzzy power aggregation operators in multiple attribute decision making;2018.0;[];In this paper, we utilize power aggregation operators to develop some Pythagorean fuzzy power aggregation operators: Pythagorean fuzzy power average operator, Pythagorean fuzzy power geometric operator, Pythagorean fuzzy power weighted average operator, Pythagorean fuzzy power weighted geometric operator, Pythagorean fuzzy power ordered weighted average operator, Pythagorean fuzzy power ordered weighted geometric operator, Pythagorean fuzzy power hybrid average operator, and Pythagorean fuzzy power hybrid geometric operator. The prominent characteristic of these proposed operators are studied. Then, we have utilized these operators to develop some approaches to solve the Pythagorean fuzzy multiple attribute decision-making problems. Finally, a practical example is given to verify the developed approach and to demonstrate its practicality and effectiveness.
2769562095;Well-posed and stable transmission problems;2018.0;[];We introduce the notion of a transmission problem to describe a general class of problems where different dynamics are coupled in time. Well-posedness and stability is analysed for continuous and d ...
2769575562;Review of constraints on vision-based gesture recognition for human–computer interaction;2018.0;[];The ability of computers to recognise hand gestures visually is essential for progress in human-computer interaction. Gesture recognition has applications ranging from sign language to medical assistance to virtual reality. However, gesture recognition is extremely challenging not only because of its diverse contexts, multiple interpretations, and spatio-temporal variations but also because of the complex non-rigid properties of the hand. This study surveys major constraints on vision-based gesture recognition occurring in detection and pre-processing, representation and feature extraction, and recognition. Current challenges are explored in detail.
2770446405;How to make key 5G wireless technologies environmental friendly: A review;2018.0;[];
2771909823;IDOL: Comprehensive & Complete LOD Insights;2017.0;[];Over the last decade, we observed a steadily increasing amount of RDF datasets made available on the web of data. The decentralized nature of the web, however, makes it hard to identify all these datasets. Even more so, when downloadable data distributions are discovered, only insufficient metadata is available to describe the datasets properly, thus posing barriers on its usefulness and reuse. In this paper, we describe an attempt to exhaustively identify the whole linked open data cloud by harvesting metadata from multiple sources, providing insights about duplicated data and the general quality of the available metadata. This was only possible by using a probabilistic data structure called Bloom filter. Finally, we published a dump file containing metadata which can further be used to enrich existent datasets.
2772679429;A novel aggregation method for Pythagorean fuzzy multiple attribute group decision making;2018.0;[];
2772766867;Hierarchical Convolutional Neural Networks for EEG-Based Emotion Recognition;2018.0;[];Traditional machine learning methods suffer from severe overfitting in EEG-based emotion reading. In this paper, we use hierarchical convolutional neural network (HCNN) to classify the positive, neutral, and negative emotion states. We organize differential entropy features from different channels as two-dimensional maps to train the HCNNs. This approach maintains information in the spatial topology of electrodes. We use stacked autoencoder (SAE), SVM, and KNN as competing methods. HCNN yields the highest accuracy, and SAE is slightly inferior. Both of them show absolute advantage over traditional shallow models including SVM and KNN. We confirm that the high-frequency wave bands Beta and Gamma are the most suitable bands for emotion reading. We visualize the hidden layers of HCNNs to investigate the feature transformation flow along the hierarchical structure. Benefiting from the strong representational learning capacity in the two-dimensional space, HCNN is efficient in emotion recognition especially on Beta and Gamma waves.
2773194082;Addressing the Issue of Routing Unfairness in Opportunistic Backhaul Networks for Collecting Sensed Data;2017.0;[];Widely deploying sensors in the environment and embedding them in physical objects is a crucial step towards realizing smart and sustainable cities. To cope with rising resource demands and limited budgets, opportunistic networks (OppNets) offer a scalable backhaul option for collecting delay-tolerant data from sensors to gateways in order to enable efficient urban operations and services. While pervasive devices such as smartphones and tablets contribute significantly to the scalability of OppNets, closely following human movement patterns and social structure introduces network characteristics that pose routing challenges. Our study on the impact of these characteristics reveals that existing routing protocols subject a key set of devices to higher resource consumption, to which their users may respond by withdrawing participation. Unfortunately, existing solutions addressing this unfairness do not guarantee achievable throughput since they are not specifically designed for sensed data collection scenarios. Based on concepts derived from the study, we suggest design guidelines for adapting applicable routing protocols to sensed data collection scenarios. We also follow our design guidelines to propose the Fair Locality Aware Routing (FLARoute) technique. Evaluating FLARoute within an existing routing protocol confirms improved fairness and throughput under conditions that compromise the performance of existing solutions.
2773434597;Rainbow $H$-factors;2006.0;[];An  $H$-factor  of a graph $G$ is a spanning subgraph of $G$ whose connected components are isomorphic to $H$.  Given a properly edge-colored graph $G$, a  rainbow  $H$-subgraph of $G$ is an $H$-subgraph of $G$ whose edges have distinct colors.  A  rainbow $H$-factor  is an $H$-factor whose components are rainbow $H$-subgraphs.  The following result is proved. If $H$ is any fixed graph with $h$ vertices then every properly edge-colored graph with $hn$ vertices and minimum degree $(1-1/\chi(H))hn+o(n)$ has a rainbow $H$-factor.
2774637605;E-learning success determinants: Brazilian empirical study;2017.0;[];   E-learning is a web-based learning ecosystem for the dissemination of information, communication, and knowledge for education and training. Understanding the impact of e-learning on society, as well as its benefits, is important to link e-learning systems to their success drivers. The aim of this study is to find the determinants of user perceived satisfaction, use, and individual impact of e-learning. This study proposes a theoretical model integrating theories of information systemsu0027 satisfaction and success in the e-learning systems. The model was empirically validated in higher education institutions and university centers in Brazil through a quantitative method of structural equation modeling. Collaboration quality, information quality, and user perceived satisfaction explain e-learning use. The drivers of user perceived satisfaction are information quality, system quality, instructor attitude toward e-learning, diversity in assessment, and learner perceived interaction with others. System quality, use, and user perceived satisfaction explain individual impact.
2774702408;Uncertain linguistic hesitant fuzzy sets and their application in multi‐attribute decision making;2018.0;[];
2775719470;Optimal Stackelberg strategies for financing a supply chain through online peer-to-peer lending;2017.0;[];   In recent years, supply chain finance (SCF) through online peer-to-peer (P2P) lending platforms has gained its popularity. We study an SCF system with a manufacturer selling a product to a retailer that faces uncertain demand over a single period. We assume that either the retailer or the manufacturer faces a capital constraint and must borrow capital through an online P2P lending platform. The platform determines a service rate for the loan, the manufacturer sets a wholesale price for the product, and the retailer chooses its order quantity for the product. We identify optimal Stackelberg strategies of the participants in the SCF system. For an SCF system with a capital-constrained retailer, we find that the retaileru0027s optimal order quantity and the manufactureru0027s optimal wholesale price decrease with the platformu0027s service rate. For an SCF system with a capital-constrained manufacturer, we find that as the platformu0027s service rate increases, the manufactureru0027s optimal wholesale price increases but the retaileru0027s optimal order quantity decreases. Our analysis suggests that it is important for the retailer and the manufacturer to take the online P2P lending platformu0027s financial decisions (such as the service rate) into account when making their operational decisions.
2778803536;Perceptions of the use of intelligent information access systems in university level active learning activities among teachers of biomedical subjects;2017.0;[];   Background  Student participation and the use of active methodologies in classroom learning are being increasingly emphasized. The use of intelligent systems can be of great help when designing and developing these types of activities. Recently, emerging disciplines such as ‘educational data mining’ and ‘learning analytics and knowledge’ have provided clear examples of the importance of the use of artificial intelligence techniques in education.    Objective  The main objective of this study was to gather expert opinions regarding the benefits of using complementary methods that are supported by intelligent systems, specifically, by intelligent information access systems, when processing texts written in natural language and the benefits of using these methods as companion tools to the learning activities that are employed by biomedical and health sciences teachers.    Methods  Eleven teachers of degree courses who belonged to the Faculties of Biomedical Sciences (BS) and Health Sciences (HS) of a Spanish university in Madrid were individually interviewed. These interviews were conducted using a mixed methods questionnaire that included 66 predefined close-ended and open-ended questions. In our study, three intelligent information access systems (i.e., BioAnnote, CLEiM and MedCMap) were successfully used to evaluate the teacher’s perceptions regarding the utility of these systems and their different methods in learning activities.    Results  All teachers reported using active learning methods in the classroom, most of which were computer programs that were used for initially designing and later executing learning activities. All teachers used case-based learning methods in the classroom, with a specific emphasis on case reports written in Spanish and/or English. In general, few or none of the teachers were familiar with the technical terms related to the technologies used for these activities such as “intelligent systems” or “concept/mental maps”. However, they clearly realized the potential applicability of such approaches in both the preparation and the effective use of these activities in the classroom. Specifically, the themes highlighted by a greater number of teachers after analyzing the responses to the open-ended questions were the usefulness of BioAnnote system to provide reliable sources of medical information and the usefulness of the bilingual nature of CLEiM system for learning medical terminology in English.    Conclusions  Three intelligent information access systems were successfully used to evaluate the teacher’s perceptions regarding the utility of these systems in learning activities. The results of this study showed that integration of reliable sources of information, bilingualism and selective annotation of concepts were the most valued features by the teachers, who also considered the incorporation of these systems into learning activities to be potentially very useful. In addition, in the context of our experimental conditions, our work provides useful insights into the way to appropriately integrate this type of intelligent information access systems into learning activities, revealing key themes to consider when developing such approaches.
2781579392;The Reproducibility Crisis and Academic Libraries;2018.0;[];In recent years, evidence has emerged from disciplines ranging from biology to economics that many scientific studies are not reproducible. This evidence has led to declarations in both the scientific and lay press that science is experiencing a “reproducibility crisis” and that this crisis has significant impacts on both science and society, including misdirected effort, funding, and policy implemented on the basis of irreproducible research. In many cases, academic libraries are the natural organizations to lead efforts to implement recommendations from journals, funders, and societies to improve research reproducibility. In this editorial, we introduce the reproducibility crisis, define reproducibility and replicability, and then discusses how academic libraries can lead institutional support for reproducible research.
2782876185;Probabilistic Model Checking and Scheduling Implementation of an Energy Router System in Energy Internet for Green Cities;2018.0;[];Energy router (ER) based system is a crucial part of the energy transmission and management under the circumstance of energy Internet for green cities. During its design process, a sound formal verification and a performance monitoring scheme are needed to check its reliability and meaningful quantitative properties. In this paper, we provide formal verification solutions for an ER-based system by proposing a continuous-time Markov chain model describing the architecture of the ER-based system. To verify real-world function of the ER-based system, we choose electricity trading to propose a Markov decision process model based on an ER subsystem to describe the trading behavior. To monitor the system performance, we project the energy scheduling process in the ER-based system, and then implement this scheduling process on top of a cloud computing experiment tool. Finally, we perform extensive experiment evaluations to investigate the system reliability properties, quantitative properties, and scheduling behaviors. The experiment verifies the effectiveness of the proposed models and the monitoring scheme.
2782879278;SACRE: Supporting contextual requirements’ adaptation in modern self-adaptive systems in the presence of uncertainty at runtime;2018.0;[];© 2018 Elsevier Ltd Runtime uncertainty such as unpredictable resource unavailability, changing environmental conditions and user needs, as well as system intrusions or faults represents one of the main current challenges of self-adaptive systems. Moreover, todayu0027s systems are increasingly more complex, distributed, decentralized, etc. and therefore have to reason about and cope with more and more unpredictable events. Approaches to deal with such changing requirements in complex todayu0027s systems are still missing. This work presents SACRE (Smart Adaptation through Contextual REquirements), our approach leveraging an adaptation feedback loop to detect self-adaptive systems’ contextual requirements affected by uncertainty and to integrate machine learning techniques to determine the best operationalization of context based on sensed data at runtime. SACRE is a step forward of our former approach ACon which focus had been on adapting the context in contextual requirements, as well as their basic implementation. SACRE primarily focuses on architectural decisions, addressing self-adaptive systems’ engineering challenges. Furthering the work on ACon, in this paper, we perform an evaluation of the entire approach in different uncertainty scenarios in real-time in the extremely demanding domain of smart vehicles. The real-time evaluation is conducted in a simulated environment in which the smart vehicle is implemented through software components. The evaluation results provide empirical evidence about the applicability of SACRE in real and complex software system domains.
2783371799;Automated Multiclass Classification of Spontaneous EEG Activity in Alzheimer’s Disease and Mild Cognitive Impairment;2018.0;[];The discrimination of early Alzheimer’s disease (AD) and its prodromal form (i.e., mild cognitive impairment, MCI) from cognitively healthy control (HC) subjects is crucial since the treatment is more effective in the first stages of the dementia. The aim of our study is to evaluate the usefulness of a methodology based on electroencephalography (EEG) to detect AD and MCI. EEG rhythms were recorded from 37 AD patients, 37 MCI subjects and 37 HC subjects. Artifact-free trials were analyzed by means of several spectral and nonlinear features: relative power in the conventional frequency bands, median frequency, individual alpha frequency, spectral entropy, Lempel–Ziv complexity, central tendency measure, sample entropy, fuzzy entropy, and auto-mutual information. Relevance and redundancy analyses were also conducted through the fast correlation-based filter (FCBF) to derive an optimal set of them. The selected features were used to train three different models aimed at classifying the trials: linear discriminant analysis (LDA), quadratic discriminant analysis (QDA) and multi-layer perceptron artificial neural network (MLP). Afterwards, each subject was automatically allocated in a particular group by applying a trial-based majority vote procedure. After feature extraction, the FCBF method selected the optimal set of features: individual alpha frequency, relative power at delta frequency band, and sample entropy. Using the aforementioned set of features, MLP showed the highest diagnostic performance in determining whether a subject is not healthy (sensitivity of 82.35% and positive predictive value of 84.85% for HC vs. all classification task) and whether a subject does not suffer from AD (specificity of 79.41% and negative predictive value of 84.38% for AD vs. all comparison). Our findings suggest that our methodology can help physicians to discriminate AD, MCI and HC.
2783517474;Rate-Maximized Scheduling in Adaptive OCDMA Systems Using Stochastic Optimization;2018.0;[];In this letter, we use stochastic optimization to schedule data transmissions in an adaptive optical code division multiple-access (OCDMA) system to maximize transmission rate. We employ Lyapunov drift theory to formulate a general optimization problem for rate-maximized scheduling in which user codes are adaptively assigned such that time-averaged transmission rate is maximized and important system parameters such as stability and BER performance are maintained. Simulation results show that the overall transmission rate in a rate-maximized scheduled OCDMA system can be up to 3 (1.2) times better than the transmission rate in its counterpart OCDMA system with fixed code assignment scheme (with rate-efficient code allocation algorithm).
2783565819;Latent Relational Metric Learning via Memory-based Attention for Collaborative Ranking;2018.0;[];This paper proposes a new neural architecture for collaborative ranking with implicit feedback. Our model, LRML (Latent Relational Metric Learning) is a novel extension of metric learning approaches for recommendation. More specifically, instead of simple push pull mechanisms between user and item pairs, we propose to learn latent relations for each user item interaction. This helps to alleviate the potential geometric inflexibility of existing metric learning approaches. This not only enables better performance but also a greater extent of modeling capability, allowing our model to scale to larger number of interactions. In order to do so, we employ a augmented memory module and learn to attend over these memory blocks to construct latent relations. The attention module is controlled by the user-item interaction, making the learned relation vector specific to each user-item pair. Hence, this can be interpreted as learning an exclusive and optimal relational translation for each user-item interaction. The proposed architecture not only demonstrates the state-of-the-art performance across multiple recommendation benchmarks. LRML outperforms other metric learning models by 6%-7.5% in terms of Hits@10 and nDCG@10 on large datasets such as Netflix and MovieLens20M. Moreover, qualitative studies also demonstrate evidence that our proposed model is able to infer and encode explicit sentiment, temporal and attribute information despite being only trained on implicit feedback. As such, this ascertains the ability of LRML to uncover hidden relational structure within implicit datasets.
2783634296;An Extended Description of MORPH: A Reference Architecture for Configuration and Behaviour Self-Adaptation;2017.0;[];An architectural approach to self-adaptive systems involves runtime change of system configuration (i.e., the system’s components, their bindings and operational parameters) and behaviour update (i.e., component orchestration). The architecture should allow for both configuration and behaviour changes selected from pre-computed change strategies and for synthesised change strategies at run-time to satisfy changes in the environment, changes in the specified goals of the system or in response to failures or degradation in quality attributes, such as performance, of the system itself. Although controlling configuration and behaviour at runtime has been discussed and applied to architectural adaptation, architectures for self-adaptive systems often compound these two aspects reducing the potential for adaptability. In this work we provide an extended description of our proposal for a reference architecture that allows for coordinated yet transparent and independent adaptation of system configuration and behaviour.
2785117529;Advanced Compressor Tree Synthesis for FPGAs;2018.0;[];This work presents novel methods for the optimization of compressor trees for FPGAs as required in many arithmetic computations. As demonstrated in recent work, important key elements for the design of efficient but fast compressor trees are target-optimized 4:2 compressors as well as generalized parallel counters (GPCs). However, the optimization of a compressor tree for minimal resources using both compressors and GPCs has not been addressed so far. As this combined optimization is a non-trivial task, three methods are proposed to find best solutions for a given problem size: 1) a heuristic that obtains compressor trees with typically less resources and fewer stages than state-of-the-art heuristics, 2) an integer linear programming (ILP)-based methodology that finds optimal compressor trees using the fewest stages possible, 3) a combined approach that partially solves the problem heuristically to reduce the search space for the ILP-based method. In all methods, the cost for pipeline registers can be included. Synthesis experiments show that the proposed methods provide pipelined compressor trees with about 40 percent less LUTs compared to trees of 2-input adders at the cost of being about 12 ...20 percent slower.
2787932447;Latent Semantic Aware Multi-view Multi-label Classification;2018.0;[];
2788276534;New Directions in Attack Tree Research: Catching up with Industrial Needs;2017.0;[];Attack trees provide a systematic way of characterizing diverse system threats. Their strengths arise from the combination of an intuitive representation of possible attacks and availability of formal mathematical frameworks for analyzing them in a qualitative or a quantitative manner. Indeed, the mathematical frameworks have become a large focus of attack tree research. However, practical applications of attack trees in industry largely remain a tedious and error-prone exercise.
2789457647;ASCA-PSO: Adaptive sine cosine optimization algorithm integrated with particle swarm for pairwise local sequence alignment;2018.0;[];   The sine cosine algorithm (SCA), a recently proposed population-based optimization algorithm, is based on the use of sine and cosine trigonometric functions as operators to update the movements of the search agents. To optimize performance, different parameters on the SCA must be appropriately tuned. Setting such parameters is challenging because they permit the algorithm to escape from local optima and avoid premature convergence. The main drawback of the SCA is that the parameter setting only affects the exploitation of the prominent regions. However, the SCA has good exploration capabilities. This article presents an enhanced version of the SCA by merging it with particle swarm optimization (PSO). PSO exploits the search space better than the operators of the standard SCA. The proposed algorithm, called ASCA-PSO, has been tested over several unimodal and multimodal benchmark functions, which show its superiority over the SCA and other recent and standard meta-heuristic algorithms. Moreover, to verify the capabilities of the SCA, the SCA has been used to solve the real-world problem of a pairwise local alignment algorithm that tends to find the longest consecutive substrings between two biological sequences. Experimental results provide evidence of the good performance of the ASCA-PSO solutions in terms of accuracy and computational time.
2789600914;Model visualization: Combining context-based graph and tree representations;2018.0;[];   A Contextual Graph is based on the Contextual Graph formalism, which allows experts to realistically model the possible ways a task can be realized (i.e., practices). The power of Contextual-Graphs relies on their capability of considering the situation-dependent data (i.e., contextual information) that characterizes a task realization. Through successful experiences applying this formalism in several fields (e.g. medicine, transport, and military) it has been identified a clear need for an alternative representation of tasks that require an immediate response to an event (e.g. an incident in the Parisian subway). Decision makers have expressed their interest in a model that allows them to quickly identify the contextual information needed to elaborate and develop a practice. Moreover, in task realizations in which either a real object (e.g. a medical image), or an object of the reasoning (e.g. a strategy for driving in a freeway) corresponds to a practice output, a graph representation is not the clearest visualization, nor the fastest support for decision makers. The purpose of this paper is to offer a tree view of practices in a Contextual Graph. Such tree representation responds to the decision makers needs, providing another outlet for analyzing a task realization and its outputs. Moreover, the automatic generation of this view was implemented as a feature of the CxG software.
2789682920;Some q‐rung orthopair fuzzy Heronian mean operators in multiple attribute decision making;2018.0;[];
2789939861;Relevant Feature Selection from a Combination of Spectral-Temporal and Spatial Features for Classification of Motor Imagery EEG;2018.0;[];This paper presents a novel algorithm (CVSTSCSP) for determining discriminative features from an optimal combination of temporal, spectral and spatial information for motor imagery brain computer interfaces. The proposed method involves four phases. In the first phase, EEG signal is segmented into overlapping time segments and bandpass filtered through frequency filter bank of variable size subbands. In the next phase, features are extracted from the segmented and filtered data using stationary common spatial pattern technique (SCSP) that can handle the non- stationarity and artifacts of EEG signal. The univariate feature selection method is used to obtain a relevant subset of features in the third phase. In the final phase, the classifier is used to build adecision model. In this paper, four univariate feature selection methods such as Euclidean distance, correlation, mutual information and Fisher discriminant ratio and two well-known classifiers (LDA and SVM) are investigated. The proposed method has been validated using the publicly available BCI competition IV dataset Ia and BCI Competition III dataset IVa. Experimental results demonstrate that the proposed method significantly outperforms the existing methods in terms of classification error. A reduction of 76.98%, 75.65%, 73.90% and 72.21% in classification error over both datasets and both classifiers can be observed using the proposed CVSTSCSP method in comparison to CSP, SBCSP, FBCSP and CVSCSP respectively.
2789947069;Multiple positive solutions for a system of impulsive integral boundary value problems with sign-changing nonlinearities;2018.0;[];   In this paper we investigate a system of impulsive integral boundary value problems with sign-changing nonlinearities. Using the fixed point theorem in double cones, we prove the existence of multiple positive solutions.
2790139232;Permutation Jaccard Distance-Based Hierarchical Clustering to Estimate EEG Network Density Modifications in MCI Subjects;2018.0;[];In this paper, a novel electroencephalographic (EEG)-based method is introduced for the quantification of brain-electrical connectivity changes over a longitudinal evaluation of mild cognitive impaired (MCI) subjects. In the proposed method, a dissimilarity matrix is constructed by estimating the coupling strength between every pair of EEG signals, Hierarchical clustering is then applied to group the related electrodes according to the dissimilarity estimated on pairs of EEG recordings. Subsequently, the connectivity density of the electrodes network is calculated. The technique was tested over two different coupling strength descriptors: wavelet coherence (WC) and permutation Jaccard distance (PJD), a novel metric of coupling strength between time series introduced in this paper. Twenty-five MCI patients were enrolled within a follow-up program that consisted of two successive evaluations, at time T0 and at time T1, three months later. At T1, four subjects were diagnosed to have converted to Alzheimer’s Disease (AD). When applying the PJD-based method, the converted patients exhibited a significantly increased PJD (   $p   ), i.e., a reduced overall coupling strength, specifically in delta and theta bands and in the overall range (0.5–32 Hz). In addition, in contrast to stable MCI patients, converted patients exhibited a network density reduction in every subband (delta, theta, alpha, and beta). When WC was used as coupling strength descriptor, the method resulted in a less sensitive and specific outcome. The proposed method, mixing nonlinear analysis to a machine learning approach, appears to provide an objective evaluation of the connectivity density modifications associated to the MCI-AD conversion, just processing noninvasive EEG signals.
2790149985;Analysis and Numerical Simulations of a Stochastic SEIQR Epidemic System with Quarantine-Adjusted Incidence and Imperfect Vaccination;2018.0;[];This paper considers a high-dimensional stochastic SEIQR (susceptible-exposed-infected-quarantined-recovered) epidemic model with quarantine-adjusted incidence and the imperfect vaccination. The main aim of this study is to investigate stochastic effects on the SEIQR epidemic model and obtain its thresholds. We first obtain the sufficient condition for extinction of the disease of the stochastic system. Then, by using the theory of Hasminskii and the Lyapunov analysis methods, we show there is a unique stationary distribution of the stochastic system and it has an ergodic property, which means the infectious disease is prevalent. This implies that the stochastic disturbance is conducive to epidemic diseases control. At last, computer numerical simulations are carried out to illustrate our theoretical results.
2790179491;Finite time blow-up for a class of parabolic or pseudo-parabolic equations;2018.0;[];"   In this paper, we study the initial boundary value problem for a class of parabolic or pseudo-parabolic equations:        u    t    −  a  Δ    u    t    −  Δ  u  +  b  u  =  k   (  t  )   |  u    |    p  −  2    u  ,     (  x  ,  t  )   ∈  Ω  ×   (  0  ,  T  )   ,     where    a  ≥  0   ,    b  u003e  −    l    1      with      l    1      being the principal eigenvalue for    −  Δ    on      H    0    1     (  Ω  )     and    k   (  t  )   u003e  0   . By using the potential well method, Levine’s concavity method and some differential inequality techniques, we obtain the finite time blow-up results provided that the initial energy satisfies three conditions: (i)    J   (    u    0    ;  0  )     0   ; (ii)    J   (    u    0    ;  0  )   ≤  d   (  ∞  )    , where    d   (  ∞  )     is a nonnegative constant; (iii)    0    J   (    u    0    ;  0  )   ≤  C  ρ   (  0  )    , where    ρ   (  0  )     involves the      L    2     -norm or      H    0    1     -norm of the initial data. We also establish the lower and upper bounds for the blow-up time. In particular, we obtain the existence of certain solutions blowing up in finite time with initial data at the Nehari manifold or at arbitrary energy level."
2790318515;Computing the Matrix Mittag-Leffler Function with Applications to Fractional Calculus;2018.0;[];"The computation of the Mittag-Leffler (ML) function with matrix arguments, and some applications in fractional calculus, are discussed. In general the evaluation of a scalar function in matrix arguments may require the computation of derivatives of possible high order depending on the matrix spectrum. Regarding the ML function, the numerical computation of its derivatives of arbitrary order is a completely unexplored topic; in this paper we address this issue and three different methods are tailored and investigated. The methods are combined together with an original derivatives balancing technique in order to devise an algorithm capable of providing high accuracy. The conditioning of the evaluation of matrix ML functions is also studied. The numerical experiments presented in the paper show that the proposed algorithm provides high accuracy, very often close to the machine precision."
2790715562;A Score Function-Based Method of Forecasting Using Intuitionistic Fuzzy Time Series;2018.0;[];Intuitionistic fuzzy set plays a vital role in data analysis and decision-making problems. In this paper, we propose an enhanced and versatile method of forecasting using the concept of intuitionistic fuzzy time series (FTS) based on their score function. The developed method has been presented in the form of simple computational steps of forecasting instead of complicated max–min compositions operator of intuitionistic fuzzy sets to compute the relational matrix R. Also, the proposed method is based on the maximum score and minimum accuracy function of intuitionistic fuzzy numbers (IFNs) to fuzzify the historical time series data. Further intuitionistic fuzzy logical relationship groups are defined and also provide a forecasted value and lies in an interval and is more appropriate rather than a crisp value. Furthermore, the proposed method has been implemented on the historical student enrollments data of University of Alabama and obtains the forecasted values which have been compared with the existing methods to show its superiority. The suitability of the proposed model has also been examined to forecast the movement of share market price of State Bank of India (SBI) at Bombay Stock Exchange (BSE). The results of the comparison of MSE and MAPE indicate that the proposed method produces more accurate forecasting results.
2791295466;Three-Dimensional Attention-Based Deep Ranking Model for Video Highlight Detection;2018.0;[];The video highlight detection task is to localize key elements (moments of useru0027s major or special interest) in a video. Most of existing highlight detection approaches extract features from the video segment as a whole without considering the difference of local features both temporally and spatially. Due to the complexity of video content, this kind of mixed features will impact the final highlight prediction. In temporal extent, not all frames are worth watching because some of them only contain the background of the environment without human or other moving objects. In spatial extent, it is similar that not all regions in each frame are highlights especially when there are lots of clutters in the background. To solve the above problem, we propose a novel three-dimensional (3-D) (spatial+temporal) attention model that can automatically localize the key elements in a video without any extra supervised annotations. Specifically, the proposed attention model produces attention weights of local regions along both the spatial and temporal dimensions of the video segment. The regions of key elements in the video will be strengthened with large weights. Thus, the more effective feature of the video segment is obtained to predict the highlight score. The proposed 3-D attention scheme can be easily integrated into a conventional end-to-end deep ranking model that aims to learn a deep neural network to compute the highlight score of each video segment. Extensive experimental results on the YouTube and SumMe datasets demonstrate that the proposed approach achieves significant improvement over state-of-the-art methods. With the proposed 3-D attention model, video highlights can be accurately retrieved in spatial and temporal dimensions without human supervision in several domains, such as gymnastics, parkour, skating, skiing, surfing, and dog activities, on the public datasets.
2791561716;A 16Gb/s/pin 8Gb GDDR6 DRAM with bandwidth extension techniques for high-speed applications;2018.0;[];"Recently the demand for high-bandwidth graphic DRAM, for game consoles and graphic cards, has dramatically increased due to the development of virtual reality, artificial intelligence, deep learning, autonomous driving cars, etc. These applications require greater data transfer speeds than pervious devices, GDDR5 [1] and GDDR5X [2], which are limited to 12Gb/s/pin. This paper introduces an 8Gb GDDR6 operating at up to 16Gb/s/pin. To exceed the prior speed limit various bandwidth extension techniques are proposed. WCK is driven with a dividing scheme to overcome speed limitations and to reduce power consumption. In addition, a dual-band architecture with different types of nibble drivers is proposed in order to cover stability of CML-to-CMOS in all frequency regions; CML nibble is used for high-speed, while CMOS nibble is used for low-speed. A DC-split scheme is implemented for duty-cycle correction and skew compensation. The bandwidth of the high-frequency divider is extended by using a proposed mode-changed flip-flop. The receiver uses a loop-unrolled one-tap decision-feedback equalizer (DFE) designed to eliminate channel inter-symbol interference (ISI). A two-stage pre-amplifier is also used for bandwidth extension. The transmitter uses a 4:1 multiplexer using a half-rate sampler, where a 1UI pulse is unnecessary to minimize the full-rate operation. To secure on-chip signal transmission characteristic, the bandwidth limitation of transistor in a DRAM process is extended by adopting an on-chip feedback EQ filter."
2791677100;Exploring Human Mobility Patterns in Urban Scenarios: A Trajectory Data Perspective;2018.0;[];Smart cities have been recognized as a promising research focus around the world. To realize smart cities, computation and utilization of big data are key factors. More specifically, exploring the patterns of human mobility based on large amounts of multi-source data plays an important role in analyzing the formation of social-economic phenomena in smart cities. However, our acquired knowledge is still very limited for smart cities. In this article, we propose an integrated computing method to rescale heterogeneous traffic trajectory data, which leverages MLE and BIC. Our analysis is based on two real datasets generated by subway smart card transactions and taxi GPS trajectories from Shanghai, China, which contain more than 451 million trading records by 14 subway lines and 34 billion GPS records by 13,695 taxis. Specifically, we quantitatively explore the patterns of human mobility on weekends and weekdays. Through logarithmic binning and data fitness, we calculate the Bayesian weights to select the best fitting distributions. In addition, we leverage three metrics to analyze the patterns of human mobility in two datasets: trip displacement, trip duration, and trip interval. We obtain several important human mobility patterns and discover quite a few interesting phenomena, which lay a solid foundation for future research.
2791899797;A novel nature-inspired algorithm for optimization: Squirrel search algorithm;2019.0;[];   This paper presents a novel nature-inspired optimization paradigm, named as squirrel search algorithm (SSA). This optimizer imitates the dynamic foraging behaviour of southern flying squirrels and their efficient way of locomotion known as gliding. Gliding is an effective mechanism used by small mammals for travelling long distances. The present work mathematically models this behaviour to realize the process of optimization. The efficiency of the proposed SSA is evaluated using statistical analysis, convergence rate analysis, Wilcoxonu0027s test and ANOVA on classical as well as modern CEC 2014 benchmark functions. An extensive comparative study is carried out to exhibit the effectiveness of SSA over other well-known optimizers in terms of optimization accuracy and convergence rate. The proposed algorithm is implemented on a real-time Heat Flow Experiment to check its applicability and robustness. The results demonstrate that SSA provides more accurate solutions with high convergence rate as compared to other existing optimizers.
2791970612;The impact of an iPad-supported annotation and sharing technology on university students' learning;2018.0;[];   iPads, or more generally tablet computers, have received rapid and widespread uptake across higher education. Despite this, there is limited evidence of how their use affects student learning within this context. This study focuses on the use of a tablet by the instructor to support the annotation and in-class sharing of studentsu0027 work to create a collaborative learning environment within a first year undergraduate subject. This paper reports the results of an empirical study looking at the effect this tablet technology has on student performance using a sample of 741 first-year accounting students. The study uses data from enrolment and attendance records, end of semester examination results and student perceptions from a survey. Results indicate that class sharing of the instructoru0027s and studentsu0027 annotation of homework through the use of a tablet is associated with an improvement in student performance on procedural or equation-based questions as well as increased student engagement. However, contrary to expectations, the introduction of in class annotations was associated with a decline in student performance on theoretical, extended response questions. The authors argue that affordances of the tablet, when used in a student-centred way, can introduce a bias towards some kinds of interactions over others. This large-scale study of in-class tablet use suggests that though the tablets may be positively associated with student engagement and satisfaction, caution must be exercised in how the use by the instructor affects the classroom environment and what students learn. These findings have particular relevance to university learning contexts with equation-centric subjects such as those in Business and STEM.
2792234394;Learning Structural Node Embeddings via Diffusion Wavelets;2018.0;[];Nodes residing in different parts of a graph can have similar structural roles within their local network topology. The identification of such roles provides key insight into the organization of networks and can be used for a variety of machine learning tasks. However, learning structural representations of nodes is a challenging problem, and it has typically involved manually specifying and tailoring topological features for each node. In this paper, we develop GraphWave, a method that represents each nodeu0027s network neighborhood via a low-dimensional embedding by leveraging heat wavelet diffusion patterns. Instead of training on hand-selected features, GraphWave learns these embeddings in an unsupervised way. We mathematically prove that nodes with similar network neighborhoods will have similar GraphWave embeddings even though these nodes may reside in very different parts of the network, and our method scales linearly with the number of edges. Experiments in a variety of different settings demonstrate GraphWaveu0027s real-world potential for capturing structural roles in networks, and our approach outperforms existing state-of-the-art baselines in every experiment, by as much as 137%.
2792333172;DOA estimation of rectilinear signals with a partly calibrated uniform linear array;2018.0;[];This paper investigates the problem of direction-of-arrival (DOA) estimation of rectilinear or strictly second-order noncircular signals with a partly calibrated uniform linear array (ULA). Consider that the uncalibrated portion of the array suffers from unknown gains and phases, an extended data model corresponding to a virtual (extended) array is presented by taking the noncircularity of the signals into account. On this basis and given the signal subspace matrix associated with the virtual array, a linear equation is derived to determine the unknown gains and phases. Then, the DOAs are found through the eigenvalue decomposition of a matrix related to the signal subspace matrix and array gains and phases. Since spatial spectrum search is not required, the proposed method is computationally efficient. Moreover, it is able to handle at most 2Mâˆ’3 rectilinear signals with a partly calibrated ULA of M elements, even though only two neighboring elements of the array are calibrated. Numerical results also demonstrate that the proposed method has remarkable performance superiority brought by the rectilinearity.
2792446208;A 16Gb LPDDR4X SDRAM with an NBTI-tolerant circuit solution, an SWD PMOS GIDL reduction technique, an adaptive gear-down scheme and a metastable-free DQS aligner in a 10nm class DRAM process;2018.0;[];High-density and high-speed DRAM requirements have been ever-increasing to achieve a better user experience for mobile systems, by adopting QHD (2560×1440), and higher display resolutions, dual cameras, augmented reality, and advanced driver-assistance systems. LPDDR4X has been the hand-held and mobile memory of choice due to its high speed (5.0Gb/s/pin [1]) and low-power data retention ( CCDMW =32f CK ), however the area overhead (6.25%), due to the additional parity arrays for a (136, 128) single-error-correction code [4], is currently limiting for mass production in terms of chip cost. This overhead can be mitigated by adopting a scaled technology node that enables a smaller chip size as well as better retention time due to ECC. This paper presents several circuit techniques to maintain LPDDR4Xu0027s high speed and low power in a 10nm class process, thereby enabling a cost-effective DRAM design with inDRAM ECC: using (1) an NBTI-tolerant circuit solution that covers whole high-speed circuit regions, (2) a sub-WL driver (SWD) PMOS GIDL-reduction technique ensures stable power recovery, (3) an adaptive IO buffer current gear-down scheme based on user-scenarios, and (4) a metastable-free DQS aligner. Figure 12.2.1 shows the top-level block diagram of the 8Gb/1channel macro, with an in-DRAM ECC using a (136, 128) single-error-correction code, similar to that of previous 20nm designs [2-4].
2793022729;DeepLink: A Deep Learning Approach for User Identity Linkage;2018.0;[];The typical aim of User Identity Linkage (UIL) is to detect when users from across different social platforms are actually one and the same individual. Existing efforts to address this problem of practical relevance span from user-profile-based, through user-generated-content-based, user-behavior-based approaches to supervised or unsupervised learning frameworks, to subspace learning-based models. Most of them often require extraction of relevant features (e.g., profile, location, biography, networks, behavior, etc.) to model the user consistently across different social networks. However, these features are mainly derived based on prior knowledge and may vary for different platforms and applications. Inspired by the recent successes of deep learning in different tasks, especially in automatic feature extraction and representation, we propose a deep neural network based algorithm for UIL, called DeepLink. It is a novel end-to-end approach in a semi-supervised learning manner, without involving any hand-crafting features. Specifically, DeepLink samples the networks and learns to encode network nodes into vector representation to capture local and global network structures which, in turn, can be used to align anchor nodes through deep neural networks. A dual learning based paradigm is exploited to learn how to transfer knowledge and update the linkage using the policy gradient method. Experiments conducted on several public datasets show that DeepLink outperforms the state-of-the-art methods in terms of both linking precision and identity-match ranking.
2793029474;Comparison of multi-objective evolutionary algorithms in hybrid Kansei engineering system for product form design;2018.0;[];"   Understanding the affective needs of customers is crucial to the success of product design. Hybrid Kansei engineering system (HKES) is an expert system capable of generating products in accordance with the affective responses. HKES consists of two subsystems: forward Kansei engineering system (FKES) and backward Kansei engineering system (BKES). In previous studies, HKES was based primarily on single-objective optimization, such that only one optimal design was obtained in a given simulation run. The use of multi-objective evolutionary algorithm (MOEA) in HKES was only attempted using the non-dominated sorting genetic algorithm-II (NSGA-II), such that very little work has been conducted to compare different MOEAs. In this paper, we propose an approach to HKES combining the methodologies of support vector regression (SVR) and MOEAs. In BKES, we constructed predictive models using SVR. In FKES, optimal design alternatives were generated using MOEAs. Representative designs were obtained using fuzzy c-means algorithm for clustering the Pareto front into groups. To enable comparison, we employed three typical MOEAs: NSGA-II, the Pareto envelope-based selection algorithm-II (PESA-II), and the strength Pareto evolutionary algorithm-2 (SPEA2). A case study of vase form design was provided to demonstrate the proposed approach. Our results suggest that NSGA-II has good convergence performance and hybrid performance; in contrast, SPEA2 provides the strong diversity required by designers. The proposed HKES is applicable to a wide variety of product design problems, while providing creative design ideas through the exploration of numerous Pareto optimal solutions."
2793439489;A 16Gb 18Gb/S/pin GDDR6 DRAM with per-bit trainable single-ended DFE and PLL-less clocking;2018.0;[];"Starting at 512Mb 6Gb/s/pin [1], GDDR5u0027s speed and density have been steadily developing for about 10 years; recently achieving 8Gb 9Gb/s/pin [2] with per-pin timing training. Although 8Gb GDDR5X can operate at 12Gb/s [3] by increasing the burst length (BL) from 8 to 16, a degradation in system performance at a data granularity of 64B is seen. The I/O specification, using PLL clocking that additionally causes PLL jitter, has not changed much compared with GDDR5. To overcome these issues, GDDR6 introduced a dual channel for a data granularity of 32B with a BL16, per-bit training of l/ REF , and an equalizer with PLL-less clocking. This paper presents a 16Gb 18Gb/s/pin GDDR6 DRAM with a die architecture and high-speed circuit techniques on 1.35V DRAM process."
2793538957;Recent optimization models and trends in location, relocation, and dispatching of emergency medical vehicles;2019.0;[];   Over the past 10 years, a considerable amount of research has been devoted to the development of models to support decision making in the particular yet important context of Emergency Medical Services (EMS). More specifically, the need for advanced strategies to take into account the uncertainty and dynamism inherent to EMS, as well as the pertinence of socially oriented objectives, such as equity, and patient medical outcomes, have brought new and exciting challenges to the field. In this context, this paper summarizes and discusses modern modeling approaches to address problems related to ambulance fleet management, particularly those related to vehicle location and relocation, as well as dispatching decisions. Although it reviews early works on static ambulance location problems, this review concentrates on recent approaches to address tactical and operational decisions, and the interaction between these two types of decisions. Finally, it concludes on the current state of the art and identifies promising research avenues in the field.
2793935950;Learning Semantics of Gestural Instructions for Human-Robot Collaboration;2018.0;[];Designed to work safely alongside humans, collaborative robots need to be capable partners in human-robot teams. Besides having key capabilities like detecting gestures, recognizing objects, grasping them, and handing them over, these robots need to seamlessly adapt their behaviour for efficient human-robot collaboration. In this context we present the fast, supervised Proactive Incremental Learning (PIL) framework for learning associations between human hand gestures and the intended robotic manipulation actions. With the proactive aspect, the robot is competent to predict the human’s intent and perform an action without waiting for an instruction. The incremental aspect enables the robot to learn associations on the fly while performing a task. It is a probabilistic, statistically-driven approach. As a proof of concept, we focus on a table assembly task where the robot assists its human partner. We investigate how the accuracy of gesture detection affects the number of interactions required to complete the task. We also conducted a human-robot interaction study with non-roboticist users comparing a proactive with a reactive robot that waits for instructions.
2794083222;A synergy of the sine-cosine algorithm and particle swarm optimizer for improved global optimization and object tracking;2018.0;[];   Due to its simplicity and efficiency, a recently proposed optimization algorithm, Sine Cosine Algorithm (SCA), has gained the interest of researchers from various fields for solving optimization problems. However, it is prone to premature convergence at local minima as it lacks internal memory. To overcome this drawback, a novel Hybrid SCA-PSO algorithm for solving optimization problems and object tracking is proposed. The      P   b  e  s  t       and      G   b  e  s  t       components of PSO (Particle Swarm Optimization) is added to traditional SCA to guide the search process for potential candidate solutions and PSO is then initialized with      P   b  e  s  t       of SCA to exploit the search space further. The proposed algorithm combines the exploitation capability of PSO and exploration capability of SCA to achieve optimal global solutions. The effectiveness of this algorithm is evaluated using 23 classical, CEC 2005 and CEC 2014 benchmark functions. Statistical parameters are employed to observe the efficiency of the Hybrid SCA-PSO qualitatively and results prove that the proposed algorithm is very competitive compared to the state-of-the-art metaheuristic algorithms. The Hybrid SCA-PSO algorithm is applied for object tracking as a real thought-provoking case study. Experimental results show that the Hybrid SCA-PSO-based tracker can robustly track an arbitrary target in various challenging conditions. To reveal the capability of the proposed algorithm, comparative studies of tracking accuracy and speed of the Hybrid SCA-PSO based tracking framework and other trackers, viz., Particle filter, Mean-shift, Particle swarm optimization, Bat algorithm, Sine Cosine Algorithm (SCA) and Hybrid Gravitational Search Algorithm (HGSA) is presented.
2794240319;A Trust Reputation Architecture for Cloud Computing Environment;2017.0;[];Cloud computing provides computational resources (processing, storage, software, network) to users in a scalable and dynamic way over the Internet. The use of these resources are impacted by issues related to privacy, security and trust. In this sense, this paper presents a trust reputation architecture applied in the cloud computing environment. The reputed trust is based on two trust indicators: objective and subjective ones. The objective trust indicator is based on the historical QoS indicators of a cloud provider. Then, the subjective trust indicator is composed of the usersu0027 feedbacks to the cloud providers. In order to evaluate the proposed architecture, simulations were performed using a P2P Network Simulator in order to emulate cloud providers and their clients. The evaluation results show the architecture applicability with low overhead.
2795152721;Online multi-layer dictionary pair learning for visual classification;2018.0;[];   Classifier training plays an important role in image classification, while a good classifier could more effectively exploit the discriminative information of input features to separate the difficult samples. Inspired by the recent advance of representation based classifiers and the success of multi-layer architectures in visual recognition, we propose a multi-layer dictionary pair learning based classifier to enhance the image classification performance. With the multi-layer structure and a nonlinear feature transform in each layer, the proposed classifier learning model could accumulate stronger discrimination capability than the previous single-layer representation based classifiers. Furthermore, to make our learning model applicable to datasets with a larger amount of samples, we propose an online training algorithm which updates model parameters with data batches. The so-called online multi-layer dictionary pair learning (OMDPL) method is evaluated on benchmark image classification datasets. With the same input features, OMDPL exhibits better classification performance than other popular classifiers.
2795726520;Deep variance network: An iterative, improved CNN framework for unbalanced training datasets;2018.0;[];   Convolutional neural network (CNN) has demonstrated its superior ability to achieve amazing accuracy in computer vision field. Nevertheless, for practical domain-specific image recognition tasks, it still remains difficult to obtain massive high-quality labeled datasets due to the strong requirements for extensive, tedious manual processing. Inspired by the well-known observation that human brain can accurately recognize objects without relying on massive congeneric examples, we propose a novel deep variance network (DVN) to further enhance the generalization ability of CNN in this paper, which could still produce higher recognition accuracy even with unbalanced training datasets than original CNN. The key idea of our DVN is built upon the intrinsic exploitation of inter-class homogeneity and intra-class heterogeneity. Towards such goal, we make the first attempt to incorporate a hierarchical Bayesian model into the powerful CNN framework, which can transfer the joint feature distribution from certain object’s complete training dataset to other object’s incomplete training dataset in an iterative way. In each training cycle, the CNN-resulted features are clustered into discrimination-related subspaces to guide the learning and adaptive adjustment of homogeneity and heterogeneity over unbalanced training datasets. In practice, we furnish several state-of-the-art deep networks with our proposed DVN, and conduct extensive experiments and comprehensive evaluations over CIFAR-10, MNIST, and SVHN benchmarks. The experiments have shown that, most of the furnished deep networks can benefit from our DVN, wherein they gain at most 6.9% accuracy improvement over CIFAR-10 benchmark, 52.83% error reduction over MNIST benchmark, and an improvement of 6.2% over SVHN datasets.
2795900623;Wrist-worn hand gesture recognition based on barometric pressure sensing;2018.0;[];Hand gestures are expressive motions that convey meaningful information. The ability for machines to extract and process the underlying meanings of these gestures is critical to many human-interactive applications. Various methods have been proposed, but the development of a more accurate, and simpler system could enable the machine and its user to exchange useful information more effectively. In this paper, a barometric-pressure-sensor-based wristband is presented as an initial proof of such concept. The wristband is composed of an array of 10 barometric pressure sensors spaced evenly around the wrist to estimate pressure profiles as tendons and muscles change with various hand gestures. Subject testing was performed to quantify classification accuracy for three groups of hand gestures: group 1) six wrist gestures, group 2) five single finger flexions, and group 3) ten Chinese number gestures. Leave-one-out cross-validation was used to compute classification accuracy. Results demonstrated classification accuracies of 98% for the wrist gestures, 95% for the single finger flexions, and 90% for Chinese number gestures. The presented pressure sensing wristband could potentially be used for a variety of applications including gesture-controlled devices, health-monitoring devices, and assistive devices for deaf-mute individuals.
2796146433;A learning approach to enhance assurances for real-time self-adaptive systems;2018.0;[];The assurance of real-time properties is prone to context variability. Providing such assurance at design time would require to check all the possible context and system variations or to predict which one will be actually used. Both cases are not viable in practice since there are too many possibilities to foresee. Moreover, the knowledge required to fully provide the assurance for self-adaptive systems is only available at runtime and therefore difficult to predict at early development stages. Despite all the efforts on assurances for self-adaptive systems at design or runtime, there is still a gap on verifying and validating real-time constraints accounting for context variability. To fill this gap, we propose a method to provide assurance of self-adaptive systems, at design- and runtime, with special focus on real-time constraints. We combine off-line requirements elicitation and model checking with on-line data collection and data mining to guarantee the systemu0027s goals, both functional and non-functional, with fine tuning of the adaptation policies towards the optimization of quality attributes. We experimentally evaluate our method on a simulated prototype of a Body Sensor Network system (BSN) implemented in OpenDaVINCI. The results of the validation are promising and show that our method is effective in providing evidence that support the provision of assurance.
2796494547;Efficient approximation of functions of some large matrices by partial fraction expansions;2019.0;[];ABSTRACTSome important applicative problems require the evaluation of functions Ψ of large and sparse and/or localized matrices A. Popular and interesting techniques for computing Ψ(A) and Ψ(A)v, w...
2796547704;Multiple-Attribute Decision-Making Based on Archimedean Bonferroni Operators of q -Rung Orthopair Fuzzy Numbers;2019.0;[];The theory of   $q$  -rung orthopair fuzzy sets (  $q$  -ROFSs) proposed by Yager effectively describes fuzzy information in the real world. Because   $q$  -ROFSs contain the parameter   $q$   and can adjust the range of expressed fuzzy information, they are superior to both intuitionistic and Pythagorean fuzzy sets. Archimedean T-norm and T-conorm (ATT) is an important tool used to generate operational rules based on the  q -rung orthopair fuzzy numbers (  $q$  -ROFNs). In comparison, the Bonferroni mean (BM) operator has an advantage because it considers the interrelationships between the different attributes. Therefore, it is an important and meaningful innovation to extend the BM operator to the   $q$  -ROFNs based upon the ATT. In this paper, we first discuss   $q$  -rung orthopair fuzzy operational rules by using ATT. Furthermore, we extend BM operator to the   $q$  -ROFNs and propose the   $q$  -rung orthopair fuzzy Archimedean BM   $(q\hbox{-}{ROFABM})$   operator and the  q -rung orthopair fuzzy weighted Archimedean BM   $(q\hbox{-}{ROFWABM})$   operator and study their desirable properties. Then, a new multiple-attribute decision-making (MADM) method is developed based on   $q\hbox{-}{ROFWABM}$   operator. Finally, we use a practical example to verify effectiveness and superiority by comparing to other existing methods.
2797681513;Real-Time Ambulance Dispatching and Relocation;2018.0;[];In this study, we develop a flexible optimization framework for real-time ambulance dispatching and relocation. In addition to ambulance redeployment, we consider a general dispatching and relocation strategy by which the decision maker has the option to (i) select any available ambulance to dispatch to a call or to queue the call and (ii) send an idle ambulance to cover the location of an ambulance just dispatched to a call. We formulate the problem as a stochastic dynamic program, and, because the state space is unbounded, an approximate dynamic programming (ADP) framework is developed to generate high-quality solutions. We assess the quality of our solutions by developing a lower bound on the expected response time and computing a lower bound on the expected fraction of late calls of any relocation policy. We test the performance of our policies and available benchmarks on an emergency medical services system in Mecklenburg County, North Carolina. The results show that our policies are near optimal and...
2798471822;A Structural Model for Students' Adoption of Learning Management Systems: An Empirical Investigation in the Higher Education Context;2018.0;[];
2798754355;Disconnected Recurrent Neural Networks for Text Categorization;2018.0;[];
2798937584;PlusEmo2Vec at SemEval-2018 Task 1: Exploiting emotion knowledge from emoji and #hashtags;2018.0;[];This paper describes our system that has been submitted to SemEval-2018 Task 1: Affect in Tweets (AIT) to solve five subtasks. We focus on modeling both sentence and word level representations of emotion inside texts through large distantly labeled corpora with emojis and hashtags. We transfer the emotional knowledge by exploiting neural network models as feature extractors and use these representations for traditional machine learning models such as support vector regression (SVR) and logistic regression to solve the competition tasks. Our system is placed among the Top3 for all subtasks we participated.
2800557164;Comparing trained and untrained teachers on their use of LMS tools using the Rasch analysis;2018.0;[];   Measuring training outcomes is important given the resources universities invest in staff learning management system (LMS) training. In this paper we show how the effect of LMS training on LMS usage can be evaluated using Rasch analysis of teachersu0027 LMS usage activity logs by making comparisons between teachers who have attended training and those who have not. Our analysis showed that teachers who attended LMS training workshops had higher LMS activity level compared with the untrained teachers. In particular, trained teachers tended to make relatively more use of ‘grade centre’ and of ‘assessment tool’ but relatively less use of ‘content’ in their teaching compared with teachers who did not attend training. The results support Rasch analysis as a useful approach for evaluating the effect of training across a large number of courses and extend understanding from findings from self-report studies of training effectiveness. They also provide insights that inform training practice and highlight its importance for development of online teaching. Implications for professional development of online teaching and the evaluation are discussed.
2800577149;Evolving dynamic fitness measures for genetic programming;2018.0;[];   This research builds on the hypothesis that the use of different fitness measures on the different generations of genetic programming (GP) is more effective than the convention of applying the same fitness measure individually throughout GP. Whereas the previous study used a genetic algorithm (GA) to induce the sequence in which fitness measures should be applied over the GP generations, this research uses a meta- (or high-level) GP to evolve a combination of the fitness measures for the low-level GP. The study finds that the meta-GP is the preferred approach to generating dynamic fitness measures. GP systems applying the generated dynamic fitness measures consistently outperform the previous approach, as well as standard GP on benchmark and real world problems. Furthermore, the generated dynamic fitness measures are shown to be reusable, whereby they can be used to solve unseen problems to optimality.
2801297575;Joint routing and aborting optimization of cooperative unmanned aerial vehicles;2018.0;[];   This paper incorporates the abort policy into the routing problem of unmanned aerial vehicles (UAV). In order to serve a number of targets, some UAVs can be deployed each visiting part of the targets. Different from other works on routing of UAVs, it is assumed that each UAV may experience shocks during the travel. In order to reduce the expected cost of UAV destruction, it is allowed that a UAV aborts the mission if it is found to have undergone too many shocks after it finishes serving a certain number of targets. The optimal routing plan together with the abort policy for each UAV are studied, with the objective to minimize the total cost consisting of the expected cost of UAV destruction and the expected cost for unvisited targets. Test case is used to illustrate the application of the framework.
2801904685;Stochastic Periodic Solution of a Susceptible-Infective Epidemic Model in a Polluted Environment under Environmental Fluctuation;2018.0;[];"It is well known that the pollution and environmental fluctuations may seriously affect the outbreak of infectious diseases (e.g., measles). Therefore, understanding the association between the periodic outbreak of an infectious disease and noise and pollution still needs further development. Here we consider a stochastic susceptible-infective (SI) epidemic model in a polluted environment, which incorporates both environmental fluctuations as well as pollution. First, the existence of the global positive solution is discussed. Thereafter, the sufficient conditions for the nontrivial stochastic periodic solution and the boundary periodic solution of disease extinction are derived, respectively. Numerical simulation is also conducted in order to support the theoretical results. Our study shows that (i) large intensity noise may help the control of periodic outbreak of infectious disease; (ii) pollution may significantly affect the peak level of infective population and cause adverse health effects on the exposed population. These results can help increase the understanding of periodic outbreak patterns of infectious diseases."
2802084755;The existence and nonexistence of entire large solutions for a quasilinear Schrödinger elliptic system by dual approach;2018.0;[];   In this paper, we establish some new results on the existence and nonexistence of radial large positive solutions for a modified Schrodinger system with a nonconvex diffusion term by a successive iteration technique and the dual approach. The necessary and sufficient condition for the existence of radial large positive solutions is established. Our results improve and extend many previous work in this field of research.
2802583537;Brownian Motus and Clustered Binary Insertion Sort methods: An efficient progress over traditional methods;2018.0;[];"   Sorting is the basic operation in every application of computer science. The paper proposes two novel sorting algorithms based on the concept of traditional Insertion Sort (IS). Firstly, Brownian Motus Insertion Sort (BMIS) based on IS is proposed. It is followed by Clustered Binary Insertion Sort (CBIS) based on the principles of Binary Insertion Sort (BIS). BIS is a binary search enhancement of IS which is a quite famous variant of it. Average case time complexity of BMIS is    O   (    n    0  .  54    )    ; and that of CBIS is    O   (  n  log  n  )    . The scenario which results into the worst case of IS is with the complexity of    O   (    n    2    )    ; and BIS with    O   (  n  log  n  )     is the best case scenario for BMIS and CBIS with complexity of    O   (  n  )    . The probability of getting a worst case scenario for BMIS and CBIS is approximately zero. Comparison of proposed algorithms with IS and BIS has been performed at    25  %  ,  50  %  ,  75  %      and     100  %    level of randomness in the initial dataset. These results lead to prove our claim of devising efficient enhancements of IS. The results further reveal that performance of BMIS and CBIS will increase with a decrease in randomness level of the dataset in comparison to its counterparts. The number of comparisons required by BMIS and CBIS will approach to    O   (  n  )     with randomness level approach to zero. So, for nearly sorted datasets, our proposed BMIS and CBIS are the best choice. Both BMIS and CBIS are  in-place ,  stable  and  online sorting  algorithm."
2802745231;Multi-attribute group decision making based on extended TOPSIS method under interval-valued intuitionistic fuzzy environment;2018.0;[];   In this paper, we propose a novel multi-attribute group decision making (MAGDM) method under interval-valued intuitionistic fuzzy environment by integrating extended TOPSIS and linear programming methods. We assume that multiple decision makers provide the input information, namely the assessment values and attribute weights using interval-valued intuitionistic fuzzy (IVIF) values for incorporating inherent uncertainty in the MAGDM process. Furthermore, the weights of decision makers given by experts to describe their importance in group decision making are assumed as IVIF values. The advantage and disadvantage scores are employed to determine the individual measure of importance for each decision maker. Aggregation of the assessment values and attributes weights are performed using two different IVIF aggregation operators. A weighted similarity measure, based upon the optimal attributes weights obtained through linear programming method, is defined for determining relative closeness coefficients for selection of the most preferred alternative. A real-world case study is provided to illustrate the working of the proposed methodology. Moreover, a thorough comparison has been done with related existing works in order to show the advantages of the methodology.
2802989963;Towards a Reuse Strategic Decision Pattern Framework – from Theories to Practices;2019.0;[];This paper demonstrates our proposed Reuse Strategic Decision Pattern Framework (RSDPF) based on blending ANP and TOPSIS techniques, enabled by the OSM model with data analytics. The motivation, related work, theory, the use and deployment, and the service deployment of the framework have been discussed in details. In this paper, RSDPF framework is demonstrated by the data analysis and interpretations based on a financial service firm. The OSM model allows 3 step of processed to be performed in one go to perform statistical tests, identify linear relations, check consistency on dataset and calculate OLS regression. The aim is to identify the actual, expected and risk rates of profitability. Code and services can be reused to compute for analysis. Service integration of the RSDPF framework has been demonstrated. Results confirm that there is a high extent of reliability. In this paper, we have demonstrated the reuse and integration of the framework supported by the case study of the financial service firm with its data analysis and service to justify our research contributions – reuse and integration in statistical data mining, knowledge and heuristic discovery and finally domain transference.
2803001434;A new trapezoidal Pythagorean fuzzy linguistic entropic combined ordered weighted averaging operator and its application for enterprise location;2018.0;[];
2803360343;A novel atom search optimization for dispersion coefficient estimation in groundwater;2019.0;[];   A new type of meta-heuristic global optimization methodology based on atom dynamics is introduced. The proposed Atom Search Optimization (ASO) approach is a population-based iterative heuristic global optimization algorithm for dealing with a diverse set of optimization problems. ASO mathematically models and mimics the atomic motion model in nature, where atoms interact with each other through interaction forces resulting form Lennard-Jones potential and constraint forces resulting from bond-length potential, the algorithm is simple and easy to implement. ASO is applied to a dispersion coefficient estimation problem, the experimental results demonstrate that ASO can outperform other well-known approaches such as Particle Swarm Optimization (PSO), Genetic Algorithm (GA) and Bacterial Foraging Optimization (BFO) and that ASO is competitive to its competitors for parameter estimation problems. The source codes of ASO are available at  https://www.mathworks.com/matlabcentral/fileexchange/67011-atom-search-optimization--aso--algorithm?s_tid=srchtitle .
2803620531;Describing Video With Attention-Based Bidirectional LSTM;2019.0;[];Video captioning has been attracting broad research attention in the multimedia community. However, most existing approaches heavily rely on static visual information or partially capture the local temporal knowledge (e.g., within 16 frames), thus hardly describing motions accurately from a global view. In this paper, we propose a novel video captioning framework, which integrates bidirectional long-short term memory (BiLSTM) and a soft attention mechanism to generate better global representations for videos as well as enhance the recognition of lasting motions in videos. To generate video captions, we exploit another long-short term memory as a decoder to fully explore global contextual information. The benefits of our proposed method are two fold: 1) the BiLSTM structure comprehensively preserves global temporal and visual information and 2) the soft attention mechanism enables a language decoder to recognize and focus on principle targets from the complex content. We verify the effectiveness of our proposed video captioning framework on two widely used benchmarks, that is, microsoft video description corpus and MSR-video to text, and the experimental results demonstrate the superiority of the proposed approach compared to several state-of-the-art methods.
2803680438;A novel induced aggregation method for intuitionistic fuzzy set and its application in multiple attribute group decision making;2018.0;[];
2803696563;A novel image segmentation method based on fast density clustering algorithm;2018.0;[];   Image segmentation is one of the key technologies for image processing. Most image segmentation methods based on clustering algorithms encountered with challenges including cluster center sensitivity, parameter dependence, low self-adaptability and cluster center determination difficulty. Accordingly, a novel image segmentation method based on fast density clustering algorithm (IS-FDC) is proposed in this paper. Pixel similarity is calculated on basis of both pixel value and its position information. IS-FDC is based on fast density clustering algorithm (FDC), in which cluster centers could be determined automatically by multiple linear regression analysis, and the only sensible parameter confidence interval is self-adaptive based on cuckoo search algorithm (CS). Currently density based clustering algorithms applied to image segmentation have difficulties, especially for its high time complexity and memory complexity. Parallel partition and scaling strategies are put forward to speed up clustering process. Multiple images from Berkeley dataset are adopted for simulations and analysis. IS-FDC is compared with several outstanding algorithms based on several evaluation indexes including both supervised and unsupervised algorithm indexes to testify that the proposed IS-FDC is outperformed. Abundant experimental results proved that IS-FDC is robust to parameters, which can automatically determine the number of segmentation and improve the accuracy of segmentation effectively.
2803870155;Behavioural intentions of using virtual reality in learning: perspectives of acceptance of information technology and learning style;2019.0;[];The use of virtual reality (VR) has become a viable alternative to conventional learning methods in various knowledge domains. Wearable head-mounted displays (HMDs) are devices that provide users with an immersive VR experience. To investigate the direct determinants affecting students’ reasons for HMD use in learning, hypotheses relating to information technology acceptance and Kolb’s learning styles were proposed and tested in this study. Participants were recruited through stratified random sampling according to the population ratio of colleges at a university in Taiwan. Students were shown a video on VR applications in learning, after which an online survey was completed. In total, 387 questionnaires were collected of which 376 were valid. An inference analysis of the samples was performed by structural equation modelling with eight exogenous latent variables, namely the four constructs of the unified theory of acceptance and use of technology (UTAUT) and the four modes of Kolb’s learning styles. All eight variables pointed to one endogenous latent variable: behavioural intention. The results showed all four constructs of the UTAUT to have a positive and significant effect on students’ behavioural intention to use HMDs in learning and only the concrete experience mode of Kolb’s learning styles to have a positive and significant effect. Based on these findings, this study provides suggestions on how to encourage HMDs use in learning to VR developers and educational institutions.
2804191162;Multi-objective parallel robotic dispensing planogram optimisation using association rule mining and evolutionary algorithms;2018.0;[];ABSTRACTThis research addresses a medication planogram optimisation problem for robotic dispensing systems (RDSs) in mail-order pharmacy automation (MOPA) facilities. A MOPA is used by a high-throughput fulfilment facility that processes a large volume of prescription orders. In MOPA facilities, each RDS unit integrates auto-dispenser devices and a robot arm to count and dispense medications automatically to complete high demand. An RDS planogram is the allocation of medications in one RDS unit and their distribution in different RDS units. A significant challenge in MOPA systems is to design an efficient planogram strategy. In this study, the RDS planogram is optimised to meet three objectives: association between medications, workload balance of RDSs, and robot arm travel distance. Association rule mining (ARM) is applied to explore the associations between medications, whereas a nonlinear mixed-integer programming (MIP) model is developed to optimise medication allocation based on ARM outputs. Four evo...
2804426336;PEA: Parallel electrocardiogram-based authentication for smart healthcare systems;2018.0;[];   Currently, ECG-based authentication is considered highly promising in terms of user identification for smart healthcare systems because of its inimitability, suitability, accessibility and comfortability. However, it is a great challenge to improve the authentication accuracy, especially for scenarios that include a large number of users. Thus, this paper proposes a parallel ECG-based authentication called PEA. Specifically, this paper proposes a hybrid ECG feature extraction method that integrated fiducial- and non-fiducial-based features to extract more comprehensive ECG features and thereby improve the authentication stability. Furthermore, this paper proposes a parallel ECG pattern recognition framework to improve the recognition efficiency in multiple ECG feature spaces. Through the experiments, the performance of the proposed authentication is verified.
2804627268;Faster individual discrete logarithms in finite fields of composite extension degree;2018.0;[];Computing discrete logarithms in finite fields is a main concern in cryptography. The best algorithms in large and medium characteristic fields (e.g., {GF}$(p^2)$, {GF}$(p^{12})$) are the Number Field Sieve and its variants (special, high-degree, tower). The best algorithms in small characteristic finite fields (e.g., {GF}$(3^{6 \cdot 509})$) are the Function Field Sieve, Jouxu0027s algorithm, and the quasipolynomial-time algorithm. The last step of this family of algorithms is the individual logarithm computation. It computes a smooth decomposition of a given target in two phases: an initial splitting, then a descent tree. While new improvements have been made to reduce the complexity of the dominating relation collection and linear algebra steps, resulting in a smaller factor basis (database of known logarithms of small elements), the last step remains at the same level of difficulty. Indeed, we have to find a smooth decomposition of a typically large element in the finite field. This work improves the initial splitting phase and applies to any nonprime finite field. It is very efficient when the extension degree is composite. It exploits the proper subfields, resulting in a much more smooth decomposition of the target. This leads to a new trade-off between the initial splitting step and the descent step in small characteristic. Moreover it reduces the width and the height of the subsequent descent tree.
2805182940;An Edge-Based Architecture to Support Efficient Applications for Healthcare Industry 4.0;2019.0;[];"Edge computing paradigm has attracted many interests in the last few years as a valid alternative to the standard cloud-based approaches to reduce the interaction timing and the huge amount of data coming from Internet of Things (IoT) devices toward the Internet. In the next future, Edge-based approaches will be essential to support time-dependent applications in the Industry 4.0 context; thus, the paper proposes  BodyEdge , a novel architecture well suited for human-centric applications, in the context of the emerging healthcare industry. It consists of a tiny mobile client module and a performing edge gateway supporting multiradio and multitechnology communication to collect and locally process data coming from different scenarios; moreover, it also exploits the facilities made available from both private and public cloud platforms to guarantee a high flexibility, robustness, and adaptive service level. The advantages of the designed software platform have been evaluated in terms of reduced transmitted data and processing time through a real implementation on different hardware platforms. The conducted study also highlighted the network conditions (data load and processing delay) in which  BodyEdge  is a valid and inexpensive solution for healthcare application scenarios."
2805246634;Combining EEG signal processing with supervised methods for Alzheimer’s patients classification;2018.0;[];"Alzheimer’s Disease (AD) is a neurodegenaritive disorder characterized by a progressive dementia, for which actually no cure is known. An early detection of patients affected by AD can be obtained by analyzing their electroencephalography (EEG) signals, which show a reduction of the complexity, a perturbation of the synchrony, and a slowing down of the rhythms. In this work, we apply a procedure that exploits feature extraction and classification techniques to EEG signals, whose aim is to distinguish patient affected by AD from the ones affected by Mild Cognitive Impairment (MCI) and healthy control (HC) samples. Specifically, we perform a time-frequency analysis by applying both the Fourier and Wavelet Transforms on 109 samples belonging to AD, MCI, and HC classes. The classification procedure is designed with the following steps: (i) preprocessing of EEG signals; (ii) feature extraction by means of the Discrete Fourier and Wavelet Transforms; and (iii) classification with tree-based supervised methods. By applying our procedure, we are able to extract reliable human-interpretable classification models that allow to automatically assign the patients into their belonging class. In particular, by exploiting a Wavelet feature extraction we achieve 83%, 92%, and 79% of accuracy when dealing with HC vs AD, HC vs MCI, and MCI vs AD classification problems, respectively. Finally, by comparing the classification performances with both feature extraction methods, we find out that Wavelets analysis outperforms Fourier. Hence, we suggest it in combination with supervised methods for automatic patients classification based on their EEG signals for aiding the medical diagnosis of dementia."
2805780866;HDTQ: Managing RDF Datasets in Compressed Space;2018.0;[];HDT (Header-Dictionary-Triples) is a compressed representation of RDF data that supports retrieval features without prior decompression. Yet, RDF datasets often contain additional graph information, such as the origin, version or validity time of a triple. Traditional HDT is not capable of handling this additional parameter(s). This work introduces HDTQ (HDT Quads), an extension of HDT that is able to represent quadruples (or quads) while still being highly compact and queryable. Two HDTQ-based approaches are introduced: Annotated Triples and Annotated Graphs, and their performance is compared to the leading open-source RDF stores on the market. Results show that HDTQ achieves the best compression rates and is a competitive alternative to well-established systems.
2807337963;A new autonomous data transmission reduction method for wireless sensors networks;2018.0;[];The inherent limitation in energy resources and computational power for sensor nodes in a Wireless Sensor Network, poses the challenge of extending the lifetime of these networks. Since radio communication is the dominant energy consuming activity, most presented approaches focused on reducing the number of data transmitted to the central workstation. This can be achieved by deploying both on the workstation and the sensor node a synchronized prediction model capable of forecasting future values. Thus, enabling the sensor node to transmit only the values that surpasses a predefined error threshold. This mechanism offers a decrease in the cost of transmission energy for a price of an increase in the cost of computational energy. Therefore, finding the right trade­off between complexity and efficiency is very important to achieve optimal results. In this paper, we present a novel data reduction method that outperforms other state of the art data reduction approaches. We demonstrated the efficiency of our algorithm using simulation on real-world data sets collected at our laboratory. The obtained results show that our method was able to achieve a data suppression ratio ranging between 93.2% and 99.8%.
2807548702;Hierarchical Multi-Clue Modelling for POI Popularity Prediction with Heterogeneous Tourist Information;2019.0;[];Predicting the popularity of Point of Interest (POI) has become increasingly crucial for location-based services, such as POI recommendation. Most of the existing methods can seldom achieve satisfactory performance due to the scarcity of POIu0027s information, which tendentiously confines the recommendation to popular scene spots, and ignores the unpopular attractions with potentially precious values. In this paper, we propose a novel approach, termed  Hierarchical Multi-Clue Fusion  ( HMCF ), for predicting the popularity of POIs. Specifically, in order to cope with the problem of data sparsity, we propose to comprehensively describe POI using various types of user generated content (UGC) (e.g., text and image) from multiple sources. Then, we devise an effective POI modelling method in a hierarchical manner, which simultaneously injects semantic knowledge as well as multi-clue representative power into POIs. For evaluation, we construct a multi-source POI dataset by collecting all the textual and visual content of several specific provinces in China from four main-stream tourism platforms during 2006 to 2017. Extensive experimental results show that the proposed method can significantly improve the performance of predicting the attractions’ popularity as compared to several baseline methods.
2808935172;Resource2Vec: Linked Data distributed representations for term discovery in automatic speech recognition;2018.0;[];   In this work we present a neural network embedding we call Resource2Vec, which is able to represent the resources that make up some Linked Data (LD) corpora. A vector representation of these resources allows more advantageous processing (in computational terms) as is the case with known word or document embeddings. We give a quantitative analysis for their study. Furthermore, we employ them in an Automatic Speech Recognition (ASR) task to demonstrate their functionality by designing a strategy for term discovery. This strategy permits out-of-vocabulary (OOV) terms in a Large Vocabulary Continuous Speech Recognition (LVCSR) system to be discovered and then put into the final transcription. First, we detect where a potential OOV term may have been uttered in the LVCSR output speech segments. Second, we carry out a candidate OOV search in some LD corpora. This search is oriented by distance measurements between the transcription context around the potential-OOV speech segment and the resources of the LD corpora in Resource2Vec format, obtaining a set of candidates. To rank them, we mainly depend on the phone transcription of that segment. Finally, we decide whether or not to incorporate a candidate into the final transcription. The results show we are able to improve the transcription in Word Error Rate (WER) terms significantly, after our strategy is used on speech in Spanish.
2809071881;Development of white matter fibre density and morphology over childhood: A longitudinal fixel-based analysis;2018.0;[];   Purpose  White matter fibre development in childhood involves dynamic changes to microstructural organisation driven by increasing axon diameter, density, and myelination. However, there is a lack of longitudinal studies that have quantified advanced diffusion metrics to identify regions of accelerated fibre maturation, particularly across the early pubertal period. We applied a novel longitudinal fixel-based analysis (FBA) framework, in order to estimate microscopic and macroscopic white matter changes over time.    Methods  Diffusion-weighted imaging (DWI) data were acquired for 59 typically developing children (27 female) aged 9–13 years  at two time-points approximately 16 months apart (time-point 1: 10.4 ± 0.4 years, time-point 2: 11.7 ± 0.5 years). Whole brain FBA was performed using the connectivity-based fixel enhancement method, to assess longitudinal changes in fibre microscopic density and macroscopic morphological measures, and how these changes are related to sex, pubertal stage, and pubertal progression. Follow-up analyses were performed in sub-regions of the corpus callosum to confirm the main findings using a Bayesian repeated measures approach.    Results  There was a statistically significant increase in fibre density over time localised to medial and posterior commissural and association fibres, including the forceps major and bilateral superior longitudinal fasciculus. Increases in fibre cross-section were substantially more widespread. The rate of fibre development was not associated with age or sex. In addition, there was no significant relationship between pubertal stage or progression and longitudinal fibre development over time. Follow-up Bayesian analyses were performed to confirm the findings, which supported the null effect of the longitudinal pubertal comparison.    Conclusion  Using a novel longitudinal fixel-based analysis framework, we demonstrate that white matter fibre density and fibre cross-section increased within a 16-month scan rescan period in specific regions. The observed increases might reflect increasing axonal diameter or axon count. Pubertal stage or progression did not influence the rate of fibre development in the early stages of puberty. Future work should focus on quantifying these measures across a wider age range to capture the full spectrum of fibre development across the pubertal period.
2809684781;A comprehensive survey on machine learning for networking: evolution, applications and research opportunities;2018.0;[];Machine Learning (ML) has been enjoying an unprecedented surge in applications that solve problems and enable automation in diverse domains. Primarily, this is due to the explosion in the availability of data, significant improvements in ML techniques, and advancement in computing capabilities. Undoubtedly, ML has been applied to various mundane and complex problems arising in network operation and management. There are various surveys on ML for specific areas in networking or for specific network technologies. This survey is original, since it jointly presents the application of diverse ML techniques in various key areas of networking across different network technologies. In this way, readers will benefit from a comprehensive discussion on the different learning paradigms and ML techniques applied to fundamental problems in networking, including traffic prediction, routing and classification, congestion control, resource and fault management, QoS and QoE management, and network security. Furthermore, this survey delineates the limitations, give insights, research challenges and future opportunities to advance ML in networking. Therefore, this is a timely contribution of the implications of ML for networking, that is pushing the barriers of autonomic network operation and management.
2810334075;Long Activity Video Understanding Using Functional Object-Oriented Network;2019.0;[];Video understanding is one of the most challenging topics in computer vision. In this paper, a four-stage video understanding pipeline is presented to simultaneously recognize all atomic actions and the single ongoing activity in a video. This pipeline uses objects and motions from the video and a graph-based knowledge representation network as prior reference. Two deep networks are trained to identify objects and motions in each video sequence associated with an action and low level image features are used to identify objects of interest in the video sequence. Confidence scores are assigned to objects of interest to represent their involvement in the action and to motion classes based on results from a deep neural network that classifies an ongoing action in video into motion classes. Confidence scores are computed for each candidate functional unit to associate them with an action using a knowledge representation network, object confidences, and motion confidences. Each action, therefore, is associated with a functional unit, and the sequence of actions is evaluated to identify the sole activity occurring in the video. The knowledge representation used in the pipeline is called the functional object-oriented network, which is a graph-based network useful for encoding knowledge about manipulation tasks. Experiments are performed on a dataset of cooking videos to test the proposed algorithm with action inference and activity classification. Experiments show that using a functional object-oriented network improves video understanding significantly.
2810397803;Deep Learning for Matching in Search and Recommendation;2018.0;[];"Matching is the key problem in both search and recommendation, that is to measure the relevance of a document to a query or the interest of a user on an item. Previously, machine learning methods have been exploited to address the problem, which learns a matching function from labeled data, also referred to as ""learning to matchu0027u0027. In recent years, deep learning has been successfully applied to matching and significant progresses have been made. Deep semantic matching models for search and neural collaborative filtering models for recommendation are becoming the state-of-the-art technologies. The key to the success of the deep learning approach is its strong ability in learning of representations and generalization of matching patterns from raw data (e.g., queries, documents, users, and items, particularly in their raw forms). In this tutorial, we aim to give a comprehensive survey on recent progress in deep learning for matching in search and recommendation. Our tutorial is unique in that we try to give a unified view on search and recommendation. In this way, we expect researchers from the two fields can get deep understanding and accurate insight on the spaces, stimulate more ideas and discussions, and promote developments of technologies. The tutorial mainly consists of three parts. Firstly, we introduce the general problem of matching, which is fundamental in both search and recommendation. Secondly, we explain how traditional machine learning techniques are utilized to address the matching problem in search and recommendation. Lastly, we elaborate how deep learning can be effectively used to solve the matching problems in both tasks."
2810419333;Quantitative Analysis of Relationship Between Hypokinetic Dysarthria and the Freezing of Gait in Parkinson’s Disease;2018.0;[];Hypokinetic dysarthria (HD) and freezing of gait (FOG) are both axial symptoms that occur in patients with Parkinson’s disease (PD). It is assumed they have some common pathophysiological mechanisms and therefore that speech disorders in PD can predict FOG deficits within the horizon of some years. The aim of this study is to employ a complex quantitative analysis of the phonation, articulation and prosody in PD patients in order to identify the relationship between HD and FOG, and establish a mathematical model that would predict FOG deficits using acoustic analysis at baseline. We enrolled 75 PD patients who were assessed by 6 clinical scales including the Freezing of Gait Questionnaire (FOG–Q). We subsequently extracted 19 acoustic measures quantifying speech disorders in the fields of phonation, articulation and prosody. To identify the relationship between HD and FOG, we performed a partial correlation analysis. Finally, based on the selected acoustic measures, we trained regression models to predict the change in FOG during a 2-year follow-up. We identified significant correlations between FOG–Q scores and the acoustic measures based on formant frequencies (quantifying the movement of the tongue and jaw) and speech rate. Using the regression models, we were able to predict a change in particular FOG–Q scores with an error of between 7.4 and 17.0 %. This study is suggesting that FOG in patients with PD is mainly linked to improper articulation, a disturbed speech rate and to intelligibility. We have also proved that the acoustic analysis of HD at the baseline can be used as a predictor of the FOG deficit during 2 years of follow-up. This knowledge enables researchers to introduce new cognitive systems that predict gait difficulties in PD patients.
2845841029;Development of an EMG-Controlled Mobile Robot;2018.0;[];This paper presents the development of a Robot Operating System (ROS)-based mobile robot control using electromyography (EMG) signals. The proposed robot’s structure is specifically designed to provide modularity and is controlled by a Raspberry Pi 3 running on top of an ROS application and a Teensy microcontroller. The EMG muscle commands are sent to the robot with hand gestures that are captured using a Thalmic Myo Armband and recognized using a k-Nearest Neighbour (k-NN) classifier. The robot’s performance is evaluated by navigating it through specific paths while solely controlling it through the EMG signals and using the collision avoidance approach. Thus, this paper aims to expand the research on the topic, introducing a more accurate classification system with a wider set of gestures, hoping to come closer to a usable real-life application.
2883145848;Rapid Airplane Detection in Remote Sensing Images Based on Multilayer Feature Fusion in Fully Convolutional Neural Networks;2018.0;[];
2883589083;Near-Field Sources Localization in Partly Calibrated Sensor Arrays With Unknown Gains and Phases;2019.0;[];In this letter, near-field sources localization in partly calibrated uniform linear arrays (ULAs) with unknown gains and phases is considered. The existing methods generally assume that the array sensor is perfectly calibrated, i.e., the error of gains and phases is not considered. However, in practical applications, the problem of unknown gains and phases generally exists and the performance of these algorithms may decay under the partly calibrated array. Aiming to solve the problem, we propose a new method to estimate the directions of arrival (DOAs) and ranges of near-field sources in partly calibrated ULAs. The proposed method include two steps. First, by making use of symmetric array geometry, a special high-order cumulant matrix is constructed to estimate the DOAs. Then, with the estimated DOAs, the corresponding ranges are estimated by defining the 1-D range function. The proposed method can effectively estimate the DOAs and ranges of near-field sources under unknown gains and phases. Numerical simulation results show the effectiveness of the proposed method.
2883617685;Forearm Motion Recognition With Noncontact Capacitive Sensing;2018.0;[];This study presents a noncontact capacitive sensing method for forearm motion recognition. A method is proposed to record upper limb motion information from muscle contractions without contact with human skin, compensating for the limitations of existing sEMG-based methods. The sensing front-ends are designed based on human forearm shapes, and the forearm limb shape changes caused by muscle contractions will be represented by capacitance signals. After implementation of the capacitive sensing system, experiments on healthy subjects are conducted to evaluate the effectiveness. Nine motion patterns combined with 16 motion transitions are investigated on seven participants. We also designed an automatic data labeling method based on inertial signals from the measured hand, which greatly accelerated the training procedure. With the capacitive sensing system and the designed recognition algorithm, the method produced an average recognition of over 92%. Correct decisions could be made with approximately a 347-ms delay from the relaxed state to the time point of motion initiation. The confounding factors that affect the performances are also analyzed, including the sliding window length, the motion types and the external disturbances. We found the average accuracy increased to 98.7% when five motion patterns were recognized. The results of the study proved the feasibility and revealed the problems of the noncontact capacitive sensing approach on upper-limb motion sensing and recognition. Future efforts in this direction could be worthwhile for achieving more promising outcomes.
2883995567;Data Provenance in the Internet of Things;2018.0;[];"There is a need to create a trusted and secure IoT environment to share information, create knowledge and perform digital transactions. Trustworthy data collection, mining and fusion is vital for the successful widespread acceptance of IoT applications. This requires not only accurate, secure, and precise data collection; but also provisioning of data provenance throughout the whole life-cycle of the IoT device. To this end, this paper introduces a provenance-based trust management solution which helps in establishing a trust relationship among communicating devices in the IoT. This work extends the Internet of Things Management Platform (IoT-MP) by assuring data provenance. Thus complementing the previous IoT-MP capabilities in preserving the privacy of users in the IoT."
2884166416;Exponential operation and aggregation operator for q‐rung orthopair fuzzy set and their decision‐making method with a new score function;2018.0;[];
2884326683;More is Better: Precise and Detailed Image Captioning Using Online Positive Recall and Missing Concepts Mining;2019.0;[];Recently, a great progress in automatic image captioning has been achieved by using semantic concepts detected from the image. However, we argue that existing concepts-to-caption framework, in which the concept detector is trained using the image-caption pairs to minimize the vocabulary discrepancy, suffers from the deficiency of insufficient concepts. The reasons are two-fold: 1) the extreme imbalance between the number of occurrence positive and negative samples of the concept and 2) the incomplete labeling in training captions caused by the biased annotation and usage of synonyms. In this paper, we propose a method, termed  online positive recall and missing concepts mining , to overcome those problems. Our method adaptively re-weights the loss of different samples according to their predictions for online positive recall and uses a two-stage optimization strategy for missing concepts mining. In this way, more semantic concepts can be detected and a high accuracy will be expected. On the caption generation stage, we explore an element-wise selection process to automatically choose the most suitable concepts at each time step. Thus, our method can generate more precise and detailed caption to describe the image. We conduct extensive experiments on the MSCOCO image captioning data set and the MSCOCO online test server, which shows that our method achieves superior image captioning performance compared with other competitive methods.
2885161904;Smart Directional Data Aggregation in VANETs;2018.0;[];The ultimate goal of a Traffic Information System (TIS) consists in properly informing vehicles about road traffic conditions in order to reduce traffic jams and consequently CO2 emission while increasing the user comfort. Therefore, the design of an efficient aggregation protocol that combines correlated traffic information like location, speed and direction known as Floating Car Data (FCD) is of paramount importance. In this paper, we introduce a new TIS data aggregation protocol called Smart Directional Data Aggregation (SDDA) able to decrease the network overload while obtaining high accurate information on traffic conditions for large road sections. To this end, we introduce three levels of messages filtering: (i) filtering all FCD messages before the aggregation process based on vehicle directions and road speed limitations, (ii) integrating a suppression technique in the phase of information gathering in order to eliminate the duplicate data, and (iii) aggregating the filtered FCD data and then disseminating it to other vehicles. The performed experiments show that the SDDA outperforms existing approaches in terms of effectiveness and efficiency.
2885789677;A Piezoresistive Sensor to Measure Muscle Contraction and Mechanomyography;2018.0;[];Measurement of muscle contraction is mainly achieved through electromyography (EMG) and is an area of interest for many biomedical applications, including prosthesis control and human machine interface. However, EMG has some drawbacks, and there are also alternative methods for measuring muscle activity, such as by monitoring the mechanical variations that occur during contraction. In this study, a new, simple, non-invasive sensor based on a force-sensitive resistor (FSR) which is able to measure muscle contraction is presented. The sensor, applied on the skin through a rigid dome, senses the mechanical force exerted by the underlying contracting muscles. Although FSR creep causes output drift, it was found that appropriate FSR conditioning reduces the drift by fixing the voltage across the FSR and provides voltage output proportional to force. In addition to the larger contraction signal, the sensor was able to detect the mechanomyogram (MMG), i.e., the little vibrations which occur during muscle contraction. The frequency response of the FSR sensor was found to be large enough to correctly measure the MMG. Simultaneous recordings from flexor carpi ulnaris showed a high correlation (Pearsonu0027s r u003e 0.9) between the FSR output and the EMG linear envelope. Preliminary validation tests on healthy subjects showed the ability of the FSR sensor, used instead of the EMG, to proportionally control a hand prosthesis, achieving comparable performances.
2885902036;Sharp conditions for the existence of a stationary distribution in one classical stochastic chemostat;2018.0;[];   This paper studies the asymptotic behaviors of one classical chemostat model in a stochastic environment. Based on the Feller property, sharp conditions are derived for the existence of a stationary distribution by using the mutually exclusive possibilities known in [11, 12] (See Lemma 2.4 for details), which closes the gap left by the Lyapunov function. Further, we obtain a sufficient condition for the extinction of the organism based on two noise-induced parameters: an analogue of the feed concentration S* and the break-even concentration λ. Results indicate that both noises have negative effects on persistence of the microorganism.
2885947181;Evaluating the quality of the LOD cloud: An empirical investigation;2018.0;[];
2886310805;An optimum ANN-based breast cancer diagnosis: Bridging gaps between ANN learning and decision-making goals;2018.0;[];"   It is difficult to overestimate the importance of appropriate breast cancer diagnosis, as the disease ranks second among all cancers that lead to death in women. Many efforts propose data analytic tools that succeed in predicting breast cancer with high accuracy; the literature is abundant with studies that report close-to-perfect prediction rates. This paper shifts the focus of improvement from higher accuracy towards better decision-making. Quantitatively, we have shown more accuracy does not always lead to better decisions, and the process of Artificial Neural Networks (ANN) learning can benefit from the inculcation of decision-making goals. We have proposed a decision-oriented ANN classification method called Life-Sensitive Self-Organizing Error-Driven (LS-SOED), which enhances ANN’s performance in decision-making. LS-SOED combines the supervised and unsupervised learning power of ANN to handle the inconclusive nature of hidden patterns in the data in such way that the best possible decisions are made, i.e. the least misclassification cost (the minimum possible loosing of life) is achieved. The learning power of SOED matches, if not excels, the best performances reported in the literature when the objective is to achieve the highest accuracy. When the objective is to minimize misclassification costs, we have shown, on average, in one dataset more than 30 years of life for a group of 283 people, and in another more than 8 years of life for a group of 57 people can be saved collectively."
2886555363;Evaluating query and storage strategies for RDF archives;2019.0;[];
2887712318;Video Captioning by Adversarial LSTM;2018.0;[];In this paper, we propose a novel approach to video captioning based on adversarial learning and long short-term memory (LSTM). With this solution concept, we aim at compensating for the deficiencies of LSTM-based video captioning methods that generally show potential to effectively handle temporal nature of video data when generating captions but also typically suffer from exponential error accumulation. Specifically, we adopt a standard generative adversarial network (GAN) architecture, characterized by an interplay of two competing processes: a “generator” that generates textual sentences given the visual content of a video and a “discriminator” that controls the accuracy of the generated sentences. The discriminator acts as an “adversary” toward the generator, and with its controlling mechanism, it helps the generator to become more accurate. For the generator module, we take an existing video captioning concept using LSTM network. For the discriminator, we propose a novel realization specifically tuned for the video captioning problem and taking both the sentences and video features as input. This leads to our proposed LSTM–GAN system architecture, for which we show experimentally to significantly outperform the existing methods on standard public datasets.
2888657195;REGAL: Representation Learning-based Graph Alignment;2018.0;[];Problems involving multiple networks are prevalent in many scientific and other domains. In particular, network alignment, or the task of identifying corresponding nodes in different networks, has applications across the social and natural sciences. Motivated by recent advancements in node representation learning for single-graph tasks, we propose REGAL (REpresentation learning-based Graph ALignment), a framework that leverages the power of automatically-learned node representations to match nodes across different graphs. Within REGAL we devise xNetMF, an elegant and principled node embedding formulation that uniquely generalizes to multi-network problems. Our results demonstrate the utility and promise of unsupervised representation learning-based network alignment in terms of both speed and accuracy. REGAL runs up to 30x faster in the representation learning stage than comparable methods, outperforms existing network alignment methods by 20 to 30% accuracy on average, and scales to networks with millions of nodes each.
2889467404;Adaptive monitoring: A systematic mapping;2019.0;[];"   Context  Adaptive monitoring is a method used in a variety of domains for responding to changing conditions. It has been applied in different ways, from monitoring systems’ customization to re-composition, in different application domains. However, to the best of our knowledge, there are no studies analyzing how adaptive monitoring differs or resembles among the existing approaches.    Objective  To characterize the current state of the art on adaptive monitoring, specifically to: (a) identify the main concepts in the adaptive monitoring topic; (b) determine the demographic characteristics of the studies published in this topic; (c) identify how adaptive monitoring is conducted and evaluated by the different approaches; (d) identify patterns in the approaches supporting adaptive monitoring.    Method  We have conducted a systematic mapping study of adaptive monitoring approaches following recommended practices. We have applied automatic search and snowballing sampling on different sources and used rigorous selection criteria to retrieve the final set of papers. Moreover, we have used an existing qualitative analysis method for extracting relevant data from studies. Finally, we have applied data mining techniques for identifying patterns in the solutions.    Results  We have evaluated 110 studies organized in 81 approaches that support adaptive monitoring. By analyzing them, we have: (1) surveyed related terms and definitions of adaptive monitoring and proposed a generic one; (2) visualized studies’ demographic data and arranged the studies into approaches; (3) characterized the main approaches’ contributions; (4) determined how approaches conduct the adaptation process and evaluate their solutions.    Conclusions  This cross-domain overview of the current state of the art on adaptive monitoring may be a solid and comprehensive baseline for researchers and practitioners in the field. Especially, it may help in identifying opportunities of research; for instance, the need of proposing generic and flexible software engineering solutions for supporting adaptive monitoring in a variety of systems."
2889545660;Atom search optimization and its application to solve a hydrogeologic parameter estimation problem;2019.0;[];   In recent years, various metaheuristic optimization methods have been proposed in scientific and engineering fields. In this study, a novel physics-inspired metaheuristic optimization algorithm, atom search optimization (ASO), inspired by basic molecular dynamics, is developed to address a diverse set of optimization problems. ASO mathematically models and mimics the atomic motion model in nature, where atoms interact through interaction forces resulting from the Lennard-Jones potential and constraint forces resulting from the bond-length potential. The proposed algorithm is simple and easy to implement. ASO is tested on a range of benchmark functions to verify its validity, qualitatively and quantitatively, and then applied to a hydrogeologic parameter estimation problem with success. The results demonstrate that ASO is superior to some classic and newly emerging algorithms in the literature and is a promising solution to real-world engineering problems.
2889553287;Hand tracking and gesture recognition using lensless smart sensors;2018.0;[];
2890564211;Brain Network Analysis of Compressive Sensed High-Density EEG Signals in AD and MCI Subjects;2019.0;[];Alzheimeru0027s disease (AD) is a neurodegenerative disorder that causes a loss of connections between neurons. The goal of this paper is to construct a complex network model of the brain-electrical activity, using high-density EEG (HD-EEG) recordings, and to compare the network organization in AD, mild cognitive impaired (MCI), and healthy control (CNT) subjects. The HD-EEG of 16 AD, 16 MCI, and 12 CNT was recorded during an eye-closed resting state. The permutation disalignment index (PDI) was used to describe the dissimilarity between EEG signals and to construct the connection matrices of the network model. The three groups were found to have significantly different ( p    $    0.001) characteristic path length (  $\lambda$  ), average clustering coefficient (CC), and the global efficiency (GE). This is the first time that HD-EEG signals of AD, MCI, and CNT have been compared and that PDI has been used to discriminate between the three groups. Considering the large amount of data originating from HD-EEG acquisition, compared to standard EEG, the aim of this paper is also to assess that compression did not alter the results of the complex network analysis. Compressive sensing was adopted to compress and reconstruct the HD-EEG signals with minimal information loss, achieving an average structural similarity index of 0.954 (AD), 0.957 (MCI), and 0.959 (CNT). When applied to the reconstructed HD-EEG, complex network analysis provided a substantially unaltered performance, compared to the analysis of the original signals:   $\lambda$  , CC, and GE of the three groups were indeed still significantly different ( p    $    0.001).
2890736686;Non-metric Similarity Graphs for Maximum Inner Product Search;2018.0;[];In this paper we address the problem of Maximum Inner Product Search (MIPS) that is currently the computational bottleneck in a large number of machine learning applications. While being similar to the nearest neighbor search (NNS), the MIPS problem was shown to be more challenging, as the inner product is not a proper metric function. We propose to solve the MIPS problem with the usage of similarity graphs, i.e., graphs where each vertex is connected to the vertices that are the most similar in terms of some similarity function. Originally, the framework of similarity graphs was proposed for metric spaces and in this paper we naturally extend it to the non-metric MIPS scenario. We demonstrate that, unlike existing approaches, similarity graphs do not require any data transformation to reduce MIPS to the NNS problem and should be used for the original data. Moreover, we explain why such a reduction is detrimental for similarity graphs. By an extensive comparison to the existing approaches, we show that the proposed method is a game-changer in terms of the runtime/accuracy trade-off for the MIPS problem.
2891321141;Classification of electroencephalogram signal for the detection of epilepsy using Innovative Genetic Programming;2019.0;[];
2891953200;Two MAGDM models based on hesitant fuzzy linguistic term sets with possibility distributions: VIKOR and TOPSIS;2019.0;[];   Hesitant fuzzy linguistic term sets (HFLTSs) with additional possibility distributions can represent a much broader range of linguistic data. This paper develops compromise solutions for multiple attribute group decision making (MAGDM) using HFLTS. Geodesic distance and possibility distribution based distance measures are introduced to calculate the consensus degrees and determine the participant importance weights for the aggregation. Two models are then proposed to derive a compromise solution for the MAGDM problems. The first model is based on the VIKOR method and is used to determine the compromise between group utility maximization and individual regret minimization. The second model is based on the TOPSIS method and seeks to identify the compromise between the distances from the ideal and anti-ideal solutions. For both models, several ideal solutions and separation measures are presented and some of the properties in the two models are proven. An example based on an assessment of a health-care waste disposal management system illustrates the feasibility and practicability of the two proposed models. A further comparative study highlights the distinct features and potential use of the presented models.
2891996945;A Novel Dynamic Cloud Service Trust Evaluation Model in Cloud Computing;2018.0;[];There are many different types of cloud services. How to choose the best cloud service for a specific application is a challenging issue. Using cloud services, usersu0027 private data could be exposed to a third party, which might not be trusted. In this paper, a dynamic cloud service trust evaluation model is proposed based on service-level agreement (SLA) and privacy-awareness. First, to make the final trust evaluation value more practical, the proposed model performs a comprehensive trust evaluation, which consists of direct, indirect, and reputation trust. Second, cloud services are divided into five levels based on their service capabilities. By analyzing SLA to determine the quality of service, the user can choose a suitable SLA. In order to protect data security of the cloud service user, we propose a data protection method based on a normal cloud model. Finally, we propose a dynamic trust update mechanism to update the direct trust. Experimental results based on a public dataset show that the proposed model effectively identifies the user community based on service preferences, improves the service requesteru0027s satisfaction, avoids malicious interference, and is accurate and feasible.
2892102150;Norm-Ranging LSH for Maximum Inner Product Search;2018.0;[];Neyshabur and Srebro proposed Simple-LSH, which is the state-of-the-art hashing method for maximum inner product search (MIPS) with performance guarantee. We found that the performance of Simple-LSH, in both theory and practice, suffers from long tails in the 2-norm distribution of real datasets. We propose Norm-ranging LSH, which addresses the excessive normalization problem caused by long tails in Simple-LSH by partitioning a dataset into multiple sub-datasets and building a hash index for each sub-dataset independently. We prove that Norm-ranging LSH has lower query time complexity than Simple-LSH. We also show that the idea of partitioning the dataset can improve other hashing based methods for MIPS. To support efficient query processing on the hash indexes of the sub-datasets, a novel similarity metric is formulated. Experiments show that Norm-ranging LSH achieves an order of magnitude speedup over Simple-LSH for the same recall, thus significantly benefiting applications that involve MIPS.
2892116073;Star-critical Ramsey numbers for large generalized fans and books;2018.0;[];   In this paper, we show that for any fixed integers    m  ≥  2    and    t  ≥  2   , the star-critical Ramsey number      r    ∗     (    K    1    +  n    K    t    ,    K    m  +  1    )   =   (  m  −  1  )   t  n  +  t    for all sufficiently large    n   . Furthermore, for any fixed integers    p  ≥  2    and    m  ≥  2   ,      r    ∗     (    K    p    +  n    K    1    ,    K    m  +  1    )   =   (  m  −  1  +  o   (  1  )   )   n    as    n  →  ∞   .
2892848289;Interval-Valued 2-Tuple Linguistic Induced Continuous Ordered Weighted Distance Measure and Its Application to Multiple Attribute Group Decision Making;2018.0;[];
2894424125;Exploring spatial-frequency-sequential relationships for motor imagery classification with recurrent neural network;2018.0;[];Conventional methods of motor imagery brain computer interfaces (MI-BCIs) suffer from the limited number of samples and simplified features, so as to produce poor performances with spatial-frequency features and shallow classifiers. Alternatively, this paper applies a deep recurrent neural network (RNN) with a sliding window cropping strategy (SWCS) to signal classification of MI-BCIs. The spatial-frequency features are first extracted by the filter bank common spatial pattern (FB-CSP) algorithm, and such features are cropped by the SWCS into time slices. By extracting spatial-frequency-sequential relationships, the cropped time slices are then fed into RNN for classification. In order to overcome the memory distractions, the commonly used gated recurrent unit (GRU) and long-short term memory (LSTM) unit are applied to the RNN architecture, and experimental results are used to determine which unit is more suitable for processing EEG signals. Experimental results on common BCI benchmark datasets show that the spatial-frequency-sequential relationships outperform all other competing spatial-frequency methods. In particular, the proposed GRU-RNN architecture achieves the lowest misclassification rates on all BCI benchmark datasets. By introducing spatial-frequency-sequential relationships with cropping time slice samples, the proposed method gives a novel way to construct and model high accuracy and robustness MI-BCIs based on limited trials of EEG signals.
2894895904;A Convolutional Neural Network approach for classification of dementia stages based on 2D-spectral representation of EEG recordings;2019.0;[];"   A data-driven machine deep learning approach is proposed for differentiating subjects with Alzheimer’s Disease (AD), Mild Cognitive Impairment (MCI) and Healthy Control (HC), by only analyzing noninvasive scalp EEG recordings. The methodology here proposed consists of evaluating the power spectral density (PSD) of the 19-channels EEG traces and representing the related spectral profiles into 2-d gray scale images (PSD-images). A customized Convolutional Neural Network with one processing module of convolution, Rectified Linear Units (ReLu) and pooling layer (CNN1) is designed to extract from PSD-images some suitable features and to perform the corresponding two and three-ways classification tasks. The resulting CNN is shown to provide better classification performance when compared to more conventional learning machines; indeed, it achieves an average accuracy of 89.8% in binary classification and of 83.3% in three-ways classification. These results encourage the use of deep processing systems (here, an engineered first stage, namely the PSD-image extraction, and a second or multiple CNN stage) in challenging clinical frameworks."
2895353730;Multi-Factorial Energy Aware Resource Management in Edge Networks;2019.0;[];Edge networks deliver computing services close to the user, unlike centralized clouds. This improves service scalability and delay-sensitive functions can be offloaded to the edge, when the latency incurred by cloud services is too high. Since services in edge networks, by their nature, are not centralized, careful design is required to achieve efficient resource utilization and low power consumption. These issues are addressed in this paper. A network device power model is formulated to explore the power dissipation characteristics of frequency scalable CMOS devices (as measured using a NetFPGA testbed). An on-demand energy-efficient resource allocation model (OERA) is designed based on this model. OERA features acceptance ratios that are 11%–17% higher than existing solutions and 9% lower power consumption. A novel algorithm is presented for resource placement in edge networks, which can accommodate higher traffic flow demands and distribution distance than existing solutions. This uses mixed integer linear programming to simultaneously maximize the aggregate flow demands and to minimize the network energy consumption. An iterative algorithm and a heuristic greedy edge network device placement algorithm are implemented that not only solve this NP-Hard problem but also significantly reduce the network energy consumption.
2895645998;Aggressive language identification using word embeddings and sentiment features;2018.0;[];
2895658904;A task-and-technique centered survey on visual analytics for deep learning model engineering;2018.0;[];   Although deep neural networks have achieved state-of-the-art performance in several artificial intelligence applications in the past decade, they are still hard to understand. In particular, the features learned by deep networks when determining whether a given input belongs to a specific class are only implicitly described concerning a considerable number of internal model parameters. This makes it harder to construct interpretable hypotheses of what the network is learning and how it is learning—both of which are essential when designing and improving a deep model to tackle a particular learning task. This challenge can be addressed by the use of visualization tools that allow machine learning experts to explore which components of a network are learning useful features for a pattern recognition task, and also to identify characteristics of the network that can be changed to improve its performance. We present a review of modern approaches aiming to use visual analytics and information visualization techniques to understand, interpret, and fine-tune deep learning models. For this, we propose a taxonomy of such approaches based on whether they provide tools for visualizing a network’s architecture, to facilitate the interpretation and analysis of the training process, or to allow for feature understanding. Next, we detail how these approaches tackle the tasks above for three common deep architectures: deep feedforward networks, convolutional neural networks, and recurrent neural networks. Additionally, we discuss the challenges faced by each network architecture and outline promising topics for future research in visualization techniques for deep learning models.
2896196878;Learning Multi-View Representation With LSTM for 3-D Shape Recognition and Retrieval;2019.0;[];Shape representation for 3-D models is an important topic in computer vision, multimedia analysis, and computer graphics. Recent multiview-based methods demonstrate promising performance for 3-D shape recognition and retrieval. However, most multiview-based methods ignore the correlations of multiple views or suffer from high computional cost. In this paper, we propose a novel multiview-based network architecture for 3-D shape recognition and retrieval. Our network combines convolutional neural networks (CNNs) with long short-term memory (LSTM) to exploit the correlative information from multiple views. Well-pretrained CNNs with residual connections are first used to extract a low-level feature of each view image rendered from a 3-D shape. Then, a LSTM and a sequence voting layer are employed to aggregate these features into a shape descriptor. The highway network and a three-step training strategy are also adopted to boost the optimization of the deep network. Experimental results on two public datasets demonstrate that the proposed method achieves promising performance for 3-D shape recognition and the state-of-the-art performance for the 3-D shape retrieval.
2896572817;Auto-Adjusting Self-Adaptive Software Systems;2018.0;[];Self-adaptive systems can cope with changes in their operating environment by modifying their structure and behavior at run time. Different kinds of changes pose different requirements on how the software should adapt: some changes may require an immediate adaptation, whereas others do not, leaving more time to find the most suitable action. To address different kinds of changes, we introduce auto-adjustment, which works by quickly assessing changes in terms of the resulting requirements on the adaptation logic (e.g., their criticality or urgency), and adjusting the adaptation logic accordingly. Thereby, auto-adjustment allows dynamically considering the trade-off between adaptation speed and adaptation quality. Experiments with an autonomic cloud resource allocation system show that auto-adjustment leads to an improved trade-off between conflicting system goals: by allowing 0.3% higher energy consumption, the number of server overloads can be reduced by 68%.
2897064646;Stroke Lesion Detection Using Convolutional Neural Networks;2018.0;[];Stroke is an injury that affects the brain tissue, mainly caused by changes in the blood supply to a particular region of the brain. As consequence, some specific functions related to that affected region can be reduced, decreasing the quality of life of the patient. In this work, we deal with the problem of stroke detection in Computed Tomography (CT) images using Convolutional Neural Networks (CNN) optimized by Particle Swarm optimization (PSO). We considered two different kinds of strokes, ischemic and hemorrhagic, as well as making available a public dataset to foster the research related to stroke detection in the human brain. The dataset comprises three different types of images for each case, i.e., the original CT image, one with the segmented cranium and an additional one with the radiological densityu0027s map. The results evidenced that CNNu0027s are suitable to deal with stroke detection, obtaining promising results.
2897230578;Automatic Curation of Sports Highlights Using Multimodal Excitement Features;2019.0;[];The production of sports highlight packages summarizing a gameu0027s most exciting moments is an essential task for broadcast media. Yet, it requires labor-intensive video editing. We propose a novel approach for auto-curating sports highlights, and demonstrate it to create a first of a kind, real-world system for the editorial aid of golf and tennis highlight reels. Our method fuses information from the players’ reactions (action recognition such as high-fives and fist pumps), players’ expressions (aggressive, tense, smiling, and neutral), spectators (crowd cheering), commentator (tone of the voice and word analysis), and game analytics to determine the most interesting moments of a game. We accurately identify the start and end frames of key shot highlights with additional metadata, such as the playeru0027s name and the whole number, or analysts input allowing personalized content summarization and retrieval. In addition, we introduce new techniques for learning our classifiers with reduced manual training data annotation by exploiting the correlation of different modalities. Our work has been demonstrated at a major golf tournament (2017 Masters) and two major international tennis tournaments (2017 Wimbledon and U.S. Open), successfully extracting highlights through the course of the sporting events. For the 2017 Masters, 54% of the clips selected by our system overlapped with the official highlights reels. Furthermore, user studies showed that 90% of the non-overlapping ones were of the same quality of the official clips for the 2017 Masters, while the automatic selection of clips for highlights of 2017 Wimbledon and 2017 US Open agreed with human preferences 80% and 84.2% of the time, respectively.
2897298014;Online Data Organizer: Micro-Video Categorization by Structure-Guided Multimodal Dictionary Learning;2019.0;[];Micro-videos have rapidly become one of the most dominant trends in the era of social media. Accordingly, how to organize them draws our attention. Distinct from the traditional long videos that would have multi-site scenes and tolerate the hysteresis, a micro-video: 1) usually records contents at one specific venue within a few seconds. The venues are structured hierarchically regarding their category granularity. This motivates us to organize the micro-videos via their venue structure. 2) timely circulates over social networks. Thus, the timeliness of micro-videos desires effective online processing. However, only 1.22% of micro-videos are labeled with venue information when uploaded at the mobile end. To address this problem, we present a framework to organize the micro-videos online. In particular, we first build a structure-guided multi-modal dictionary learning model to learn the concept-level micro-video representation by jointly considering their venue structure and modality relatedness. We then develop an online learning algorithm to incrementally and efficiently strengthen our model, as well as categorize the micro-videos into a tree structure. Extensive experiments on a real-world data set validate our model well. In addition, we have released the codes to facilitate the research in the community.
2897754576;From Neural Re-Ranking to Neural Ranking: Learning a Sparse Representation for Inverted Indexing;2018.0;[];"The availability of massive data and computing power allowing for effective data driven neural approaches is having a major impact on machine learning and information retrieval research, but these models have a basic problem with efficiency. Current neural ranking models are implemented as multistage rankers: for efficiency reasons, the neural model only re-ranks the top ranked documents retrieved by a first-stage efficient ranker in response to a given query. Neural ranking models learn dense representations causing essentially every query term to match every document term, making it highly inefficient or intractable to rank the whole collection. The reliance on a first stage ranker creates a dual problem: First, the interaction and combination effects are not well understood. Second, the first stage ranker serves as a ""gate-keeper"" or filter, effectively blocking the potential of neural models to uncover new relevant documents. In this work, we propose a standalone neural ranking model (SNRM) by introducing a sparsity property to learn a latent sparse representation for each query and document. This representation captures the semantic relationship between the query and documents, but is also sparse enough to enable constructing an inverted index for the whole collection. We parameterize the sparsity of the model to yield a retrieval model as efficient as conventional term based models. Our model gains in efficiency without loss of effectiveness: it not only outperforms the existing term matching baselines, but also performs similarly to the recent re-ranking based neural models with dense representations. Our model can also take advantage of pseudo-relevance feedback for further improvements. More generally, our results demonstrate the importance of sparsity in neural IR models and show that dense representations can be pruned effectively, giving new insights about essential semantic features and their distributions."
2897829713;Mobile cloud computing based stroke healthcare system;2019.0;[];   Information technology has recently seen a huge progress in innovative healthcare technologies that rendered healthcare data bigger. Connectivity on 7/24 basis between human to device and device to device have a crucial role in individuals’ lives. Therefore, Mobile Cloud System (MCC) has become an indispensable tool. Parallel with the rapid developments in the Internet of Things, convergence has become an important issue. Our proposed method, accordingly, can be converged with mobile-cloud environments with cloud computing in handling healthcare information. This study uses Virtual Dedicated Server (VDS) as 4 VCPU and 8 GB RAM and proposes a model based on the Android based mobile phones for stroke patients with cardioembolic (689) and cryptogenic (528) subtypes. The system set up through this study has two basic application elements which are mobile application and server application. Artificial Neural Network (ANN) module is beneficial for classifying the two stroke subtypes while server application is used for saving the data from the patients. Accordingly, our model guarantees availability, security, and scalability as a system for stroke patients applying Stroke dataset for ANN algorithm, Multilayer Perceptron Algorithm (MLP), which has been done for the first time in literature with big data in this scope. The main contributions are: (1) The outcomes will display an individual unique social insurance framework. (2) The outcomes will be utilized for the distinguishing proof of stroke-related data to be gathered by cell phones that are Android based. (3) Stroke patients will find out about their condition of well-being through an ANN application programming interface, which will provide a sort of organization for the patients. Overall, an efficient and user-friendly stroke determination human services framework has been presented through this Healthcare System for patients.
2898178966;A CORS-Based Differential Correction Approach for AIS Mobile Stations;2018.0;[];
2898233200;Generalized Latent Multi-View Subspace Clustering;2019.0;[];Subspace clustering is an effective method that has been successfully applied to many applications. Here we propose a novel subspace clustering model for multi-view data using a latent representation termed Latent Multi-View Subspace Clustering (LMSC). Unlike most existing single-view subspace clustering methods, which directly reconstruct data points using original features, our method explores underlying complementary information from multiple views and simultaneously seeks the underlying latent representation. Using the complementarity of multiple views, the latent representation depicts data more comprehensively than each individual view, accordingly making subspace representation more accurate and robust. We proposed two LMSC formulations: linear LMSC (lLMSC), based on linear correlations between latent representation and each view, and generalized LMSC (gLMSC), based on neural networks to handle general relationships. The proposed method can be efficiently optimized under the Augmented Lagrangian Multiplier with Alternating Direction Minimization (ALM-ADM) framework. Extensive experiments on diverse datasets demonstrate the effectiveness of the proposed method.
2898896120;Cost Efficient Edge Intelligence Framework Using Docker Containers;2018.0;[];The emergence of edge computing has its basis in the integration of the Internet of Things (IoT) with the Cloud computing. In order to make it possible, management technologies of data centers have to be combined with significantly more limited devices. The Docker technology, which provides a very lightweight and effective virtualization solution, can be utilized to manage, deploy and distribute edge/cloud applications onto clusters (that, in our case, will be composed by lightweight and small board devices–such as Raspberry Pi). We apply this on the human activity identification scenario. These types of edge devices can be very useful especially in cases when the combination of low costs and robustness is desirable due to various reasons and conditions. In our work, we propose and analyze a framework based on the distributed edge/cloud paradigm. It is able to provide an advantageous combination of various benefits and lower costs of data processing performed at the edge instead of central servers. Support Vector Machine (SVM) has been utilized for recognizing human activity via the proposed framework. The results of the use case are presented in detail in this paper along with the simulated experiment.
2899304489;Some q‐rung orthopair fuzzy maclaurin symmetric mean operators and their applications to potential evaluation of emerging technology commercialization;2019.0;[];
2899430821;An Induced Hesitant Linguistic Aggregation Operator and Its Application for Creating Fuzzy Ontology;2018.0;[];
2899508431;Tuning self-adaptation in cyber-physical systems through architectural homeostasis;2019.0;[];   Self-adaptive software-intensive cyber-physical systems (sasiCPS) encounter a high level of run-time uncertainty. State-of-the-art architecture-based self-adaptation approaches assume designing against a fixed set of situations that warrant self-adaptation. As a result, failures may appear when sasiCPS operate in environment conditions they are not specifically designed for. In response, we propose to increase the homeostasis of sasiCPS, i.e., the capacity to maintain an operational state despite run-time uncertainty, by introducing run-time changes to the architecture-based self-adaptation strategies according to environment stimuli. In addition to articulating the main idea of architectural homeostasis, we introduce four mechanisms that reify the idea: (i) collaborative sensing, (ii) faulty component isolation from adaptation, (iii) enhancing mode switching, and (iv) adjusting guards in mode switching. Moreover, our experimental evaluation of the four mechanisms in two different case studies confirms that allowing a complex system to change its self-adaptation strategies helps the system recover from run-time errors and abnormalities and keep it in an operational state.
2900290300;Robust pixelwise saliency detection via progressive graph rankings;2019.0;[];   In this paper, we propose a novel saliency detection method based on superpixel-to-pixel level optimization. First, we segment the input image into superpixels under four scales. For each scale, we construct a k-regular basic graph with these superpixels as nodes. Furthermore, we enlarge the basic graph with virtual absorbing nodes and utilize absorbing Markov chain ranking to calculate background-based saliency. Second, for each scale, we obtain robust foreground queries from the previous result, and use manifold ranking to obtain foreground-based saliency. Third, a regularized random walk ranking based on the pixelwise graph for each scale is used to diffuse the saliency values among pixels. Finally, we obtain four saliency maps for the input image and integrate them together for the final saliency map. Extensive experiments on several challenging datasets reveal that the proposed method performs better in terms of precision, recall and F-measure values. Despite complex backgrounds, our method performs better in detecting small and/or multiple salient objects than other state-of-the-art methods as a whole.
2900501367;New q‐rung orthopair fuzzy partitioned Bonferroni mean operators and their application in multiple attribute decision making;2019.0;[];
2900786131;Classification of multiple motor imagery using deep convolutional neural networks and spatial filters;2019.0;[];   Brain–Computer Interfaces (BCI) are systems that translate brain activity patterns into commands for an interactive application, and some of them recognize patterns generated by motor imagery. Currently, these systems present performances and methodologies that still are not practical enough for realistic applications. Therefore, this paper proposes two methodologies for multiple motor imagery classification. Both methodologies use features extracted by a variant of Discriminative Filter Bank Common Spatial Pattern (DFBCSP) presented in this paper. The frequency bands selection in this variant is carried out by a novel iterative algorithm that selects the frequency band that attains the highest classification accuracy for specific binary classification. For each binary combination of classes, a frequency band is selected. The resulting samples are then set into a matrix which feeds one or many Convolutional Neural Networks previously optimized by using a Bayesian optimization. The first methodology applies a Convolutional Neural Network (CNN) for the classification of all classes and the second is a modular network composed of four expert CNNs. In this modular network, each expert CNN performs a binary classification, and a fully connected network analyzes their results. To validate both approaches two datasets were used, the BCI competition IV dataset 2a and another presented in this paper recorded from eight subjects by using the OpenBCI device. The experimental results demonstrated an improvement in the classification accuracy over many classic intelligent recognition methods, without a high computation time in order that they can be implemented in an online application.
2901284226;Quasi-random graphs;1989.0;[];
2901492641;Spoken Conversational AI in Video Games: Emotional Dialogue Management Increases User Engagement;2018.0;[];In a traditional role-playing game (RPG) conversing with a Non-Playable Character (NPC) typically appears somewhat unrealistic and can break immersion and user engagement. In commercial games, the player usually selects one of several possible predefined conversation options which are displayed as text or labels on the screen, to progress the conversation. In contrast, we first present a spoken conversational interface, built using a state-of-the-art open-domain social conversational AI developed for the Amazon Alexa Challenge, which was modified for use in a video game. This system is designed to keep users engaged in the conversation -- which we measure by time taken speaking with the character. In particular, we use emotion detection and emotional dialogue management to enhance the conversational experience. We then evaluate the contribution of emotion detection and conversational responses in a spoken dialogue system for a role-playing video game. In order to do this, two prototypes of the same game were created: one system using sentiment analysis and emotional modelling and the other system that does not detect or react to emotions. Both systems use a spoken conversational AI system where the user can freely talk to a Non-Playable-Character using unconstrained speech input.
2901536328;Predictors of excessive internet use among adolescents in Spain: The relevance of the relationship between parents and their children;2019.0;[];   Since the introduction of the concept in the late nineties, Internet addiction has been a growing phenomenon becoming a public health issue that cannot be ignored nowadays. Focusing on adolescents, previous research has analyzed the prevalence of excessive Internet use and determined several predictors such as socio-demographic characteristics, personality traits or emotions. To move on this topic, this paper draws attention to the relationship between parents and their children. Using a nationally representative sample of 37,486 students aged 14–18 years old in Spain, an under researched European country, this study finds that care received from parents [B = −0.141, SE = −0.033] and parentsu0027 knowledge about where [B = −0.065, SE = −0.032] and with whom [B = −0.232, SE = −0.032] the adolescent is while he/she goes out at night are associated with lower levels of excessive Internet use. In turn, fixing clear rules does not help reduce excessive Internet use. These results, together with the influence of other control variables, offer important insights and implications to prevent adolescents from excessive Internet use.
2901565938;A hybrid GSA-GA algorithm for constrained optimization problems;2019.0;[];   In this paper, a new hybrid GSA-GA algorithm is presented for the constraint nonlinear optimization problems with mixed variables. In it, firstly the solution of the algorithm is tuned up with the gravitational search algorithm and then each solution is upgraded with the genetic operators such as selection, crossover, mutation. The performance of the algorithm is tested on the several benchmark design problems with different nature of the objectives, constraints and the decision variables. The obtained results from the proposed approach are compared with the several existing approaches result and found to be very profitable. Finally, obtained results are verified with some statistical testing.
2901962466;On the elicitation of criteria weights in PROMETHEE-based ranking methods for a mobile application;2019.0;[];   Today, almost everybody has a smartphone and applications have been developed to help users to take decisions (e.g. which hotel to choose, which museum to visit, etc). In order to improve the recommendations of the mobile application, it is crucial to elicit the preference structures of the user. As problems are often based on several criteria, multicriteria decision aiding methods are most adequate in these cases, and past works have proposed indirect eliciting approaches for multicriteria decision aiding methods. However, they often do not aim of reducing as much as possible the cognitive efforts required by the user. This is prerequisite of mobile applications as they are used by everybody. In this work, the weights to assign to the evaluation criteria in a PROMETHEE-based ranking approach are unknown, and therefore must be elicited indirectly either from a partial ranking provided by the user or from the selection of his/her most preferred alternative into a subset of reference alternatives. In the latter case, the cognitive effort required by the decision-maker is minimal. Starting from a linear optimisation model aimed at searching for the most discriminating vector of weights, three quadratic variants are proposed subsequently to overcome the issues arising from the linear model. An iterative quadratic optimisation model is proposed to fit the real setting in which the application should operate, where the eliciting procedure must be launched iteratively and converge over time to the vector of weights, which are the weights that the user implicitly assigns to the evaluation criteria. Finally, three experiments are performed to confirm the effectiveness and the differences between the proposed models.
2902418216;Linguistic‐induced ordered weighted averaging operator for multiple attribute group decision‐making;2019.0;[];
2903477899;Rational Krylov methods for functions of matrices with applications to fractional partial differential equations;2019.0;[];   In this paper we propose a new choice of poles to define reliable rational Krylov methods. These methods are used for approximating function of positive definite matrices. In particular, the fractional power and the fractional resolvent are considered because of their importance in the numerical solution of fractional partial differential equations. The numerical experiments on some fractional partial differential equation models confirm that the proposed approach is promising.
2903559293;EVM-CNN: Real-Time Contactless Heart Rate Estimation From Facial Video;2019.0;[];With the increase in health consciousness, noninvasive body monitoring has aroused interest among researchers. As one of the most important pieces of physiological information, researchers have remotely estimated the heart rate (HR) from facial videos in recent years. Although progress has been made over the past few years, there are still some limitations, like the processing time increasing with accuracy and the lack of comprehensive and challenging datasets for use and comparison. Recently, it was shown that HR information can be extracted from facial videos by spatial decomposition and temporal filtering. Inspired by this, a new framework is introduced in this paper to remotely estimate the HR under realistic conditions by combining spatial and temporal filtering and a convolutional neural network. Our proposed approach shows better performance compared with the benchmark on the MMSE-HR dataset in terms of both the average HR estimation and short-time HR estimation. High consistency in short-time HR estimation is observed between our method and the ground truth.
2904468863;A group decision making sustainable supplier selection approach using extended TOPSIS under interval-valued Pythagorean fuzzy environment;2019.0;[];   Due to the increasing awareness of environmental and social issues, sustainable supplier selection becomes an important problem. The aim of this paper is to develop a novel group decision making sustainable supplier selection approach using extended Techniques for Order Preferences by Similarity to Ideal Solution (TOPSIS) under interval-valued Pythagorean fuzzy environment. Sustainable supplier selection often involves uncertain information due to the subjective nature of human judgments, and the interval-valued Pythagorean fuzzy set (IVPFS) has great ability to address strong fuzziness, ambiguity and inexactness during the decision-making process. The first contribution of this research is to use the IVPFS to capture the uncertain information of decision makers. Moreover, sustainable supplier selection often involves multiple decision makers from different groups. The second contribution of this research is to develop a group decision making approach for sustainable supplier selection. TOPSIS is the most commonly used technique in sustainable supplier selection. The third contribution of this research is to propose an extended TOPSIS method by integrating distance and similarity between alternatives concurrently to evaluate performances of suppliers. In this research, the group decision making approach and extended TOPSIS method is also extended to IVPFSs. Finally, experiments are conducted to verify the feasibility and efficiency of the proposed sustainable supplier selection approach. Experiments results show that the proposed approach is effective and efficient to help decision makers to select optimal sustainable suppliers. Therefore, the proposed approach can be applied by managers to evaluate and determine appropriate suppliers in sustainable supplier selection process.
2904704195;A comparison of fuzzy DEA and fuzzy TOPSIS in sustainable supplier selection: Implications for sourcing strategy;2019.0;[];   This paper presents a comparative analysis of the outcomes achieved when two widely applied methods for supplier selection—the ‘technique for order of preference by similarity to ideal solution’ (TOPSIS) and ‘data envelopment analysis’—are applied to the problem of identifying the most preferred sustainable suppliers. Both fuzzy DEA and fuzzy TOPSIS are applied to a common dataset of logistics service providers in Sweden. The results reveal that TOPSIS outperforms DEA in terms of both calculation complexity and sensitivity to changes in the number of suppliers. However, output rankings from the two models are found to be less than perfectly correlated. The paper concludes that utilizing both methods, as applied to just a small number of evaluation criteria and a relatively low level of detail in the data, produces a useful pooled shortlist of potential sustainable suppliers. This can then form the basis for a second stage application where either of the methods may be applied to a greater number of criteria that are specified to a higher level of detail. Even more critically, the results also have the potential to point to specific aspects for discussion when negotiating price and service quality commitments with potential sustainable suppliers.
2904724061;DeepDynamicHand: A Deep Neural Architecture for Labeling Hand Manipulation Strategies in Video Sources Exploiting Temporal Information;2018.0;[];Humans are capable of complex manipulation interactions with the environment, relying on the intrinsic adaptability and compliance of their hands. Recently, soft robotic manipulation has attempted to reproduce such an extraordinary behavior, through the design of deformable yet robust end-effectors. To this goal, the investigation of human behavior has become crucial to correctly inform technological developments of robotic hands that can successfully exploit environmental constraint as humans actually do. Among the different tools robotics can leverage on to achieve this objective, deep learning has emerged as a promising approach for the study and then the implementation of neuro-scientific observations on the artificial side. However, current approaches tend to neglect the dynamic nature of hand pose recognition problems, limiting the effectiveness of these techniques in identifying sequences of manipulation primitives underpinning action generation, e.g. during purposeful interaction with the environment. In this work, we propose a vision-based supervised Hand Pose Recognition method which, for the first time, takes into account temporal information to identify meaningful sequences of actions in grasping and manipulation tasks . More specifically, we apply Deep Neural Networks to automatically learn features from hand posture images that consist of frames extracted from grasping and manipulation task videos with objects and external environmental constraints. For training purposes, videos are divided into intervals, each associated to a specific action by a human supervisor. The proposed algorithm combines a Convolutional Neural Network to detect the hand within each video frame and a Recurrent Neural Network to predict the hand action in the current frame, while taking into consideration the history of actions performed in the previous frames. Experimental validation has been performed on two datasets of dynamic hand-centric strategies, where subjects regularly interact with objects and environment. Proposed architecture achieved a very good classification accuracy on both datasets, reaching performance up to 94%, and outperforming state of the art techniques. The outcomes of this study can be successfully applied to robotics, e.g for planning and control of soft anthropomorphic manipulators.
2905305602;Feel-good robotics: requirements on touch for embodiment in assistive robotics;2018.0;[];"The feeling of embodiment, i.e., experiencing the body as belonging to oneself and being able to integrate objects into oneu0027s bodily self-representation, is a key aspect of human self-consciousness and has been shown to importantly shape human cognition. An extension of such feelings toward robots has been argued as being crucial for assistive technologies aiming at restoring, extending, or simulating sensorimotor functions. Empirical and theoretical work illustrates the importance of sensory feedback for the feeling of embodiment and also immersion; we focus on the the perceptual level of touch and the role of tactile feedback in various assistive robotic devices. We critically review how different facets of tactile perception in humans, i.e., affective, social, and self-touch, might influence embodiment. This is particularly important as current assistive robotic devices - such as prostheses, orthoses, exoskeletons, and devices for teleoperation-often limit touch low-density and spatially constrained haptic feedback, i.e., the mere touch sensation linked to an action. Here, we analyze, discuss, and propose how and to what degree tactile feedback might increase the embodiment of certain robotic devices, e.g., prostheses, and the feeling of immersion in human-robot interaction, e.g., in teleoperation. Based on recent findings from cognitive psychology on interactive processes between touch and embodiment, we discuss technical solutions for specific applications, which might be used to enhance embodiment, and facilitate the study of how embodiment might alter human-robot interactions. We postulate that high-density and large surface sensing and stimulation are required to foster embodiment of such assistive devices."
2905588001;Data Lifecycle Challenges in Production Machine Learning: A Survey;2018.0;[];Machine learning has become an essential tool for gleaning knowledge from data and tackling a diverse set of computationally hard tasks. However, the accuracy of a machine learned model is deeply tied to the data that it is trained on. Designing and building robust processes and tools that make it easier to analyze, validate, and transform data that is fed into large-scale machine learning systems poses data management challenges. Drawn from our experience in developing data-centric infrastructure for a production machine learning platform at Google, we summarize some of the interesting research challenges that we encountered, and survey some of the relevant literature from the data management and machine learning communities. Specifically, we explore challenges in three main areas of focus - data understanding, data validation and cleaning, and data preparation. In each of these areas, we try to explore how different constraints are imposed on the solutions depending on where in the lifecycle of a model the problems are encountered and who encounters them.
2906809961;Some q-rung orthopair fuzzy point weighted aggregation operators for multi-attribute decision making;2019.0;[];q-Rung orthopair fuzzy sets, originally proposed by Yager, can dynamically adjust the range of indication of decision information by changing a parameter q based on the different hesitation degree, and point operator is a useful aggregation technology that can control the uncertainty of valuating data from some experts and thus get intensive information in the process of decision making. However, the existing point operators are not available for decision-making problems under q-Rung orthopair fuzzy environment. Thus, in this paper, we firstly propose some new point operators to make it conform to q-rung orthopair fuzzy numbers (q-ROFNs). Then, associated with classic arithmetic and geometric operators, we propose a new class of point weighted aggregation operators to aggregate q-rung orthopair fuzzy information. These proposed operators can redistribute the membership and non-membership in q-ROFNs according to different principle. Furthermore, based on these operators, a novel approach to multi-attribute decision making (MADM) in q-rung orthopair fuzzy context is introduced. Finally, we give a practical example to illustrate the applicability of the new approach. The experimental results show that the novel MADM method outperforms the existing MADM methods for dealing with MADM problems.
2907080401;Short time Fourier transformation and deep neural networks for motor imagery brain computer interface recognition;2018.0;[];
2907166662;Collective Reconstructive Embeddings for Cross-Modal Hashing;2019.0;[];In this paper, we study the problem of cross-modal retrieval by hashing-based approximate nearest neighbor search techniques. Most existing cross-modal hashing works mainly address the issue of multi-modal integration complexity using the same mapping and similarity calculation for data from different media types. Nonetheless, this may cause information loss during the mapping process due to overlooking the specifics of each individual modality. In this paper, we propose a simple yet effective cross-modal hashing approach, termed collective reconstructive embeddings (CRE), which can simultaneously solve the heterogeneity and integration complexity of multi-modal data. To address the heterogeneity challenge, we propose to process heterogeneous types of data using different modality-specific models. Specifically, we model textual data with cosine similarity-based reconstructive embedding to alleviate the data sparsity to the greatest extent, while for image data, we utilize the Euclidean distance to characterize the relationships of the projected hash codes. Meanwhile, we unify the projections of text and image to the Hamming space into a common reconstructive embedding through rigid mathematical reformulation, which not only reduces the optimization complexity significantly but also facilitates the inter-modal similarity preservation among different modalities. We further incorporate the code balance and uncorrelation criteria into the problem and devise an efficient iterative algorithm for optimization. Comprehensive experiments on four widely used multimodal benchmarks show that the proposed CRE can achieve a superior performance compared with the state of the art on several challenging cross-modal tasks.
2907221105;Complete radiation boundary conditions for the Helmholtz equation I: waveguides;2019.0;[];We consider the use of complete radiation boundary conditions for the solution of the Helmholtz equation in waveguides. A general analysis of well-posedness, convergence, and finite element approximation is given. In addition, methods for the optimization of the boundary condition parameters are considered. The theoretical results are illustrated by some simple numerical experiments.
2907536214;Multilevel Model for Video Object Segmentation Based on Supervision Optimization;2019.0;[];In this work, we present a supervised object segmentation algorithm for unconstrained video. Instead of arbitrarily picking a few frames for manual labeling, as in many existing supervised methods, the proposed method selects frames in a more reasonable manner, called supervision optimization. For this, we formulate a principled objective function by inferring the propagation error from appearance and motion clues. After this, we construct a multilevel segmentation model, which consists of low-level and high-level features. On the low level, image pixels are used for a more accurate estimation of motion and segmentation. On the high level, image segments are considered for a more semantic classification of the foreground and background. By integrating these in one segmentation graph, the result can be further improved by leveraging the knowledge from both levels. In experiments, the proposed approach is evaluated by different measures, and the results on a benchmark demonstrate the effectiveness in comparison with other state-of-the-art algorithms.
2908114608;Stationary distribution of a stochastic food chain chemostat model with general response functions;2019.0;[];   In this paper, we focus on a food chain chemostat model with general response functions, perturbed by white noise. Under appropriate assumptions, we establish sufficient conditions for the existence of a unique ergodic stationary distribution by using stochastic Lyapunov analysis method. Our main effort is to construct the suitable Lyapunov function.
2909525106;Iterative learning control for differential inclusions of parabolic type with noninstantaneous impulses;2019.0;[];   In this paper, we present a numerical solution for a finite time complete tracking problem based on the iterative learning control technique for dynamical systems governed by partial differential inclusions of parabolic type with noninstantaneous impulses. By imposing a standard Lipschitz condition on a set-valued mapping and applying conventional P-type updating laws with an initial iterative learning mechanism, we successfully establish an iterative learning process for the tracking problem and conduct a novel convergence analysis with the help of Steiner-type selectors. Sufficient conditions are presented for ensuring asymptotical convergence of the tracking error to zero. Numerical examples are provided to verify the effectiveness of the proposed method with a suitable selection of set-valued mappings.
2909979782;Towards Human-Centric Aggregation via Ordered Weighted Aggregation Operators and Linguistic Data Summaries: A New Perspective on Zadeh's Inspirations;2019.0;[];"This work presents a new perspective on how Zadehu0027s ideas related to fuzzy logic and computing with words have influenced the crucial issue of information aggregation and have led to what may be called a human-centric aggregation. We indicate a need to develop tools and techniques to reflect some fine shades of meaning regarding what can be considered the very purpose of human-centric aggregation, notably stated by various modalities in natural language specifications, in particular the usuality. We advocate the use of the ordered weighted average (OWA) operator, which is a formidable tool that can easily be tailored to a user?s intention as to the purpose and method of aggregation, generalizing many simple and natural aggregation types, such as the arithmetic mean, maximum and minimum, and probability. We show some of the most representative extensions and generalizations, including the induced OWA, the generalized OWA, the probabilistic OWA, and the OWA distance. We show their use in the basic case of the aggregation of numerical values and in social choice (voting) results. Then, we claim that linguistic data summaries in Yager?s sense can be considered an ""ultimately human consistent"" form of human-centric aggregation and show how the OWA operators can be used therein."
2910050977;Clutter-based gain and phase calibration for monostatic MIMO radar with partly calibrated array;2019.0;[];   The gain and phase calibration of monostatic multiple-input multiple-output (MIMO) radars with partly calibrated arrays is addressed and a clutter-based orthogonal projection (OP) method is introduced. When the transmitter and receiver of a monostatic MIMO radar are both uniform linear arrays with the same array spacing, a non-zero OP matrix for the joint transmit-receive array steering vectors can be found. The clutter output power projected on the OP matrix is zero if array is perfectly calibrated. Hence, the transmitter gain and phase can be calibrated by minimizing clutter projection output power for a well-calibrated receiver subarray. Subsequently, the unknown receiver gain and phase errors can be estimated with a similar projection method. Numerical simulation results confirm the effectiveness of the proposed method.
2911489589;IoT-enabled smart grid via SM: An overview;2019.0;[];   Power quality and reliability issues are big challenges to both service provider and consumers in conventional power grids. The ongoing technological advancements in the Internet of Things (IoT) era provide better solutions to enhance the management of these challenges and enforce the measures of a Smart Grid (SG). Advanced Metering Infrastructure (AMI) and Smart Metering (SM) technologies are enabler technologies that can modernize the conventional power grid through exposing the hidden details of electrical power by introducing two-way communication scheme during power transaction process between utilities and consumers. Throughout literature, AMI and SM technologies are widely discussed. However, few studies discuss the role of SM in power quality and reliability monitoring in IoT-enabled SGs. Hence, the paper aims to comprehensively review the feasibility of employing SM for power quality and reliability monitoring. First, we provide a detailed overview about the SMs, wireless communication technologies, and routing algorithms as enabling technologies in AMI. Then, we categorize the existing literature works that target power quality and reliability monitoring. Finally, open research issues are outlined based on shortages in the existing literature.
2911561799;Scenario optimization;1991.0;[];
2911722481;Deep learning IoT system for online stroke detection in skull computed tomography images;2019.0;[];   Cerebral vascular accidents (CVA) affect about 16 million people worldwide annually. CVA, also know as stroke, is a serious global health problem, and can cause significant physical limitations to those affected. Computed tomography is the most appropriate procedure to diagnose and evaluate the dimensions and magnitude of a stroke. Thus, in this article we present an Internet of Things (IoT) framework for the classification of stroke from CT images applying Convolutional Neural Networks (CNN) in order to identifying a healthy brain, an ischemic stroke or a hemorrhagic stroke. Following the Transfer Learning concept CNN was combined with different consolidated Machine Learning methods such as Bayesian Classifier, Multilayer Perceptron, k-Nearest Neighbor, Random Forest and Support Vector Machines. Our approach contributes to the automation of the diagnostic process by a competent method that is able to obtain information imperceptible to the human eye, and thus it contributes to a more precise diagnosis. In addition, with the advent of IoT, a highly efficient and flexible new instrument emerges to address issues related to health care services and specifically in our approach can provide remote diagnoses and monitoring of patients. The approach was validated by analyzing the parameters Accuracy, F1-Score, Recall, Precision and processing time. The results showed that CNN obtained 100% Accuracy, F1-Score, Recall and Precision in combination with most of the classifiers tested. The shortest training and test times were 0.015 s and 0.001 s, respectively, both in combination with the Bayesian Classifier. Thus, our proposed approach demonstrates efficiency and reliability to detect strokes.
2911736392;TRAC-1 Shared Task on Aggression Identification: IIT(ISM)$@$COLING'18.;2018.0;[];
2912026968;Cyberbullying Detection Task: the EBSI-LIA-UNAM System (ELU) at COLING’18 TRAC-1;2018.0;[];
2912102236;Benchmarking Aggression Identification in Social Media.;2018.0;[];
2912123473;Aggression Detection in Social Media: Using Deep Neural Networks, Data Augmentation, and Pseudo Labeling.;2018.0;[];
2912248199;Fully Connected Neural Network with Advance Preprocessor to Identify Aggression over Facebook and Twitter.;2018.0;[];
2912478203;SaGe: Web Preemption for Public SPARQL Query Services;2019.0;[];To provide stable and responsive public SPARQL query services, data providers enforce quotas on server usage. Queries which exceed these quotas are interrupted and deliver partial results. Such interruption is not an issue if it is possible to resume queries execution afterward. Unfortunately, there is no preemption model for the Web that allows for suspending and resuming SPARQL queries. In this paper, we propose SaGe: a SPARQL query engine based on Web preemption. SaGe allows SPARQL queries to be suspended by the Web server after a fixed time quantum and resumed upon client request. Web preemption is tractable only if its cost in time is negligible compared to the time quantum. The challenge is to support the full SPARQL query language while keeping the cost of preemption negligible. Experimental results demonstrate that SaGe outperforms existing SPARQL query processing approaches by several orders of magnitude in term of the average total query execution time and the time for first results.
2912748147;Preference Learning;2010.0;[];"The topic of preferences is a new branch of machine learning and data mining, and it has attracted considerable attention in artificial intelligence research in previous years. It involves learning from observations that reveal information about the preferences of an individual or a class of individuals. Representing and processing knowledge in terms of preferences is appealing as it allows one to specify desires in a declarative way, to combine qualitative and quantitative modes of reasoning, and to deal with inconsistencies and exceptions in a flexible manner. And, generalizing beyond training data, models thus learned may be used for preference prediction. This is the first book dedicated to this topic, and the treatment is comprehensive. The editors first offer a thorough introduction, including a systematic categorization according to learning task and learning technique, along with a unified notation. The first half of the book is organized into parts on label ranking, instance ranking, and object ranking; while the second half is organized into parts on applications of preference learning in multiattribute domains, information retrieval, and recommender systems. The book will be of interest to researchers and practitioners in artificial intelligence, in particular machine learning and data mining, and in fields such as multicriteria decision-making and operations research."
2912918913;Aggression Identification and Multi Lingual Word Embeddings.;2018.0;[];
2912889105;Unsupervised learning;1999.0;[];
2912994242;Aggression Detection in Social Media using Deep Neural Networks.;2018.0;[];
2913044599;Merging Datasets for Aggressive Text Identification.;2018.0;[];
2913245224;Type-2 Fuzzy Envelope of Hesitant Fuzzy Linguistic Term Set: A New Representation Model of Comparative Linguistic Expression;2019.0;[];The use of hesitant fuzzy linguistic term sets contributes to the elicitation of comparative linguistic expressions in decision contexts when experts hesitate among different linguistic terms to provide their assessments. Since the existing representation models for linguistic expressions based on hesitant fuzzy linguistic term sets do not consider properly the uncertainty caused by the inherent vagueness of such linguistic expressions, it is necessary to improve their modeling to cope with such vagueness. In this paper, we propose a new fuzzy envelope for the hesitant fuzzy linguistic term sets in form of type-2 fuzzy sets for representing comparative linguistic expressions. Such an envelope overcomes the limitation of existing representations in coping with inherent uncertainties and facilitates the processes of computing with words for linguistic decision making problems dealing with comparative linguistic expressions.
2913440437;Cyber-aggression Detection using Cross Segment-and-Concatenate Multi-Task Learning from Text.;2018.0;[];
2913474415;Filtering Aggression from the Multilingual Social Media Feed.;2018.0;[];
2913799641;Exertion Games;2016.0;[];"Advances in human-computer interaction HCI technologies have led to emerging computer game systems that foster physical exertion as part of the interaction; we call them exertion games. These games highlight a body-centric perspective on our interactions with computers, in contrast to traditional mouse, keyboard and gamepad interactions, not just in terms of their physical interface, but also in terms of the experiences that they support. As a result, exertion games show great promise in facilitating not only health benefits, but also novel play experiences. However, to realize this promise, exertion games need to be well designed, not only in terms of technical aspects involving the sensing of the active body, but also in relation to the experiential perspective of an active human body. This article provides an overview of existing work on exertion games, outlines a spectrum of exertion games, and presents an analysis of key enabling technologies. We also position exertion games within a broader HCI context by reviewing and examining different design approaches and frameworks for building exertion games. Finally, the article concludes with directions for future work."
2913929186;Adaptive Independent Subspace Analysis of Brain Magnetic Resonance Imaging Data;2019.0;[];Methods for image registration, segmentation, and visualization of magnetic resonance imaging (MRI) data are used widely to help medical doctors in supporting diagnostics. The large amount and complexity of MRI data require looking for new methods that allow for efficient processing of this data. Here, we propose using the adaptive independent subspace analysis (AISA) method to discover meaningful electroencephalogram activity in the MRI scan data. The results of AISA (image subspaces) are analyzed using image texture analysis methods to calculate first order, gray-level co-occurrence matrix, gray-level size-zone matrix, gray-level run-length matrix, and neighboring gray-tone difference matrix features. The obtained feature space is mapped to the 2D space using the t-distributed stochastic neighbor embedding method. The classification results achieved using the k-nearest neighbor classifier with 10-fold cross-validation have achieved 94.7% of accuracy (and f-score of 0.9356) from the real autism spectrum disorder dataset.
2914011592;Multi-scale aggregation network for temporal action proposals;2019.0;[];   Temporal action detection is a very challenging and valuable task for video analysis and applications. The detection results, to a great extent, rely on the quality of temporal action proposals. However, temporal actions in videos vary dramatically, e.g. from a fraction of a second to minutes, which causes much difficulties for accurate temporal action proposals. In this paper, we propose a multi-scale aggregation network to overcome those variations for temporal action proposals. Our proposed network generates an actionness score sequence for the input video to automatically perceive the duration of actions, and thus can dynamically generate corresponding lengths of action proposals for them. For more reliable actionness prediction, we propose to adaptively explore the intrinsic short and long dependencies in action by two multi-scale aggregation strategies: unit level multi-scale aggregation and proposal level multi-scale aggregation. We also propose to take the soft labelling to facilitate the actionness prediction for the units near the action boundaries. Extensive experiments on THUMOS14 dataset have demonstrated the effectiveness of our proposed method.
2914034656;Combining Shallow and Deep Learning for Aggressive Text Detection.;2018.0;[];
2914317128;IRIT at TRAC 2018.;2018.0;[];
2914491155;Aggression Identification Using Deep Learning and Data Augmentation.;2018.0;[];
2914569437;Textual Aggression Detection through Deep Learning.;2018.0;[];
2915594435;Towards Privacy-Preserving Malware Detection Systems for Android;2018.0;[];Android is the primary target for mobile malware. To protect users, phone vendors (e.g., Samsung and Huawei) usually leverage third-party security service providers (e.g., VirusTotal and Qihoo 360) to detect malicious apps in app stores and collect appsu0027 runtime behaviors on usersu0027 phones to further spot malware missed in the previous step. However, this practice could cause privacy concerns to phone vendors, users and security service providers. Specifically, phone vendors do not want to share apps (including the paid ones) with security service providers, while the latter do not want to share the malware signatures with the former. Moreover, users do not want to expose appsu0027 runtime behaviors to third parties. These concerns would cause a real dilemma for each involved party. In this paper, we propose a privacy-preserving malware detection system for Android, in which the privacy (or assets) of phone vendors, users, and security service providers are protected. It detects malicious apps in phone vendoru0027s app stores and on usersu0027 phones, without directly sharing apps, appsu0027 runtime behaviors, and malware signatures to other parties. We implement a prototype system called PPMDroid and apply several optimizations to save bandwidth and speed up the process. Extensive evaluation results with real malware samples demonstrate the effectiveness and efficiency of our system.
2915596699;A hierarchical gravitational search algorithm with an effective gravitational constant;2019.0;[];   Gravitational search algorithm (GSA) inspired by the law of gravity is a swarm intelligent optimization algorithm. It utilizes the gravitational force to implement the interaction and evolution of individuals. The conventional GSA achieves several successful applications, but it still faces a premature convergence and a low search ability. To address these two issues, a hierarchical GSA with an effective gravitational constant (HGSA) is proposed from the viewpoint of population topology. Three contrastive experiments are carried out to analyze the performances between HGSA and other GSAs, heuristic algorithms and particle swarm optimizations (PSOs) on function optimization. Experimental results demonstrate the effective property of HGSA due to its hierarchical structure and gravitational constant. A component-wise experiment is also established to further verify the superiority of HGSA. Additionally, HGSA is applied to several real-world optimization problems so as to verify its good practicability and performance. Finally, the time complexity analysis is discussed to conclude that HGSA has the same computational efficiency in comparison with other GSAs.
2916340702;A hybrid biogeography-based optimization with variable neighborhood search mechanism for no-wait flow shop scheduling problem;2019.0;[];   The no-wait flow shop scheduling problem (NWFSP) plays an essential role in the manufacturing industry. Inspired by the overall process of biogeography theory, the standard biogeography-based optimization (BBO) was constructed with migration and mutation operators. In this paper, a hybrid biogeography-based optimization with variable neighborhood search (HBV) is implemented for solving the NWFSP with the makespan criterion. The modified NEH and the nearest neighbor mechanism are employed to generate a potential initial population. A hybrid migration operator, which combines the path relink technique and the block-based self-improvement strategy, is designed to accelerate the convergence speed of HBV. The iterated greedy (IG) algorithm is introduced into the mutation operator to obtain a promising solution in exploitation phase. A variable neighbor search strategy, which is based on the block neighborhood structure and the insert neighborhood structure, is designed to perform the local search around the current best solution in each generation. Furthermore, the global convergence performance of the HBV is analyzed with the Markov model. The computational results and comparisons with other state-of-art algorithms based on Taillard and VRF benchmark show that the efficiency and performance of HBV for solving NWFSP.
2916778556;Math Doesn't Have to be Hard: Logic Block Architectures to Enhance Low-Precision Multiply-Accumulate on FPGAs;2019.0;[];Recent work has shown that using low-precision arithmetic in Deep Neural Network (DNN) inference acceleration can yield large efficiency gains with little or no accuracy degradation compared to half or single precision floating-point by enabling more MAC operations per unit area. The most efficient precision is a complex function of the DNN application, structure and required accuracy, which makes the variable precision capabilities of FPGAs very valuable. We propose three logic block architecture enhancements to increase the density and reduce the delay of multiply-accumulate (MAC) operations implemented in the soft fabric. Adding another level of carry chain to the ALM (extra carry chain architecture) leads to a 1.5x increase in MAC density, while ensuring a small impact on general designs as it adds only 2.6% FPGA tile area and a representative critical path delay increase of 0.8%. On the other hand, our highest impact option, which combines our 4-bit Adder architecture with a 9-bit Shadow Multiplier, increases MAC density by 6.1x, at the cost of larger tile area and representative critical path delay overheads of 16.7% and 9.8%, respectively.
2919373174;Improved Biogeography-Based Optimization Algorithm and Its Application to Clustering Optimization and Medical Image Segmentation;2019.0;[];
2919803975;Agent Based Information Security Framework for Hybrid Cloud Computing;2019.0;[];
2919979744;Harris hawks optimization: Algorithm and applications;2019.0;[];   In this paper, a novel population-based, nature-inspired optimization paradigm is proposed, which is called Harris Hawks Optimizer (HHO). The main inspiration of HHO is the cooperative behavior and chasing style of Harris’ hawks in nature called surprise pounce. In this intelligent strategy, several hawks cooperatively pounce a prey from different directions in an attempt to surprise it. Harris hawks can reveal a variety of chasing patterns based on the dynamic nature of scenarios and escaping patterns of the prey. This work mathematically mimics such dynamic patterns and behaviors to develop an optimization algorithm. The effectiveness of the proposed HHO optimizer is checked, through a comparison with other nature-inspired techniques, on 29 benchmark problems and several real-world engineering problems. The statistical results and comparisons show that the HHO algorithm provides very promising and occasionally competitive results compared to well-established metaheuristic techniques. Source codes of HHO are publicly available at  http://www.alimirjalili.com/HHO.html  and  http://www.evo-ml.com/2019/03/02/hho .
2920990744;Some cosine similarity measures and distance measures between q‐rung orthopair fuzzy sets;2019.0;[];
2921062571;Assistant Vehicle Localization Based on Three Collaborative Base Stations via SBL-Based Robust DOA Estimation;2019.0;[];As a promising research area in Internet of Things (IoT), Internet of Vehicles (IoV) has attracted much attention in wireless communication and network. In general, vehicle localization can be achieved by the global positioning systems (GPSs). However, in some special scenarios, such as cloud cover, tunnels or some places where the GPS signals are weak, GPS cannot perform well. The continuous and accurate localization services cannot be guaranteed. In order to improve the accuracy of vehicle localization, an assistant vehicle localization method based on direction-of-arrival (DOA) estimation is proposed in this paper. The assistant vehicle localization system is composed of three base stations (BSs) equipped with a multiple input multiple output (MIMO) array. The locations of vehicles can be estimated if the positions of the three BSs and the DOAs of vehicles estimated by the BSs are known. However, the DOA estimated accuracy maybe degrade dramatically when the electromagnetic environment is complex. In the proposed method, a sparse Bayesian learning (SBL)-based robust DOA estimation approach is first proposed to achieve the off-grid DOA estimation of the target vehicles under the condition of nonuniform noise, where the covariance matrix of nonuniform noise is estimated by a least squares (LSs) procedure, and a grid refinement procedure implemented by finding the roots of a polynomial is performed to refine the grid points to reduce the off-grid error. Then, according to the DOA estimation results, the target vehicle is cross-located once by each two BSs in the localization system. Finally, robust localization can be realized based on the results of three-time cross-location. Plenty of simulation results demonstrate the effectiveness and superiority of the proposed method.
2921483513;A Hybrid Feature Extraction Method With Regularized Extreme Learning Machine for Brain Tumor Classification;2019.0;[];Brain cancer classification is an important step that depends on the physician’s knowledge and experience. An automated tumor classification system is very essential to support radiologists and physicians to identify brain tumors. However, the accuracy of current systems needs to be improved for suitable treatments. In this paper, we propose a hybrid feature extraction method with a regularized extreme learning machine (RELM) for developing an accurate brain tumor classification approach. The approach starts by preprocessing the brain images by using a min–max normalization rule to enhance the contrast of brain edges and regions. Then, the brain tumor features are extracted based on a hybrid method of feature extraction. Finally, a RELM is used for classifying the type of brain tumor. To evaluate and compare the proposed approach, a set of experiments is conducted on a new public dataset of brain images. The experimental results proved that the approach is more effective compared with the existing state-of-the-art approaches, and the performance in terms of classification accuracy improved from 91.51% to 94.233% for the experiment of the random holdout technique.
2922034350;A multiple feature construction method based on gravitational search algorithm;2019.0;[];   One of the most important issues to tackle in data classification has been the existence of non-informative features in feature sets. Therefore, feature construction (FC) is an important pre-processing task to construct discriminating features from the original ones. Gravitational search algorithm (GSA) is a powerful swarm-based metaheuristic algorithm, which has been improved and adapted to represent multiple new constructed features. Most of the swarm-based algorithms entail a population of individuals while one individual would be returned as an optimal solution at the end of the process. In this paper, the solutions’ structure of GSA have been changed in a way that each individual can be considered as a part of the solution, and the final result consists of the whole population. Consequently, each individual is a constructed feature aiming to achieve a population of good features. In other words, the proposed method is a novel multiple feature construction (MFC) method based on the GSA which is called in brief GSAMFC. The experimental results on thirteen standard data sets demonstrate that the proposed GSAMFC is highly beneficial for providing suitable and small feature subsets as well as improving the classifier accuracy. The obtained results of GSAMFC and those of the competing algorithms prove the proficiency of the proposed method.
2923735516;Toward a European exascale ecosystem: the EuroHPC joint undertaking;2019.0;[];
2924079966;EEG-Based Brain-Computer Interfaces Using Motor-Imagery: Techniques and Challenges;2019.0;[];Electroencephalography (EEG)-based brain-computer interfaces (BCIs), particularly those using motor-imagery (MI) data, have the potential to become groundbreaking technologies in both clinical and entertainment settings. MI data is generated when a subject imagines the movement of a limb. This paper reviews state-of-the-art signal processing techniques for MI EEG-based BCIs, with a particular focus on the feature extraction, feature selection and classification techniques used. It also summarizes the main applications of EEG-based BCIs, particularly those based on MI data, and finally presents a detailed discussion of the most prevalent challenges impeding the development and commercialization of EEG-based BCIs.
2924973947;Research on the assessment of classroom teaching quality with q‐rung orthopair fuzzy information based on multiparametric similarity measure and combinative distance‐based assessment;2019.0;[];
2926314329;On Challenges in Machine Learning Model Management.;2018.0;[];
2930250717;A model specification for the design of trust negotiations;2019.0;[];   Trust negotiation is a type of trust management model for establishing trust between entities by a mutual exchange of credentials. This approach was designed for online environments, where the attributes of users, such as skills, habits, behaviour and experience are unknown. Required criteria of trust negotiation must be supported by a trust negotiation model in order to provide a functional, adequately robust and efficient application. Such criteria were identified previously. In this paper we are presenting a model specification using a UML-based notation for the design of trust negotiation. This specification will become a part of the Software Development Life Cycle, which will provide developers a strong tool for incorporating trust and trust-related issues into the software they create. The specification defines components and their layout for the provision of the essential functionality of trust negotiation on one side as well as optional, additional features on the other side. The extra features make trust negotiation more robust, applicable for more scenarios and may provide a privacy protection functionality.
2932060911;Multi-Region Risk-Sensitive Cognitive Ensembler for Accurate Detection of Attention-Deficit/Hyperactivity Disorder;2019.0;[];In this paper, we present a multi-region ensemble classifier approach (MRECA) using a cognitive ensemble of classifiers for accurate identification of attention-deficit/hyperactivity disorder (ADHD) subjects. This approach is developed using the features extracted from the structural MRIs of three different developing brain regions, viz., the amygdala, caudate, and hippocampus. For this study, the structural magnetic resonance imaging (sMRI) data provided by the ADHD-200 consortium has been used to identify the following three classes of ADHD, viz., ADHD-combined, ADHD-inattentive, and the TDC (typically developing control). From the sMRIs of the amygdala, caudate, and hippocampus regions of the brain from the ADHD-200 data, multiple feature sets were obtained using a feature-selecting genetic algorithm (FSGA), in a wraparound approach using an extreme learning machine (ELM) basic classifier. An improved crossover operator for the FSGA has been developed for obtaining higher accuracies compared with other existing crossover operators. From the multiple feature sets and the corresponding ELM classifiers, a classifier-selecting genetic algorithm (CSGA) has been developed to identify the top performing feature sets and their ELM classifiers. These classifiers are then combined using a risk-sensitive hinge loss function to form a risk-sensitive cognitive ensemble classifier resulting in a simultaneous multiclass classification of ADHD with higher accuracies. Performance evaluation of the multi-region ensemble classifier is presented under the following three scenarios, viz., region-based individual (best) classifier, region-based ensemble classifier, and finally a multiple-region-based ensemble classifier. The study results clearly indicate that the proposed “multi-region ensemble classification approach” (MRECA) achieves a much higher classification accuracy of ADHD data (normally a difficult problem because of the variations in the data) compared with other existing methods.
2933602294;Knowledge Graphs: New Directions for Knowledge Representation on the Semantic Web (Dagstuhl Seminar 18371);2019.0;[];"The increasingly pervasive nature of the Web, expanding to devices and things in  :[13],""along with new trends in Artificial Intelligence call for new paradigms and a new look  :[29],""Representation and Processing at scale for the Semantic Web. The emerging, but  :[42],""be concretely shaped concept of ""Knowledge Graphs"" provides an excellent unifying  :[54],""this current status of Semantic Web research. More than two decades of Semantic  provides a :[68],""solid basis and a promising technology and standards stack to interlink  and :[82],""knowledge on the Web. However, neither are applications for Knowledge  :[94],""such limited to Linked Open Data, nor are instantiations of Knowledge Graphs in  :[108],""while often inspired by – limited to the core Semantic Web stack. This report documents  and the :[124],""outcomes of Dagstuhl Seminar 18371 ""Knowledge Graphs: New Directions  :[29],""Representation on the Semantic :[136],""Web"", where a group of experts from academia  :[149],""discussed fundamental questions around these topics for a week in early September  the :[162],""following: what are :[82],""knowledge graphs? Which applications do we see to  :[175],""open research questions still need :[42],""be addressed and which technology gaps still need  :[189],""closed?"
2934195809;Divergence measure of Pythagorean fuzzy sets and its application in medical diagnosis;2019.0;[];   The Pythagorean fuzzy set (PFS) which is an extension of intuitionistic fuzzy set, is more capable of expressing and handling the uncertainty under uncertain environments, so that it was broadly applied in a variety of fields. Whereas, how to measure PFSs’ distance appropriately is still an open issue. It is well known that the square root of Jensen–Shannon divergence is a true metric in the probability distribution space which is a useful measure of distance. On account of this point, a novel divergence measure between PFSs is proposed by taking advantage of the Jensen–Shannon divergence in this paper, called as PFSJS distance. This is the first work to consider the divergence of PFSs for measuring the discrepancy of data from the perspective of the relative entropy. The new PFSJS distance measure has some desirable merits, in which it meets the distance measurement axiom and can better indicate the discrimination degree of PFSs. Then, numerical examples demonstrate that the PFSJS distance can avoid generating counter-intuitive results which is more feasible, reasonable and superior than existing distance measures. Additionally, a new algorithm based on the PFSJS distance measure is designed to solve the problems of medical diagnosis. By comparing the different methods in the medical diagnosis application, it is found that the new algorithm is as efficient as the other methods. These results prove that the proposed method is practical in dealing with the medical diagnosis problems.
2938963980;Optimal mission abort policy for systems subject to random shocks based on virtual age process;2019.0;[];   Safety critical systems such as aircrafts, submarines and space stations are required to perform various missions. To enhance the survivability of such systems, a mission can be aborted when a certain malfunction or incident condition is satisfied and a rescue procedure should be activated. This paper develops a novel mission abort policy for systems experiencing both internal failure and external shocks. Failure process of the system can be divided into two stages from new to the initialization of a defect, and from that to failure. Motivated by the virtual age concept, the impact of external shocks is characterized by random virtual age increment in the two stages. We consider a policy where a mission is aborted if the duration in defective state is larger than a given threshold. Under the stochastic failure model and mission abort policy, mission success probability and system survivability are evaluated and the optimal abort threshold balancing the tradeoff between the system survivability and the mission success probability is investigated. A numerical example on a cooling system in chemical reactors is given to illustrate the applicability of the abort policy.
2939362715;HiGRU: Hierarchical Gated Recurrent Units for Utterance-level Emotion Recognition;2019.0;[];"In this paper, we address three challenges in utterance-level emotion recognition in dialogue systems: (1) the same word can deliver different emotions in different contexts; (2) some emotions are rarely seen in general dialogues; (3) long-range contextual information is hard to be effectively captured. We therefore propose a hierarchical Gated Recurrent Unit (HiGRU) framework with a lower-level GRU to model the word-level inputs and an upper-level GRU to capture the contexts of utterance-level embeddings. Moreover, we promote the framework to two variants, HiGRU with individual features fusion (HiGRU-f) and HiGRU with self-attention and features fusion (HiGRU-sf), so that the word/utterance-level individual inputs and the long-range contextual information can be sufficiently utilized. Experiments on three dialogue emotion datasets, IEMOCAP, Friends, and EmotionPush demonstrate that our proposed HiGRU models attain at least 8.7%, 7.5%, 6.0% improvement over the state-of-the-art methods on each dataset, respectively. Particularly, by utilizing only the textual feature in IEMOCAP, our HiGRU models gain at least 3.8% improvement over the state-of-the-art conversational memory network (CMN) with the trimodal features of text, video, and audio."
2941110559;A review on the self and dual interactions between machine learning and optimisation;2019.0;[];"Machine learning and optimisation are two growing fields of artificial intelligence with an enormous number of computer science applications. The techniques in the former area aim to learn knowledge from data or experience, while the techniques from the latter search for the best option or solution to a given problem. To employ these techniques automatically and effectively aligning with the real aim of artificial intelligence, both sets of techniques are frequently hybridised, interacting with each other and themselves. This study focuses on such interactions aiming at (1) presenting a broad overview of the studies on self and dual interactions between machine learning and optimisation; (2) providing a useful tutorial for researchers and practitioners in both fields in support of collaborative work through investigation of the recent advances and analyses of the advantages and disadvantages of different techniques to tackle the same or similar problems; (3) clarifying the overlapping terminologies having different meanings used in both fields; (4) identifying research gaps and potential research directions."
2941914178;Human emotion recognition using deep belief network architecture;2019.0;[];   Recently, deep learning methodologies have become popular to analyse physiological signals in multiple modalities via hierarchical architectures for human emotion recognition. In most of the state-of-the-arts of human emotion recognition, deep learning for emotion classification was used. However, deep learning is mostly effective for deep feature extraction. Therefore, in this research, we applied unsupervised deep belief network (DBN) for depth level feature extraction from fused observations of Electro-Dermal Activity (EDA), Photoplethysmogram (PPG) and Zygomaticus Electromyography (zEMG) sensors signals. Afterwards, the DBN produced features are combined with statistical features of EDA, PPG and zEMG to prepare a feature-fusion vector. The prepared feature vector is then used to classify five basic emotions namely Happy, Relaxed, Disgust, Sad and Neutral. As the emotion classes are not linearly separable from the feature-fusion vector, the Fine Gaussian Support Vector Machine (FGSVM) is used with radial basis function kernel for non-linear classification of human emotions. Our experiments on a public multimodal physiological signal dataset show that the DBN, and FGSVM based model significantly increases the accuracy of emotion recognition rate as compared to the existing state-of-the-art emotion classification techniques.
2942508766;Simplified and Enhanced Multiple Level Nested Arrays Exploiting High-Order Difference Co-Arrays;2019.0;[];Based on the high-order difference co-array concept, an enhanced four-level nested array (E-FL-NA) is first proposed, which optimizes the consecutive lags at the fourth-order difference co-array stage. To simplify sensor location formulations for comprehensive illustration and also convenient structure construction, a simplified and enhanced four-level nested array (SE-FL-NA) is then proposed, whose performance is compromised but still better than the four-level nested array (FL-NA). This simplified structure is further extended to the higher order case with multiple sub-arrays, referred to as simplified and enhanced multiple level nested arrays (SE-ML-NAs), where significantly increased degrees of freedom can be provided and exploited for underdetermined direction of arrival estimation. Simulation results are provided to demonstrate the performance of the proposed E-FL-NA, whereas a higher number of detectable sources is achieved by the SE-ML-NA with a limited number of physical sensors.
2942571977;SIMAS: smart IoT model for acute stroke avoidance;2019.0;[];
2944198247;Precise Point Positioning Using Dual-Frequency GNSS Observations on Smartphone;2019.0;[];The update of the Android system and the emergence of the dual-frequency GNSS chips enable smartphones to acquire dual-frequency GNSS observations. In this paper, the GPS L1/L5 and Galileo E1/E5a dual-frequency PPP (precise point positioning) algorithm based on RTKLIB and GAMP was applied to analyze the positioning performance of the Xiaomi Mi 8 dual-frequency smartphone in static and kinematic modes. The results showed that in the static mode, the RMS position errors of the dual-frequency smartphone PPP solutions in the E, N, and U directions were 21.8 cm, 4.1 cm, and 11.0 cm, respectively, after convergence to 1 m within 102 min. The PPP of dual-frequency smartphone showed similar accuracy with geodetic receiver in single-frequency mode, while geodetic receiver in dual-frequency mode has higher accuracy. In the kinematic mode, the positioning track of the smartphone dual-frequency data had severe fluctuations, the positioning tracks derived from the smartphone and the geodetic receiver showed approximately difference of 3–5 m.
2944813347;Common spatial patterns combined with phase synchronization information for classification of EEG signals;2019.0;[];   The common spatial patterns (CSP) approach is a classical and representative technique of optimizing spatial filters of electroencephalogram (EEG) signals in the community of brain computer interfaces (BCI). It, however, utilizes only amplitude information of the EEG signals. The phase information of the multi-channel EEG series, on the other hand, plays an important role in characterizing brain activities. In this paper, we consider enhancing the classification performance of CSP by making explicit use of information of the phase synchronization. An index, termed as rank-weighted phase lag index (rWPLI), is introduced to qualify the intrinsic phase synchronization. The rWPLI features are then incorporated into the CSP framework via three ways of feature combinations. The classification experiments on real EEG data sets of BCI competitions show the effectiveness of the proposed framework.
2946502251;ICFR: An effective incremental collaborative filtering based recommendation architecture for personalized websites;2019.0;[];To solve the problem that users’ retrieval intentions are seldom considered by personalized websites, we propose an improved incremental collaborative filtering (CF)-based recommendation implementation method (ICFR) in this paper. The ICFR model uses one of the most popular recommendation algorithms – the collaborative filtering recommendation algorithm – for personalized websites. This paper first uses a CF algorithm to obtain the relationship between user preferences and recommended content. Second, the browsing behaviour information of users is extracted by analysing Web logs and is then converted into ratings. Finally, an incremental algorithm is designed to update historical user preference data. Based on this established model, we propose some cases for this architecture, which illustrate that the ICFR model is suitable for personalized website recommendations.
2946627595;A QoS-aware virtual machine scheduling method for energy conservation in cloud-based cyber-physical systems;2019.0;[];Nowadays, with the development of cyber-physical systems (CPS), there are an increasing amount of applications deployed in the CPS to connect cyber space with physical world better and closer than ever. Furthermore, the cloud-based CPS bring massive computing and storage resource for CPS, which enables a wide range of applications. Meanwhile, due to the explosive expansion of applications deployed on the CPS, the energy consumption of the cloud-based CPS has received wide concern. To improve the energy efficiency in the cloud environment, the virtualized technology is employed to manage the resources, and the applications are generally hosted by virtual machines (VMs). However, it remains challenging to meet the Quality-of-Service (QoS) requirements. In view of this challenge, a QoS-aware VM scheduling method for energy conservation, named QVMS, in cloud-based CPS is designed. Technically, our scheduling problem is formalized as a standard multi-objective problem first. Then, the Non-dominated Sorting Genetic Algorithm III (NSGA-III) is adopted to search the optimal VM migration solutions. Besides, SAW (Simple Additive Weighting) and MCDM (Multiple Criteria Decision Making) are employed to select the most optimal scheduling strategy. Finally, simulations and experiments are conducted to verify the effectiveness of our proposed method.
2946699206;A lightweight and cost effective edge intelligence architecture based on containerization technology;2019.0;[];The integration of Cloud computing and Internet of Things led to rapid growth in the edge computing field. This would not be achievable without combining the data centers’ managing systems with much more restrained technologies. One significantly effective and lightweight solution to this issue is presented by the Docker technology. It is able to manage virtualization process and can therefore be used to distribute, deploy and manage cloud and edge applications assigned into the clusters. In our study, this technology was represented by the Raspberry Pi devices, which are convenient thanks to their low cost, robust applicability and lightweight nature. Our application scenario focuses on identification of the human activities. In this paper, we suggest and evaluate an architecture on the basis of the distributed edge/cloud integration paradigm. We explain all of its advantages which lie in the combination of affordability and several other benefits provided by the fact that data processing is conducted by the edge devices instead of the central server. To recognize and identify human activity, the Regularized Extreme Leaning Machine (RELM) was engaged in our architecture. Our study presents detailed information about our use case scenario and the experimental simulation we performed.
2947910571;Player Selection for a National Football Team using Fuzzy AHP and Fuzzy TOPSIS.;2019.0;[];
2949120650;EmojiNet: Building a Machine Readable Sense Inventory for Emoji;2016.0;[];Emoji are a contemporary and extremely popular way to enhance electronic communication. Without rigid semantics attached to them, emoji symbols take on different meanings based on the context of a message. Thus, like the word sense disambiguation task in natural language processing, machines also need to disambiguate the meaning or sense of an emoji. In a first step toward achieving this goal, this paper presents EmojiNet, the first machine readable sense inventory for emoji. EmojiNet is a resource enabling systems to link emoji with their context-specific meaning. It is automatically constructed by integrating multiple emoji resources with BabelNet, which is the most comprehensive multilingual sense inventory available to date. The paper discusses its construction, evaluates the automatic resource creation process, and presents a use case where EmojiNet disambiguates emoji usage in tweets. EmojiNet is available online for use at this http URL.
2949275038;PIR-DSP: An FPGA DSP Block Architecture for Multi-precision Deep Neural Networks;2019.0;[];Quantisation is a key optimisation strategy to improve the performance of floating-point deep neural network (DNN) accelerators. Digital signal processing (DSP) blocks on field-programmable gate arrays are not efficiently utilised when the accelerator precision is much lower than the DSP precision. Through three modifications to Xilinx DSP48E2 DSP blocks, we address this issue for important computations in embedded DNN accelerators, namely the standard, depth-wise, and pointwise convolutional layers. First, we propose a flexible precision, run-time decomposable multiplier architecture for CNN implementations. Second, we propose a significant upgrade to DSPDSP interconnect, providing a semi-2D low precision chaining capability which supports our low-precision multiplier. Finally, we improve data reuse via a register file which can also be configured as FIFO. Compared with the 27 × 18-bit mode in the Xilinx DSP48E2, our Precision, Interconnect, and Reuseoptimised DSP (PIR-DSP) offers a 6× improvement in multiplyaccumulate operations per DSP in the 9 × 9-bit case, 12× for 4 × 4 bits, and 24× for 2 × 2 bits. We estimate that PIR-DSP decreases the run time energy to 31/19/13% of the original value in a 9/4/2-bit MobileNet-v2 DNN implementation.
2949413858;RESTful or RESTless -- Current State of Today's Top Web APIs;2019.0;[];Recent developments in the world of services on the Web show that both the number of available Web APIs as well as the applications built on top is constantly increasing. This trend is commonly attributed to the wide adoption of the REST architectural principles. Still, the development of Web APIs is rather autonomous and it is up to the providers to decide how to implement, expose and describe the Web APIs. The individual implementations are then commonly documented in textual form as part of a webpage, showing a wide variety in terms of content, structure and level of detail. As a result, client application developers are forced to manually process and interpret the documentation. Before we can achieve a higher level of automation and can make any significant improvement to current practices and technologies, we need to reach a deeper understanding of their similarities and differences. Therefore, in this paper we present a thorough analysis of the most popular Web APIs through the examination of their documentation. We provide conclusions about common description forms, output types, usage of API parameters, invocation support, level of reusability, API granularity and authentication details. The collected data builds a solid foundation for identifying deficiencies and can be used as a basis for devising common standards and guidelines for Web API development.
2949773717;On the Complexity of Solving Markov Decision Problems;2013.0;[];Markov decision problems (MDPs) provide the foundations for a number of problems of interest to AI researchers studying automated planning and reinforcement learning. In this paper, we summarize results regarding the complexity of solving MDPs and the running time of MDP solution algorithms. We argue that, although MDPs can be solved efficiently in theory, more study is needed to reveal practical algorithms for solving large problems quickly. To encourage future research, we sketch some alternative methods of analysis that rely on the structure of MDPs.
2950112902;LSTMs with Attention for Aggression Detection;2018.0;[];In this paper, we describe the system submitted for the shared task on Aggression Identification in Facebook posts and comments by the team Nishnik. Previous works demonstrate that LSTMs have achieved remarkable performance in natural language processing tasks. We deploy an LSTM model with an attention unit over it. Our system ranks 6th and 4th in the Hindi subtask for Facebook comments and subtask for generalized social media data respectively. And it ranks 17th and 10th in the corresponding English subtasks.
2950133940;Distributed Representations of Words and Phrases and their Compositionality;2013.0;[];"The recently introduced continuous Skip-gram model is an efficient method for learning high-quality distributed vector representations that capture a large number of precise syntactic and semantic word relationships. In this paper we present several extensions that improve both the quality of the vectors and the training speed. By subsampling of the frequent words we obtain significant speedup and also learn more regular word representations. We also describe a simple alternative to the hierarchical softmax called negative sampling. An inherent limitation of word representations is their indifference to word order and their inability to represent idiomatic phrases. For example, the meanings of ""Canada"" and ""Air"" cannot be easily combined to obtain ""Air Canada"". Motivated by this example, we present a simple method for finding phrases in text, and show that learning good vector representations for millions of phrases is possible."
2950138901;Less is More: Learning Highlight Detection from Video Duration.;2019.0;[];Highlight detection has the potential to significantly ease video browsing, but existing methods often suffer from expensive supervision requirements, where human viewers must manually identify highlights in training videos. We propose a scalable unsupervised solution that exploits video duration as an implicit supervision signal. Our key insight is that video segments from shorter user-generated videos are more likely to be highlights than those from longer videos, since users tend to be more selective about the content when capturing shorter videos. Leveraging this insight, we introduce a novel ranking framework that prefers segments from shorter videos, while properly accounting for the inherent noise in the (unlabeled) training data. We use it to train a highlight detector with 10M hashtagged Instagram videos. In experiments on two challenging public video highlight detection benchmarks, our method substantially improves the state-of-the-art for unsupervised highlight detection.
2950515166;Improving the Polynomial time Precomputation of Frobenius Representation Discrete Logarithm Algorithms - Simplified Setting for Small Characteristic Finite Fields.;2014.0;[];
2951118135;RiTUAL-UH at TRAC 2018 Shared Task: Aggression Identification.;2018.0;[];"This paper presents our system for ""TRAC 2018 Shared Task on Aggression Identification"". Our best systems for the English dataset use a combination of lexical and semantic features. However, for Hindi data using only lexical features gave us the best results. We obtained weighted F1- measures of 0.5921 for the English Facebook task (ranked 12th), 0.5663 for the English Social Media task (ranked 6th), 0.6292 for the Hindi Facebook task (ranked 1st), and 0.4853 for the Hindi Social Media task (ranked 2nd)."
2951479717;FPGA Architecture Enhancements for Efficient BNN Implementation;2018.0;[];Binarized neural networks (BNNs) are ultra-reduced precision neural networks, having weights and activations restricted to single-bit values. BNN computations operate on bitwise data, making them particularly amenable to hardware implementation. In this paper, we first analyze BNN implementations on contemporary commercial 20nm FPGAs. We then propose two lightweight architectural changes that significantly improve the logic density of FPGA BNN implementations. The changes involve incorporating additional carry-chain circuitry into logic elements, where the additional circuitry is connected in a specific way to benefit BNN computations. The architectural changes are evaluated in the context of state-of-the-art Intel and Xilinx FPGAs and shown to provide over 2x area reduction in the key BNN computational task (the XNOR-popcount sub-circuit), at a modest performance cost of less than 2%.
2951627131;TBI2Flow: Travel behavioral inertia based long-term taxi passenger flow prediction;2019.0;[];Taxis are one of the representative modes of traffic systems. However, with the emergence of shared cars led by DiDi and Uber in recent years, the traditional taxi companies are facing unprecedented competitions. Without personalized data collected from the mobile devices, passenger flow prediction based on vehicle GPS records presents a unique solution that can improve taxis’ operating efficiency while preserving personal privacy. In this paper, we propose the Travel Behavioral Inertia (TBI) from taxi GPS records, which embodies Driver Inertia (DI) and Passenger Inertia (PI). Then we integrate TBI with other features to construct multi-dimensional features and predict taxi passenger flow based on a deep learning algorithm. We call the entire framework TBI2Flow. Extensive experiments demonstrate that TBI features has outstanding contribution to passenger flow prediction and TBI2Flow outperforms state-of-the-art methods including time series-based method and other deep learning-based methods on long-term taxi passenger flow prediction.
2951701153;Semi-supervised Convolutional Neural Networks for Text Categorization via Region Embedding;2015.0;[];This paper presents a new semi-supervised framework with convolutional neural networks (CNNs) for text categorization. Unlike the previous approaches that rely on word embeddings, our method learns embeddings of small text regions from unlabeled data for integration into a supervised CNN. The proposed scheme for embedding learning is based on the idea of two-view semi-supervised learning, which is intended to be useful for the task of interest even though the training is done on unlabeled data. Our models achieve better results than previous approaches on sentiment classification and topic classification tasks.
2951762009;Information measures for q ‐rung orthopair fuzzy sets;2019.0;[];
2952186347;Two-Stream Convolutional Networks for Action Recognition in Videos;2014.0;[];"We investigate architectures of discriminatively trained deep Convolutional Networks (ConvNets) for action recognition in video. The challenge is to capture the complementary information on appearance from still frames and motion between frames. We also aim to generalise the best performing hand-crafted features within a data-driven learning framework.  :[47,118],""contribution is three-fold. First, we propose a two-stream ConvNet architecture which incorporates spatial and temporal networks. Second, we demonstrate that a ConvNet trained on multi-frame dense optical flow is able to achieve very good performance in spite of limited training data. Finally, we show that multi-task learning, applied to two different action classification datasets, can be used to increase the amount of training data and improve the performance on both.  architecture is trained and evaluated on the standard video actions benchmarks of UCF-101 and HMDB-51, where it is competitive with the state of the art. It also exceeds by a large margin previous attempts to use deep nets for video classification."
2952417223;An accurate multi-modal biometric identification system for person identification via fusion of face and finger print;2019.0;[];Internet of things (IoT) have entirely revolutionized the industry. However, the cyber-security of IoT enabled cyber-physical systems is still one of the main challenges. The success of cyber-physical system is highly reliant on its capability to withstand cyberattacks. Biometric identification is the key factor responsible for the provision of secure cyber-physical system. The conventional unimodal biometric systems do not have the potential to provide the required level of security for cyber-physical system. The unimodal biometric systems are affected by a variety of issues like noisy sensor data, non-universality, susceptibility to forgery and lack of invariant representation. To overcome these issues and to provide higher-security enabled cyber-physical systems, the combination of different biometric modalities is required. To ensure a secure cyber-physical system, a novel multi-modal biometric system based on face and finger print is proposed in this work. Finger print matching is performed using alignment-based elastic algorithm. For the improved facial feature extraction, extended local binary patterns (ELBP) are used. For the effective dimensionality reduction of extracted ELBP feature space, local non-negative matrix factorization is used. Score level fusion is performed for the fusion. Experimental evaluation is done on FVC 2000 DB1, FVC 2000 DB2, ORL (ATu0026T) and YALE databases. The proposed method achieved a high recognition accuracy of 99.59%.
2954614122;Human Evaluation of Wheelchair Robot for Active Postural Support (WRAPS);2019.0;[];
2956007097;FCMDT: A novel fuzzy cognitive maps dynamic trust model for cloud federated identity management;2019.0;[];   Efficient identity management system has become one of the fundamental requirements for ensuring safe, secure and transparent use of cloud services. In such a borderless environment, entities belonging to different network domains need to cooperate dynamically with each other by exchanging and sharing a significant amount of personal information in a scalable, effective and seamless manner. The traditional approach to address this challenge has been identity federation, aiming to simplify the user experience by aggregating distributed rights and permissions. However, the current federated identity management solutions are missing mechanisms to achieve agile and dynamic trust management, which remains one of the biggest obstacles to their wide adoption in cloud computing. In this paper, we aim to address this issue by introducing a novel dynamic trust model for Federated Identity Management. The proposed model relies on fuzzy cognitive maps for modelling and evaluating trust relationships between the involved entities in federated identity management systems. This trust mechanism facilitates the creation of trust relationships between prior unknown entities in a secure and dynamic way and makes Federated Identity Management systems more scalable and flexible to deploy and maintain in cloud computing environments. In addition, we propose a set of trust features for Federated Identity Management, which serves as a basis for modelling and quantifying the trust level of unknown entities. The effectiveness of the proposed trust model is proven through performance analysis and experimental results.
2958718920;Master data management for manufacturing big data: a method of evaluation for data network;2019.0;[];In the process of manufacturing, a large amount of manufacturing data is produced by different departments and different domain. In order to realise data sharing and linkage among supply chains, master data management method has been used. Through master data management, the key data can be shared and distributed uniformly. However, since these cross-domain data form a data network through the association of master data, how to evaluate the effectiveness and rationality of this network becomes the major issue in the proposed method. In this paper, a model of the master data network is built based on the theory of set pair analysis. In order to verify the master data, an evaluation method for the network is proposed. Finally, a case was presented to validate this network model and evaluation method.
2961595003;Intelligent maintenance frameworks of large-scale grid using genetic algorithm and K-Mediods clustering methods;2019.0;[];"Large-scale power grids, especially smart grid systems, consist of a huge amount of complex computerized electronic devices, such as smart meters. A smart maintenance system is desired to schedule and send maintenance worker to locations where any computerized devices become faulty. A grid management system (GMS) is purposely designed in the way that the following three conditions are generally fulfilled: 1) all workers are working on full capacity everyday; 2) the highest severity level faulty devices are fixed the quickest; and 3) the overall traveling distance/time is minimized. In this study, two intelligent grid maintenance framework are proposed considering the above three conditioned based on two state-of-arts algorithms, namely, genetic algorithm and K-mediods clustering method, respectively. Five real-world datasets collected from five different local cities/counties in eastern China are adopted and applied to verify the effectiveness of the two proposed intelligent grid maintenance frameworks."
2962689739;On the Safety of Machine Learning: Cyber-Physical Systems, Decision Sciences, and Data Products;2017.0;[]; Machine learning algorithms increasingly influence our decisions and interact with us in all parts of our daily lives. Therefore, just as we consider the safety of power plants, highways, ...
2962707028;Crowdsourced live streaming over the cloud;2015.0;[];Empowered by todayu0027s rich tools for media generation and distribution, and the convenient Internet access, crowdsourced streaming generalizes the single-source streaming paradigm by including massive contributors for a video channel. It calls a joint optimization along the path from crowdsourcers, through streaming servers, to the end-users to minimize the overall latency. The dynamics of the video sources, together with the globalized request demands and the high computation demand from each sourcer, make crowdsourced live streaming challenging even with powerful support from modern cloud computing. In this paper, we present a generic framework that facilitates a cost-effective cloud service for crowdsourced live streaming. Through adaptively leasing, the cloud servers can be provisioned in a fine granularity to accommodate geo-distributed video crowdsourcers. We present an optimal solution to deal with service migration among cloud instances of diverse lease prices. It also addresses the location impact to the streaming quality. To understand the performance of the proposed strategies in the realworld, we have built a prototype system running over the planetlab and the Amazon/Microsoft Cloud. Our extensive experiments demonstrate that the effectiveness of our solution in terms of deployment cost and streaming quality.
2962723569;Self-Learning Cloud Controllers: Fuzzy Q-Learning for Knowledge Evolution;2015.0;[];Auto-scaling features enable cloud applications to maintain enough resources to satisfy demand spikes, reduce costs and keep performance in check. Most auto-scaling strategies rely on a predefined set of rules to scale up/down the required resources depending on the application usage. Those rules are however difficult to devise and generalize, and users are often left alone tuning auto-scale parameters of essentially blackbox applications. In this paper, we propose a novel fuzzy reinforcement learning controller, FQL4KE, which automatically scales up or down resources to meet performance requirements. The Q-Learning technique, a model-free reinforcement learning strategy, frees users of most tuning parameters. FQL4KE has been successfully applied and we therefore think that a fuzzy controller with Q-Learning is indeed a promising combination for auto-scaling resources.
2962767366;Inductive Representation Learning on Large Graphs;2017.0;[];"Low-dimensional embeddings of nodes in large graphs have proved extremely useful in a variety of prediction tasks, from content recommendation to identifying protein functions. However, most existing approaches require that all nodes in the graph are present during training of the embeddings; these previous approaches are inherently transductive and do not naturally generalize to unseen nodes. Here we present GraphSAGE, a general, inductive framework that leverages node feature information (e.g., text attributes) to efficiently generate node embeddings. Instead of training individual embeddings for each node, we learn a function that generates embeddings by sampling and aggregating features from a nodeu0027s local neighborhood. Our algorithm outperforms strong baselines on three inductive node-classification benchmarks: we classify the category of unseen nodes in evolving information graphs based on citation and Reddit post data, and we show that our algorithm generalizes to completely unseen graphs using a multi-graph dataset of protein-protein interactions."
2962772361;Advances in Pre-Training Distributed Word Representations.;2017.0;[];
2962975459;Aggression-annotated Corpus of Hindi-English Code-mixed Data.;2018.0;[];
2963026768;Universal Language Model Fine-tuning for Text Classification;2018.0;[];
2963056065;Asymmetric LSH (ALSH) for Sublinear Time Maximum Inner Product Search (MIPS);2014.0;[];We present the first provably sublinear time hashing algorithm for approximate Maximum Inner Product Search (MIPS). Searching with (un-normalized) inner product as the underlying similarity measure is a known difficult problem and finding hashing schemes for MIPS was considered hard. While the existing Locality Sensitive Hashing (LSH) framework is insufficient for solving MIPS, in this paper we extend the LSH framework to allow asymmetric hashing schemes. Our proposal is based on a key observation that the problem of finding maximum inner products, after independent asymmetric transformations, can be converted into the problem of approximate near neighbor search in classical settings. This key observation makes efficient sublinear hashing scheme for MIPS possible. Under the extended asymmetric LSH (ALSH) framework, this paper provides an example of explicit construction of provably fast hashing scheme for MIPS. Our proposed algorithm is simple and easy to implement. The proposed hashing scheme leads to significant computational savings over the two popular conventional LSH schemes: (i) Sign Random Projection (SRP) and (ii) hashing based on p-stable distributions for L2 norm (L2LSH), in the collaborative filtering task of item recommendations on Netflix and Movielens (10M) datasets.
2963139378;Geometric transformations of multidimensional color images based on NASS;2016.0;[];We present quantum algorithms to realize geometric transformations (two-point swappings, symmetric flips, local flips, orthogonal rotations, and translations) based on an n-qubit normal arbitrary superposition state (NASS). These transformations are implemented using quantum circuits consisting of basic quantum gates, which are constructed with polynomial numbers of single-qubit and two-qubit gates. Complexity analysis shows that the global operators (symmetric flips, local flips, orthogonal rotations) can be implemented with O(n) gates. The proposed geometric transformations are used to facilitate applications of quantum images with low complexity.
2963150697;Mask R-CNN;2017.0;[];We present a conceptually simple, flexible, and general framework for object instance segmentation. Our approach efficiently detects objects in an image while simultaneously generating a high-quality segmentation mask for each instance. The method, called Mask R-CNN, extends Faster R-CNN by adding a branch for predicting an object mask in parallel with the existing branch for bounding box recognition. Mask R-CNN is simple to train and adds only a small overhead to Faster R-CNN, running at 5 fps. Moreover, Mask R-CNN is easy to generalize to other tasks, e.g., allowing us to estimate human poses in the same framework. We show top results in all three tracks of the COCO suite of challenges, including instance segmentation, bounding-box object detection, and person keypoint detection. Without tricks, Mask R-CNN outperforms all existing, single-model entries on every task, including the COCO 2016 challenge winners. We hope our simple and effective approach will serve as a solid baseline and help ease future research in instance-level recognition. Code will be made available.
2963178340;On Fairness and Calibration;2017.0;[];"The machine learning community has become increasingly concerned with the potential for bias and discrimination in predictive models. This has motivated a growing line of work on what it means for a classification procedure to be ""fair."" In this paper, we investigate the tension between minimizing error disparity across different population groups while maintaining calibrated probability estimates. We show that calibration is compatible only with a single error constraint (i.e. equal false-negatives rates across groups), and show that any algorithm that satisfies this relaxation is no better than randomizing a percentage of predictions for an existing classifier. These unsettling findings, which extend and generalize existing results, are empirically confirmed on several datasets."
2963199603;On the Singular Values of Matrices with Displacement Structure;2017.0;[];Matrices with displacement structure such as Pick, Vandermonde, and Hankel matrices appear in a diverse range of applications. In this paper, we use an extremal problem involving rational functions to derive explicit bounds on the singular values of such matrices. For example, we show that the $k$th singular value of a real $n\times n$ positive definite Hankel matrix, $H_n$, is bounded by $C\rho^{-k/\log n}\|H_n\|_2$ with explicitly given constants $Cu003e0$ and $\rhou003e1$, where $\|H_n\|_2$ is the spectral norm. This means that a real $n\times n$ positive definite Hankel matrix can be approximated, up to an accuracy of $\epsilon\|H_n\|_2$ with $0u003c\epsilonu003c1$, by a rank $\mathcal{O}(\log n\log(1/\epsilon) )$ matrix. Analogous results are obtained for Pick, Cauchy, real Vandermonde, Lowner, and certain Krylov matrices.
2963253230;Learning to generate long-term future via hierarchical prediction;2017.0;[];We propose a hierarchical approach for making long-term predictions of future frames. To avoid inherent compounding errors in recursive pixel-level prediction, we propose to first estimate high-level structure in the input frames, then predict how that structure evolves in the future, and finally by observing a single frame from the past and the predicted high-level structure, we construct the future frames without having to observe any of the pixel-level predictions. Long-term video prediction is difficult to perform by recurrently observing the predicted frames because the small errors in pixel space exponentially amplify as predictions are made deeper into the future. Our approach prevents pixel-level error propagation from happening by removing the need to observe the predicted frames. Our model is built with a combination of LSTM and analogy-based encoder-decoder convolutional neural networks, which independently predict the video structure and generate the future frames, respectively. In experiments, our model is evaluated on the Human 3.6M and Penn Action datasets on the task of long-term pixel-level video prediction of humans performing actions and demonstrate significantly better results than the state-of-the-art.
2963291843;emoji2vec: Learning Emoji Representations from their Description;2016.0;[];Many current natural language processing applications for social media rely on representation learning and utilize pre-trained word embeddings. There currently exist several publicly-available, pre-trained sets of word embeddings, but they contain few or no emoji representations even as emoji usage in social media has increased. In this paper we release emoji2vec, pre-trained embeddings for all Unicode emoji which are learned from their description in the Unicode emoji standard. The resulting emoji embeddings can be readily used in downstream social natural language processing applications alongside word2vec. We demonstrate, for the downstream task of sentiment analysis, that emoji embeddings learned from short descriptions outperforms a skip-gram model trained on a large collection of tweets, while avoiding the need for contexts in which emoji need to appear frequently in order to estimate a representation.
2963298867;GRM: Group Regularity Mobility Model;2017.0;[];In this work we propose, implement, and evaluate Group Regularity Model (GRM), a novel mobility model that accounts for the role of group meetings regularity in human mobility. We show that existing mobility models for humans do not capture the regularity of human group meetings present in real mobility traces. We characterize the statistical properties of such group meetings in real mobility traces and design GRM accordingly. We show that GRM maintains the typical pairwise contact properties of real traces, such as contact duration and inter-contact time distributions. In addition, GRM accounts for the role of group mobility, presenting group meetings regularity and social communitiesu0027 structure. Finally, we evaluate state-of-art social-aware protocols for opportunistic routing and show that their performance in synthetic traces generated by GRM is similar to their performance in real-world traces.
2963305757;Full Flow: Optical Flow Estimation By Global Optimization over Regular Grids;2016.0;[];We present a global optimization approach to optical flow estimation. The approach optimizes a classical optical flow objective over the full space of mappings between discrete grids. No descriptor matching is used. The highly regular structure of the space of mappings enables optimizations that reduce the computational complexity of the algorithmu0027s inner loop from quadratic to linear and support efficient matching of tens of thousands of nodes to tens of thousands of displacements. We show that one-shot global optimization of a classical Horn-Schunck-type objective over regular grids at a single resolution is sufficient to initialize continuous interpolation and achieve state-of-the-art performance on challenging modern benchmarks.
2963410018;Improving LSTM-based Video Description with Linguistic Knowledge Mined from Text;2016.0;[];
2963482775;Predicting Future Instance Segmentation by Forecasting Convolutional Features;2018.0;[];"Anticipating future events is an important prerequisite towards intelligent behavior. Video forecasting has been studied as a proxy task towards this goal. Recent work has shown that to predict semantic segmentation of future frames, forecasting at the semantic level is more effective than forecasting RGB frames and then segmenting these. In this paper we consider the more challenging problem of future instance segmentation, which additionally segments out individual objects. To deal with a varying number of output labels per image, we develop a predictive model in the space of fixed-sized convolutional features of the Mask R-CNN instance segmentation model. We apply  the ""detection head"" of Mask R-CNN on the predicted features to produce the instance segmentation of future frames. Experiments show that this approach significantly improves over strong baselines based on optical flow and repurposed instance segmentation architectures."
2963508088;Good enough practices in scientific computing.;2017.0;[];Author summary Computers are now essential in all branches of science, but most researchers are never taught the equivalent of basic lab skills for research computing. As a result, data can get lost, analyses can take much longer than necessary, and researchers are limited in how effectively they can work with software and data. Computing workflows need to follow the same practices as lab projects and notebooks, with organized data, documented steps, and the project structured for reproducibility, but researchers new to computing often donu0027t know where to start. This paper presents a set of good computing practices that every researcher can adopt, regardless of their current level of computational skill. These practices, which encompass data management, programming, collaborating with colleagues, organizing projects, tracking work, and writing manuscripts, are drawn from a wide variety of published sources from our daily lives and from our work with volunteer organizations that have delivered workshops to over 11,000 people since 2010.
2963517218;Linearized GMM Kernels and Normalized Random Fourier Features;2017.0;[];"The method of ""random Fourier features (RFF)"" has become a popular tool for approximating the ""radial basis function (RBF)"" kernel. The variance of RFF is actually large. Interestingly, the variance can be substantially reduced by a simple normalization step as we theoretically demonstrate. We name the improved scheme as the ""normalized RFF (NRFF)"", and we provide a technical proof of the asymptotic variance of NRFF, as validated by simulations.   We also propose the ""generalized min-max (GMM)"" kernel as a measure of data similarity, where data vectors can have both positive and negative entries. GMM is positive definite as there is an associate hashing method named ""generalized consistent weighted sampling (GCWS)"" which linearizes this (nonlinear) kernel. We provide an extensive empirical evaluation of the RBF and GMM kernels on more than 50 datasets. For a majority of the datasets, the (tuning-free) GMM kernel outperforms the best-tuned RBF kernel.   We then conduct extensive classification experiments for comparing the linearized RBF kernel using NRFF with the linearized GMM kernel using GCWS. We observe that, in order to reach a similar accuracy, GCWS typically requires substantially fewer samples than NRFF, even on datasets where the original RBF kernel outperforms the original GMM kernel. As the training, storage, and processing costs are directly proportional to the sample size, our experiments can help demonstrate that GCWS would be a more practical scheme for large-scale machine learning applications.   The empirical success of GCWS (compared to NRFF) can also be explained theoretically, from at least two aspects. Firstly, the relative variance (normalized by the squared expectation) of GCWS is substantially smaller than that of NRFF, except for the very high similarity region (where the variances of both methods approach zero). Secondly, if we are allowed to make a general model assumption on the data, then we can show analytically that GCWS exhibits much smaller variance than NRFF for estimating the same object (e.g., the RBF kernel), except for the very high similarity region."
2963611811;Stochastically Transitive Models for Pairwise Comparisons: Statistical and Computational Issues;2017.0;[];There are various parametric models for analyzing pairwise comparison data, including the Bradley–Terry–Luce (BTL) and Thurstone models, but their reliance on strong parametric assumptions is limiting. In this paper, we study a flexible model for pairwise comparisons, under which the probabilities of outcomes are required only to satisfy a natural form of stochastic transitivity. This class includes parametric models, including the BTL and Thurstone models as special cases, but is considerably more general. We provide various examples of models in this broader stochastically transitive class for which classical parametric models provide poor fits. Despite this greater flexibility, we show that the matrix of probabilities can be estimated at the same rate as in standard parametric models up to logarithmic terms. On the other hand, unlike in the BTL and Thurstone models, computing the minimax-optimal estimator in the stochastically transitive model is non-trivial, and we explore various computationally tractable alternatives. We show that a simple singular value thresholding algorithm is statistically consistent but does not achieve the minimax rate. We then propose and study algorithms that achieve the minimax rate over interesting sub-classes of the full stochastically transitive class. We complement our theoretical results with thorough numerical simulations.
2963782415;PWC-Net: CNNs for Optical Flow Using Pyramid, Warping, and Cost Volume;2018.0;[];We present a compact but effective CNN model for optical flow, called PWC-Net. PWC-Net has been designed according to simple and well-established principles: pyramidal processing, warping, and the use of a cost volume. Cast in a learnable feature pyramid, PWC-Net uses the current optical flow estimate to warp the CNN features of the second image. It then uses the warped features and features of the first image to construct a cost volume, which is processed by a CNN to estimate the optical flow. PWC-Net is 17 times smaller in size and easier to train than the recent FlowNet2 model. Moreover, it outperforms all published optical flow methods on the MPI Sintel final pass and KITTI 2015 benchmarks, running at about 35 fps on Sintel resolution (1024 A— 436) images. Our models are available on our project website.
2963843052;Hierarchical Recurrent Neural Encoder for Video Representation with Application to Captioning;2016.0;[];Recently, deep learning approach, especially deep Convolutional Neural Networks (ConvNets), have achieved overwhelming accuracy with fast processing speed for image classification. Incorporating temporal structure with deep ConvNets for video representation becomes a fundamental problem for video content analysis. In this paper, we propose a new approach, namely Hierarchical Recurrent Neural Encoder (HRNE), to exploit temporal information of videos. Compared to recent video representation inference approaches, this paper makes the following three contributions. First, our HRNE is able to efficiently exploit video temporal structure in a longer range by reducing the length of input information flow, and compositing multiple consecutive inputs at a higher level. Second, computation operations are significantly lessened while attaining more non-linearity. Third, HRNE is able to uncover temporal tran-sitions between frame chunks with different granularities, i.e. it can model the temporal transitions between frames as well as the transitions between segments. We apply the new method to video captioning where temporal information plays a crucial role. Experiments demonstrate that our method outperforms the state-of-the-art on video captioning benchmarks.
2963944538;Quasirandomness in Hypergraphs;2018.0;[];An $n$-vertex graph $G$ of edge density $p$ is considered to be  quasirandom  if it shares several important properties with the random graph $G(n,p)$. A well-known theorem of Chung, Graham and Wilson states that many such `typicalu0027 properties are asymptotically equivalent and, thus, a graph $G$ possessing one such property automatically satisfies the others.  In recent years, work in this area has focused on uncovering more quasirandom graph properties and on extending the known results to other discrete structures. In the context of hypergraphs, however, one may consider several different notions of quasirandomness. A complete description of these notions has been provided recently by Towsner, who proved several central equivalences using an analytic framework. We give short and purely combinatorial proofs of the main equivalences in Towsneru0027s result.
2964008753;Android Fragmentation in Malware Detection;2019.0;[];   Differences between Android versions affect not only application developers but also make the task of securing Android harder, as it is not easy to keep track of updates. In this paper, we first systematically analyze the Android framework, which includes APIs and enforced manifest permissions to realize the inconsistency currently exists in the OS. To carry out the analysis, fine-grained machine learning-based classifiers are constructed out of predefined malicious-benign datasets to perform the task of malware detection. We propose the use of multiple feature vectors to build machine learning-based models targeting different ranges of Android API levels. As a result, the process of choosing optimal learning features becomes more efficient while avoids complicating the machine learning model unnecessarily. Also, top features extracted from machine learning models provide us the insights about how important each of them is to specific Android versions. We eventually observe the improvement of detection rates in those fine-grained classifiers compared to a single classifier.
2964011431;Weakly-Supervised Visual Instrument-Playing Action Detection in Videos;2019.0;[];Music videos are one of the most popular types of video streaming services, and instrument playing is among the most common scenes in such videos. In order to understand the instrument-playing scenes in the videos, it is important to know what instruments are played, when they are played, and where the playing actions occur in the scene. While audio-based recognition of instruments has been widely studied, the visual aspect of music instrument playing remains largely unaddressed in the literature. One of the main obstacles is the difficulty in collecting annotated data of the action locations for training-based methods. To address this issue, we propose a weakly supervised framework to find when and where the instruments are played in the videos. We propose using two auxiliary models: 1) a sound model and 2) an object model to provide supervision for training the instrument-playing action model. The sound model provides temporal supervisions, while the object model provides spatial supervisions. They together can simultaneously provide temporal and spatial supervisions. The resulting model only needs to analyze the visual part of a music video to deduce which, when, and where instruments are played. We found that the proposed method significantly improves localization accuracy. We evaluate the result of the proposed method temporally and spatially on a small dataset (a total of 5400 frames) that we manually annotated.
2964121718;Soft-NMS — Improving Object Detection with One Line of Code;2017.0;[];Non-maximum suppression is an integral part of the object detection pipeline. First, it sorts all detection boxes on the basis of their scores. The detection box M with the maximum score is selected and all other detection boxes with a significant overlap (using a pre-defined threshold) with M are suppressed. This process is recursively applied on the remaining boxes. As per the design of the algorithm, if an object lies within the predefined overlap threshold, it leads to a miss. To this end, we propose Soft-NMS, an algorithm which decays the detection scores of all other objects as a continuous function of their overlap with M. Hence, no object is eliminated in this process. Soft-NMS obtains consistent improvements for the coco-style mAP metric on standard datasets like PASCAL VOC2007 (1.7% for both R-FCN and Faster-RCNN) and MS-COCO (1.3% for R-FCN and 1.1% for Faster-RCNN) by just changing the NMS algorithm without any additional hyper-parameters. Using Deformable-RFCN, Soft-NMS improves state-of-the-art in object detection from 39.8% to 40.9% with a single model. Further, the computational complexity of Soft-NMS is the same as traditional NMS and hence it can be efficiently implemented. Since Soft-NMS does not require any extra training and is simple to implement, it can be easily integrated into any object detection pipeline. Code for Soft-NMS is publicly available on GitHub http://bit.ly/2nJLNMu.
2964173101;Automatic Curation of Golf Highlights Using Multimodal Excitement Features;2017.0;[];The production of sports highlight packages summarizing a game’s most exciting moments is an essential task for broadcast media. Yet, it requires labor-intensive video editing. We propose a novel approach for auto-curating sports highlights, and use it to create a real-world system for the editorial aid of golf highlight reels. Our method fuses information from the players’ reactions (action recognition such as high-fives and fist pumps), spectators (crowd cheering), and commentator (tone of the voice and word analysis) to determine the most interesting moments of a game. We accurately identify the start and end frames of key shot highlights with additional metadata, such as the player’s name and the hole number, allowing personalized content summarization and retrieval. In addition, we introduce new techniques for learning our classifiers with reduced manual training data annotation by exploiting the correlation of different modalities. Our work has been demonstrated at a major golf tournament, successfully extracting highlights from live video streams over four consecutive days.
2964258169;On the Fast Computation of the Weight Enumerator Polynomial and the $t$ Value of Digital Nets over Finite Abelian Groups;2013.0;[];In this paper we introduce digital nets over finite abelian groups which contain digital nets over finite fields and certain rings as a special case. We prove a MacWilliams-type identity for such digital nets. This identity can be used to compute the strict $t$-value of a digital net over finite abelian groups. If the digital net has $N$ points in the $s$-dimensional unit cube $[0,1)^s$, then the $t$-value can be computed in $\mathcal{O}(N s \log N)$ operations and the weight enumerator polynomial can be computed in $\mathcal{O}(N s (\log N)^2)$ operations, where operations mean arithmetic of integers. By precomputing some values the number of operations of computing the weight enumerator polynomial can be reduced further.
2964333408;RTK with the Assistance of an IMU-Based Pedestrian Navigation Algorithm for Smartphones.;2019.0;[];Real-time kinematic (RTK) technique is widely used in modern society because of its high accuracy and real-time positioning. The appearance of Android P and the application of BCM47755 chipset make it possible to use single-frequency RTK and dual-frequency RTK on smartphones. The Xiaomi Mi 8 is the first dual-frequency Global Navigation Satellite System (GNSS) smartphone equipped with BCM47755 chipset. However, the performance of RTK in urban areas is much poorer compared with its performance under the open sky because the satellite signals can be blocked by the buildings and trees. RTK canu0027t provide the positioning results in some specific areas such as the urban canyons and the crossings under an overpass. This paper combines RTK with an IMU-based pedestrian navigation algorithm. We utilize attitude and heading reference system (AHRS) algorithm and zero velocity update (ZUPT) algorithm based on micro electro mechanical systems (MEMS) inertial measurement unit (IMU) in smartphones to assist RTK for the sake of improving positioning performance in urban areas. Some tests are carried out to verify the performance of RTK on the Xiaomi Mi 8 and we respectively assess the performances of RTK with and without the assistance of an IMU-based pedestrian navigation algorithm in urban areas. Results on actual tests show RTK with the assistance of an IMU-based pedestrian navigation algorithm is more robust and adaptable to complex environments than that without it.
2964345792;MAttNet: Modular Attention Network for Referring Expression Comprehension;2018.0;[];"In this paper, we address referring expression comprehension: localizing an image region described by a natural language expression. While most recent work treats expressions as a single unit, we propose to decompose them into three modular components related to subject appearance, location, and relationship to other objects. This allows us to flexibly adapt to expressions containing different types of information in an end-to-end framework. In our model, which we call the Modular Attention Network (MAttNet), two types of attention are utilized: language-based attention that learns the module weights as well as the word/phrase attention that each module should focus on; and visual attention that allows the subject and relationship modules to focus on relevant image components. Module weights combine scores from all three modules dynamically to output an overall score. Experiments show that MAttNet outperforms previous state-of-the-art methods by a large margin on both bounding-box-level and pixel-level comprehension tasks. Demo1 and code2 are provided."
2964857609;Clique-factors in sparse pseudorandom graphs;2019.0;[];   We prove that for any     t  ≥  3     there exist constants     c  u003e  0     and      n    0      such that any    d   -regular    n   -vertex graph    G    with     t  ∣  n  ≥    n    0       and second largest eigenvalue in absolute value    λ    satisfying     λ  ≤  c    d    t    ∕    n    t  −  1       contains a      K    t     -factor, that is, vertex-disjoint copies of      K    t      covering every vertex of    G   . The result generalizes to broader setting of jumbled graphs, which were introduced by Thomason in the eighties.
2964899339;Low-Cost and Highly Sensitive Wearable Sensor Based on Napkin for Health Monitoring;2019.0;[];The development of sensors with high sensitivity, good flexibility, low cost, and capability of detecting multiple inputs is of great significance for wearable electronics. Herein, we report a napkin-based wearable capacitive sensor fabricated by a novel, low-cost, and facile strategy. The capacitive sensor is composed of two pieces of electrode plates manufactured by spontaneous assembly of silver nanowires (NWs) on a polydimethylsiloxane (PDMS)-patterned napkin. The sensor possesses high sensitivity (u003e7.492 kPa−1), low cost, and capability for simultaneous detection of multiple signals. We demonstrate that the capacitive sensor can be applied to identify a variety of human physiological signals, including finger motions, eye blinking, and minute wrist pulse. More interestingly, the capacitive sensor comfortably attached to the temple can simultaneously monitor eye blinking and blood pulse. The demonstrated sensor shows great prospects in the applications of human–machine interface, prosthetics, home-based healthcare, and flexible touch panels.
2966243013;Trapezoidal Pythagorean fuzzy linguistic entropic combined ordered weighted Minkowski distance operator based on preference relations;2019.0;[];
2968409638;Deriving event data sharing in IoT systems using formal modelling and analysis;2019.0;[];   The increasing presence and utilisation of IoT systems raises many fundamental security and privacy issues that require robust approaches in understanding the behaviour of IoT systems and tackling those issues. In previous works, we demonstrated how some of the security and privacy questions in IoT systems could be answered by means of using federated identity management and authorisation frameworks, such as OAuth, intelligent gateways and personal cloud systems. In this paper, we take these works into a more fundamental level by formally modelling and analysing the OAuthing personal cloud-based IoT system. We demonstrate that this exercise reveals how data is shared across the system, and therefore how security and privacy guarantees can be established at a fundamental level.
2968632614;On generalized similarity measures for Pythagorean fuzzy sets and their applications to multiple attribute decision‐making;2019.0;[];
2969179193;Divergence-based cross entropy and uncertainty measures of Atanassov’s intuitionistic fuzzy sets with their application in decision making;2019.0;[];   The uncertainty measure of Atanassov’s intuitionistic fuzzy sets (AIFSs) is important for information discrimination under intuitionistic fuzzy environment. Although many entropy measures and knowledge measures haven been proposed to depict uncertainty of AIFSs, how to measure the uncertainty of AIFSs is still an open topic. The relation between uncertainty and other measures like entropy measures, fuzziness and intuitionism is not clear. This paper introduces uncertainty measures by using new defined divergence-based cross entropy measure of AIFSs. Axiomatic properties of the developed uncertainty measure are analysis, together with the monotony property of uncertainty degree with respect to fuzziness and intuitionism. To adjust the contribution of fuzzy entropy and intuitionistic entropy on the total uncertainty, the proposed cross entropy and uncertainty measures are parameterized. Numerical examples indicate the effectiveness and agility of the biparametric uncertainty measure in quantifying uncertainty degree. Then we apply the cross entropy and uncertainty measures into an optimal model to determine attribute weights in multi-attribute group decision making (MAGDM) problems. A new method for intuitionistic fuzzy MAGDM problems is proposed to show the efficiency of proposed measures in applications. It is demonstrated by application examples that the proposed measures can get reasonable results coinciding with other existing methods.
2969522623;Secure limitation analysis of public-key cryptography for smart card settings;2019.0;[];Smart cards are widely used in high security applications due to their self-contained nature. At the same time, the security of smart card has become an urgent problem in the field of intelligent environment. Public-key Cryptography is the main means to solve the security problems based on smart card password authentication and identity authentication protocol. This paper reviews the security issues of public key cryptography used in smart cards from the perspective of information theory. By constructing a attackers channel, we model the Public-key Cryptography process in the way of an adversary to capture the attack ability in the Public-key Cryptography setting. Then, we convert the secure problems of Public-key Cryptography into the attack channels capacity of adversaries that the maximum value of the average mutual information is the secure limitations of a Public-key Cryptography scheme, which is a reachable theoretic limitation of secure communication parties. Finally, we give the bounds of insecure for public-key encryption and signature in different secure levels, and analyze and discuss the secure limitation.
2969853667;Spiking neural networks for deep learning and knowledge representation: Editorial.;2019.0;[];
2970032879;From crowdsourcing to crowdmining: using implicit human intelligence for better understanding of crowdsourced data;2019.0;[];With the development of mobile social networks, more and more crowdsourced data are generated on the Web or collected from real-world sensing. The fragment, heterogeneous, and noisy nature of online/offline crowdsourced data, however, makes it difficult to be understood. Traditional content-based analyzing methods suffer from potential issues such as computational intensiveness and poor performance. To address them, this paper presents CrowdMining. In particular, we observe that the knowledge hidden in the process of data generation, regarding individual/crowd behavior patterns (e.g., mobility patterns, community contexts such as social ties and structure) and crowd-object interaction patterns (flickering or tweeting patterns) are neglected in crowdsourced data mining. Therefore, a novel approach that leverages implicit human intelligence (implicit HI) for crowdsourced data mining and understanding is proposed. Two studies titled CrowdEvent and CrowdRoute are presented to showcase its usage, where implicit HIs are extracted either from online or offline crowdsourced data. A generic model for CrowdMining is further proposed based on a set of existing studies. Experiments based on real-world datasets demonstrate the effectiveness of CrowdMining.
2970332774;A novel outranking sorting approach based on interval‐valued Pythagorean uncertain linguistic Euclidean distance for material supplier grading;2019.0;[];
2970364834;A New Vegetation Index to Detect Periodically Submerged Mangrove Forest Using Single-Tide Sentinel-2 Imagery;2019.0;[];
2970888294;From affect, behavior, and cognition to personality: an integrated personal character model for individual-like intelligent artifacts;2019.0;[];An individual-like intelligent artifact is a special kind of humanoid which resembles a human being in assimilating aspects of its real human counterpart’s cognition and neurological functions. Such an individual-like intelligent artifact could have a number of far-reaching applications, such as in creating a digital clone of an individual and bringing about forms of digital immortality. Although such intelligent artifacts have been created in various forms, such as physical robots or digital avatars, these creations are still far from modeling the inner cognitive and neurological mechanisms of an individual human. To imbue individual-like intelligent artifacts with the characteristics of individuals, we propose a Personal Character Model that consists of personality, the characteristics of affect, behavior, and cognition, and the relations between these characteristics. According to differential psychology and personality psychology, personality is the set of essential characteristics that make a person unique whereas characteristics in affect, behavior, and cognition explain a person’s stable and  personality in their diverse daily behavior. In addition, relations among these characteristics serve as a bridge from one characteristic to another. To illustrate the computing process of the personal character model, we first designed three experiments to collect physiological data and behavior data from twenty participants. Then we selected data features from the collected data using correlational analysis. Finally, we computed several representative characteristics from selected data features and represented the computed results.
2971146131;The existence and nonexistence of entire large solutions for a quasilinear Schrödinger elliptic system by dual approach;2020.0;[];   In this paper, we establish some new results for the existence and nonexistence of radial large positive solutions for the modified Schrodinger system with a nonconvex diffusion term. Our main tools are the successive iteration technique and dual approach. A necessary and sufficient condition for the existence of radial large positive solutions is also given, which improves and extends many previous work in this field of research.
2976125790;Discovering Attractive Segments in the User Generated Video Streams;2019.0;[];
2979077288;A Time-Frequency based Machine Learning System for Brain States Classification via EEG Signal Processing;2019.0;[];In the last decades, the use of Machine Learning (ML) algorithms have been widely employed to aid clinicians in the difficult diagnosis of neurological disorders, such as Alzheimer’s disease (AD). In this context, here, a data-driven ML system for classifying Electroencephalographic (EEG) segments (i.e. epochs) of patients affected by AD, Mild Cognitive Impairment (MCI) and Healthy Control (HC) individuals, is introduced. Specifically, the proposed ML system consists of evaluating the average Time-Frequency Map (aTFM) related to a 19-channels EEG epoch and extracting some statistical coefficients (i.e. mean, standard deviation, skewness, kurtosis and entropy) from the main five conventional EEG sub-bands (or EEG-rhythms: delta, theta, alpha 1 , alpha 2 , beta). Afterwards, the time-frequency features vector is fed into an Autoeconder (AE), a Multi-Layer Perceptron (MLP), a Logistic Regression (LR) and a Support Vector Machine (SVM) based classifier to perform the 2-ways EEG epoch-classification tasks: AD vs HC and AD vs MCI. The performances of the proposed approach have been evaluated on a dataset of 189 EEG signals (63 AD, 63 MCI and 63 HC), recorded during an eye-closed resting condition at IRCCS Centro Neurolesi Bonino Pulejo of Messina (Italy). Experimental results reported that the 1-hidden layer MLP (MLP 1 ) outperformed all the other developed learning systems as well as recently proposed state-of-the-art methods, achieving accuracy rate up to 95.76 % ± 0.0045 and 86.84% ± 0.0098 in AD vs HC and AD vs MCI classification, respectively.
2981578638;CRA-Net: Composed Relation Attention Network for Visual Question Answering.;2019.0;[];
2981877040;Real-Time EMG Based Pattern Recognition Control for Hand Prostheses: A Review on Existing Methods, Challenges and Future Implementation;2019.0;[];Upper limb amputation is a condition that significantly restricts the amputees from performing their daily activities. The myoelectric prosthesis, using signals from residual stump muscles, is aimed at restoring the function of such lost limbs seamlessly. Unfortunately, the acquisition and use of such myosignals are cumbersome and complicated. Furthermore, once acquired, it usually requires heavy computational power to turn it into a user control signal. Its transition to a practical prosthesis solution is still being challenged by various factors particularly those related to the fact that each amputee has different mobility, muscle contraction forces, limb positional variations and electrode placements. Thus, a solution that can adapt or otherwise tailor itself to each individual is required for maximum utility across amputees. Modified machine learning schemes for pattern recognition have the potential to significantly reduce the factors (movement of users and contraction of the muscle) affecting the traditional electromyography (EMG)-pattern recognition methods. Although recent developments of intelligent pattern recognition techniques could discriminate multiple degrees of freedom with high-level accuracy, their efficiency level was less accessible and revealed in real-world (amputee) applications. This review paper examined the suitability of upper limb prosthesis (ULP) inventions in the healthcare sector from their technical control perspective. More focus was given to the review of real-world applications and the use of pattern recognition control on amputees. We first reviewed the overall structure of pattern recognition schemes for myo-control prosthetic systems and then discussed their real-time use on amputee upper limbs. Finally, we concluded the paper with a discussion of the existing challenges and future research recommendations.
2988958963;LW-CoEdge: a lightweight virtualization model and collaboration process for edge computing;2019.0;[];Edge Computing is a novel paradigm that extends Cloud Computing by moving the computation closer to the end users and/or data sources. When considering Edge Computing, it is possible to design a three-tier architecture (comprising tiers for the cloud devices, edge devices, and end devices) which is useful to meet emerging IoT applications that demand low latency, geo-localization, and energy efficiency. Like the Cloud, the Edge Computing paradigm relies on virtualization. An Edge Computing virtualization model provides a set of virtual nodes (VNs) built on top of the physical devices that make up the three-tier architecture. It also provides the processes of provisioning and allocating VNs to IoT applications at the edge of the network. Performing these processes efficiently and cost-effectively raises a resource management challenge. Applying the traditional cloud virtualization models (typically centralized) to virtualize the edge tier, are unsuitable to handle emerging IoT application scenarios due to the specific features of the edge nodes, such as geographical distribution, heterogeneity and, resource constraints. Therefore, we propose a novel distributed and lightweight virtualization model targeting the edge tier, encompassing the specific requirements of IoT applications. We designed heuristic algorithms along with a P2P collaboration process to operate in our virtualization model. The algorithms perform (i) a distributed resource management process, and (ii) data sharing among neighboring VNs. The distributed resource management process provides each edge node with decision-making capability, engaging neighboring edge nodes to allocate or provision on-demand VNs. Thus, the distributed resource management improves system performance, serving more requests and handling edge node geographical distribution. Meanwhile, data sharing reduces the data transmissions between end devices and edge nodes, saving energy and reducing data traffic for IoT-edge infrastructures.
2990135320;A survey on data provenance in IoT;2019.0;[];
3000383752;Are we better off with just one ontology on the Web;2020.0;[];
1228064353;Semantic Web: Concepts, Technologies and Applications;2006.0;[];The Future of the Internet.- Concepts.- Ontology in Computer Science.- Knowledge Representation in Description Logic.- RDF and RDF Schema.- OWL.- Rule Languages.- Semantic Web Services.- Technologies.- Methods for Ontology Development.- Ontology Sources.- Semantic Web Software Tools.- Applications.- Software Agents.- Semantic Desktop.- Ontology Applications in Art.- Geospatial Semantic Web.
1483804921;Functions of Matrices: Theory and Computation;2008.0;[];"A thorough and elegant treatment of the theory of matrix functions and numerical methods for computing them, including an overview of applications, new and unpublished research results, and improved algorithms. Key features include a detailed treatment of the matrix sign function and matrix roots; a development of the theory of conditioning and properties of the Frechet derivative; Schur decomposition; block Parlett recurrence; a thorough analysis of the accuracy, stability, and computational cost of numerical methods; general results on convergence and stability of matrix iterations; and a chapter devoted to the f(A)b problem. Ideal for advanced courses and for self-study, its broad content, references and appendix also make this book a convenient general reference. Contains an extensive collection of problems with solutions and MATLAB implementations of key algorithms."
1488542496;Web-Based virtual learning environments: a research framekwork and a preliminary assessment of effectiveness in basic IT skills training;2001.0;[];"Internet technologies are having a significant impact on the learning industry. For-profit organizations and traditional institutions of higher education have developed and are using web-based courses, but little is known about their effectiveness compared to traditional classroom education. Our work focuses on the effectiveness of a web-based virtual learning environment (VLE) in the context of basic information technology skills  :[59],""article provides three main contributions. First, it introduces and defines the concept of VLE, discussing how a VLE differs from the traditional classroom and differentiating it from the related, but narrower, concept of computer aided instruction (CAI). Second, it presents a framework of VLE effectiveness, grounded in the technology-mediated learning literature, which frames the VLE research domain, and addresses the relationship between the main constructs. Finally, it focuses on one essential VLE design variable, learner control, and compares a web-based VLE to a traditional classroom through a longitudinal experimental  :[149],""results indicate that, in the context of IT basic skills training in undergraduate education, there are no significant differences in performance between students enrolled in the two environments. However, the VLE leads to higher reported computer self-efficacy, while participants report being less satisfied with the learning process."
1522301498;Adam: A Method for Stochastic Optimization;2014.0;[];We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.
1527575280;Unifying Visual-Semantic Embeddings with Multimodal Neural Language Models.;2014.0;[];"Inspired by recent advances in multimodal learning and machine translation, we introduce an encoder-decoder pipeline that learns (a): a multimodal joint embedding space with images and text and (b): a novel language model for decoding distributed representations from our space. Our pipeline effectively unifies joint image-text embedding models with multimodal neural language models. We introduce the structure-content neural language model that disentangles the structure of a sentence to its content, conditioned on representations produced by the encoder. The encoder allows one to rank images and sentences while the decoder can generate novel descriptions from scratch. Using LSTM to encode sentences, we match the state-of-the-art performance on Flickr8K and Flickr30K without using object detections. We also set new best results when using the 19-layer Oxford convolutional network. Furthermore we show that with linear encoders, the learned embedding space captures multimodal regularities in terms of vector space arithmetic e.g. *image of a blue car* - ""blue"" + ""red"" is near images of red cars. Sample captions generated for 800 images are made available for comparison."
1570515303;Computing logarithms in GF(2 n );1985.0;[];Consider the finite field having q elements and denote it by GF(q). Let α be a generator for the nonzero elements of GF(q). Hence, for any element b≠0 there exists an integer x, 0≤x≤q−2, such that b=αx. We call x the discrete logarithm of b to the base α and we denote it by x=log α b and more simply by log b when the base is fixed for the discussion. The discrete logarithm problem is stated as follows:
1571117462;Autonomic Middleware for Automotive Embedded Systems;2009.0;[];This chapter describes DySCAS: an advanced autonomic platform-independent middleware framework for automotive embedded systems. The concepts and architecture are motivated and described in detail, focusing on the need for, and achievement of, high flexibility and automatic run-time reconfiguration. The design of the middleware is positioned with respect to the way it overcomes the specific technical, environmental, and performance challenges of the automotive domain. Self-management is achieved in terms of automatic configuration for context-aware behavior, resource-use efficiency, and self-healing to handle run-time detected faults. The self-management is governed by the use of policies distributed throughout the middleware components. The simulation techniques that have been used for extensive validation are described and some key results presented. A reference implementation is presented, illustrating the way in which the various concepts and mechanisms can be realized and orchestrated.
1599058448;Anticipating the future by watching unlabeled video.;2015.0;[];"In many computer vision applications, machines will need to reason beyond the present, and predict the future. This task is challenging because it requires leveraging extensive commonsense knowledge of the world that is difficult to write down. We believe that a promising resource for efficiently obtaining this knowledge is through the massive amounts of readily available unlabeled video. In this paper, we present a large scale framework that capitalizes on temporal structure in unlabeled video to learn to anticipate both actions and objects in the future. The key idea behind our approach is that we can train deep networks to predict the visual representation of images in the future. We experimentally validate this idea on two challenging ""in the wild"" video datasets, and our results suggest that learning with unlabeled videos significantly helps forecast actions and anticipate objects."
1603536826;Voronoi diagrams and Delaunay triangulations;1997.0;[];The Voronoi diagram of a set of sites partitions space into regions one per site the region for a site s consists of all points closer to s than to any other site The dual of the Voronoi diagram the Delaunay triangulation is the unique triangulation so that the circumsphere of every triangle contains no sites in its interior Voronoi diagrams and Delaunay triangulations have been rediscovered or applied in many areas of math ematics and the natural sciences they are central topics in computational geometry with hundreds of papers discussing algorithms and extensions Section discusses the de nition and basic properties in the usual case of point sites in R with the Euclidean metric while section gives basic algorithms Some of the many extensions obtained by varying metric sites environment and constraints are discussed in section Section nishes with some interesting and nonobvious structural properties of Voronoi diagrams and Delaunay triangulations
1606551523;Artificial Neural Networks: Theory and Applications;1998.0;[];
1686810756;VERY DEEP CONVOLUTIONAL NETWORKS FOR LARGE-SCALE IMAGE RECOGNITION;2014.0;[];In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3x3) convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve state-of-the-art results. We have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision.
1799366690;Network In Network;2013.0;[];"We propose a novel deep network structure called ""Network In Network"" (NIN) to enhance model discriminability for local patches within the receptive field. The conventional convolutional layer uses linear filters followed by a nonlinear activation function to scan the input. Instead, we build micro neural networks with more complex structures to  the data within the receptive field. We instantiate the micro neural network with a multilayer perceptron, which is a potent function approximator. The feature maps are obtained by sliding the micro networks over the input in a similar manner as CNN; they are then fed into the next layer. Deep NIN can be implemented by stacking mutiple of the above described structure. With enhanced local modeling via the micro network, we are able to utilize global average pooling over feature maps in the classification layer, which is easier to interpret and less prone to overfitting than traditional fully connected layers. We demonstrated the state-of-the-art classification performances with NIN on CIFAR-10 and CIFAR-100, and reasonable performances on SVHN and MNIST datasets."
1800613700;Proof equivalence in MLL is PSPACE-complete;2016.0;[];"MLL proof equivalence is the problem of deciding whether two proofs  :[11],""linear logic are related by a series of inference  is :[21],""also known as the word problem for star-autonomous categories.  :[32],""has shown the problem to be equivalent to a rewiring problem on  :[45],""which are not canonical for full MLL due to the presence of the  :[59],""Drawing from recent work on reconfiguration problems, in this paper  shown :[70],""that MLL proof equivalence is PSPACE-complete, using a reduction  :[81],""Constraint Logic. An important consequence of the result  the :[90],""existence of a satisfactory notion of proof nets for MLL with units  :[104],""out (under current complexity assumptions). The PSPACE-hardness  to equivalence of :[112],""normal forms in MELL without units, where  :[123],""rule for the exponentials induces a similar rewiring problem."
1969067552;Fast Communication: A cumulant-based approach for direction finding in the presence of mutual coupling;2014.0;[];To estimate the direction-of-arrivals (DOAs) of non-Gaussian signals impinging on a uniform linear array (ULA) with unknown mutual coupling, a fourth-order cumulants (FOC)-based approach is presented. A series of FOC matrices are constructed by taking advantage of the banded symmetric Toeplitz structure of the mutual coupling matrix (MCM). It is shown that the DOAs can be estimated in a closed form based on these matrices. Compared with the conventional FOC-based method which only uses the middle subarray, the proposed approach makes use of the whole array and it is able to achieve better performance. Moreover, in the proposed approach the DOAs are determined by the eigenvalue decomposition instead of the exhaustive grid search and hence, it is computationally efficient. Finally, compared with the middle subarray method, the proposed one is applicable to scenarios with stronger mutual coupling. A number of numerical examples are provided to validate essential performance improvements achieved by means of the proposed approach.
1978390553;Three-point finite-difference schemes, Padé and the spectral Galerkin method. I. One-sided impedance approximation;2001.0;[];A method for calculating special grid placement for three-point schemes which yields exponential superconvergence of the Neumann to Dirich-let map has been suggested earlier. Here we show that such a grid placement can yield impedance which is equivalent to that of a spectral Galerkin method, or more generally to that of a spectral Galerkin-Petrov method. In fact we show that. for every stable Galerkin-Petrov method there is a three-point scheme which yields the same solution at the boundary. We discuss the application of this result to partial differential equations and give numerical examples. We also show equivalence at one corner of a two-dimensional optimal grid with a spectral Galerkin method.
1982300822;Understanding individual human mobility patterns;2008.0;[];The mapping of large-scale human movements is important for urban planning, traffic forecasting and epidemic prevention. Work in animals had suggested that their foraging might be explained in terms of a random walk, a mathematical rendition of a series of random steps, or a Levy flight, a random walk punctuated by occasional larger steps. The role of Levy statistics in animal behaviour is much debated — as explained in an accompanying News Feature — but the idea of extending it to human behaviour was boosted by a report in 2006 of Levy flight-like patterns in human movement tracked via dollar bills. A new human study, based on tracking the trajectory of 100,000 cell-phone users for six months, reveals behaviour close to a Levy pattern, but deviating from it as individual trajectories show a high degree of temporal and spatial regularity: work and other commitments mean we are not as free to roam as a foraging animal. But by correcting the data to accommodate individual variation, simple and predictable patterns in human travel begin to emerge. The cover photo (by Cesar Hidalgo) captures human mobility in New Yorku0027s Grand Central Station. This study used a sample of 100,000 mobile phone users whose trajectory was tracked for six months to study human mobility patterns. Displacements across all users suggest behaviour close to the Levy-flight-like pattern observed previously based on the motion of marked dollar bills, but with a cutoff in the distribution. The origin of the Levy patterns observed in the aggregate data appears to be population heterogeneity and not Levy patterns at the level of the individual. Despite their importance for urban planning1, traffic forecasting2 and the spread of biological3,4,5 and mobile viruses6, our understanding of the basic laws governing human motion remains limited owing to the lack of tools to monitor the time-resolved location of individuals. Here we study the trajectory of 100,000 anonymized mobile phone users whose position is tracked for a six-month period. We find that, in contrast with the random trajectories predicted by the prevailing Levy flight and random walk models7, human trajectories show a high degree of temporal and spatial regularity, each individual being characterized by a time-independent characteristic travel distance and a significant probability to return to a few highly frequented locations. After correcting for differences in travel distances and the inherent anisotropy of each trajectory, the individual travel patterns collapse into a single spatial probability distribution, indicating that, despite the diversity of their travel history, humans follow simple reproducible patterns. This inherent similarity in travel patterns could impact all phenomena driven by human mobility, from epidemic prevention to emergency response, urban planning and agent-based modelling.
1984697505;Graph Colouring and the Probabilistic Method;2001.0;[];The term Probabilistic Method refers to the proof of deterministic statements using probabilistic tools. The method has been successfully applied to a number of problems in the field of graph colouring. We survey some of the results thereby obtained. The talk is intended to be accessible and short on details. We will first define graph colouring, explain the type of graph colouring problems which tend to attract interest. We then explain the probabilistic tools which are used to solve them, and why we would expect the type of tools that are used to be effective for solving the types of problems typically studied.
1990746954;2009 Special Issue: Single-trial classification of vowel speech imagery using common spatial patterns;2009.0;[];With the goal of providing a speech prosthesis for individuals with severe communication impairments, we propose a control scheme for brain-computer interfaces using vowel speech imagery. Electroencephalography was recorded in three healthy subjects for three tasks, imaginary speech of the English vowels /a/ and /u/, and a no action state as control. Trial averages revealed readiness potentials at 200 ms after stimulus and speech related potentials peaking after 350 ms. Spatial filters optimized for task discrimination were designed using the common spatial patterns method, and the resultant feature vectors were classified using a nonlinear support vector machine. Overall classification accuracies ranged from 68% to 78%. Results indicate significant potential for the use of vowel speech imagery as a speech prosthesis controller.
1990766815;2014 Special Issue: Exploring personalized searches using tag-based user profiles and resource profiles in folksonomy;2014.0;[];With the increase in resource-sharing websites such as YouTube and Flickr, many shared resources have arisen on the Web. Personalized searches have become more important and challenging since users demand higher retrieval quality. To achieve this goal, personalized searches need to take usersu0027 personalized profiles and information needs into consideration. Collaborative tagging (also known as folksonomy) systems allow users to annotate resources with their own tags, which provides a simple but powerful way for organizing, retrieving and sharing different types of social resources. In this article, we examine the limitations of previous tag-based personalized searches. To handle these limitations, we propose a new method to model user profiles and resource profiles in collaborative tagging systems. We use a normalized term frequency to indicate the preference degree of a user on a tag. A novel search method using such profiles of users and resources is proposed to facilitate the desired personalization in resource searches. In our framework, instead of the keyword matching or similarity measurement used in previous works, the relevance measurement between a resource and a user query (termed the query relevance) is treated as a fuzzy satisfaction problem of a useru0027s query requirements. We implement a prototype system called the Folksonomy-based Multimedia Retrieval System (FMRS). Experiments using the FMRS data set and the MovieLens data set show that our proposed method outperforms baseline methods.
2010219174;P $^{2}$ CySeMoL: Predictive, Probabilistic Cyber Security Modeling Language;2015.0;[];"This paper presents the Predictive, Probabilistic Cyber Security Modeling Language (P   $^{2}$      CySeMoL), an attack graph tool that can be used to estimate the cyber security of enterprise architectures. P   $^{2}$      CySeMoL includes theory on how attacks and defenses relate quantitatively; thus, users must only model their assets and how these are connected in order to enable calculations. The performance of PCySeMoL enables quick calculations of large object models. It has been validated on both a component level and a system level using literature, domain experts, surveys, observations, experiments and case studies."
2011869755;Approximating the independence number via the j -function;1998.0;[];We describe an approximation algorithm for the independence number of a graph. If a graph onn vertices has an independence numbern/k + m for some fixed integerk ⩾ 3 and somem u003e 0, the algorithm finds, in random polynomial time, an independent set of size\(\tilde \Omega (m^{{3 \mathord{\left/ {\vphantom {3 {(k + 1)}}} \right. \kern-\nulldelimiterspace} {(k + 1)}}} )\), improving the best known previous algorithm of Boppana and Halldorsson that finds an independent set of size Ω(m1/(k−1)) in such a graph. The algorithm is based on semi-definite programming, some properties of the Lovaszϑ-function of a graph and the recent algorithm of Karger, Motwani and Sudan for approximating the chromatic number of a graph. If theϑ-function of ann vertex graph is at leastMn1−2/k for some absolute constantM, we describe another, related, efficient algorithm that finds an independent set of sizek. Several examples show the limitations of the approach and the analysis together with some related arguments supply new results on the problem of estimating the largest possible ratio between theϑ-function and the independence number of a graph onn vertices. © 1998 The Mathematical Programming Society, Inc. Published by Elsevier Science B.V.
2013646023;Fast communication: DOA estimation based on fourth-order cumulants with unknown mutual coupling;2009.0;[];Most of the existing direction of arrival (DOA) estimation algorithms with mutual coupling are generally based on the second-order statistics while assuming that the noise is additive white Gaussian noise. In this paper, a method based on fourth-order cumulants (FOC) is proposed for DOA estimation with uniform linear array (ULA) in the presence of unknown mutual coupling. The method can be applied when non-Gaussian signals coexist with unknown colored Gaussian noise. The unknown mutual coupling can be blindly compensated by the inherent mechanism of the proposed method and the DOA of signals can be accurately estimated without any calibration sources. Since we use the FOC, the Gaussian noise is greatly suppressed and we can obtain a good estimation performance in low SNR condition. Simulation results demonstrate the effectiveness of our method.
2021324335;Learning in massive open online courses;2015.0;[];Various social media mining approaches were adopted for analyzing MOOC learning.The daily, weekly, and monthly trends of MOOC-related tweets were investigated.Sentiment analysis indicated mixed public opinion toward MOOCs.The influencers of sentimental retweets about MOOCs were identified. Because many massive open online courses (MOOCs) have adopted social media tools for large student audiences to co-create knowledge and engage in collective learning processes, this study adopted various social media mining approaches to investigate Twitter messages related to MOOC learning. The first approach adopted in this study was calculating the important descriptive statistics of MOOC-related tweets and examining the daily, weekly, and monthly trends of MOOC that appeared on Twitter. This information can enable MOOC practitioners to observe participantsu0027 temporal activities on social media and ascertain the most effective time to post or analyze tweets. Secondly, we investigated how public sentiment toward MOOC learning can be assessed according to related tweets. Because the availability and popularity of opinion-rich social networking services are increasing for MOOC communities, our findings from the sentiment analysis of Twitter data can afford substantial insights into participant perceptions of MOOC learning. Third, we analyzed the positive and negative retweets related to MOOCs and identified the influencers of these retweets. Social network diagrams were also developed to reveal how sentimental messages about MOOCs on Twitter were disseminated from the top influencers with the highest number of positive/negative retweets about MOOCs. Analyzing the relationships among top retweet users is vital to MOOC practitioners because they can use this information to filter or recommend MOOC-related messages to the influencers. In short, the findings pertaining social media mining in this study afford a holistic understanding of MOOC trends, public sentiment toward MOOC learning, and the influencers of MOOC-related retweets.
2021848912;The Permutations 123 p 4 … p m and 321 p 4 … p m are Wilf-Equivalent;2000.0;[];Write p1, p2…p m for the permutation matrix δ pi, j . Let S n (M) be the set of n×n permutation matrices which do not contain the m×m permutation matrix M as a submatrix. In [7] Simion and Schmidt show bijectively that |S n (123) |=|S n (213) |. In [9] this was generalised to a bijection between S n (12 p3…p m ) and S n (21 p3…p m ). In the present paper we obtain a bijection between S n (123 p4…p m ) and S n (321 p4…p m ).
2030877481;"CAP twelve years later: How the ""rules"" have changed";2012.0;[];The CAP theorem asserts that any networked shared-data system can have only two of three desirable properties. However, by explicitly handling partitions, designers can optimize consistency and availability, thereby achieving some trade-off of all three. The featured Web extra is a podcast from Software Engineering Radio, in which the host interviews Dwight Merriman about the emerging NoSQL movement, the three types of nonrelational data stores, Breweru0027s CAP theorem, and much more.
2043219147;The capacitated standard response fire protection sitting problem: deterministic and probabilistic models;1993.0;[];We propose and present computational experience on a model that sites capacitated stations, engine companies and truck companies in such a way that the population or calls covered by the standard response is maximized. The standard response to a fire alarm, as defined by the Insurance Services Office (ISO), consists typically of at least three engine companiesand at least two trucks companies, located within standard distances (different for engines and trucks) from every demand point. The model utilized limits the capacity of each station toC j vehicles which could be either engines or trucks or a mixture. The possibility that at the time a call comes in the full standard response will not be available within the applicable distance standards is considered as well in a probabilistic standard response model.
2050357215;Clustering techniques for stock location and order-picking in a distribution;1999.0;[];   In this paper, the stock location and order-picking problems in a distribution center where items are distributed in less-than-case-lot quantities are addressed. Clustering techniques are applied to group items in the slots of gravity-flow racks and to sequence the picking lists by customers. Based on the order-item-quantity rule, two similarity measures are defined for items and customers, respectively. A zero–one integer programming model developed for optimal items- and customers-grouping is presented. An exact primal–dual type algorithm is explored and implemented, where real world data collected from a local distribution center is employed. Simulation results indicate the potential benefits of the proposed techniques.    Scope and purpose   In this article, issues of stock location and order-picking in a distribution center where the characteristics of dependent customer demands are thoroughly examined. Clustering techniques are applied to extract the correlated information from the customer orders, which is then used for optimizing the stock location and picking process. Instead of applying existing algorithms, a novel primal–dual-type algorithm explored to solve the clustering problem is developed. An example using real world data collected from a local distribution center is presented. Simulation studies using WITNESS simulator reveal the benefits of the stock location achieved by the proposed algorithm.
2066482960;Existence and uniqueness of symmetric positive solutions of 2n-order nonlinear singular boundary value problems☆;2013.0;[];   By applying an iterative technique, a necessary and sufficient condition is obtained for the existence of symmetric positive solutions of    2  n   -order nonlinear singular boundary value problems. At the same time, we also show the uniqueness of the symmetric positive solution.
2068655514;Transitivity frameworks for reciprocal relations: cycle-transitivity versus FG-transitivity;2005.0;[];For a reciprocal relation Q on a set of alternatives A, two transitivity frameworks which generalize both T-transitivity and stochastic transitivity are compared: the framework of cycle-transitivity, introduced by the present authors (Soc. Choice Welf., to appear) and which is based upon the ordering of the numbers Q(a,b), Q(b,c) and Q(c,a) for all (a,b,c)@?A^3, and the framework of FG-transitivity, introduced by Switalski (Fuzzy Sets and Systems 137 (2003) 85) as an immediate generalization of stochastic transitivity. The rules that enable to express FG-transitivity in the form of cycle-transitivity and cycle-transitivity in the form of FG-transitivity, illustrate that for reciprocal relations the concept of cycle-transitivity provides a framework that can cover more types of transitivity than does the concept of FG-transitivity.
2086046934;ELECTREGKMS: Robust ordinal regression for outranking methods;2011.0;[];We present a new method, called ELECTREGKMS, which employs robust ordinal regression to construct a set of outranking models compatible with preference information. The preference information supplied by the decision maker (DM) is composed of pairwise comparisons stating the truth or falsity of the outranking relation for some real or fictitious reference alternatives. Moreover, the DM specifies some ranges of variation of comparison thresholds on considered pseudo-criteria. Using robust ordinal regression, the method builds a set of values of concordance indices, concordance thresholds, indifference, preference, and veto thresholds, for which all specified pairwise comparisons can be restored. Such sets are called compatible outranking models. Using these models, two outranking relations are defined, necessary and possible. Whether for an ordered pair of alternatives there is necessary or possible outranking depends on the truth of outranking relation for all or at least one compatible model, respectively. Distinguishing the most certain recommendation worked out by the necessary outranking, and a possible recommendation worked out by the possible outranking, ELECTREGKMS answers questions of robustness concern. The method is intended to be used interactively with incremental specification of pairwise comparisons, possibly with decreasing confidence levels. In this way, the necessary and possible outranking relations can be, respectively, enriched or impoverished with the growth of the number of pairwise comparisons. Furthermore, the method is able to identify troublesome pieces of preference information which are responsible for incompatibility. The necessary and possible outranking relations are to be exploited as usual outranking relations to work out recommendation in choice or ranking problems. The introduced approach is illustrated by a didactic example showing how ELECTREGKMS can support real-world decision problems.
2108646579;Sentiment Analysis and Opinion Mining;2012.0;[];Sentiment analysis and opinion mining is the field of study that analyzes peopleu0027s opinions, sentiments, evaluations, attitudes, and emotions from written language. It is one of the most active research areas in natural language processing and is also widely studied in data mining, Web mining, and text mining. In fact, this research has spread outside of computer science to the management sciences and social sciences due to its importance to business and society as a whole. The growing importance of sentiment analysis coincides with the growth of social media such as reviews, forum discussions, blogs, micro-blogs, Twitter, and social networks. For the first time in human history, we now have a huge volume of opinionated data recorded in digital form for analysis. Sentiment analysis systems are being applied in almost every business and social domain because opinions are central to almost all human activities and are key influencers of our behaviors. Our beliefs and perceptions of reality, and the choices we make, are largely conditioned on how others see and evaluate the world. For this reason, when we need to make a decision we often seek out the opinions of others. This is true not only for individuals but also for organizations. This book is a comprehensive introductory and survey text. It covers all important topics and the latest developments in the field with over 400 references. It is suitable for students, researchers and practitioners who are interested in social media analysis in general and sentiment analysis in particular. Lecturers can readily use it in class for courses on natural language processing, social media analysis, text mining, and data mining. Lecture slides are also available online.
2115627867;Review: A review of novelty detection;2014.0;[];Novelty detection is the task of classifying test data that differ in some respect from the data that are available during training. This may be seen as u0027u0027one-class classificationu0027u0027, in which a model is constructed to describe u0027u0027normalu0027u0027 training data. The novelty detection approach is typically used when the quantity of available u0027u0027abnormalu0027u0027 data is insufficient to construct explicit models for non-normal classes. Application includes inference in datasets from critical systems, where the quantity of available normal data is very large, such that u0027u0027normalityu0027u0027 may be accurately modelled. In this review we aim to provide an updated and structured investigation of novelty detection research papers that have appeared in the machine learning literature during the last decade.
2116661285;Muiltiobjective optimization using nondominated sorting in genetic algorithms;1994.0;[];In trying to solve multiobjective optimization problems, many traditional methods scalarize the objective vector into a single objective. In those cases, the obtained solution is highly sensitive to the weight vector used in the scalarization process and demands that the user have knowledge about the underlying problem. Moreover, in solving multiobjective problems, designers may be interested in a set of Pareto-optimal points, instead of a single point. Since genetic algorithms (GAs) work with a population of points, it seems natural to use GAs in multiobjective optimization problems to capture a number of solutions simultaneously. Although a vector evaluated GA (VEGA) has been implemented by Schaffer and has been tried to solve a number of multiobjective problems, the algorithm seems to have bias toward some regions. In this paper, we investigate Goldbergu0027s notion of nondominated sorting in GAs along with a niche and speciation method to find multiple Pareto-optimal points simultaneously. The proof-of-principle results obtained on three problems used by Schaffer and others suggest that the proposed method can be extended to higher dimensional and more difficult multiobjective problems. A number of suggestions for extension and application of the algorithm are also discussed.
2124637492;Statistical mechanics of complex networks;2001.0;[];"The emergence of order in natural systems is a constant source of inspiration for both physical and biological sciences. While the spatial order characterizing for example the crystals has been the basis of many advances in contemporary physics, most complex systems in nature do not offer such high degree of order. Many of these systems form complex networks whose nodes are the elements of the system and edges represent the interactions between them.  complex networks :[73],""have been described by the random graph theory founded in 1959 by Paul Erdohs and Alfred Renyi. One of the defining features of random graphs is that they are statistically homogeneous, and their degree distribution (characterizing the spread in the number of edges starting from a node) is a Poisson distribution. In contrast, recent empirical studies, including the work of our group, indicate that the topology of real networks is much richer than that of random graphs. In particular, the degree distribution of real networks is a power-law, indicating a heterogeneous topology in which the majority of the nodes :[73],""have a small degree, but there is a significant fraction of highly connected nodes that play an important role in the connectivity of the network.  :[200,260],""scale-free topology of real networks has very important consequences on their functioning. For example, we :[73],""have discovered that :[200,260],""scale-free networks are extremely resilient to the random disruption of their nodes. On the other hand, the selective removal of the nodes with highest degree induces a rapid breakdown of the network to isolated subparts that cannot communicate with each other.  non-trivial scaling of the degree distribution of real networks is also an indication of their assembly and evolution. Indeed, our modeling studies :[73],""have shown us that there are general principles governing the evolution of networks. Most networks start from a small seed and grow by the addition of new nodes which attach to the nodes already in the system. This process obeys preferential attachment: the new nodes are more likely to connect to nodes with already high degree. We :[73],""have proposed a simple model based on these two principles wich was able to reproduce the power-law degree distribution of real networks. Perhaps even more importantly, this model paved the way to a new paradigm of network modeling, trying to capture the evolution of networks, not just their static topology."
2126581182;Sentiment Analysis and Subjectivity;2010.0;[];Textual information in the world can be broadly categorized into two main types: facts and opinions. Facts are objective expressions about entities, events and their properties. Opinions are usually subjective expressions that describe people’s sentiments, appraisals or feelings toward entities, events and their properties. The concept of opinion is very broad. In this chapter, we only focus on opinion expressions that convey people’s positive or negative sentiments. Much of the existing research on textual information processing has been focused on mining and retrieval of factual information, e.g., information retrieval, Web search, text classification, text clustering and many other text mining and natural language processing tasks. Little work had been done on the processing of opinions until only recently. Yet, opinions are so important that whenever we need to make a decision we want to hear others’ opinions. This is not only true for individuals but also true for organizations. One of the main reasons for the lack of study on opinions is the fact that there was little opinionated text available before the World Wide Web. Before the Web, when an individual needed to make a decision, he/she typically asked for opinions from friends and families. When an organization wanted to find the opinions or sentiments of the general public about its products and services, it conducted opinion polls, surveys, and focus groups. However, with the Web, especially with the explosive growth of the usergenerated content on the Web in the past few years, the world has been transformed. The Web has dramatically changed the way that people express their views and opinions. They can now post reviews of products at merchant sites and express their views on almost anything in Internet forums, discussion groups, and blogs, which are collectively called the user-generated content. This online wordof-mouth behavior represents new and measurable sources of information with many practical applications. Now if one wants to buy a product, he/she is no longer limited to asking his/her friends and families because there are many product reviews on the Web which give opinions of existing users of the product. For a company, it may no longer be necessary to conduct surveys, organize focus groups or employ external consultants in order to find consumer opinions about its products and those of its competitors because the user-generated content on the Web can already give them such information.
2127274797;Rainbow Turán Problems;2007.0;[];For a fixed graph $H$, we define the rainbow Turan number $\ex^*(n,H)$ to be the maximum number of edges in a graph on $n$ vertices that has a proper edge-colouring with no rainbow $H$. Recall that the (ordinary) Turan number $\ex(n,H)$ is the maximum number of edges in a graph on $n$ vertices that does not contain a copy of $H$. For any non-bipartite $H$ we show that $\ex^*(n,H)=(1+o(1))\ex(n,H)$, and if $H$ is colour-critical we show that $\ex^{*}(n,H)=\ex(n,H)$. When $H$ is the complete bipartite graph $K_{s,t}$ with $s \leq t$ we show $\ex^*(n,K_{s,t}) = O(n^{2-1/s})$, which matches the known bounds for $\ex(n,K_{s,t})$ up to a constant. We also study the rainbow Turan problem for even cycles, and in particular prove the bound $\ex^*(n,C_6) = O(n^{4/3})$, which is of the correct order of magnitude.
2138249740;Computing $A^alpha, log(A)$, and Related Matrix Functions by Contour Integrals;2008.0;[];New methods are proposed for the numerical evaluation of $f(\mathbf{A})$ or $f(\mathbf{A}) b$, where $f(\mathbf{A})$ is a function such as $\mathbf{A}^{1/2}$ or $\log (\mathbf{A})$ with singularities in $(-\infty,0]$ and $\mathbf{A}$ is a matrix with eigenvalues on or near $(0,\infty)$. The methods are based on combining contour integrals evaluated by the periodic trapezoid rule with conformal maps involving Jacobi elliptic functions. The convergence is geometric, so that the computation of $f(\mathbf{A})b$ is typically reduced to one or two dozen linear system solves, which can be carried out in parallel.
2139598698;Scientific Utopia: II - Restructuring Incentives and Practices to Promote Truth Over Publishability;2012.0;[];An academic scientist’s professional success depends on publishing. Publishing norms emphasize novel, positive results. As such, disciplinary incentives encourage design, analysis, and reporting decisions that elicit positive results and ignore negative results. Prior reports demonstrate how these incentives inflate the rate of false effects in published science. When incentives favor novelty over replication, false results persist in the literature unchallenged, reducing efficiency in knowledge accumulation. Previous suggestions to address this problem are unlikely to be effective. For example, a journal of negative results publishes otherwise unpublishable reports. This enshrines the low status of the journal and its content. The persistence of false findings can be meliorated with strategies that make the fundamental but  accuracy motive—getting it right—competitive with the more tangible and concrete incentive—getting it published. This article develops strategies for improving scientific practices and knowledge accumulation that account for ordinary human motivations and biases.
2145535121;A Comparison between Two Main Academic Literature Collections: Web of Science and Scopus Databases;2013.0;[];Nowadays, the world’s scientific community has been publishing an enormous number of papers in different scientific fields. In such environment, it is essential to know which databases are equally efficient and objective for literature searches. It seems that two most extensive databases are Web of Science and Scopus. Besides searching the literature, these two databases used to rank journals in terms of their productivity and the total citations received to indicate the journals impact, prestige or influence. This article attempts to provide a comprehensive comparison of these databases to answer frequent questions which researchers ask, such as: How Web of Science and Scopus are different? In which aspects these two databases are similar? Or, if the researchers are forced to choose one of them, which one should they prefer? For answering these questions, these two databases will be compared based on their qualitative and quantitative characteristics.
2149684865;Text Categorization with Suport Vector Machines: Learning with Many Relevant Features;1998.0;[];This paper explores the use of Support Vector Machines (SVMs) for learning text classifiers from examples. It analyzes the particular properties of learning with text data and identifies why SVMs are appropriate for this task. Empirical results support the theoretical findings. SVMs achieve substantial improvements over the currently best performing methods and behave robustly over a variety of different learning tasks. Furthermore they are fully automatic, eliminating the need for manual parameter tuning.
2173520492;Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks;2015.0;[];In recent years, supervised learning with convolutional networks (CNNs) has seen huge adoption in computer vision applications. Comparatively, unsupervised learning with CNNs has received less attention. In this work we hope to help bridge the gap between the success of CNNs for supervised learning and unsupervised learning. We introduce a class of CNNs called deep convolutional generative adversarial networks (DCGANs), that have certain architectural constraints, and demonstrate that they are a strong candidate for unsupervised learning. Training on various image datasets, we show convincing evidence that our deep convolutional adversarial pair learns a hierarchy of representations from object parts to scenes in both the generator and discriminator. Additionally, we use the learned features for novel tasks - demonstrating their applicability as general image representations.
2177847924;Compression of Deep Convolutional Neural Networks for Fast and Low Power Mobile Applications;2015.0;[];Although the latest high-end smartphone has powerful CPU and GPU, running deeper convolutional neural networks (CNNs) for complex tasks such as ImageNet classification on mobile devices is challenging. To deploy deep CNNs on mobile devices, we present a simple and effective scheme to compress the entire CNN, which we call one-shot whole network compression. The proposed scheme consists of three steps: (1) rank selection with variational Bayesian matrix factorization, (2) Tucker decomposition on kernel tensor, and (3) fine-tuning to recover accumulated loss of accuracy, and each step can be easily implemented using publicly available tools. We demonstrate the effectiveness of the proposed scheme by testing the performance of various compressed CNNs (AlexNet, VGGS, GoogLeNet, and VGG-16) on the smartphone. Significant reductions in model size, runtime, and energy consumption are obtained, at the cost of small loss in accuracy. In addition, we address the important implementation level issue on 1?1 convolution, which is a key operation of inception module of GoogLeNet as well as CNNs compressed by our proposed scheme.
2187569332;The YASGUI family of SPARQL clients1;2016.0;[];The size and complexity of the Semantic Web and its technology stack makes it difficult to query. Access to Linked Data could be greatly facilitated if it were supported by a tool with a strong focus on usability. In this paper we present the YASGUI family of SPARQL clients, a continuation of the YASGUI tool introduced more than two years ago. The YASGUI family of SPARQL clients enables publishers to improve ease of access for their SPARQL endpoints, and gives consumers of Linked Data a robust, feature-rich and user friendly SPARQL editor. We show that the YASGUI family had significant impact on the landscape of Linked Data management: YASGUI components are integrated in state-of-the-art triple-stores and Linked Data applications, and used as front-end by a large number of Linked Data publishers. Additionally, we show that the YASGUI web service - which provides access to any SPARQL endpoint - has a large and growing user base amongst Linked Data consumers.
2202969221;The entire large solutions for a quasilinear Schrödinger elliptic equation by the dual approach;2016.0;[];   In this paper, we consider a Modified Nonlinear Schrodinger equation    Δ  u  −  Δ   (     |  u  |     2  γ    )      |  u  |     2  γ  −  2    u  =  q   (  x  )   F   (  u  )     on       R     n     (  n  ≥  3  )    , where    γ  u003e    1    2     ,    q    is a nonnegative continuous function on       R     n     ,    F    is a continuous positive and non-decreasing function on     [  0  ,  ∞  )    . We establish sufficient conditions for the existence and nonexistence of positive entire large solutions for the given equation by using the dual approach.
2248556341;Deep multi-scale video prediction beyond mean square error;2015.0;[];Learning to predict future images from a video sequence involves the construction of an internal representation that models the image evolution accurately, and therefore, to some degree, its content and dynamics. This is why pixel-space video prediction may be viewed as a promising avenue for unsupervised feature learning. In addition, while optical flow has been a very studied problem in computer vision for a long time, future frame prediction is rarely approached. Still, many vision applications could benefit from the knowledge of the next frames of videos, that does not require the complexity of tracking every pixel trajectories. In this work, we train a convolutional network to generate future frames given an input sequence. To deal with the inherently blurry predictions obtained from the standard Mean Squared Error (MSE) loss function, we propose three different and complementary feature learning strategies: a multi-scale architecture, an adversarial training method, and an image gradient difference loss function. We compare our predictions to different published results based on recurrent neural networks on the UCF101 dataset
2308318555;A survey on object detection in optical remote sensing images;2016.0;[];   Object detection in optical remote sensing images, being a fundamental but challenging problem in the field of aerial and satellite image analysis, plays an important role for a wide range of applications and is receiving significant attention in recent years. While enormous methods exist, a deep review of the literature concerning generic object detection is still lacking. This paper aims to provide a review of the recent progress in this field. Different from several previously published surveys that focus on a specific object class such as building and road, we concentrate on more generic object categories including, but are not limited to, road, building, tree, vehicle, ship, airport, urban-area. Covering about 270 publications we survey (1) template matching-based object detection methods, (2) knowledge-based object detection methods, (3) object-based image analysis (OBIA)-based object detection methods, (4) machine learning-based object detection methods, and (5) five publicly available datasets and three standard evaluation metrics. We also discuss the challenges of current studies and propose two promising research directions, namely deep learning-based feature representation and weakly supervised learning-based geospatial object detection. It is our hope that this survey will be beneficial for the researchers to have better understanding of this research field.
2340541605;Analytic Combinatorics;2009.0;[];Analytic Combinatorics is a self-contained treatment of the mathematics underlying the analysis of discrete structures, which has emerged over the past several decades as an essential tool in the understanding of properties of computer programs and scientific models with applications in physics, biology and chemistry. Thorough treatment of a large number of classical applications is an essential aspect of the presentation. Written by the leaders in the field of analytic combinatorics, this text is certain to become the definitive reference on the topic. The text is complemented with exercises, examples, appendices and notes to aid understanding therefore, it can be used as the basis for an advanced undergraduate or a graduate course on the subject, or for self-study.
2396642682;On the Function Field Sieve and the Impact of Higher Splitting Probabilities: Application to Discrete Logarithms in F 2 1971 .;2013.0;[];In this paper we propose a binary field variant of the JouxLercier medium-sized Function Field Sieve, which results not only in complexities as low as Lqn(1/3, (4/9) ) for computing arbitrary logarithms, but also in an heuristic polynomial time algorithm for finding the discrete logarithms of degree one and two elements when the field has a subfield of an appropriate size. To illustrate the efficiency of the method, we have successfully solved the DLP in the finite fields with 2 and 2 elements, setting a record for binary fields.
2413904250;Very Deep Convolutional Networks for Natural Language Processing.;2016.0;[];The dominant approach for many NLP tasks are recurrent neural networks, in particular LSTMs, and convolutional neural networks. However, these architectures are rather shallow in comparison to the deep convolutional networks which are very successful in computer vision. We present a new architecture for text processing which operates directly on the character level and uses only small convolutions and pooling operations. We are able to show that the performance of this model increases with the depth: using up to 29 convolutional layers, we report significant improvements over the state-of-the-art on several public text classification tasks. To the best of our knowledge, this is the first time that very deep convolutional nets have been applied to NLP.
2495052195;The acceptance and use of interactive whiteboards among teachers;2016.0;[];With the spread of new educational technology such as the interactive whiteboard (IWB) teachers, as potential users, need to adapt their teaching in order to successfully utilize it. Despite considerable efforts in motivating teachers to use new educational technology, there are mixed feelings about whether to accept and use this technology in the classroom or not. In this study we propose to extend the Unified Theory of Acceptance and Use of Technology (UTAUT) with a new moderator variable user type in order to investigate differences in the UTAUT determinants between pre- and post-adopters of IWBs. The results of the study showed significant differences in causal effect sizes between pre- and post-adopters for several paths of the proposed research model. When compared to post-adopters, we can see that for pre-adopters: 1) social influence has a bigger impact on behavioral intentions, 2) performance expectancy more strongly affects attitudes toward using IWBs, and 3) there is a difference in attitudes towards using IWB on usersu0027 potential use of IWBs. For post-adopters: 4) the facilitating conditions have a bigger impact on the actual use of IWBs, and 5) behavioral intention is a stronger predictor of the actual use of IWBs when compared with pre-adopters. The Unified Theory of Acceptance and Use of Technology (UTAUT) model was extended.New moderator variables user types and teaching experiences were proposed.The extended UTAUT model was validated in the case of interactive whiteboards.Significant differences in determinants between pre- and post-adopters were shown.The biggest difference was in the path between behavioral intentions and use.
2498322372;Analysis and Design of Adaptive OCDMA Passive Optical Networks;2017.0;[];Optical code division multiple access (OCDMA) systems can support multiple classes of service by differentiating code parameters, power level, and diversity order. In this paper, we analyze bit error rate (BER) performance of a multiclass 1-D/2-D OCDMA system and propose a new approximation method that can be used to generate accurate estimation of system BER using a simple mathematical form. The proposed approximation provides insight into proper system level analysis, system level design and sensitivity of system performance to the factors such as code parameters, power level, and diversity order. Considering code design, code cardinality, and system performance constraints, two design problems are defined and their optimal solutions are provided. We, then, propose an adaptive OCDMA passive optical networks (OCDMA-PON) that adaptively shares unused resources of inactive users among active ones to improve upstream system performance. Using the approximated BER expression and defined design problems, two adaptive code allocation algorithms for the adaptive OCDMA-PON are presented and their performances are evaluated by simulation. Simulation results show that the adaptive code allocation algorithms can increase average transmission rate or decrease average transmitted optical power of optical network units for dynamic traffic patterns. According to the simulation results, for an adaptive OCDMA-PON with BER value of    $10^{-7}$   and user activity probability of 0.5, transmission rate (transmitted optical power) can be increased (decreased) by a factor of 2.53 (0.25) compared to fixed code assignment.
2511794618;Convolutional Neural Networks for Text Categorization: Shallow Word-level vs. Deep Character-level.;2016.0;[];"This paper reports the performances of shallow word-level convolutional neural networks (CNN), our earlier work (2015), on the eight datasets with relatively large training data that were used for testing the very deep character-level CNN in Conneau et al. (2016). Our findings are as follows. The shallow word-level CNNs achieve better error rates than the error rates reported in Conneau et al., though the results should be interpreted with some consideration due to the unique pre-processing of Conneau et al. The shallow word-level CNN uses more parameters and therefore requires more storage than the deep character-level CNN; however, the shallow word-level CNN computes much faster."
2518185797;Internet addiction among Chinese adolescents;2014.0;[];Adolescents with IA experience more unfavorable parental behavior than the non-IA.Adolescents with IA have lower capacity of self-control than the non-IA.Parental behavior and self-control both predict Internet addiction directly.Self-control also acts as a mediator between parental behavior and Internet addiction.The results are discussed from the view of Chinese Confucian filial piety. A cross-sectional study of a large, middle-school student sample (N=966) was presented in this paper aiming to examine how parental behavior and self-control influence Internet addiction (IA) among Chinese adolescents. Fifty-one adolescents (the top 5% of IA score distribution) were categorized as at high risk. Males were more likely addicted to Internet than females. MANOVA demonstrated that, compared with non-IA group, adolescents with IA revealed lower mean score for parental positive support behavior and higher for parental negative control behavior and had lower capacity of self-control. SEM analyses revealed that low capacity of self-control had a negative correlation with parentsu0027 positive support and a positive correlation with negative control. More importantly, Internet addiction was explained negatively by parentsu0027 positive support and positively by parentsu0027 negative control and individual low capacity of self-control. Further mediating analyses indicated that self-control accounted for an indirect role between parental behavior and adolescentsu0027 Internet addiction. The findings of the present study are of significance in investigating adolescentsu0027 problem behaviors and very helpful to provide educational advice for intervening in these behaviors. Moreover, the present findingu0027s potential relevance to Confucian styles of filial parenting was discussed.
2519515778;Exploring students' awareness and perceptions;2016.0;[];This study investigates studentsu0027 awareness and perceptions of m-learning and examines the factors affecting studentsu0027 behavioral intention to adopt m-learning, by using a modified research model that integrate technology acceptance model (perceived usefulness and perceived ease of use) and unified theory of acceptance and use of technology (social influence) along with other factors (m-learning services and mobile limitations). In addition, control (gender, field of study, study level) and moderator variables (mobile capabilities, level of mobile usage, and frequent use of m-services) were introduced to verify the individual differences between respondents on the key factors affecting the adoption and usage of m-learning. Structural equations modeling and path analysis were used to test the hypotheses and the proposed model. The results revealed that perceived usefulness and perceived ease of use were found to be the primary factors driving studentsu0027 intentions to use m-learning. Both m-learning services and social influence have positive effects on the acceptance of m-learning, while mobile limitations were found to be the main obstacle restraining studentsu0027 participation in a m-learning environment. Most of the control variables yield no significant differences between students, but all the moderator variables were found to be significant determinants that can influence students to adopt m-learning. Overall, students have great potential to engage and integrate mobile technology into their educational environment. An extended model is proposed based on TAM and UTAUT in the context of m-learning.The extended model reflects studentsu0027 awareness and perceptions of m-learning.The most significant factors driving m-learning adoption were PEOU and PU.The main obstacles affecting studentsu0027 use of m-learning were mobile limitations.The most vital determinants that affect m-learning usage were moderator variables.
2528884737;Ischemic Stroke Identification Based on EEG and EOG using 1D Convolutional Neural Network and Batch Normalization;2016.0;[];In 2015, stroke was the number one cause of death in Indonesia. The majority type of stroke is ischemic. The standard tool for diagnosing stroke is CT-Scan. For developing countries like Indonesia, the availability of CT-Scan is very limited and still relatively expensive. Because of the availability, another device that potential to diagnose stroke in Indonesia is EEG. Ischemic stroke occurs because of obstruction that can make the cerebral blood flow (CBF) on a person with stroke has become lower than CBF on a normal person (control) so that the EEG signal have a deceleration. On this study, we perform the ability of 1D Convolutional Neural Network (1DCNN) to construct classification model that can distinguish the EEG and EOG stroke data from EEG and EOG control data. To accelerate training process our model we use Batch Normalization. Involving 62 person data object and from leave one out the scenario with five times repetition of measurement we obtain the average of accuracy 0.86 (F-Score 0.861) only at 200 epoch. This result is better than all over shallow and popular classifiers as the comparator (the best result of accuracy 0.69 and F-Score 0.72 ). The feature used in our study were only 24 handcrafted feature with simple feature extraction process.
2549733045;Generalized Pythagorean Fuzzy Geometric Aggregation Operators Using Einstein t‐Norm and t‐Conorm for Multicriteria Decision‐Making Process;2017.0;[];The objective of this paper is to present some series of geometric-aggregated operators under Pythagorean fuzzy environment by relaxing the condition that the sum of the degree of membership functions is less than one with the square sum of the degree of membership functions is less than one. Under these environments, aggregator operators, namely, Pythagorean fuzzy Einstein weighted geometric, Pythagorean fuzzy Einstein ordered weighted geometric, generalized Pythagorean fuzzy Einstein weighted geometric, and generalized Pythagorean fuzzy Einstein ordered weighted geometric operators, are proposed in this paper. Some of its properties have also been investigated in details. Finally, an illustrative example for multicriteria decision-making problems of alternatives is taken to demonstrate the effectiveness of the approach.
2559463885;EEGNet: a compact convolutional neural network for EEG-based brain–computer interfaces;2018.0;[];"Brain computer interfaces (BCI) enable direct communication with a computer, using neural activity as the control signal. This neural signal is generally chosen from a variety of well-studied electroencephalogram (EEG) signals. For a given BCI paradigm, feature extractors and classifiers are tailored to the distinct characteristics of its expected EEG control signal, limiting its application to that specific signal. Convolutional Neural Networks (CNNs), which have been used in computer vision and speech recognition, have successfully been applied to EEG-based BCIs; however, they have mainly been applied to single BCI paradigms and thus it remains unclear how these architectures generalize to other paradigms. Here, we ask if we can design a single CNN architecture to accurately classify EEG signals from different BCI paradigms, while simultaneously being as compact as possible. In this work we introduce EEGNet, a compact convolutional network for EEG-based BCIs. We introduce the use of depthwise and separable convolutions to construct an EEG-specific model which encapsulates well-known EEG feature extraction concepts for BCI. We compare EEGNet to current state-of-the-art approaches across four BCI paradigms: P300 visual-evoked potentials, error-related negativity responses (ERN), movement-related cortical potentials (MRCP), and sensory motor rhythms (SMR). We show that EEGNet generalizes across paradigms better than the reference algorithms when only limited training data is available. We demonstrate three different approaches to visualize the contents of a trained EEGNet model to enable interpretation of the learned features. Our results suggest that EEGNet is robust enough to learn a wide variety of interpretable features over a range of BCI tasks, suggesting that the observed performances were not due to artifact or noise sources in the data."
2586433286;Analysis of human tissue densities;2017.0;[];A new algorithm (AHTD) is proposed to extract image features based on human tissue densities in medical images.AHTD is used to extract features from lung and brain CT images.AHTD is compared against three traditional feature extraction methods.The influence of the extraction method on the classification accuracy was assessed using four machine learning techniques.The results confirm the superiority and suitability of AHTD for use with medical images. Identification of diseases based on processing and analysis of medical images is of great importance for medical doctors to assist them in their decision making. In this work, a new feature extraction method based on human tissue density patterns, named Analysis of Human Tissue Densities (AHTD) is presented. The proposed method uses radiological densities of human tissues in Hounsfield Units to tackle the extraction of suitable features from medical images. This new method was compared against: the Gray Level Co-occurrence Matrix, Hus moments, Statistical moments, Zernikes moments, Elliptic Fourier features, Tamuras features and the Statistical Co-occurrence Matrix. Four machine learning classifiers were applied to each feature extractor for two CT image datasets:, one to classify lung disease in CT images of the thorax and the other to classify stroke in CT images of the brain. The attributes were extracted from the lung images in 5.2ms and obtained an accuracy of 99.01% for the detection and classification of lung diseases, while the attributes from the brain images were extracted in 3.8ms and obtained an accuracy of 98.81% for the detection and classification of stroke. These results show that the proposed method can be used to classify diseases in medical images, and can be used in real-time applications due to its fast extraction time of suitable attributes.
2587019100;Comparative Study of CNN and RNN for Natural Language Processing.;2017.0;[];Deep neural networks (DNN) have revolutionized the field of natural language processing (NLP). Convolutional neural network (CNN) and recurrent neural network (RNN), the two main types of DNN architectures, are widely explored to handle various NLP tasks. CNN is supposed to be good at extracting position-invariant features and RNN at modeling units in sequence. The state of the art on many NLP tasks often switches due to the battle between CNNs and RNNs. This work is the first systematic comparison of CNN and RNN on a wide range of representative NLP tasks, aiming to give basic guidance for DNN selection.
2588336250;A survey on data preprocessing for data stream mining;2017.0;[];Data preprocessing and reduction have become essential techniques in current knowledge discovery scenarios, dominated by increasingly large datasets. These methods aim at reducing the complexity inherent to real-world datasets, so that they can be easily processed by current data mining solutions. Advantages of such approaches include, among others, a faster and more precise learning process, and more understandable structure of raw data. However, in the context of data preprocessing techniques for data streams have a long road ahead of them, despite online learning is growing in importance thanks to the development of Internet and technologies for massive data collection. Throughout this survey, we summarize, categorize and analyze those contributions on data preprocessing that cope with streaming data. This work also takes into account the existing relationships between the different families of methods (feature and instance selection, and discretization). To enrich our study, we conduct thorough experiments using the most relevant contributions and present an analysis of their predictive performance, reduction rates, computational time, and memory usage. Finally, we offer general advices about existing data stream preprocessing algorithms, as well as discuss emerging future challenges to be faced in the domain of data stream preprocessing.
2590019597;Multi-view learning overview;2017.0;[];This overview reviews theoretical underpinnings of multi-view learning.It provides comprehensive introduction for the recent developments of multi-view learning methods.It attempts to identify promising venues and point out some specific challenges. Multi-view learning is an emerging direction in machine learning which considers learning with multiple views to improve the generalization performance. Multi-view learning is also known as data fusion or data integration from multiple feature sets. Since the last survey of multi-view machine learning in early 2013, multi-view learning has made great progress and developments in recent years, and is facing new challenges. This overview first reviews theoretical underpinnings to understand the properties and behaviors of multi-view learning. Then multi-view learning methods are described in terms of three classes to offer a neat categorization and organization. For each category, representative algorithms and newly proposed algorithms are presented. The main feature of this survey is that we provide comprehensive introduction for the recent developments of multi-view learning methods on the basis of coherence with early methods. We also attempt to identify promising venues and point out some specific challenges which can hopefully promote further research in this rapidly developing field.
2601688613;The existence and uniqueness of positive monotone solutions for a class of nonlinear Schrdinger equations on infinite domains;2017.0;[];In this paper, by constructing a new weighted norm method and analysis technique, we establish the conditions for the existence and uniqueness of positive monotone solutions to a class of nonlinear Schrdinger equations in planar exterior domains. Some examples are also given to demonstrate the application of our main results.
2607306668;The relative performance of ensemble methods with deep convolutional neural networks for image classification;2018.0;[];Artificial neural networks have been successfully applied to a variety of machine learning tasks, including image recognition, semantic segmentation, and machine translation. However, few studies f...
2612174832;Modelling of information diffusion on social networks with applications to WeChat;2017.0;[];Traces of user activities recorded in online social networks open new possibilities to systematically understand the information diffusion process on social networks. From the online social network WeChat, we collected a large number of information cascade trees, each of which tells the spreading trajectory of a message/information such as which user creates the information and which users view or forward the information shared by which neighbours. In this work, we propose two heterogeneous non-linear models, one for the topologies of the information cascade trees and the other for the stochastic process of information diffusion on a social network. Both models are validated by the WeChat data in reproducing and explaining key features of cascade trees.
2615953416;Stochastic Simulation;1987.0;[];
2618609858;Better Text Understanding Through Image-To-Text Transfer;2017.0;[];Generic text embeddings are successfully used in a variety of tasks. However, they are often learnt by capturing the co-occurrence structure from pure text corpora, resulting in limitations of their ability to generalize. In this paper, we explore models that incorporate visual information into the text representation. Based on comprehensive ablation studies, we propose a conceptually simple, yet well performing architecture. It outperforms previous multimodal approaches on a set of well established benchmarks. We also improve the state-of-the-art results for image-related text datasets, using orders of magnitude less data.
2619998782;Entire blow-up solutions for a quasilinear p-Laplacian Schrödinger equation with a non-square diffusion term;2017.0;[];   In this paper, we study the entire blow-up solutions for a quasilinear    p   -Laplacian Schrodinger elliptic equation with a non-square diffusion term. By using the dual approach and some new iterative techniques, the difficulty due to the non-square diffusion term and the    p   -Laplacian operator is overcome and the nonexistence and existence of entire blow-up solutions are established.
2725765108;Threshold Dynamics of a Stochastic Model with Vertical Transmission and Vaccination;2017.0;[];A stochastic  model with vertical transmission and vaccination is proposed and investigated in this paper. The threshold dynamics are explored when the noise is small. The conditions for the extinction or persistence of infectious diseases are deduced. Our results show that large noise can lead to the extinction of infectious diseases which is conducive to epidemic diseases control.
2742947535;Video Highlights Detection and Summarization with Lag-Calibration based on Concept-Emotion Mapping of Crowd-sourced Time-Sync Comments;2017.0;[];"With the prevalence of video sharing, there are increasing demands for automatic video digestion such as highlight detection. Recently, platforms with crowdsourced time-sync video comments have emerged worldwide, providing a good opportunity for highlight detection. However, this task is non-trivial: (1) time-sync comments often lag behind their corresponding shot; (2) time-sync comments are semantically sparse and noisy; (3) to determine which shots are highlights is highly subjective. The present paper aims to tackle these challenges by proposing a framework that (1) uses concept-mapped lexical-chains for lag calibration; (2) models video highlights based on comment intensity and combination of emotion and concept concentration of each shot; (3) summarize each detected highlight using improved SumBasic with emotion and concept mapping. Experiments on large real-world datasets show that our highlight detection method and summarization method both outperform other benchmarks with considerable margins."
2756292711;Some q-Rung Orthopai Fuzzy Bonferroni Mean Operators and Their Application to Multi-Attribute Group Decision Making;2018.0;[];In the real multi-attribute group decision making (MAGDM), there will be a mutual relationship between different attributes. As we all know, the Bonferroni mean (BM) operator has the advantage of considering interrelationships between parameters. In addition, in describing uncertain information, the eminent characteristic of q-rung orthopair fuzzy sets (q-ROFs) is that the sum of the qth power of the membership degree and the qth power of the degrees of non-membership is equal to or less than 1, so the space of uncertain information they can describe is broader. In this paper, we combine the BM operator with q-rung orthopair fuzzy numbers (q-ROFNs) to propose the q-rung orthopair fuzzy BM (q-ROFBM) operator, the q-rung orthopair fuzzy weighted BM (q-ROFWBM) operator, the q-rung orthopair fuzzy geometric BM (q-ROFGBM) operator, and the q-rung orthopair fuzzy weighted geometric BM (q-ROFWGBM) operator, then the MAGDM methods are developed based on these operators. Finally, we use an example to illustrate the MAGDM process of the proposed methods. The proposed methods based on q-ROFWBM and q-ROFWGBM operators are very useful to deal with MAGDM problems.
2765235648;FP-BNN;2018.0;[];Deep neural networks (DNNs) have attracted significant attention for their excellent accuracy especially in areas such as computer vision and artificial intelligence. To enhance their performance, technologies for their hardware acceleration are being studied. FPGA technology is a promising choice for hardware acceleration, given its low power consumption and high flexibility which makes it suitable particularly for embedded systems. However, complex DNN models may need more computing and memory resources than those available in many current FPGAs. This paper presents FP-BNN, a binarized neural network (BNN) for FPGAs, which drastically cuts down the hardware consumption while maintaining acceptable accuracy. We introduce a Resource-Aware Model Analysis (RAMA) method, and remove the bottleneck involving multipliers by bit-level XNOR and shifting operations, and the bottleneck of parameter access by data quantization and optimized on-chip storage. We evaluate the FP-BNN accelerator designs for MNIST multi-layer perceptrons (MLP), Cifar-10 ConvNet, and AlexNet on a Stratix-V FPGA system. An inference performance of Tera opartions per second with acceptable accuracy loss is obtained, which shows improvement in speed and energy efficiency over other computing platforms.
2795093135;Predicting Future Instance Segmentations by Forecasting Convolutional Features;2018.0;[];"Anticipating future events is an important prerequisite towards intelligent behavior. Video forecasting has been studied as a proxy task towards this goal. Recent work has shown that to predict semantic segmentation of future frames, forecasting at the semantic level is more effective than forecasting RGB frames and then segmenting these. In this paper we consider the more challenging problem of future instance segmentation, which additionally segments out individual objects. To deal with a varying number of output labels per image, we develop a predictive model in the space of fixed-sized convolutional features of the Mask R-CNN instance segmentation model. We apply the ""detection head"" of Mask R-CNN on the predicted features to produce the instance segmentation of future frames. Experiments show that this approach significantly improves over baselines based on optical flow."
2883737864;Preference-based Online Learning with Dueling Bandits: A Survey.;2018.0;[];In machine learning, the notion of multi-armed bandits refers to a class of online learning problems, in which an agent is supposed to simultaneously explore and exploit a given set of choice alternatives in the course of a sequential decision process. In the standard setting, the agent learns from stochastic feedback in the form of real-valued rewards. In many applications, however, numerical reward signals are not readily available -- instead, only weaker information is provided, in particular relative preferences in the form of qualitative comparisons between pairs of alternatives. This observation has motivated the study of variants of the multi-armed bandit problem, in which more general representations are used both for the type of feedback to learn from and the target of prediction. The aim of this paper is to provide a survey of the state of the art in this field, referred to as preference-based multi-armed bandits or dueling bandits. To this end, we provide an overview of problems that have been considered in the literature as well as methods for tackling them. Our taxonomy is mainly based on the assumptions made by these methods about the data-generating process and, related to this, the properties of the preference-based feedback.
2891946740;FINN- R : An End-to-End Deep-Learning Framework for Fast Exploration of Quantized Neural Networks;2018.0;[];Convolutional Neural Networks have rapidly become the most successful machine-learning algorithm, enabling ubiquitous machine vision and intelligent decisions on even embedded computing systems. While the underlying arithmetic is structurally simple, compute and memory requirements are challenging. One of the promising opportunities is leveraging reduced-precision representations for inputs, activations, and model parameters. The resulting scalability in performance, power efficiency, and storage footprint provides interesting design compromises in exchange for a small reduction in accuracy. FPGAs are ideal for exploiting low-precision inference engines leveraging custom precisions to achieve the required numerical accuracy for a given application. In this article, we describe the second generation of the FINN framework, an end-to-end tool that enables design-space exploration and automates the creation of fully customized inference engines on FPGAs. Given a neural network description, the tool optimizes for given platforms, design targets, and a specific precision. We introduce formalizations of resource cost functions and performance predictions and elaborate on the optimization algorithms. Finally, we evaluate a selection of reduced precision neural networks ranging from CIFAR-10 classifiers to YOLO-based object detection on a range of platforms including PYNQ and AWS F1, demonstrating new unprecedented measured throughput at 50 TOp/s on AWS F1 and 5 TOp/s on embedded devices.
29513559;On how to be a dialogician;2005.0;[];We will take as one of the main issues of this paper the challenge which the dialogical approaches offer to the relation of semantics and pragmatics concerning the concept of proof (strategy) and proposition (game). While our aim here will be to present the main technical and philosophical features of what can be seen as the dialogical approach to logic, illustrated through both very well known and new dialogics, we would also like to delineate the common pragmatic attitude which constitutes the cohesive force within the dialogical universe.
592218986;Emerging direction in embedded and ubiquitous computing : EUC 2007 Workshops : TRUST, WSOC, NCUS, UUWSN, USN, ESO, and SECUBIQ Taipei, Taiwan, December 17-20, 2007 : proceedings;2007.0;[];Trustworthiness, Reliability and Services in Ubiquitous and Sensor Networks.- Attack-Resilient Random Key Distribution Scheme for Distributed Sensor Networks.- A Critical Approach to Privacy Research in Ubiquitous Environments - Issues and Underlying Assumptions.- The Case Study of Information Security System for International Airports.- Quantitative Evaluation of Intrusion Tolerant Systems Subject to DoS Attacks Via Semi-Markov Cost Models.- An Efficient Mutual Authentication Scheme for EPCglobal Class-1 Generation-2 RFID System.- UPS - An Ubiquitous Proximity eService for Trust Collaboration.- Obligations for Privacy and Confidentiality in Distributed Transactions.- Multi-channel Enhancements for IEEE 802.11-Based Multi-hop Ad-Hoc Wireless Networks.- An Intelligent Event-Driven Interface Agent for Interactive Digital Contents in Ubiquitous Environments.- A Loop-Based Key Management Scheme for Wireless Sensor Networks.- A MAC Protocol with Little Idle Listening for Wireless Sensor Networks.- Security Technologies Based on Home Gateway for Making Smart Home Secure.- Layered Peer to Peer Streaming Using Hidden Markov Models.- Optimum Power Controller for Random Number Generator in the Crypto Module of Ubiquitous Computing Environment.- Problem Localization for Automated System Management in Ubiquitous Computing.- System and Software for Wireless SoC.- A High Speed Analog to Digital Converter for Ultra Wide Band Applications.- Design and DSP Software Implementation of Mobile WiMAX Baseband Transceiver Functions.- Cross-Layer Design for IEEE 802.16-2005 System Using Platform-Based Methodologies.- A Dynamic Frequency Allocation Scheme for IEEE 802.16 OFDMA-Based WMANs Using Hungary Algorithm.- Wireless Network Management System for WiMAX / Wi-Fi Mesh Networks.- An Implementation of QoS Framework for Heterogeneous Networks.- An Energy-Efficient MAC Design for IEEE 802.15.4-Based Wireless Sensor Networks.- A Cross-Layer Signaling and Middleware Platform for Multi-interface Mobile Devices.- Enhanced Sleep Mode Operations for Energy Saving in IEEE 802.16e.- Enhanced Fingerprint-Based Location Estimation System in Wireless LAN Environment.- Improving Channel Scanning Procedures for WLAN Handoffs.- Network Centric Ubiquitous Systems.- A Multicast Extension for Enhanced Mobile IP by Home Agent Handover.- Autonomic Multi-server Distribution in Flash Crowds Alleviation Network.- Generic Energy-Efficient Geographic Routing for Ad-Hoc Wireless Networks.- Description of a New Feature Meta-model.- Studying of Multi-dimensional Based Replica Management in Object Storage System.- Communication Model Exploration for Distributed Embedded Systems and System Level Interpretations.- An End-to-End QoS Adaptation Architecture for the Integrated IntServ and DiffServ Networks.- Ubiquitous Laboratory: A Research Support Environment for Ubiquitous Learning Based on Sensor Networks.- Intelligent Monitoring Using Wireless Sensor Networks.- On the Design of Micro-mobility for Mobile Network.- ANSWER: Adaptive Network Selection in WLAN/UMTS EnviRonment.- Self-authorized Public Key Management for Home Networks.- A Cross-Layered Diagnostician in OSGi Platform for Home Network.- Ubiquitous Underwater Acoustic-Sensor Network.- LaMSM: Localization Algorithm with Merging Segmented Maps for Underwater Sensor Networks.- TinyOS-Based Gateway for Underwater Acoustics/Radio Frequency Communication.- An Energy Scheduling Algorithm for Ensuring the Pre-determined Lifetime in Sensor Network.- Underwater Acoustic Communication and Modem-Based Navigation Aids.- State-of-the-Art in MAC Protocols for Underwater Acoustics Sensor Networks.- An Ultrasonic Sensor Based Low-Power Acoustic Modem for Underwater Communication in Underwater Wireless Sensor Networks.- UWA-NAV - Energy Efficient Error Control Scheme for Underwater Acoustic Sensor Network.- Underwater Wideband Source Localization Using the Interference Pattern Matching.- A New Virtual Select Database Operation for Wireless Sensor Networks.- GT2 - Reduced Wastes Time Mechanism for Underwater Acoustic Sensor Network.- RFID and Ubiquitous Sensor Networks.- Comparative Evaluation of Probabilistic and Deterministic Tag Anti-collision Protocols for RFID Networks.- An Efficient Mutual Authentication Protocol on RFID Tags.- HGLAP - Hierarchical Group-Index Based Lightweight Authentication Protocol for Distributed RFID System.- Target Classification in Sparse Sampling Acoustic Sensor Networks Using IDDC Algorithm.- Scriptable Sensor Network Based Home-Automation.- Applying Situation Awareness to Mobile Proactive Information Delivery.- Embedded Software Optimization.- Energy-Efficiency on a Variable-Bitrate Device.- The Secure DAES Design for Embedded System Application.- Software Power Peak Reduction on Smart Card Systems Based on Iterative Compiling.- Simultaneous Operation Scheduling and Operation Delay Selection to Minimize Cycle-by-Cycle Power Differential.- A Simple Approach to Robust Optimal Pole Assignment of Decentralized Stochastic Singularly-Perturbed Computer Controlled Systems.- Assured-Timeliness Integrity Protocols for Distributable Real-Time Threads with in Dynamic Distributed Systems.- Evaluating Modeling Solutions on Their Ability to Support the Partitioning of Automotive Embedded Systems.- Security in Ubiquitous Computing.- Security Analysis of the Certificateless Signature Scheme Proposed at SecUbiq 2006.- New Efficient Certificateless Signature Scheme.- A Practical Identity-Based Signature Scheme from Bilinear Map.- Linkable Ring Signatures from Linear Feedback Shift Register.- A Simple and Efficient Key Exchange Scheme Against the Smart Card Loss Problem.- A Key Distribution Scheme Preventing Collusion Attacks in Ubiquitous Heterogeneous Sensor Networks.- Token-Based Authenticated Key Establishment Protocols for Three-Party Communication.- Two Approaches on Pairwise Key Path Establishment for Sensor Networks.- An Efficient Authentication Protocol for RFID Systems Resistant to Active Attacks.- Low-Cost and Strong-Security RFID Authentication Protocol.- A Ticket Based Binding Update Authentication Method for Trusted Nodes in Mobile IPv6 Domain.
