{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "040446f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\galdw\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\huggingface_hub\\file_download.py:142: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\galdw\\.cache\\huggingface\\hub\\models--sentence-transformers--sentence-t5-xxl. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    }
   ],
   "source": [
    "import networkx as nx\n",
    "import numpy as np\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "class CitationGraphAbstractGenerator:\n",
    "    def __init__(self, embedding_model=\"sentence-transformers/sentence-t5-xxl\"):\n",
    "        \"\"\"\n",
    "        Initialize the citation graph abstract generator\n",
    "        \n",
    "        Args:\n",
    "            embedding_model: Model name for embedding abstracts\n",
    "        \"\"\"\n",
    "        self.embedding_model = SentenceTransformer(embedding_model)\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "        self.generator = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
    "        self.citation_graph = nx.DiGraph()\n",
    "        \n",
    "    def load_citation_graph(self, papers):\n",
    "        \"\"\"\n",
    "        Load papers and their citations into a graph\n",
    "        \n",
    "        Args:\n",
    "            papers: List of dictionaries, each containing 'id', 'abstract', and 'citations' (list of paper ids)\n",
    "        \"\"\"\n",
    "        # Add nodes (papers) to the graph\n",
    "        for paper in papers:\n",
    "            self.citation_graph.add_node(\n",
    "                paper['id'], \n",
    "                abstract=paper['abstract'],\n",
    "                embedding=None  # Will be populated later\n",
    "            )\n",
    "            \n",
    "        # Add edges (citations)\n",
    "        for paper in papers:\n",
    "            for cited_paper_id in paper['citations']:\n",
    "                if cited_paper_id in self.citation_graph:\n",
    "                    self.citation_graph.add_edge(paper['id'], cited_paper_id)\n",
    "                    \n",
    "        print(f\"Graph created with {self.citation_graph.number_of_nodes()} nodes and {self.citation_graph.number_of_edges()} edges\")\n",
    "        \n",
    "    def compute_embeddings(self):\n",
    "        \"\"\"\n",
    "        Compute and store embeddings for all papers in the graph\n",
    "        \"\"\"\n",
    "        for node_id in self.citation_graph.nodes():\n",
    "            abstract = self.citation_graph.nodes[node_id]['abstract']\n",
    "            embedding = self.embedding_model.encode(abstract)\n",
    "            self.citation_graph.nodes[node_id]['embedding'] = embedding\n",
    "        \n",
    "        print(\"Embeddings computed for all nodes\")\n",
    "    \n",
    "    def create_new_node(self):\n",
    "        \"\"\"\n",
    "        Create a new node in the graph without any links\n",
    "        \n",
    "        Returns:\n",
    "            new_node_id: ID of the new node\n",
    "        \"\"\"\n",
    "        # Create new node ID\n",
    "        new_node_id = f\"new_paper_{len([n for n in self.citation_graph.nodes() if 'new_paper' in str(n)])}\"\n",
    "        \n",
    "        # Add new node to graph without any connections\n",
    "        self.citation_graph.add_node(new_node_id, abstract=None, embedding=None)\n",
    "        \n",
    "        print(f\"Created new node {new_node_id} (no connections yet)\")\n",
    "        return new_node_id\n",
    "    \n",
    "    def predict_links(self, node_id):\n",
    "        \"\"\"\n",
    "        Predict potential links for a new node using only structural information\n",
    "        with stochastic elements similar to random graph generation.\n",
    "        \n",
    "        This method determines the number of links dynamically based on network properties\n",
    "        and uses structural network measures with controlled randomness to select links.\n",
    "        \n",
    "        Args:\n",
    "            node_id: ID of the node to predict links for\n",
    "            \n",
    "        Returns:\n",
    "            predicted_links: List of node IDs that are predicted to be linked\n",
    "        \"\"\"\n",
    "        \n",
    "        # Get all nodes except the target and other new nodes\n",
    "        candidate_nodes = [n for n in self.citation_graph.nodes() \n",
    "                        if n != node_id and 'new_paper' not in str(n)]\n",
    "        \n",
    "        if not candidate_nodes:\n",
    "            print(\"No candidate nodes available for link prediction\")\n",
    "            return []\n",
    "        \n",
    "        # STEP 1: Calculate network properties to determine num_links\n",
    "        \n",
    "        # Calculate degree distribution\n",
    "        in_degrees = dict(self.citation_graph.in_degree())\n",
    "        out_degrees = dict(self.citation_graph.out_degree())\n",
    "        total_degrees = {n: in_degrees.get(n, 0) + out_degrees.get(n, 0) for n in self.citation_graph.nodes()}\n",
    "        \n",
    "        # Analyze degree distribution for random graph model parameters\n",
    "        avg_degree = np.mean(list(total_degrees.values()))\n",
    "        std_degree = np.std(list(total_degrees.values()))\n",
    "        max_degree = max(total_degrees.values()) if total_degrees else 1\n",
    "        \n",
    "        # Get clustering coefficient - represents transitivity in the network\n",
    "        clustering_coef = nx.average_clustering(self.citation_graph)\n",
    "        \n",
    "        # STEP 2: Determine number of links using network properties\n",
    "        \n",
    "        # Base the number of links on degree distribution\n",
    "        # For citation networks, out-degree (references) follows a different distribution\n",
    "        # than in-degree (citations)\n",
    "        avg_out_degree = np.mean(list(out_degrees.values()))\n",
    "        \n",
    "        # Calculate num_links based on network's average out-degree with some variation\n",
    "        # This mimics how many papers a typical new paper in this network would cite\n",
    "        base_num_links = max(1, int(round(avg_out_degree)))\n",
    "        \n",
    "        # Add stochastic element - similar to random graph models, but informed by the \n",
    "        # actual degree variance in the network\n",
    "        degree_variance_factor = std_degree / (avg_degree + 1)  # Normalized variance\n",
    "        stochastic_factor = np.random.normal(loc=1.0, scale=degree_variance_factor)\n",
    "        stochastic_factor = max(0.5, min(stochastic_factor, 1.5))  # Keep within reasonable bounds\n",
    "        \n",
    "        # Apply stochastic factor\n",
    "        num_links = max(1, int(round(base_num_links * stochastic_factor)))\n",
    "        \n",
    "        # Ensure we don't predict more links than available candidates\n",
    "        num_links = min(num_links, len(candidate_nodes))\n",
    "        \n",
    "        # Log the network analysis and link count decision\n",
    "        print(f\"Network analysis: avg_out_degree={avg_out_degree:.2f}, clustering={clustering_coef:.2f}, \"\n",
    "            f\"degree_variance={degree_variance_factor:.2f}\")\n",
    "        print(f\"Dynamically determined num_links={num_links} for node {node_id} \"\n",
    "            f\"(stochastic_factor={stochastic_factor:.2f})\")\n",
    "        \n",
    "        # STEP 3: Calculate purely structural scores for link candidates\n",
    "        \n",
    "        # Calculate key structural metrics\n",
    "        # 1. Preferential attachment (rich-get-richer effect)\n",
    "        # Papers with more citations are more likely to be cited again\n",
    "        pref_attach_scores = {}\n",
    "        total_citations = sum(in_degrees.values())\n",
    "        for n in candidate_nodes:\n",
    "            if total_citations > 0:\n",
    "                pref_attach_scores[n] = in_degrees.get(n, 0) / total_citations\n",
    "            else:\n",
    "                pref_attach_scores[n] = 1.0 / len(candidate_nodes)\n",
    "        \n",
    "        # 2. Edge formation via transitivity (triangle closing)\n",
    "        # If papers A and B both cite paper C, then a new paper citing A might also cite B\n",
    "        # We approximate this using clustering coefficient and degree centrality\n",
    "        transitivity_scores = {}\n",
    "        degree_centrality = nx.degree_centrality(self.citation_graph)\n",
    "        for n in candidate_nodes:\n",
    "            # Papers with higher centrality and in clusters are more likely to be co-cited\n",
    "            transitivity_scores[n] = degree_centrality.get(n, 0) * clustering_coef\n",
    "        \n",
    "        # 3. Calculate recency score if 'year' attribute exists\n",
    "        # Recent papers are more likely to be cited by new papers\n",
    "        recency_scores = {}\n",
    "        current_year = 2025  # Example current year\n",
    "        for n in candidate_nodes:\n",
    "            if 'year' in self.citation_graph.nodes[n]:\n",
    "                year = self.citation_graph.nodes[n]['year']\n",
    "                # Linear decay over 10 years\n",
    "                recency_scores[n] = max(0, 1 - (current_year - year) / 10)\n",
    "            else:\n",
    "                recency_scores[n] = 0.5  # Default value\n",
    "        \n",
    "        # STEP 4: Calculate randomness factor similar to random graph models\n",
    "        \n",
    "        # Calculate random probability for each node\n",
    "        # Different random graph models use different probability distributions:\n",
    "        # - Erdős–Rényi: uniform probabilities\n",
    "        # - Barabási–Albert: preferential attachment\n",
    "        # - Watts–Strogatz: high clustering and short paths\n",
    "        \n",
    "        # We'll use a hybrid approach with citation network characteristics\n",
    "        \n",
    "        # Generate random component for each node\n",
    "        random_scores = {n: np.random.rand() for n in candidate_nodes}\n",
    "        \n",
    "        # STEP 5: Combine structural scores with controlled randomness\n",
    "        \n",
    "        # Set randomness parameter to control stochastic vs. structural influence\n",
    "        # Lower clustering suggests more random connections are needed\n",
    "        randomness_factor = max(0.2, min(0.5, 1.0 - clustering_coef))\n",
    "        \n",
    "        combined_scores = {}\n",
    "        for n in candidate_nodes:\n",
    "            # Calculate deterministic structural score\n",
    "            structural_score = (\n",
    "                0.5 * pref_attach_scores.get(n, 0) +   # Preferential attachment (citations)\n",
    "                0.3 * transitivity_scores.get(n, 0) +  # Transitivity (cluster formation)\n",
    "                0.2 * recency_scores.get(n, 0)         # Recency\n",
    "            )\n",
    "            \n",
    "            # Combine structural and random components\n",
    "            combined_scores[n] = (\n",
    "                (1 - randomness_factor) * structural_score + \n",
    "                randomness_factor * random_scores[n]\n",
    "            )\n",
    "        \n",
    "        # STEP 6: Select links using weighted random sampling\n",
    "        # This mimics the probabilistic edge formation in random graph models\n",
    "        # while respecting structural properties\n",
    "        \n",
    "        # Convert scores to probabilities\n",
    "        total_score = sum(combined_scores.values())\n",
    "        if total_score == 0:\n",
    "            probabilities = [1.0/len(candidate_nodes) for _ in candidate_nodes]\n",
    "        else:\n",
    "            probabilities = [combined_scores[n]/total_score for n in candidate_nodes]\n",
    "        \n",
    "        # Perform weighted sampling without replacement\n",
    "        try:\n",
    "            selected_indices = np.random.choice(\n",
    "                range(len(candidate_nodes)), \n",
    "                size=num_links, \n",
    "                replace=False, \n",
    "                p=probabilities\n",
    "            )\n",
    "            predicted_links = [candidate_nodes[i] for i in selected_indices]\n",
    "        except ValueError as e:\n",
    "            # Fallback if there's an issue with probability distribution\n",
    "            print(f\"Sampling error: {e}. Using deterministic selection instead.\")\n",
    "            predicted_links = sorted(combined_scores.items(), key=lambda x: x[1], reverse=True)[:num_links]\n",
    "            predicted_links = [n for n, _ in predicted_links]\n",
    "        \n",
    "        print(f\"Predicted {len(predicted_links)} links for node {node_id} \"\n",
    "            f\"with randomness factor {randomness_factor:.2f}\")\n",
    "        \n",
    "        return predicted_links\n",
    "    \n",
    "    def add_links(self, node_id, link_ids):\n",
    "        \"\"\"\n",
    "        Add links from a node to other nodes\n",
    "        \n",
    "        Args:\n",
    "            node_id: ID of the source node\n",
    "            link_ids: List of target node IDs\n",
    "        \"\"\"\n",
    "        for target_id in link_ids:\n",
    "            if target_id in self.citation_graph:\n",
    "                self.citation_graph.add_edge(node_id, target_id)\n",
    "        \n",
    "        print(f\"Added {len(link_ids)} links from node {node_id}\")\n",
    "    \n",
    "    def generate_abstract_from_links(self, node_id):\n",
    "        \"\"\"\n",
    "        Generate an abstract for a node based on its links\n",
    "        \n",
    "        Args:\n",
    "            node_id: ID of the node\n",
    "            \n",
    "        Returns:\n",
    "            abstract: Generated abstract\n",
    "        \"\"\"\n",
    "        # Get linked nodes\n",
    "        linked_nodes = list(self.citation_graph.successors(node_id))\n",
    "        \n",
    "        if not linked_nodes:\n",
    "            print(\"No links found for the node\")\n",
    "            return None\n",
    "        \n",
    "        # Get abstracts of linked papers\n",
    "        linked_abstracts = [self.citation_graph.nodes[n]['abstract'] for n in linked_nodes]\n",
    "        \n",
    "        # Create embeddings for the linked papers\n",
    "        linked_embeddings = [self.citation_graph.nodes[n]['embedding'] for n in linked_nodes]\n",
    "        \n",
    "        # Create an aggregated embedding for the new node\n",
    "        aggregated_embedding = np.mean(linked_embeddings, axis=0)\n",
    "        self.citation_graph.nodes[node_id]['embedding'] = aggregated_embedding\n",
    "        \n",
    "        # Create a prompt for the generator based on linked papers\n",
    "        prompt = f\"Based on the following research papers, write a novel abstract for a new paper that builds upon and extends these ideas:\\n\\n\"\n",
    "        prompt += \"\\n\\n\".join([f\"Paper {i+1}: {abstract}\" for i, abstract in enumerate(linked_abstracts)])\n",
    "        prompt += \"\\n\\nWrite a cohesive abstract that integrates concepts from these papers and proposes a novel approach or finding:\"\n",
    "        \n",
    "        # Generate abstract\n",
    "        inputs = self.tokenizer(prompt, return_tensors=\"pt\")\n",
    "        \n",
    "        outputs = self.generator.generate(\n",
    "            inputs.input_ids,\n",
    "            max_length=len(inputs.input_ids[0]) + 250,  # Allow for a reasonably sized abstract\n",
    "            num_return_sequences=1,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            do_sample=True\n",
    "        )\n",
    "        \n",
    "        generated_text = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        \n",
    "        # Extract just the abstract part\n",
    "        abstract = generated_text.split(\"a novel approach or finding:\")[1].strip() if \"a novel approach or finding:\" in generated_text else generated_text\n",
    "        \n",
    "        # Store the abstract\n",
    "        self.citation_graph.nodes[node_id]['abstract'] = abstract\n",
    "        \n",
    "        return abstract\n",
    "    \n",
    "    def evaluate_abstract(self, node_id):\n",
    "        \"\"\"\n",
    "        Evaluate the generated abstract\n",
    "        \n",
    "        Args:\n",
    "            node_id: ID of the node\n",
    "            \n",
    "        Returns:\n",
    "            metrics: Dictionary of evaluation metrics\n",
    "        \"\"\"\n",
    "        # Get the abstract and its embedding\n",
    "        abstract = self.citation_graph.nodes[node_id]['abstract']\n",
    "        \n",
    "        if abstract is None:\n",
    "            print(\"No abstract found for the node\")\n",
    "            return None\n",
    "        \n",
    "        # Compute embedding if not already done\n",
    "        if self.citation_graph.nodes[node_id]['embedding'] is None:\n",
    "            embedding = self.embedding_model.encode(abstract)\n",
    "            self.citation_graph.nodes[node_id]['embedding'] = embedding\n",
    "        else:\n",
    "            embedding = self.citation_graph.nodes[node_id]['embedding']\n",
    "        \n",
    "        # Get linked nodes\n",
    "        linked_nodes = list(self.citation_graph.successors(node_id))\n",
    "        \n",
    "        if not linked_nodes:\n",
    "            print(\"No links found for the node\")\n",
    "            return None\n",
    "        \n",
    "        # Compute similarity with linked papers\n",
    "        similarities = []\n",
    "        for n in linked_nodes:\n",
    "            other_embedding = self.citation_graph.nodes[n]['embedding']\n",
    "            similarity = cosine_similarity([embedding], [other_embedding])[0][0]\n",
    "            similarities.append(similarity)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        metrics = {\n",
    "            'avg_similarity': np.mean(similarities),\n",
    "            'max_similarity': np.max(similarities),\n",
    "            'min_similarity': np.min(similarities),\n",
    "            'std_similarity': np.std(similarities),\n",
    "            'num_linked_papers': len(linked_nodes)\n",
    "        }\n",
    "        \n",
    "        return metrics\n",
    "    \n",
    "    def visualize_graph(self, highlight_node=None):\n",
    "        \"\"\"\n",
    "        Visualize the citation graph\n",
    "        \n",
    "        Args:\n",
    "            highlight_node: Node to highlight in the visualization\n",
    "        \"\"\"\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        \n",
    "        # Define node colors\n",
    "        node_colors = []\n",
    "        for node in self.citation_graph.nodes():\n",
    "            if node == highlight_node:\n",
    "                node_colors.append('red')\n",
    "            elif 'new_paper' in str(node):\n",
    "                node_colors.append('green')\n",
    "            else:\n",
    "                node_colors.append('lightblue')\n",
    "        \n",
    "        # Define node sizes based on in-degree (citation count)\n",
    "        node_sizes = []\n",
    "        for node in self.citation_graph.nodes():\n",
    "            in_degree = self.citation_graph.in_degree(node)\n",
    "            node_sizes.append(300 + in_degree * 50)\n",
    "        \n",
    "        # Create layout\n",
    "        pos = nx.spring_layout(self.citation_graph, seed=42)\n",
    "        \n",
    "        # Draw the graph\n",
    "        nx.draw(\n",
    "            self.citation_graph, \n",
    "            pos=pos, \n",
    "            with_labels=True, \n",
    "            node_color=node_colors,\n",
    "            node_size=node_sizes, \n",
    "            alpha=0.7, \n",
    "            arrows=True\n",
    "        )\n",
    "        \n",
    "        plt.title(\"Citation Graph with Generated Papers\")\n",
    "        plt.show()\n",
    "\n",
    "# Example usage\n",
    "def run_demo():\n",
    "    # Create sample papers with abstracts and citations\n",
    "    papers = [\n",
    "        {\n",
    "            'id': 'paper1',\n",
    "            'abstract': \"This paper introduces a new method for graph-based text generation using attention mechanisms and transformer models.\",\n",
    "            'citations': [],\n",
    "            'year': 2020\n",
    "        },\n",
    "        {\n",
    "            'id': 'paper2',\n",
    "            'abstract': \"We propose a novel approach to citation network analysis using reinforcement learning and knowledge graphs.\",\n",
    "            'citations': ['paper1'],\n",
    "            'year': 2021\n",
    "        },\n",
    "        {\n",
    "            'id': 'paper3',\n",
    "            'abstract': \"This work extends previous research on knowledge graphs with transformer architectures and bidirectional encodings.\",\n",
    "            'citations': ['paper1', 'paper2'],\n",
    "            'year': 2022\n",
    "        },\n",
    "        {\n",
    "            'id': 'paper4',\n",
    "            'abstract': \"Our research combines graph neural networks with language models for scientific discovery and automated hypothesis generation.\",\n",
    "            'citations': ['paper2', 'paper3'],\n",
    "            'year': 2023\n",
    "        },\n",
    "        {\n",
    "            'id': 'paper5',\n",
    "            'abstract': \"This paper presents a novel approach to feature propagation in citation networks using diffusion models.\",\n",
    "            'citations': ['paper1', 'paper4'],\n",
    "            'year': 2024\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    # Initialize the generator\n",
    "    generator = CitationGraphAbstractGenerator()\n",
    "    \n",
    "    # Load citation graph\n",
    "    generator.load_citation_graph(papers)\n",
    "    \n",
    "    # Compute embeddings for existing papers\n",
    "    generator.compute_embeddings()\n",
    "    \n",
    "    # Create a new node\n",
    "    new_node_id = generator.create_new_node()\n",
    "    \n",
    "    # Predict links for the new node\n",
    "    predicted_links = generator.predict_links(new_node_id, num_links=3)\n",
    "    print(f\"Predicted links: {predicted_links}\")\n",
    "    \n",
    "    # Add the predicted links\n",
    "    generator.add_links(new_node_id, predicted_links)\n",
    "    \n",
    "    # Generate abstract based on the links\n",
    "    abstract = generator.generate_abstract_from_links(new_node_id)\n",
    "    print(f\"\\nGenerated Abstract:\\n{abstract}\\n\")\n",
    "    \n",
    "    # Evaluate the abstract\n",
    "    metrics = generator.evaluate_abstract(new_node_id)\n",
    "    print(\"Evaluation Metrics:\")\n",
    "    for metric, value in metrics.items():\n",
    "        print(f\"- {metric}: {value:.4f}\")\n",
    "    \n",
    "    # Visualize the graph\n",
    "    generator.visualize_graph(highlight_node=new_node_id)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_demo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc33910a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
