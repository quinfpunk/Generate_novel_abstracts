{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0401327c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\galdw\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from LinkPredictor import EnhancedLinkPredictor\n",
    "from EncoderDecoder import BottleneckT5Autoencoder\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import nltk\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "except LookupError:\n",
    "    nltk.download('punkt')\n",
    "try:\n",
    "    nltk.data.find('corpora/stopwords')\n",
    "except LookupError:\n",
    "    nltk.download('stopwords')\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import unicodedata\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "import spacy\n",
    "#!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "54622bd8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>year</th>\n",
       "      <th>references</th>\n",
       "      <th>abstract</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3005773274</td>\n",
       "      <td>Malware classification algorithm using advance...</td>\n",
       "      <td>2020.0</td>\n",
       "      <td>[]</td>\n",
       "      <td>Recently, Internet of Drones (IoD) are issu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3002219790</td>\n",
       "      <td>Generalized transitivity: A systematic compari...</td>\n",
       "      <td>2020.0</td>\n",
       "      <td>[79079207, 170687150, 756957829, 1517040319, 1...</td>\n",
       "      <td>Reciprocal relations are binary relations Q...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2999570098</td>\n",
       "      <td>A New DGNSS Positioning Infrastructure for And...</td>\n",
       "      <td>2020.0</td>\n",
       "      <td>[1978255623, 2109436662, 2146023544, 218050761...</td>\n",
       "      <td>One’s position has become an important piece o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2998778732</td>\n",
       "      <td>A New Cycle Slip Detection and Repair Method U...</td>\n",
       "      <td>2020.0</td>\n",
       "      <td>[2482503770]</td>\n",
       "      <td>The detection and repair of the cycle slip is ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2972569439</td>\n",
       "      <td>Existence and concentration of positive ground...</td>\n",
       "      <td>2020.0</td>\n",
       "      <td>[]</td>\n",
       "      <td>This paper is concerned with the following ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           id                                              title    year  \\\n",
       "0  3005773274  Malware classification algorithm using advance...  2020.0   \n",
       "1  3002219790  Generalized transitivity: A systematic compari...  2020.0   \n",
       "2  2999570098  A New DGNSS Positioning Infrastructure for And...  2020.0   \n",
       "3  2998778732  A New Cycle Slip Detection and Repair Method U...  2020.0   \n",
       "4  2972569439  Existence and concentration of positive ground...  2020.0   \n",
       "\n",
       "                                          references  \\\n",
       "0                                                 []   \n",
       "1  [79079207, 170687150, 756957829, 1517040319, 1...   \n",
       "2  [1978255623, 2109436662, 2146023544, 218050761...   \n",
       "3                                       [2482503770]   \n",
       "4                                                 []   \n",
       "\n",
       "                                            abstract  \n",
       "0     Recently, Internet of Drones (IoD) are issu...  \n",
       "1     Reciprocal relations are binary relations Q...  \n",
       "2  One’s position has become an important piece o...  \n",
       "3  The detection and repair of the cycle slip is ...  \n",
       "4     This paper is concerned with the following ...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('dblp_processing/dblp_papers_2020_SampleWithRefs.csv', sep=';')\n",
    "df['abstract'] = df['abstract'].fillna('')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cc33910a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CitationGraphAbstractGenerator:\n",
    "    def __init__(self, embedding_model_name='t5', linkPredictor_model_name='enhanced'):\n",
    "        \"\"\"\n",
    "        Initialize the citation graph abstract generator\n",
    "        \n",
    "        Args:\n",
    "            embedding_model: Model for embedding abstracts\n",
    "            linkPredictor_model: Model for link prediction\n",
    "        \"\"\"\n",
    "        # Download nltk resources if needed\n",
    "\n",
    "        self.embedding_model_name = embedding_model_name\n",
    "        self.linkPredictor_model_name = linkPredictor_model_name\n",
    "        if embedding_model_name=='t5':\n",
    "            self.embedding_model = BottleneckT5Autoencoder()\n",
    "        if linkPredictor_model_name=='enhanced':\n",
    "            self.linkPredictor_model = EnhancedLinkPredictor()\n",
    "        self.citation_graph = nx.DiGraph()\n",
    "        self.stop_words = list(stopwords.words('english'))\n",
    "\n",
    "    def _update_linkPredictor(self):\n",
    "        \"\"\"\n",
    "        Update the linkPredictor model, used when we update the grap (e.g., for new papers)\n",
    "        \"\"\"\n",
    "        if self.linkPredictor_model_name=='enhanced':\n",
    "            self.linkPredictor_model = EnhancedLinkPredictor(self.citation_graph)\n",
    "        \n",
    "\n",
    "    def load_citation_graph(self, papers):\n",
    "        \"\"\"\n",
    "        Load papers and their references into a graph\n",
    "        \n",
    "        Args:\n",
    "            papers: List of dictionaries, each containing 'id', 'abstract', and 'references' (list of paper ids)\n",
    "        \"\"\"\n",
    "        # Add nodes (papers) to the graph\n",
    "        for paper in papers:\n",
    "            # Extract key concepts from the abstract\n",
    "            key_concepts = self.extract_key_concepts(paper['abstract'])\n",
    "            \n",
    "            self.citation_graph.add_node(\n",
    "                paper['id'], \n",
    "                abstract=paper['abstract'],\n",
    "                embedding=None,  # Will be populated later\n",
    "                key_concepts=key_concepts  # Added key_concepts property\n",
    "            )\n",
    "            \n",
    "        # Add edges (references)\n",
    "        for paper in papers:\n",
    "            for cited_paper_id in paper['references']:\n",
    "                if cited_paper_id in self.citation_graph:\n",
    "                    self.citation_graph.add_edge(paper['id'], cited_paper_id)\n",
    "                    \n",
    "        print(f\"Graph created with {self.citation_graph.number_of_nodes()} nodes and {self.citation_graph.number_of_edges()} edges\")\n",
    "        \n",
    "    def extract_key_concepts(self,text, score_threshold=0.2):\n",
    "        \"\"\"\n",
    "        Extract key concepts from an abstract using TF-IDF with a score threshold.\n",
    "\n",
    "        Args:\n",
    "            text (str): Input text (e.g., an abstract).\n",
    "            score_threshold (float): Minimum TF-IDF score to consider as a concept.\n",
    "\n",
    "        Returns:\n",
    "            list: List of key concepts.\n",
    "        \"\"\"\n",
    "        # Tokenization and cleaning (removing special characters and non-alphanumeric tokens)\n",
    "        tokens = re.findall(r'\\b\\w+\\b', text.lower())\n",
    "\n",
    "        # Remove stop words\n",
    "        filtered_tokens = [t for t in tokens if t not in self.stop_words and len(t) > 2]  # Exclude stop words and short words\n",
    "\n",
    "        if len(filtered_tokens) < 5:\n",
    "            # If there are fewer than 5 meaningful tokens, return the tokens directly\n",
    "            return list(set(filtered_tokens))\n",
    "\n",
    "        # Apply TF-IDF to extract important terms\n",
    "        tfidf = TfidfVectorizer(ngram_range=(1, 3), stop_words='english')\n",
    "        \n",
    "        try:\n",
    "            tfidf_matrix = tfidf.fit_transform([text])\n",
    "            feature_names = tfidf.get_feature_names_out()\n",
    "            scores = tfidf_matrix.toarray()[0]\n",
    "\n",
    "            # Pair terms with their TF-IDF scores\n",
    "            scored_concepts = [(feature_names[i], scores[i]) for i in range(len(scores))]\n",
    "\n",
    "            # Filter terms based on the score threshold\n",
    "            scored_concepts = sorted(scored_concepts, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "            # Only include concepts above the threshold\n",
    "            filtered_concepts = [concept for concept, score in scored_concepts if score >= score_threshold]\n",
    "\n",
    "            return filtered_concepts\n",
    "\n",
    "        except ValueError as e:\n",
    "            # If TF-IDF fails (empty vocabulary), return tokens or an error message\n",
    "            print(f\"Error with TF-IDF: {e}\")\n",
    "            return list(set(filtered_tokens))\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"TF-IDF failed: {e}\")\n",
    "            print(f\"Tokens: {tokens}\")\n",
    "            print(f\"Type of tokens: {type(tokens)}\")\n",
    "            return list(set(tokens))[:10]\n",
    "\n",
    "        \n",
    "    def compute_embeddings(self):\n",
    "        \"\"\"\n",
    "        Compute and store embeddings for all papers in the graph\n",
    "        \"\"\"\n",
    "        for node_id in self.citation_graph.nodes():\n",
    "            abstract = self.citation_graph.nodes[node_id]['abstract']\n",
    "            \n",
    "            if not abstract:\n",
    "                print(f\"Skipping node {node_id}: Abstract is empty or None.\")\n",
    "                continue  # Skip this node if abstract is empty\n",
    "            \n",
    "            # Ensure abstract is a valid string\n",
    "            if isinstance(abstract, str):\n",
    "                print(f\"Embedding abstract for node {node_id}: {abstract[:100]}...\")  # Debug output for first 100 characters\n",
    "                embedding = self.embedding_model.embed(abstract)  # Embedding computation\n",
    "                self.citation_graph.nodes[node_id]['embedding'] = embedding\n",
    "            else:\n",
    "                print(f\"Invalid abstract format for node {node_id}: {abstract}\")\n",
    "            \n",
    "        print(\"Embeddings computed for all nodes\")\n",
    "    \n",
    "    def create_new_node(self):\n",
    "        \"\"\"\n",
    "        Create a new node in the graph without any links\n",
    "        \n",
    "        Returns:\n",
    "            new_node_id: ID of the new node\n",
    "        \"\"\"\n",
    "        # Create new node ID\n",
    "        new_node_id = f\"new_paper_{len([n for n in self.citation_graph.nodes() if 'new_paper' in str(n)])}\"\n",
    "        \n",
    "        # Add new node to graph without any connections\n",
    "        self.citation_graph.add_node(new_node_id, abstract=None, embedding=None, key_concepts=[])\n",
    "\n",
    "        predicted_links = self.predict_links(new_node_id)  # Predict links for the new node\n",
    "\n",
    "        # Add the predicted links\n",
    "        self.add_links(new_node_id, predicted_links)\n",
    "        \n",
    "        # Generate abstract based on the links\n",
    "        self.generate_abstract_from_links(new_node_id)\n",
    "\n",
    "        # Update the target node's embedding and key concepts\n",
    "        self._update_linkPredictor()\n",
    "\n",
    "        print(f\"Created new node {new_node_id} (no connections yet)\")\n",
    "        return new_node_id\n",
    "    \n",
    "    def predict_links(self, node_id):\n",
    "        \"\"\"\n",
    "        Predict potential links for a new node using only structural information\n",
    "        with stochastic elements similar to random graph generation.\n",
    "        \n",
    "        This method determines the number of links dynamically based on network properties\n",
    "        and uses structural network measures with controlled randomness to select links.\n",
    "        \n",
    "        Args:\n",
    "            node_id: ID of the node to predict links for\n",
    "            \n",
    "        Returns:\n",
    "            predicted_links: List of node IDs that are predicted to be linked\n",
    "        \"\"\"\n",
    "        \n",
    "        # Get all nodes except the target and other new nodes\n",
    "        \n",
    "        predicted_links = self.linkPredictor_model.predict_links(node_id)\n",
    "        \n",
    "        return predicted_links\n",
    "    \n",
    "    def add_links(self, node_id, link_ids):\n",
    "        \"\"\"\n",
    "        Add links from a node to other nodes\n",
    "        \n",
    "        Args:\n",
    "            node_id: ID of the source node\n",
    "            link_ids: List of target node IDs\n",
    "        \"\"\"\n",
    "        for target_id in link_ids:\n",
    "            if target_id in self.citation_graph:\n",
    "                self.citation_graph.add_edge(node_id, target_id)\n",
    "\n",
    "        print(f\"Added {len(link_ids)} links from node {node_id}\")\n",
    "    \n",
    "    def generate_abstract_from_links(self, node_id):\n",
    "        \"\"\"\n",
    "        Generate an abstract for a node based on its links using embedding averaging\n",
    "        \n",
    "        Args:\n",
    "            node_id: ID of the node to generate an abstract for\n",
    "        \"\"\"\n",
    "        # Get linked nodes\n",
    "        linked_nodes = list(self.citation_graph.successors(node_id))\n",
    "        \n",
    "        if not linked_nodes:\n",
    "            print(\"No links found for the node\")\n",
    "            return None\n",
    "        \n",
    "        # Get abstracts of linked papers\n",
    "        linked_abstracts = [self.citation_graph.nodes[n]['abstract'] for n in linked_nodes]\n",
    "        \n",
    "        # Create embeddings for the linked papers if they don't exist\n",
    "        for n in linked_nodes:\n",
    "            if self.citation_graph.nodes[n]['embedding'] is None:\n",
    "                embedding = self.embedding_model.encode(self.citation_graph.nodes[n]['abstract'])\n",
    "                self.citation_graph.nodes[n]['embedding'] = embedding\n",
    "        \n",
    "        # Get embeddings of linked papers\n",
    "        linked_embeddings = [self.citation_graph.nodes[n]['embedding'] for n in linked_nodes]\n",
    "        \n",
    "        # Create an aggregated embedding for the new node by averaging\n",
    "        aggregated_embedding = np.mean(linked_embeddings, axis=0)\n",
    "        self.citation_graph.nodes[node_id]['embedding'] = aggregated_embedding\n",
    "        \n",
    "        # Extract just the abstract part\n",
    "        abstract = self.embedding_model.generate_from_latent(aggregated_embedding)\n",
    "        \n",
    "        # Store the abstract\n",
    "        self.citation_graph.nodes[node_id]['abstract'] = abstract\n",
    "        \n",
    "        # Extract key concepts from the abstract\n",
    "        key_concepts = self.extract_key_concepts(abstract)\n",
    "        self.citation_graph.nodes[node_id]['key_concepts'] = key_concepts\n",
    "        \n",
    "        print(f\"Generated abstract with {len(key_concepts)} key concepts: {', '.join(key_concepts)}\")\n",
    "        \n",
    "    \n",
    "    def calculate_concept_similarity(self, concepts1, concepts2, threshold=0.7):\n",
    "        \"\"\"\n",
    "        Calculate similarity between two sets of concepts\n",
    "        \n",
    "        Args:\n",
    "            concepts1: First list of concepts\n",
    "            concepts2: Second list of concepts\n",
    "            threshold: Similarity threshold for considering concepts as matching\n",
    "            \n",
    "        Returns:\n",
    "            float: Percentage of concepts1 that have a match in concepts2\n",
    "        \"\"\"\n",
    "        if not concepts1 or not concepts2:\n",
    "            return 0.0\n",
    "        \n",
    "        # Encode concepts to get embeddings\n",
    "        embeddings1 = self.embedding_model.encode(concepts1)\n",
    "        embeddings2 = self.embedding_model.encode(concepts2)\n",
    "        \n",
    "        # Calculate similarity matrix\n",
    "        similarity_matrix = cosine_similarity(embeddings1, embeddings2)\n",
    "        \n",
    "        # Count concepts in concepts1 that have a match above threshold in concepts2\n",
    "        matched_concepts = 0\n",
    "        for i in range(len(concepts1)):\n",
    "            if any(similarity_matrix[i, j] > threshold for j in range(len(concepts2))):\n",
    "                matched_concepts += 1\n",
    "        \n",
    "        # Return percentage\n",
    "        return matched_concepts / len(concepts1)\n",
    "    \n",
    "    def evaluate_node_concepts(self, node_id, threshold=0.7):\n",
    "        \"\"\"\n",
    "        Evaluate the concepts of a node against its linked nodes\n",
    "        \n",
    "        Args:\n",
    "            node_id: ID of the node to evaluate\n",
    "            threshold: Similarity threshold for considering concepts as matching\n",
    "            \n",
    "        Returns:\n",
    "            dict: Dictionary of evaluation metrics\n",
    "        \"\"\"\n",
    "        # Get node concepts\n",
    "        node_concepts = self.citation_graph.nodes[node_id]['key_concepts']\n",
    "        \n",
    "        if not node_concepts:\n",
    "            print(f\"No concepts found for node {node_id}\")\n",
    "            return None\n",
    "        \n",
    "        # Get linked nodes\n",
    "        linked_nodes = list(self.citation_graph.successors(node_id))\n",
    "        \n",
    "        if not linked_nodes:\n",
    "            print(f\"No links found for node {node_id}\")\n",
    "            return None\n",
    "        \n",
    "        # Calculate concept similarity for each linked node\n",
    "        similarity_percentages = []\n",
    "        all_linked_concepts = set()\n",
    "        \n",
    "        for linked_id in linked_nodes:\n",
    "            linked_concepts = self.citation_graph.nodes[linked_id]['key_concepts']\n",
    "            all_linked_concepts.update(linked_concepts)\n",
    "            \n",
    "            similarity = self.calculate_concept_similarity(node_concepts, linked_concepts, threshold)\n",
    "            similarity_percentages.append(similarity)\n",
    "        \n",
    "        # Calculate new concepts (concepts not in any linked node)\n",
    "        new_concepts = []\n",
    "        for concept in node_concepts:\n",
    "            # Encode concept\n",
    "            concept_embedding = self.embedding_model.encode([concept])[0]\n",
    "            \n",
    "            # Check if the concept exists in any linked node\n",
    "            is_new = True\n",
    "            for linked_id in linked_nodes:\n",
    "                linked_concepts = self.citation_graph.nodes[linked_id]['key_concepts']\n",
    "                if not linked_concepts:\n",
    "                    continue\n",
    "                    \n",
    "                linked_embeddings = self.embedding_model.encode(linked_concepts)\n",
    "                similarities = cosine_similarity([concept_embedding], linked_embeddings)[0]\n",
    "                \n",
    "                if any(sim > threshold for sim in similarities):\n",
    "                    is_new = False\n",
    "                    break\n",
    "                    \n",
    "            if is_new:\n",
    "                new_concepts.append(concept)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        metrics = {\n",
    "            'avg_concept_similarity': np.mean(similarity_percentages) if similarity_percentages else 0.0,\n",
    "            'new_concepts_count': len(new_concepts),\n",
    "            'new_concepts_percentage': len(new_concepts) / len(node_concepts) if node_concepts else 0.0,\n",
    "            'new_concepts': new_concepts\n",
    "        }\n",
    "        \n",
    "        return metrics\n",
    "    \n",
    "    def evaluate_abstract(self, node_id):\n",
    "        \"\"\"\n",
    "        Evaluate the generated abstract\n",
    "        \n",
    "        Args:\n",
    "            node_id: ID of the node\n",
    "            \n",
    "        Returns:\n",
    "            metrics: Dictionary of evaluation metrics\n",
    "        \"\"\"\n",
    "        # Get the abstract and its embedding\n",
    "        abstract = self.citation_graph.nodes[node_id]['abstract']\n",
    "        \n",
    "        if abstract is None:\n",
    "            print(\"No abstract found for the node\")\n",
    "            return None\n",
    "        \n",
    "        # Compute embedding if not already done\n",
    "        if self.citation_graph.nodes[node_id]['embedding'] is None:\n",
    "            embedding = self.embedding_model.encode(abstract)\n",
    "            self.citation_graph.nodes[node_id]['embedding'] = embedding\n",
    "        else:\n",
    "            embedding = self.citation_graph.nodes[node_id]['embedding']\n",
    "        \n",
    "        # Get linked nodes\n",
    "        linked_nodes = list(self.citation_graph.successors(node_id))\n",
    "        \n",
    "        if not linked_nodes:\n",
    "            print(\"No links found for the node\")\n",
    "            return None\n",
    "        \n",
    "        # Compute similarity with linked papers\n",
    "        similarities = []\n",
    "        for n in linked_nodes:\n",
    "            other_embedding = self.citation_graph.nodes[n]['embedding']\n",
    "            similarity = cosine_similarity([embedding], [other_embedding])[0][0]\n",
    "            similarities.append(similarity)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        metrics = {\n",
    "            'avg_similarity': np.mean(similarities),\n",
    "            'max_similarity': np.max(similarities),\n",
    "            'min_similarity': np.min(similarities),\n",
    "            'std_similarity': np.std(similarities),\n",
    "            'num_linked_papers': len(linked_nodes)\n",
    "        }\n",
    "        \n",
    "        # Add concept evaluation metrics\n",
    "        concept_metrics = self.evaluate_node_concepts(node_id)\n",
    "        if concept_metrics:\n",
    "            metrics.update(concept_metrics)\n",
    "        \n",
    "        return metrics\n",
    "    \n",
    "    def visualize_graph(self, highlight_node=None):\n",
    "        \"\"\"\n",
    "        Visualize the citation graph\n",
    "        \n",
    "        Args:\n",
    "            highlight_node: Node to highlight in the visualization\n",
    "        \"\"\"\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        \n",
    "        # Define node colors\n",
    "        node_colors = []\n",
    "        for node in self.citation_graph.nodes():\n",
    "            if node == highlight_node:\n",
    "                node_colors.append('red')\n",
    "            elif 'new_paper' in str(node):\n",
    "                node_colors.append('green')\n",
    "            else:\n",
    "                node_colors.append('lightblue')\n",
    "        \n",
    "        # Define node sizes based on in-degree (citation count)\n",
    "        node_sizes = []\n",
    "        for node in self.citation_graph.nodes():\n",
    "            in_degree = self.citation_graph.in_degree(node)\n",
    "            node_sizes.append(300 + in_degree * 50)\n",
    "        \n",
    "        # Create layout\n",
    "        pos = nx.spring_layout(self.citation_graph, seed=42)\n",
    "        \n",
    "        # Draw the graph\n",
    "        nx.draw(\n",
    "            self.citation_graph, \n",
    "            pos=pos, \n",
    "            with_labels=True, \n",
    "            node_color=node_colors,\n",
    "            node_size=node_sizes, \n",
    "            alpha=0.7, \n",
    "            arrows=True\n",
    "        )\n",
    "        \n",
    "        plt.title(\"Citation Graph with Generated Papers\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1604f5d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\galdw\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\huggingface_hub\\file_download.py:144: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\galdw\\.cache\\huggingface\\hub\\models--google--t5-v1_1-base. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Converting from SentencePiece and Tiktoken failed, if a converter for SentencePiece is available, provide a model path with a SentencePiece tokenizer.model file.Currently available slow->fast converters: ['AlbertTokenizer', 'BartTokenizer', 'BarthezTokenizer', 'BertTokenizer', 'BigBirdTokenizer', 'BlenderbotTokenizer', 'CamembertTokenizer', 'CLIPTokenizer', 'CodeGenTokenizer', 'ConvBertTokenizer', 'DebertaTokenizer', 'DebertaV2Tokenizer', 'DistilBertTokenizer', 'DPRReaderTokenizer', 'DPRQuestionEncoderTokenizer', 'DPRContextEncoderTokenizer', 'ElectraTokenizer', 'FNetTokenizer', 'FunnelTokenizer', 'GPT2Tokenizer', 'HerbertTokenizer', 'LayoutLMTokenizer', 'LayoutLMv2Tokenizer', 'LayoutLMv3Tokenizer', 'LayoutXLMTokenizer', 'LongformerTokenizer', 'LEDTokenizer', 'LxmertTokenizer', 'MarkupLMTokenizer', 'MBartTokenizer', 'MBart50Tokenizer', 'MPNetTokenizer', 'MobileBertTokenizer', 'MvpTokenizer', 'NllbTokenizer', 'OpenAIGPTTokenizer', 'PegasusTokenizer', 'Qwen2Tokenizer', 'RealmTokenizer', 'ReformerTokenizer', 'RemBertTokenizer', 'RetriBertTokenizer', 'RobertaTokenizer', 'RoFormerTokenizer', 'SeamlessM4TTokenizer', 'SqueezeBertTokenizer', 'T5Tokenizer', 'UdopTokenizer', 'WhisperTokenizer', 'XLMRobertaTokenizer', 'XLNetTokenizer', 'SplinterTokenizer', 'XGLMTokenizer', 'LlamaTokenizer', 'CodeLlamaTokenizer', 'GemmaTokenizer', 'Phi3Tokenizer']",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\transformers\\convert_slow_tokenizer.py:1587\u001b[39m, in \u001b[36mTikTokenConverter.extract_vocab_merges_from_model\u001b[39m\u001b[34m(self, tiktoken_url)\u001b[39m\n\u001b[32m   1586\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1587\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtiktoken\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mload\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m load_tiktoken_bpe\n\u001b[32m   1588\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'tiktoken'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\transformers\\convert_slow_tokenizer.py:1730\u001b[39m, in \u001b[36mconvert_slow_tokenizer\u001b[39m\u001b[34m(transformer_tokenizer, from_tiktoken)\u001b[39m\n\u001b[32m   1726\u001b[39m     logger.info(\u001b[33m\"\u001b[39m\u001b[33mConverting from Tiktoken\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   1727\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mTikTokenConverter\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1728\u001b[39m \u001b[43m        \u001b[49m\u001b[43mvocab_file\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtransformer_tokenizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvocab_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1729\u001b[39m \u001b[43m        \u001b[49m\u001b[43madditional_special_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtransformer_tokenizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43madditional_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m-> \u001b[39m\u001b[32m1730\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconverted\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1731\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\transformers\\convert_slow_tokenizer.py:1624\u001b[39m, in \u001b[36mTikTokenConverter.converted\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1623\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mconverted\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> Tokenizer:\n\u001b[32m-> \u001b[39m\u001b[32m1624\u001b[39m     tokenizer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1625\u001b[39m     tokenizer.pre_tokenizer = pre_tokenizers.Sequence(\n\u001b[32m   1626\u001b[39m         [\n\u001b[32m   1627\u001b[39m             pre_tokenizers.Split(Regex(\u001b[38;5;28mself\u001b[39m.pattern), behavior=\u001b[33m\"\u001b[39m\u001b[33misolated\u001b[39m\u001b[33m\"\u001b[39m, invert=\u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[32m   1628\u001b[39m             pre_tokenizers.ByteLevel(add_prefix_space=\u001b[38;5;28mself\u001b[39m.add_prefix_space, use_regex=\u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[32m   1629\u001b[39m         ]\n\u001b[32m   1630\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\transformers\\convert_slow_tokenizer.py:1617\u001b[39m, in \u001b[36mTikTokenConverter.tokenizer\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1616\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mtokenizer\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m1617\u001b[39m     vocab_scores, merges = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mextract_vocab_merges_from_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mvocab_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1618\u001b[39m     tokenizer = Tokenizer(BPE(vocab_scores, merges, fuse_unk=\u001b[38;5;28;01mFalse\u001b[39;00m))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\transformers\\convert_slow_tokenizer.py:1589\u001b[39m, in \u001b[36mTikTokenConverter.extract_vocab_merges_from_model\u001b[39m\u001b[34m(self, tiktoken_url)\u001b[39m\n\u001b[32m   1588\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1589\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1590\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m`tiktoken` is required to read a `tiktoken` file. Install it with `pip install tiktoken`.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1591\u001b[39m     )\n\u001b[32m   1593\u001b[39m bpe_ranks = load_tiktoken_bpe(tiktoken_url)\n",
      "\u001b[31mValueError\u001b[39m: `tiktoken` is required to read a `tiktoken` file. Install it with `pip install tiktoken`.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      2\u001b[39m papers = df.to_dict(orient=\u001b[33m'\u001b[39m\u001b[33mrecords\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Initialize the generator\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m generator = \u001b[43mCitationGraphAbstractGenerator\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# Load citation graph\u001b[39;00m\n\u001b[32m      8\u001b[39m generator.load_citation_graph(papers)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 15\u001b[39m, in \u001b[36mCitationGraphAbstractGenerator.__init__\u001b[39m\u001b[34m(self, embedding_model_name, linkPredictor_model_name)\u001b[39m\n\u001b[32m     13\u001b[39m \u001b[38;5;28mself\u001b[39m.linkPredictor_model_name = linkPredictor_model_name\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m embedding_model_name==\u001b[33m'\u001b[39m\u001b[33mt5\u001b[39m\u001b[33m'\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m     \u001b[38;5;28mself\u001b[39m.embedding_model = \u001b[43mBottleneckT5Autoencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m linkPredictor_model_name==\u001b[33m'\u001b[39m\u001b[33menhanced\u001b[39m\u001b[33m'\u001b[39m:\n\u001b[32m     17\u001b[39m     \u001b[38;5;28mself\u001b[39m.linkPredictor_model = EnhancedLinkPredictor()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\galdw\\Downloads\\Centrale Part2\\MLNS\\MLNS_final_project\\EncoderDecoder.py:7\u001b[39m, in \u001b[36mBottleneckT5Autoencoder.__init__\u001b[39m\u001b[34m(self, model_path, device)\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, model_path=\u001b[33m\"\u001b[39m\u001b[33mgoogle/t5-v1_1-base\u001b[39m\u001b[33m\"\u001b[39m, device=\u001b[33m'\u001b[39m\u001b[33mgpu\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch.cuda.is_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m'\u001b[39m):\n\u001b[32m      6\u001b[39m     \u001b[38;5;28mself\u001b[39m.device = device\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m     \u001b[38;5;28mself\u001b[39m.tokenizer = \u001b[43mAutoTokenizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_max_length\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m512\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      8\u001b[39m     \u001b[38;5;28mself\u001b[39m.model = AutoModelForCausalLM.from_pretrained(model_path, trust_remote_code=\u001b[38;5;28;01mTrue\u001b[39;00m).to(\u001b[38;5;28mself\u001b[39m.device)\n\u001b[32m      9\u001b[39m     \u001b[38;5;28mself\u001b[39m.model.eval()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\transformers\\models\\auto\\tokenization_auto.py:1028\u001b[39m, in \u001b[36mAutoTokenizer.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[39m\n\u001b[32m   1025\u001b[39m tokenizer_class_py, tokenizer_class_fast = TOKENIZER_MAPPING[\u001b[38;5;28mtype\u001b[39m(config)]\n\u001b[32m   1027\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m tokenizer_class_fast \u001b[38;5;129;01mand\u001b[39;00m (use_fast \u001b[38;5;129;01mor\u001b[39;00m tokenizer_class_py \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m-> \u001b[39m\u001b[32m1028\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtokenizer_class_fast\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1029\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1030\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m tokenizer_class_py \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\transformers\\tokenization_utils_base.py:2062\u001b[39m, in \u001b[36mPreTrainedTokenizerBase.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, cache_dir, force_download, local_files_only, token, revision, trust_remote_code, *init_inputs, **kwargs)\u001b[39m\n\u001b[32m   2059\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   2060\u001b[39m         logger.info(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mloading file \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m from cache at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresolved_vocab_files[file_id]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m2062\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_from_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2063\u001b[39m \u001b[43m    \u001b[49m\u001b[43mresolved_vocab_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2064\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2065\u001b[39m \u001b[43m    \u001b[49m\u001b[43minit_configuration\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2066\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43minit_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2067\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2068\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2069\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2070\u001b[39m \u001b[43m    \u001b[49m\u001b[43m_commit_hash\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcommit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2071\u001b[39m \u001b[43m    \u001b[49m\u001b[43m_is_local\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_local\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2072\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2073\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2074\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\transformers\\tokenization_utils_base.py:2302\u001b[39m, in \u001b[36mPreTrainedTokenizerBase._from_pretrained\u001b[39m\u001b[34m(cls, resolved_vocab_files, pretrained_model_name_or_path, init_configuration, token, cache_dir, local_files_only, _commit_hash, _is_local, trust_remote_code, *init_inputs, **kwargs)\u001b[39m\n\u001b[32m   2300\u001b[39m \u001b[38;5;66;03m# Instantiate the tokenizer.\u001b[39;00m\n\u001b[32m   2301\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2302\u001b[39m     tokenizer = \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43minit_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43minit_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2303\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m import_protobuf_decode_error():\n\u001b[32m   2304\u001b[39m     logger.info(\n\u001b[32m   2305\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mUnable to load tokenizer model from SPM, loading from TikToken will be attempted instead.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2306\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m(Google protobuf error: Tried to load SPM model with non-SPM vocab file).\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   2307\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\transformers\\models\\t5\\tokenization_t5_fast.py:119\u001b[39m, in \u001b[36mT5TokenizerFast.__init__\u001b[39m\u001b[34m(self, vocab_file, tokenizer_file, eos_token, unk_token, pad_token, extra_ids, additional_special_tokens, add_prefix_space, **kwargs)\u001b[39m\n\u001b[32m    114\u001b[39m     logger.warning_once(\n\u001b[32m    115\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mYou set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    116\u001b[39m     )\n\u001b[32m    117\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33mfrom_slow\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m119\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    120\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvocab_file\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvocab_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    121\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtokenizer_file\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtokenizer_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    122\u001b[39m \u001b[43m    \u001b[49m\u001b[43meos_token\u001b[49m\u001b[43m=\u001b[49m\u001b[43meos_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    123\u001b[39m \u001b[43m    \u001b[49m\u001b[43munk_token\u001b[49m\u001b[43m=\u001b[49m\u001b[43munk_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    124\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpad_token\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpad_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    125\u001b[39m \u001b[43m    \u001b[49m\u001b[43mextra_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    126\u001b[39m \u001b[43m    \u001b[49m\u001b[43madditional_special_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43madditional_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    127\u001b[39m \u001b[43m    \u001b[49m\u001b[43madd_prefix_space\u001b[49m\u001b[43m=\u001b[49m\u001b[43madd_prefix_space\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    128\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    129\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    131\u001b[39m \u001b[38;5;28mself\u001b[39m.vocab_file = vocab_file\n\u001b[32m    132\u001b[39m \u001b[38;5;28mself\u001b[39m._extra_ids = extra_ids\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\transformers\\tokenization_utils_fast.py:139\u001b[39m, in \u001b[36mPreTrainedTokenizerFast.__init__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    137\u001b[39m     \u001b[38;5;28mself\u001b[39m.vocab_file = kwargs.get(\u001b[33m\"\u001b[39m\u001b[33mvocab_file\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    138\u001b[39m     \u001b[38;5;28mself\u001b[39m.additional_special_tokens = kwargs.get(\u001b[33m\"\u001b[39m\u001b[33madditional_special_tokens\u001b[39m\u001b[33m\"\u001b[39m, [])\n\u001b[32m--> \u001b[39m\u001b[32m139\u001b[39m     fast_tokenizer = \u001b[43mconvert_slow_tokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrom_tiktoken\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    140\u001b[39m     slow_tokenizer = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    141\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\transformers\\convert_slow_tokenizer.py:1732\u001b[39m, in \u001b[36mconvert_slow_tokenizer\u001b[39m\u001b[34m(transformer_tokenizer, from_tiktoken)\u001b[39m\n\u001b[32m   1727\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m TikTokenConverter(\n\u001b[32m   1728\u001b[39m         vocab_file=transformer_tokenizer.vocab_file,\n\u001b[32m   1729\u001b[39m         additional_special_tokens=transformer_tokenizer.additional_special_tokens,\n\u001b[32m   1730\u001b[39m     ).converted()\n\u001b[32m   1731\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1732\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1733\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mConverting from SentencePiece and Tiktoken failed, if a converter for SentencePiece is available, provide a model path \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1734\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mwith a SentencePiece tokenizer.model file.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1735\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCurrently available slow->fast converters: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(SLOW_TO_FAST_CONVERTERS.keys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m   1736\u001b[39m     )\n",
      "\u001b[31mValueError\u001b[39m: Converting from SentencePiece and Tiktoken failed, if a converter for SentencePiece is available, provide a model path with a SentencePiece tokenizer.model file.Currently available slow->fast converters: ['AlbertTokenizer', 'BartTokenizer', 'BarthezTokenizer', 'BertTokenizer', 'BigBirdTokenizer', 'BlenderbotTokenizer', 'CamembertTokenizer', 'CLIPTokenizer', 'CodeGenTokenizer', 'ConvBertTokenizer', 'DebertaTokenizer', 'DebertaV2Tokenizer', 'DistilBertTokenizer', 'DPRReaderTokenizer', 'DPRQuestionEncoderTokenizer', 'DPRContextEncoderTokenizer', 'ElectraTokenizer', 'FNetTokenizer', 'FunnelTokenizer', 'GPT2Tokenizer', 'HerbertTokenizer', 'LayoutLMTokenizer', 'LayoutLMv2Tokenizer', 'LayoutLMv3Tokenizer', 'LayoutXLMTokenizer', 'LongformerTokenizer', 'LEDTokenizer', 'LxmertTokenizer', 'MarkupLMTokenizer', 'MBartTokenizer', 'MBart50Tokenizer', 'MPNetTokenizer', 'MobileBertTokenizer', 'MvpTokenizer', 'NllbTokenizer', 'OpenAIGPTTokenizer', 'PegasusTokenizer', 'Qwen2Tokenizer', 'RealmTokenizer', 'ReformerTokenizer', 'RemBertTokenizer', 'RetriBertTokenizer', 'RobertaTokenizer', 'RoFormerTokenizer', 'SeamlessM4TTokenizer', 'SqueezeBertTokenizer', 'T5Tokenizer', 'UdopTokenizer', 'WhisperTokenizer', 'XLMRobertaTokenizer', 'XLNetTokenizer', 'SplinterTokenizer', 'XGLMTokenizer', 'LlamaTokenizer', 'CodeLlamaTokenizer', 'GemmaTokenizer', 'Phi3Tokenizer']"
     ]
    }
   ],
   "source": [
    "# Create sample papers with abstracts and references\n",
    "papers = df.to_dict(orient='records')\n",
    "\n",
    "# Initialize the generator\n",
    "generator = CitationGraphAbstractGenerator()\n",
    "\n",
    "# Load citation graph\n",
    "generator.load_citation_graph(papers)\n",
    "\n",
    "# Compute embeddings for existing papers\n",
    "generator.compute_embeddings()\n",
    "\n",
    "# Create a new node\n",
    "new_node_id = generator.create_new_node()\n",
    "\n",
    "# Evaluate the abstract\n",
    "metrics = generator.evaluate_abstract(new_node_id)\n",
    "print(\"Evaluation Metrics:\")\n",
    "for metric, value in metrics.items():\n",
    "    if isinstance(value, float):\n",
    "        print(f\"- {metric}: {value:.4f}\")\n",
    "    elif isinstance(value, list):\n",
    "        print(f\"- {metric}: {value}\")\n",
    "    else:\n",
    "        print(f\"- {metric}: {value}\")\n",
    "\n",
    "# Visualize the graph\n",
    "generator.visualize_graph(highlight_node=new_node_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e7ec852",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
