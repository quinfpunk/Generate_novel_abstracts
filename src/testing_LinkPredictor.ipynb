{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04aeae50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "from LinkPredictor import EnhancedLinkPredictor\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from collections import defaultdict\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import os\n",
    "import urllib.request\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6fd2efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load classical citation networks\n",
    "def load_citation_dataset(dataset_name):\n",
    "    \"\"\"\n",
    "    Load a classical citation network dataset\n",
    "\n",
    "    Args:\n",
    "        dataset_name: Name of the dataset ('cora', 'citeseer', 'arxiv', or 'web-stanford')\n",
    "\n",
    "    Returns:\n",
    "        NetworkX DiGraph object\n",
    "    \"\"\"\n",
    "\n",
    "    G = nx.DiGraph()\n",
    "    display_print = True\n",
    "\n",
    "    try:\n",
    "        if dataset_name.lower() == 'cora':\n",
    "            print(\"Loading Cora dataset...\")\n",
    "            data_url = 'https://linqs-data.soe.ucsc.edu/public/lbc/cora.tgz'\n",
    "            local_file = 'cora.tgz'\n",
    "\n",
    "            # Download dataset if not exists\n",
    "            if not os.path.exists('cora'):\n",
    "                print(f\"Downloading Cora dataset...\")\n",
    "                urllib.request.urlretrieve(data_url, local_file)\n",
    "\n",
    "                # Extract the file\n",
    "                import tarfile\n",
    "                with tarfile.open(local_file, 'r:gz') as tar:\n",
    "                    tar.extractall()\n",
    "\n",
    "                # Clean up\n",
    "                os.remove(local_file)\n",
    "\n",
    "            # Read the cora.cites file\n",
    "            with open('cora/cora.cites', 'r') as f:\n",
    "                for line in f:\n",
    "                    citing, cited = map(int, line.strip().split('\\t'))\n",
    "                    G.add_edge(citing, cited)\n",
    "\n",
    "            # Add paper metadata\n",
    "            with open('cora/cora.content', 'r') as f:\n",
    "                for line in f:\n",
    "                    parts = line.strip().split('\\t')\n",
    "                    paper_id = int(parts[0])\n",
    "                    features = parts[1:-1]\n",
    "                    label = parts[-1]\n",
    "\n",
    "                    if paper_id not in G:\n",
    "                        G.add_node(paper_id)\n",
    "\n",
    "                    G.nodes[paper_id]['label'] = label\n",
    "                    # Add a simple year attribute for recency calculation (using a random year between 2000-2015)\n",
    "                    G.nodes[paper_id]['year'] = random.randint(2000, 2015)\n",
    "\n",
    "\n",
    "        elif dataset_name.lower() == 'arxiv':\n",
    "            print(\"Loading arXiv HEP-TH citation network...\")\n",
    "            dataset_url = 'https://snap.stanford.edu/data/cit-HepTh.txt.gz'\n",
    "            local_file = 'cit-HepTh.txt.gz'\n",
    "            extracted_file = 'cit-HepTh.txt'\n",
    "\n",
    "            # Download dataset if not exists\n",
    "            if not os.path.exists(extracted_file):\n",
    "                print(f\"Downloading arXiv HEP-TH citation network...\")\n",
    "                urllib.request.urlretrieve(dataset_url, local_file)\n",
    "\n",
    "                # Extract the file\n",
    "                import gzip\n",
    "                import shutil\n",
    "                with gzip.open(local_file, 'rb') as f_in:\n",
    "                    with open(extracted_file, 'wb') as f_out:\n",
    "                        shutil.copyfileobj(f_in, f_out)\n",
    "\n",
    "                # Clean up\n",
    "                os.remove(local_file)\n",
    "\n",
    "            # Read the citation network\n",
    "            with open(extracted_file, 'r') as f:\n",
    "                for line in f:\n",
    "                    if line.startswith('#'):\n",
    "                        continue\n",
    "                    parts = line.strip().split('\\t')\n",
    "                    if len(parts) == 2:\n",
    "                        citing, cited = parts\n",
    "                        G.add_edge(citing, cited)\n",
    "\n",
    "            # Add random years for papers (1992-2003 as per the dataset description)\n",
    "            for node in G.nodes():\n",
    "                G.nodes[node]['year'] = random.randint(1992, 2003)\n",
    "\n",
    "        elif dataset_name.lower() == 'pubmed':\n",
    "            print(\"Loading PubMed dataset...\")\n",
    "            data_url = 'https://linqs-data.soe.ucsc.edu/public/Pubmed-Diabetes.tgz'\n",
    "            local_file = 'Pubmed-Diabetes.tgz'\n",
    "\n",
    "            # Download dataset if not exists\n",
    "            if not os.path.exists('Pubmed-Diabetes'):\n",
    "                print(f\"Downloading PubMed dataset...\")\n",
    "                urllib.request.urlretrieve(data_url, local_file)\n",
    "\n",
    "                # Extract the file\n",
    "                import tarfile\n",
    "                with tarfile.open(local_file, 'r:gz') as tar:\n",
    "                    tar.extractall()\n",
    "\n",
    "                # Clean up\n",
    "                os.remove(local_file)\n",
    "\n",
    "            # Reading the 'data/Pubmed-Diabetes.DIRECTED.cites.tab' file\n",
    "            cite_file = 'Pubmed-Diabetes/data/Pubmed-Diabetes.DIRECTED.cites.tab'\n",
    "            with open(cite_file, 'r') as f:\n",
    "                # Skip header line\n",
    "                next(f)\n",
    "                for line in f:\n",
    "                    parts = line.strip().split('\\t')\n",
    "                    if len(parts) >= 2:\n",
    "                        # Format is sometimes \"paper:<number>\\tpaper:<number>\"\n",
    "                        source = parts[0].replace('paper:', '')\n",
    "                        target = parts[1].replace('paper:', '')\n",
    "                        G.add_edge(source, target)\n",
    "\n",
    "            # Try to add node attributes if available\n",
    "            try:\n",
    "                node_file = 'Pubmed-Diabetes/data/Pubmed-Diabetes.NODE.paper.tab'\n",
    "                with open(node_file, 'r') as f:\n",
    "                    # Skip header line\n",
    "                    next(f)\n",
    "                    for line in f:\n",
    "                        parts = line.strip().split('\\t')\n",
    "                        if len(parts) >= 3:\n",
    "                            paper_id = parts[0].replace('paper:', '')\n",
    "                            if paper_id in G or paper_id in G.nodes():\n",
    "                                # Extract year if available, otherwise use a random year\n",
    "                                year = random.randint(1990, 2010)\n",
    "                                label = parts[1] if len(parts) > 1 else \"unknown\"\n",
    "                                G.nodes[paper_id]['year'] = year\n",
    "                                G.nodes[paper_id]['label'] = label\n",
    "            except Exception as e:\n",
    "                print(f\"Could not load node attributes for PubMed: {e}\")\n",
    "                # Add random years\n",
    "                for node in G.nodes():\n",
    "                    G.nodes[node]['year'] = random.randint(1990, 2010)\n",
    "                    \n",
    "        else:\n",
    "            print(f\"Unknown dataset '{dataset_name}', please choose from: cora, citeseer, arxiv, web-stanford, or pubmed\")\n",
    "            display_print = False\n",
    "            return None\n",
    "\n",
    "        if display_print:\n",
    "            print(f\"Loaded {dataset_name} dataset\")\n",
    "\n",
    "        return G\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading dataset {dataset_name}: {e}\")\n",
    "        print(\"Please check your internet connection or try again later.\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1810f1f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Configure plt for better visualization\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "# List of datasets to analyze\n",
    "datasets = ['cora', 'pubmed', 'arxiv']  # You can add other datasets as needed\n",
    "\n",
    "# Store the results for each dataset\n",
    "all_results = {}\n",
    "\n",
    "for dataset_name in datasets:\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Analyzing {dataset_name.upper()} dataset\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    # Load the dataset\n",
    "    citation_graph = load_citation_dataset(dataset_name)\n",
    "    \n",
    "    # Initialize the link predictor\n",
    "    predictor = EnhancedLinkPredictor(citation_graph)\n",
    "    \n",
    "    # Evaluate the link prediction with title and basic stats before table\n",
    "    results = predictor.evaluate(num_test_nodes=400, test_ratio=0.3)\n",
    "    \n",
    "    # Store results\n",
    "    all_results[dataset_name] = results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
